{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Practicus AI Documentation","text":""},{"location":"#about-practicus-ai","title":"About Practicus AI","text":"<p>Practicus AI is a unified platform for Generative AI and Data Intelligence. It helps you move from initial concepts to production-grade AI and Data solutions. Whether you are an experienced engineer or new to Data and AI, the platform is designed to be both reliable and easy to navigate.</p>"},{"location":"#select-benefits","title":"Select Benefits","text":"<p>Practicus AI brings together Generative AI, self-service AI and Data tools, MLOps, Orchestration, Kubernetes infrastructure management, and Observability into a single platform. This integrated, but flexible approach simplifies the process of working with AI and Data at scale.</p> <p></p>"},{"location":"#platform-components","title":"Platform Components","text":"<p>The Practicus AI Platform is built on an enterprise-grade core, with optional add-ons to extend functionality. This structure supports consistent deployment, governance, and scalability. It also provides a unified environment for managing applications, runtimes, and advanced analytics tools.</p> <p></p>"},{"location":"#core-platform","title":"Core Platform","text":"<p>The core platform establishes the foundation for a secure, scalable, and versatile AI environment. It consists of four layers: Applications, Mesh, Runtime, and Infrastructure. Each layer offers distinct capabilities\u2014such as creating and sharing GenAI applications, hosting advanced models and microservices, managing service meshes for reliability, and ensuring efficient use of GPUs and other resources.</p> <p></p>"},{"location":"#add-ons","title":"Add-Ons","text":"<p>In addition to the core platform, Practicus AI offer a variety of add-ons that can be selectively incorporated into your environment. These add-ons are protected by enterprise single sign-on and integrate smoothly with core platform and other add-on components. As we continue to expand the add-on library, you\u2019ll have access to a growing range of tools and features, ensuring that your platform remains both adaptable and secure as your AI initiatives evolve.</p>"},{"location":"#practicus-ais-core-principles","title":"Practicus AI's Core Principles","text":"<p>While the core platform and its add-ons define what you can accomplish with Practicus AI, a set of guiding principles ensures that every feature, integration, and improvement remains aligned with our core values of security, flexibility, and usability.</p>"},{"location":"#7-core-principles-of-the-practicus-ai-platform","title":"7 Core Principles of the Practicus AI Platform","text":"<ol> <li> <p>Security &amp; Compliance First:    The platform must maintain enterprise-level security and compliance at all times, enforcing measures such as SSO with MFA/OTP and strict data governance to safeguard sensitive information.</p> </li> <li> <p>Cloud-Native &amp; Vendor-Agnostic:    The platform must remain cloud-native and adhere to open-source and CNCF standards, ensuring it can be deployed in any environment\u2014on-premises, in the cloud, or within air-gapped configurations\u2014without creating vendor lock-in.</p> </li> <li> <p>Ease of Use &amp; Extensibility:    The platform must provide a unified interface that simplifies setup, enables rapid deployment, and supports extensibility, adapting smoothly to evolving requirements.</p> </li> <li> <p>Operational Simplicity for Technical Teams:    The platform must reduce operational complexity, allowing technical users to focus on designing and refining AI solutions rather than managing infrastructure overhead.</p> </li> <li> <p>Real-Time Observability &amp; Scalability:    The platform must offer continuous insights into performance and health, alongside automatic scaling and optimized GPU utilization, ensuring efficiency as workloads grow.</p> </li> <li> <p>Accessibility for Non-Technical Users:    The platform must deliver interfaces and tools that are easy to navigate, allowing non-technical users to engage in data exploration, analysis, and insight generation without requiring advanced expertise.</p> </li> <li> <p>Comprehensive Analytics &amp; Generative AI:    The platform must integrate a full spectrum of analytics capabilities, from traditional to advanced and generative AI, supporting informed decision-making and fostering innovation across diverse use cases.</p> </li> </ol>"},{"location":"#next-steps","title":"Next Steps","text":"<p>As you continue through this documentation, you will find sections tailored to different types of users. Technical teams can explore detailed operational information, while those new to AI can learn the fundamentals. Consider this the starting point for understanding how Practicus AI supports your AI and data initiatives.</p> <p>Next: Getting Started</p> <p><sup><sup>Practicus AI docs for v25.5.4 (Built on 2025-10-25 11:11)</sup></sup></p>"},{"location":"admin_pod_for_user_folder_inspection/","title":"Kubernetes Pod Definition: Admin Access for User Folder Inspection","text":""},{"location":"admin_pod_for_user_folder_inspection/#description","title":"Description","text":"<p>In the Practicus platform, there are situations where administrators need to inspect users' <code>/my</code> folders, which are automatically mounted via PVs/PVCs. This document aims to provide a Kubernetes Pod definition that allows administrators to mount a specified user's <code>/my</code> folder into their own pod to perform this inspection.</p>"},{"location":"admin_pod_for_user_folder_inspection/#solution","title":"Solution","text":"<p>The following YAML definition allows an administrator to mount <code>{{TARGET_USERNAME}}</code>'s personal folder into their pod at <code>/home/ubuntu/{{TARGET_USERNAME}}</code>.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  # Pod name for the admin viewer pod. Replace {{TARGET_USERNAME}} with the actual username.\n  name: prt-pod-user-folder-viewer-{{TARGET_USERNAME}}\n  namespace: prt-ns\nspec:\n  restartPolicy: Never\n  volumes:\n    # Volume definition for mounting the target user's Persistent Volume Claim (PVC).\n    # Replace {{TARGET_USERNAME}} with the actual username.\n    - name: prt-vol-user-folder-viewer-{{TARGET_USERNAME}}\n      persistentVolumeClaim:\n        # Claim name for the target user's PVC. Replace {{TARGET_USERNAME}} with the actual username.\n        claimName: prt-pvc-my-{{TARGET_USERNAME}}\n  containers:\n    - name: ubuntu\n      image: ubuntu:22.04\n      command: [\"/bin/bash\", \"-c\", \"sleep infinity\"] \n      volumeMounts:\n        # Mount the volume defined above. Replace {{TARGET_USERNAME}} with the actual username.\n        - name: prt-vol-user-folder-viewer-{{TARGET_USERNAME}}\n          # Mount path inside the pod where the user's folder will be accessible.\n          # Replace {{TARGET_USERNAME}} with the actual username.\n          mountPath: /home/ubuntu/{{TARGET_USERNAME}}\n</code></pre>"},{"location":"admin_pod_for_user_folder_inspection/#usage-instructions","title":"Usage Instructions","text":"<ol> <li>Save the YAML content above to a file (e.g., <code>admin-viewer-pod.yaml</code>).</li> <li>Replace the placeholders <code>{{TARGET_USERNAME}}</code> within the YAML with the actual username of the user you wish to inspect.</li> <li>To deploy the pod to your Kubernetes cluster:     <pre><code>kubectl apply -f admin-viewer-pod.yaml -n prt-ns\n# or if you are using OpenShift:\noc apply -f admin-viewer-pod.yaml -n prt-ns\n</code></pre></li> <li>Once the pod is ready, you can exec into it to inspect the user's folder:     <pre><code>kubectl exec -it prt-pod-user-folder-viewer-{{TARGET_USERNAME}} -n prt-ns -- bash\n# or if you are using OpenShift:\noc exec -it prt-pod-user-folder-viewer-{{TARGET_USERNAME}} -n prt-ns -- bash\n</code></pre></li> <li>Inside the pod, navigate to the folder using <code>cd /home/ubuntu/{{TARGET_USERNAME}}</code>.</li> <li>Remember to delete the pod when you are finished:     <pre><code>kubectl delete pod prt-pod-user-folder-viewer-{{TARGET_USERNAME}} -n prt-ns\n# or if you are using OpenShift:\noc delete pod prt-pod-user-folder-viewer-{{TARGET_USERNAME}} -n prt-ns\n</code></pre></li> </ol>"},{"location":"admin_pod_for_user_folder_inspection/#note","title":"Note","text":"<p>This pod has a minimal configuration. You may need to customize fields such as <code>image</code>, <code>resources</code>, <code>securityContext</code>, etc., according to your environment's security and resource requirements.</p>"},{"location":"advanced/","title":"Advanced Features","text":"<p>In this section we will look at some advanced functionality of Practicus AI. </p>"},{"location":"advanced/#production-data-pipelines","title":"Production Data Pipelines","text":"<p>Typically, an ML workflow will depend on several cycles of data preparation steps before the final ML training can be done. This is rarely a one time task, since your ML model might eventually start drifting as time goes by (not predicting accurately anymore). When this happens, the first thing to try is usually re-training with new, fresh data. Which means you have to reload data from raw data sources, and apply data preparation steps again.  Instead of preparing data manually every time you train, some teams create automated data pipelines. These pipelines typically run on a schedule, for instance daily, creating clean and ready-to-train-on data sets. </p> <p>You can easily build data pipelines with Practicus AI, and embed them into existing data integration platforms.  All you need is a platform that can run Python. Please see some performance tips below in the example code </p> <p>Achieving this is straightforward, you can simply \"chain\" data preparation steps using .dp files, and not necessarily the updated Excel files for performance reasons. </p> <p>Example data pipeline:</p> <pre><code>import practicus\n# 1) create a Python .py file for the data pipeline\n# 2) place your data loading code as usual, from databases, S3 etc. \ndf = load_data_as_usual()\n# 3) execute changes in .dp files one after another\npracticus.apply_changes(df, \"data_prep_1.dp\", inplace=True)\npracticus.apply_changes(df, \"data_prep_2.dp\", inplace=True)\n# ...\npracticus.apply_changes(df, \"data_prep_10.dp\", inplace=True)\n# pipeline is completed\nsave_to_final_dest_as_usual(df)\n\n# Performance Tips: \n# 1) Pandas Data Frame will work faster with practicus, especially for very large datasets \n# 2) You can use \"inplace=True\" updates, to avoid creating new data frames \n# 3) you can delete sorting and column reordering commands in the .dp files, since your \n# ML model training will not care. Especially sorting is a relatively expensive operation.  \n# column reordering with Pandas Data Frame will also trigger creating a new df. \n</code></pre> <p>Deployment</p> <p>Once the data pipeline Python code (.py) is ready like the above, you can then \"pip install practicus\" in the data integration platform and run the .py file as usual. Please note that Practicus AI uses Java (&gt;= v8.0) behind the scenes for some complex operations. Although it is unlikely, if there is no Java runtime (JRE) installed on the data integration platform, Practicus AI will download a local copy of the JRE, first time it runs. This local Java installation is purely a \"file download\" operation of a stripped down JRE, and will not need any root/admin privileges.  You can also manually trigger downloading the local JRE operation in advance by calling practicus.install_jre(), or install Java (&gt;= v8.0) yourself. We recommend Open\u00a0JDK.  </p>"},{"location":"advanced/#dp-file-structure","title":"DP file structure","text":"<p>Practicus AI can detect changes made inside an Excel (.xlsx) file when you call detect_changes() or apply_changes()  functions. The recorded changes are then saved to a simple text file with the extension .dp (data prep).</p> <p>The .dp file is intended to be easily consumed by users that are not programmers or data scientists. The goal is to create a happy medium so that different user personas can view the changes detected in the .dp file and collaborate on a data science project.</p> <p>The .dp file does not necessarily need to only include changes that are detected by Practicus AI. As you can read below, you can freely add your own changes or remove existing ones, and then finally ask Practicus AI to run these data prep steps on your data sets to complete data preparation. </p> <p>The below is a simple .dp file example. </p> <p></p>"},{"location":"advanced/#input_columns","title":"Input_Columns","text":"<pre><code>Input_Columns = [Column Name 1], [Column Name 2], ..\n</code></pre> <p>This is an optional section, but it is recommended to use for data validation. When Practicus AI starts analyzing an Excel (.xlsx) file, it will detect all the visible column names and add them in the Input_Columns section.  When you execute apply_changes() with a new data set, Practicus AI will compare the column names of the input data set to Input_Columns and give you a warning if they are not the same. For example, let's assume you have a data set with Col_1, Col_2, Col_3 columns, export to .xlsx, make some changes and then run detect_changes(). You will see Input_Columns = [Col_1], [Col_2], [Col_3]. Let's assume you then go ahead and delete  Col_2 in your data set, and apply the .dp file. You would get a warning that the input columns do not match with the input data set. </p> <p>Example: </p> <pre><code>Input_Columns = [CRIM], [ZN], [INDUS]\n</code></pre>"},{"location":"advanced/#drop_column","title":"Drop_Column","text":"<pre><code>Drop_Column(Col[name of the column to delete])\n</code></pre> <p>Will delete a column from the data set.  Practicus AI will add a Drop_Column command for both deleted and hidden columns in an Excel (.xlsx) file. The only difference is that deleted columns will run early on in the .dp file and hidden columns will be deleted later since formulas or filters can depend on them. It is common that a user creates a new Excel column, uses a formula and then hides the old column that the formula uses. </p> <p>Example: </p> <pre><code>Drop_Column(Col[ZN])\n</code></pre>"},{"location":"advanced/#update","title":"Update","text":"<pre><code>Update(Col[name of the column to update] from_value to to_value)\n</code></pre> <p>Updates all the rows for a certain column from one value to another. For example, updating all missing values to 0. When a manual update of a cell is detected in the Excel file, an Update command is added to the .dp file. You do not need to make the same update to all cells, just one ie enough to apply the change for that column. If the same value is updated to different values manually, only the first detected update will run. Other updates will be commented out. You can review these update commands and make changes on the .dp file as needed.</p> <p>Examples:</p> <pre><code>Update(Col[CHAS] blank to 1.0)\nUpdate(Col[text field] \"abc\" to \"def\")\nUpdate(Col[ZN] 5 to 10)\n# The below Update command is commented out automatically,\n# since the same value (5) was updated to 10 in one Excel cell and to 11 in another\n# Update(Col[ZN] 5 to 11)\n</code></pre>"},{"location":"advanced/#rename_column","title":"Rename_Column","text":"<pre><code>Rename_Column(Col[current name] to Col[new name])\n</code></pre> <p>Will rename a column. Practicus AI will only analyze top n rows to detect potential renames and can miss some of them in complex cases. Please feel free to manually detect these and add them to the .dp file. </p> <p>Example: </p> <pre><code>Rename_Column(Col[petal_width] to Col[petal width])\n</code></pre>"},{"location":"advanced/#functions-and-formulas","title":"Functions and formulas","text":"<pre><code>Col[name of the column] = EXCEL_FUNCTION( .. EMBEDDED_FUNCTIONS(..) .. ) + OPERATORS\n</code></pre> <p>Using functions to create formulas is probably one of the most powerful features of Excel. The same is true for Practicus AI data prep use case as well. Practicus AI currently interprets over 200+ Excel functions and applies them with custom created Python code to your data set to perform the data transformation.  If Practicus AI ever encounters an Excel function that it doesn't understand, it will create a Python template for you to provide the missing functionality. Please read more about this in custom functions' section below.   Practicus AI currently only supports functions and formulas to run on the same row.  For things like averages of all values for  a particular column, you can use separate sheets or pivot tables to do yor analysis, and then finally perform the data preparation steps on individual rows.  </p> <p>Examples:</p> <pre><code># Operators\nCol[A] = Col[A] + 1\nCol[A] = (Col[A] + Col[B]) / 2\n# Mathematical functions\nCol[A] = SQRT(Col[A]) + POWER(Col[B], 2)\n# Statistical\nCol[A] = MAX(Col[B], Col[C], Col[D]) / AVERAGE(Col[B], Col[C], Col[D])\n# Logical\nCol[A] = IF(Col[B] &gt; 1, \"value greater than 1\", \"it is not\")\nCol[A] = IF( AND(Col[B] &gt; 1, Col[C] &gt; 1), \"both values greater than 1\", \"not\")\n# Text\nCol[first name] = UPPER(Col[first name]) \n# splitting text, i.e. John Brown to John in one column and Brown in another \nCol[first name] = LEFT(Col[full name], SEARCH(\" \", Col[full name]) - 1) \nCol[last name] = RIGHT(Col[full name], LEN(Col[full name]) - SEARCH(\" \", Col[full name])) \n</code></pre> <p>Please note that columns with formulas are currently placed in the .dp file in order that they appear in Excel. If a formula column depends on another, but appears prior in Excel you can face execution order issues. For instance, let's assume we have A = B + 1 formula that appears in the 2nd Excel column. And B = [some existing column] + 1 appears in 3rd Excel column. In the .dp file you will see A = B + 1 first, and it's evaluation will fail since B is not defined yet.  To resolve this kind of issue, you can 1) change the order of columns in Excel so that it matches the execution order of formulas, or 2) change the order in .dp file manually.</p>"},{"location":"advanced/#filtering-with-remove_except","title":"Filtering with Remove_Except","text":"<pre><code>Remove_Except(Col[name of the column] criteria)\n</code></pre> <p>Removes (filters) all the rows that do not match the criteria. Criteria can be \"is in [values]\", logical operators like &gt;, &lt;, &gt;=, &lt;=, = and != and corresponding value, \"and\", \"or\".</p> <p>Examples:</p> <pre><code># remove all values on column RAD, and only keep 4, 5 and 6\nRemove_Except(Col[RAD] is in [4, 5, 6])\n# only keep values RAD &gt; 5 and ZN &lt; 10  \nRemove_Except(Col[RAD] &gt; 5 and Col[ZN] &lt;10)\n# remove all 5's   \nRemove_Except(Col[RAD] != 5)\n# keep only 5's   \nRemove_Except(Col[RAD] = 5)\n</code></pre>"},{"location":"advanced/#sort_columns","title":"Sort_Columns","text":"<pre><code>Sort_Columns(Col[name of first column to sort], .., ascending[True|False, ..])\n</code></pre> <p>Sort values for column(s), ascending or descending. You can sort on as many columns as needed. </p> <p>Example: </p> <pre><code>Sort_Columns(Col[AGE], Col[INDUS], ascending[True, False])\n</code></pre> <p>Sorts on column \"AGE\" from smallest to largest first, and then for the same AGE values sorts on column INDUS, largest to smallest.</p>"},{"location":"advanced/#reorder_columns","title":"Reorder_Columns","text":"<pre><code>Reorder_Columns([column name], [another column name], ..)\n</code></pre> <p>All new columns are appended to the end of the data set by default.  Reorder_Columns command changes the order of the columns. Please note that since this command has to create a new data set internally, inplace=True updates for apply_changes() will not work. inplace=False is the default behaviour for apply_changes() function. </p> <p>Example:</p> <pre><code>Reorder_Columns([CRIM], [CHAS], [NOX])\n</code></pre>"},{"location":"advanced/#custom-functions-udfs","title":"Custom Functions (UDFs)","text":"<pre><code>Col[name of the column] = MY_UDF(..)\n</code></pre> <p>Practicus AI supports defining your own User Defined Functions (UDFs) in a .dp file. If Practicus AI.apply_changes() function encounters a function name that it doesn't know, it will create a placeholder Python .py file for you to be completed later. The same will also happen if Practicus AI encounters an Excel function it doesn't recognize. </p> <p>Practicus AI custom function names follow Excel syntax: capitol letters, \"A-Z\" and numbers  \"0-9\", with the exception of an underscore  \"_\". For example MY_FUNCTION_2(). MyFunction2() is not a valid Excel or Practicus AI function name.   </p> <p>Example: </p> <p>Place the below line in \"sample.dp\" file</p> <pre><code>...\nCol[A] = MY_FUNCTION(Col[B], Col[C], 1, 2, 3)\n...\n</code></pre> <p>and run </p> <pre><code>df2 = practicus.apply_changes(df, \"sample.dp\")\n</code></pre> <p>This will create \"sample_dp.py\" file, place the missing function and raise a NotImplemented error. </p> <pre><code>def practicus_my_function(row, *args):\n    # you can define your function and then delete the below line\n    raise NotImplementedError('MY_FUNCTION')\n\n    # 'row' is a Pandas Data Frame row that you can access *any* column\n    result = row['B'] + row['C'] + args[2] + args[3] + args[4]\n    # the below does the same, without accessing row \n    result = args[0] + args[1] + args[2] + args[3] + args[4]\n\n    return result\n\n# please note that the above function template is simplified for documentation purposes\n# what you get will have more lines, including proper exception handling\n</code></pre> <p>After you fill the missing function template, you can re-run practicus.apply_changes() in your notebook, and your UDF MY_FUNCTION() will execute for all the rows on the data frame, in combination with other commands. </p> <p>You can also mix and match different UDFs and Excel functions on the same line as you wish. </p> <p>Example:</p> <pre><code>COL[A] = MY_FUNC( MY_FUNC2(Col[A]), SIN(Col[B]) )\n</code></pre> <p>The above will create UDF .py placeholders for MY_FUNC( ) and MY_FUNC2( ), and also use the standard Excel function SIN( ).  </p>"},{"location":"airflow/","title":"Airflow integration","text":"<p>Create production ready data processing code with one-click, embed into Airflow or other orchestration engines.  Choose your preferred data processing engine including Pandas, DASK, RAPIDS (GPU), RAPIDS + DASK (multi-GPU) and SPARK.</p> <p>After Airflow Code export is completed, you can see, run and stop the DAGs created with the Airflow tab. You can also run and schedule more than one task sequentially.</p> <p></p> <p>You can see and edit your Airflow code exported with Jupyter Notebook and continue your operations programmatically.You can also work on the data pipelines you have created.</p> <p></p> <p>Please view the Modern Data Pipelines section to learn more about out how Practicus AI data pipelines work by default.</p>"},{"location":"analyze/","title":"Analyze","text":"<p>Easily create charts and profile your data with one click.  Quickly understand how your data is distributed and identify hard to find correlations.</p>"},{"location":"analyze/#profile","title":"Profile","text":"<p>Generate the profile of the data you use with the profile feature in the analyze section.</p> <p></p> <p></p>"},{"location":"analyze/#graph","title":"Graph","text":"<p>Let's use the graph feature in the analysis section.</p> <p></p> <p>You can choose the graph style(Plot, Bar, Horizontal Bar, Histogram, Scatter Plot) you want to create from the dialog. Then enter the x and y columns of the graph you want to create.</p> <p></p> <p></p> <p>You can perform operations on the chart with the features in the menu bar, and you can save the chart if you want.</p>"},{"location":"analyze/#quick-stats","title":"Quick Stats","text":"<p>You can see quick statistics of the columns on data.</p> <p></p> <p>It gives you statistical data about the columns such as count, mean, std, min, 25%, 50%, 75% and max.</p> <p></p>"},{"location":"analyze/#quick-pivot","title":"Quick Pivot","text":"<p>Quick pivot allows you to quickly summarize large amounts of data in an interactive way. You can choose Sum, Median, Min, Max, Std. Dev., Variance, Product as Summary Method.</p> <p></p>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#export_data","title":"export_data","text":"<pre><code>export_data(data_set, output_path, columns=None, reindex=False)\n</code></pre> <p>Exports a dataset to an Excel file (.xlsx) to be used with Microsoft Excel, Google Sheets, LibreOffice etc. </p> <p>data_set: Pandas DataFrame (recommended), NumPy Array or Python list. For large datasets, we recommend Pandas DataFrame.</p> <p>output_path: The path where the Excel file will be written. If there is no file extension, Practicus AI will automatically add .xlsx to the file name.  </p> <p>columns: Optional, default is None. We highly recommend providing column names if the data_set does not include them. If no column names are found, Practicus AI will automatically add names for you, such as column_1, column_2 etc. These names will not lead to the best user experience.   </p> <p>reindex: Optional, default is False. If a Pandas DataFrame is used to export_data function, Practicus AI will use the index found in this data frame, or create one if none found. After several cycles of \"exporting and applying changes\", indexes might start jumping from value to value, such as 0, 5, 14 because of  filtering values. Although it is perfectly fine for Practicus AI to work with sparse index values like these, in some cases it can confuse end users. When reindex=True, Practicus AI will reset the index, so it will become like 0, 1, 2 again.</p> <pre><code># Example 1, exporting basic Python Array\n\n# define a basic array\narr = [[1,2,3],\n       [4,5,6],\n       [7,8,9]]\n\nimport practicus\n\n# write to Excel. Since no colum names are provided, practicus will create new names\npracticus.export_data(arr, \"basic_array.xlsx\")\n\n# Now, with column names\npracticus.export_data(arr, \"basic_array_2.xlsx\", columns=['x1', 'x2', 'x3'])\n\n\n# Example 2, exporting NumPy Array\nfrom sklearn.datasets import load_boston\n\ndata_set = load_boston()\nnumpy_array = data_set.data\n\npracticus.export_data(numpy_array, \"boston_house_numpy.xlsx\", \n                  columns=data_set.feature_names)\n\n\n# Example 3, exporting Pandas Data Frame (recommended)\nimport pandas as pd\ndf = pd.DataFrame(data=data_set.data, columns=data_set.feature_names)\n\n# no need to pass columns since the pandas data frame already has them \npracticus.export_data(df, \"boston_house_pandas.xlsx\")\n\n\n# Example 4, reindexing \n\n# let's delete first 5 rows from the data frame \ndf = df.drop(df.index[[0,1,2,3,4]])\n\n# reindexing will start the index from 0 again\npracticus.export_data(df, \"boston_house_pandas_reindexed.xlsx\", reindex=True)\n</code></pre>"},{"location":"api-reference/#apply_changes","title":"apply_changes","text":"<pre><code>data_set2 = apply_changes(data_set, input_path, columns=None, inplace=False, reindex=False)\n</code></pre> <p>Applies detected or passed changes to the data_set. The way apply_changes() work depends on what kind of file is passed in input_path. If an Excel (.xlsx) file is passed, apply_changes() first executes detect_changes() to find out what kind of changes are made to the Excel (.xlsx) file, writes these changed to a .dp file and then applies these changes to the data_set. If input_path is a .dp file, detect_changes() is not called, and updates are applied directly from the .dp file.  </p> <p>data_set: Pandas DataFrame, NumPy Array or Python list. For large datasets, Pandas DataFrame will perform faster.</p> <p>input_path: This can be either an Excel (.xlsx) file with some changes in it, or a data prep (.dp) file that has changes recorded in it.   </p> <p>columns: Optional, default is None. We highly recommend providing column names if the data_set does not include them. If no column names found, Practicus AI will automatically add names for you, such as column_1, column_2 etc.  Being consistent in column names between export_data() and apply_changes() is important since the column names in the Excel file and the data_set we later try to apply changes on should match. </p> <p>inplace: Optional, default is False. If inplace=True, Practicus AI will apply updates directly on the data_set provided as an input, and will not return anything back. Please note that inplace editing only works  if a Pandas DataFrame is provided as an input. inplace editing might have some performance benefits for very large datasets. We discourage using inplace updates unless it is necessary, since it becomes harder to retry code. I.e. if the applied changes do not work as you expect, you can open the auto-generated .dp file, make changes and can re-run apply_changes(). This will not work with inplace=True.</p> <p>reindex: Optional, default is False. Only meaningful if a Pandas Data Frame is used. After several cycles of \"exporting and applying changes\", indexes might start jumping from value to value i.e. 0, 5, 14 due to filtering of values. Although it is perfectly fine for Practicus AI to work with sparse indexes, in some cases these kinds of indexing can confuse end users. When reindex=True, Practicus AI will reset the index so the returned data_set will have indexes reset, i.e. 0,1,2,3. </p> <p>Returns: The exact same type of data_set, with changes applied. If a Pandas DataFrame as passed as input value, this function return a Pandas DataFrame. Same for NumPy Array and Python Lists. If inplace=True, returns nothing.  Other than returning the data_set, apply_changes also writes a new .dp file with changes detected in it, if an Excel (.xlsx) file is passed in input_path. </p> <pre><code># Example 1\n\narr = [[1,2,3],\n       [4,5,6],\n       [7,8,9]]\n\nimport practicus\npracticus.export_data(arr, \"basic_array_2.xlsx\", columns=['x1', 'x2', 'x3'])\n\n# now open the Excel file and delete first column, x1\n</code></pre> <pre><code># running the  below will detect the change we made (drop column), \n# apply it to arr and return a new Array  \narr2 = practicus.apply_changes(arr, \"basic_array.xlsx\", columns=['x1', 'x2', 'x3'])\n# You should see something like the below  in output window.\n</code></pre> <pre><code>Detecting changes made in basic_array.xlsx\nSaved changes to data preparation file basic_array.dp in 0.21 seconds.\n\nApplying changes from basic_array.dp\nInput_Columns = [x1], [x2], [x3]\n Input columns match the DataFrame.\n\n# Columns deleted in Excel \nRunning: Drop_Column(Col[x1])\n Completed. \n\nAll steps completed.\n</code></pre> <pre><code># Example 2, inplace updates. (Pandas data frame is required for this to work)\nimport pandas as pd\ndf = pd.DataFrame(data=arr, columns=['x1', 'x2', 'x3'])\n\n# with inplace=True, apply_changes() doesn't return anything \npracticus.apply_changes(df, \"basic_array.xlsx\", inplace=True)\n# confirm x1 column is dropped\ndisplay(df)\n\n#Example 3, reindexing\n# now open basic_array.xlsx and put a filter on x2, uncheck 2 and leave 5 and 8 only\n# (to filter in Excel, you can click the drop down arrow next to the column name, x2)   \n# reindexing will start the index from 0 again\ndf = pd.DataFrame(data=arr, columns=['x1', 'x2', 'x3'])\ndf2 = practicus.apply_changes(df, \"basic_array.xlsx\", reindex=True)\ndisplay(df2)\n</code></pre> <pre><code>  x2  x3\n0  5   6\n1    8   9\n</code></pre>"},{"location":"api-reference/#detect_changes","title":"detect_changes","text":"<pre><code>detect_changes(input_path, output_path=None)\n</code></pre> <p>This function detects changes a user makes inside the Excel (.xlsx) file and writes the detected changes to a data prep (.dp) file. Once the data prep (.dp) file is created, you can review and make changes as needed and then finally run using apply_changes function.     </p> <p>input_path: Excel (.xlsx) file path with some changes in it. </p> <p>output_path: Optional. The path for the data prep (.dp) file that the detected changes will be written to. If no file name is passed, Practicus AI uses the same name of the input_path and replaces .xlsx with .dp. </p> <p>Returns: The path of the .dp file. If output_path is provided this function returns that name, if output_path was empty, returns the name of the filename that is generated.   </p> <pre><code># Example 1 \n\n# the below code detects all changes made in the Excel file, \n# and writes the result into basic_array.dp\npracticus.detect_changes(\"basic_array.xlsx\")\n\n# open basic_array.dp file, you will see the below\n</code></pre> <pre><code>Input_Columns = [x1], [x2], [x3]\n\n# Columns deleted in Excel \nDrop_Column(Col[x1])\n\n# Filtered rows in Excel \nRemove_Except(Col[x2] is in [5, 8])\n</code></pre> <pre><code># let's assume we don't want to filter anymore. \n# Place a # in front of Remove_Except(..) to comment it out, and save the .dp file\n\n# now let's apply changes to arr, but this time directly from .dp file\n# instead of .xlsx file.\narr2 = practicus.apply_changes(arr, \"basic_array.dp\", columns=['x1', 'x2', 'x3'])\n\ndisplay(arr2)\n# the result will be: [[2, 3], [5, 6], [8, 9]]\n# first column (x1) dropped, but teh value 2 is \"not\" filtered out , \n# since we commented the filter line out with # in the .dp file \n</code></pre>"},{"location":"api-reference/#export_model","title":"export_model","text":"<pre><code>export_model(model, output_path, columns=\"\", target_name=\"\", num_rows=1000)\n</code></pre> <p>Exports a model to an Excel (.xlsx) file, so that users can understand how the model works, and make predictions by entering new values. </p> <p>model: A Python model, pipeline, or the path of a .pmml file. If a model is passed, Practicus AI can read Linear Regression,  Logistic Regression, Decision Trees and Support Vector Machine models. If a pipeline is passed, Practicus AI can read several pre-processing steps like StandardScaler, MinMaxScaler etc., in addition to the model trained as part of the pipeline. Practicus AI can also read .pmml files, especially for models built outside the Python environment like R, KNIME etc.   </p> <p>output_path: The path of the Excel (.xlsx) file to export.</p> <p>columns: Optional, if a .pmml file is passed as model and has column names in it. Otherwise, required. The column names that will be used as input features for the model. For a basic linear model,  y = 2 * x1 + 3 * x2,  'x1' and 'x2' are the input feature names. These names can be used as column names in the Excel file.</p> <p>target_name: Optional, if a .pmml file is passed as model and has column names in it. Otherwise, required. The target column name that the model predicts for. For a basic linear model,  y = 2 * x1 + 3 * x2,  'y' would be the target name.</p> <p>num_rows: Optional, defaults to 1000 rows. Indicates the number of rows to use in the Excel (.xlsx) file to make predictions on. Max 1,048,576 rows are supported, but the performance will depend on model's complexity, and the user's computer speed. Please gradually increase and test number of rows before sending the .xlsx file to others.     </p> <p>Returns: The path of the .dp file. If output_path is provided this function returns that name, if output_path was empty, returns the name of the filename that is generated.   See some limitations below.</p> <pre><code># Example 1, model exporting\n# ... model training code here ... \nsome_model.fit(X, Y)\n\nimport practicus\npracticus.export_model(some_model, output_path=\"some_model.xlsx\",\n                   columns=['X1', 'X2'], target_name=\"Some Target\", num_rows=100)\n\n# Example 2, pipeline exporting\n# ... model and pipeline code here ...\nmy_pipeline = make_pipeline(\n    SomePreProcessing(),\n    SomeOtherPreProcessing(),  \n    SomeModelTraining())\n\npracticus.export_model(my_pipeline, output_path=\"some_model.xlsx\",\n                   columns=['Column 1', 'Column 2'], target_name=\"Target\", num_rows=150)\n\n\n# Example 3, pmml model exporting\n\n# let's assume we have a model trained in R, and saved in a .pmml file\n# let's also assume the .pmml file has column and target names in it\npracticus.export_model(\"some_R_model.pmml\", output_path=\"some_R_model.xlsx\", num_rows=150)\n\n# Please check the Samples' section for more.. \n</code></pre>"},{"location":"api-reference/#export_model-limitations","title":"export_model limitations","text":"<p>Practicus AI is new, and we are looking forward to hearing your feedback on adding new features. some of our current limitations are: </p> <ul> <li> <p>Exported models need to have fewer than 16,384 columns.  </p> </li> <li> <p>Maximum 1 million rows (1,048,576) are supported for both data preparation and model exporting. </p> </li> <li> <p>Model exporting currently work with Linear Regression, Logistic Regression, Decision Trees and Support Vector Machines. According to Kaggle forums, Google search trends and other forums, these are by far the most popular modeling techniques potentially covering 90+% of the predictive ML analytics use cases. Practicus AI currently doesn't target model exporting for cognitive use cases and deep learning frameworks, since the resulting Excel file would become very complicated making  model debugging and model explainability challenging.</p> </li> </ul>"},{"location":"automating_kerberos_ticket_renewal_with_cron/","title":"Automating Kerberos Ticket Renewal with Cron","text":""},{"location":"automating_kerberos_ticket_renewal_with_cron/#introduction","title":"Introduction","text":"<p>Kerberos tickets are essential for authenticating to various services, but they have a limited lifespan. When a ticket expires, you lose access to those services until you re-authenticate. Manually renewing tickets is a hassle, especially for automated processes. This notebook will guide you through creating a startup script that leverages the <code>cron</code> utility to automatically renew your Kerberos tickets at regular intervals, ensuring uninterrupted access to Kerberos-authenticated services.</p>"},{"location":"automating_kerberos_ticket_renewal_with_cron/#why-automate-renewal-with-cron","title":"Why Automate Renewal with Cron?","text":"<p><code>cron</code> is a powerful, time-based job scheduler in Unix-like operating systems. It lets you schedule commands or scripts to run automatically at specific times or intervals. Automating Kerberos ticket renewal with <code>cron</code> offers several key advantages:</p> <ul> <li>Uninterrupted Access: Ensures continuous access to Kerberos-authenticated services without manual intervention.</li> <li>Increased Reliability: Reduces the risk of authentication failures caused by expired tickets in automated workflows.</li> <li>Operational Efficiency: Frees up your time from the repetitive task of manual ticket renewal.</li> </ul>"},{"location":"automating_kerberos_ticket_renewal_with_cron/#prerequisites","title":"Prerequisites","text":"<p>Before we start, make sure you have the following:</p> <ul> <li>Kerberos Principal (e.g., <code>your_user{{USER_NAME}}@{{REALM_NAME}}</code>).</li> <li>In this notebook, we'll automatically create a keytab file. While typically provided by a Kerberos administrator, our startup script handles this for convenience in certain scenarios.</li> </ul>"},{"location":"automating_kerberos_ticket_renewal_with_cron/#how-the-script-was-encoded","title":"How the Script Was Encoded","text":"<p>The shell script content was converted into the long <code>base64</code> string you'll use by piping the script's raw text content to the <code>base64</code> command. Here's a breakdown of the process:</p> <p>The Original Script Content</p> <p>This is the multi-line shell script designed to create your keytab, get an initial Kerberos ticket, and set up the <code>cron</code> job. Each line performs a specific action:</p> <pre><code>{\n  echo addent -password -p {{USER_NAME}}@{{REALM_NAME}} -k 1 -e aes256-cts\n  echo {{PASSWORD}}\n  echo wkt /var/practicus/{{USER_NAME}}.keytab\n  echo q\n} | ktutil\nchmod 700 /var/practicus/{{USER_NAME}}.keytab\nkinit -V -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h\nservice cron start\n(crontab -l 2&gt;/dev/null; echo '0 */11 * * * kinit -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h') | crontab -\nsh /var/practicus/.practicus/entry.sh\n</code></pre> <ul> <li><code>{</code>: This starts a command group for <code>ktutil</code>.</li> <li><code>echo addent -password -p {{USER_NAME}}@{{REALM_NAME}} -k 1 -e aes256-cts</code>: Adds a principal entry to the keytab using a password.</li> <li><code>echo {{PASSWORD}}</code>: Provides the password for the principal.</li> <li><code>echo wkt /var/practicus/{{USER_NAME}}.keytab</code>: Specifies the output keytab file path.</li> <li><code>echo q</code>: Quits <code>ktutil</code>.</li> <li><code>} | ktutil</code>: Pipes all the preceding <code>echo</code> commands into <code>ktutil</code> to create the keytab.</li> <li><code>chmod 700 /var/practicus/{{USER_NAME}}.keytab</code>: Sets secure permissions for the keytab (owner only: read, write, execute).</li> <li><code>kinit -V -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h</code>: Obtains an initial Kerberos TGT (Ticket Granting Ticket) using the keytab, valid for 12 hours.</li> <li><code>service cron start</code>: Ensures the <code>cron</code> service is running.</li> <li><code>(crontab -l 2&gt;/dev/null; echo '0 */11 * * * kinit -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h') | crontab -</code>: Adds a cron job to renew the ticket every 11 hours. Note that <code>{{USER_NAME}}@{{REALM_NAME}}</code> appears to be a specific service principal that will have its ticket renewed.</li> <li><code>sh /var/practicus/.practicus/entry.sh</code>: Executes another script. Its exact function depends on the content of that specific script, which is not defined here.</li> </ul> <p>Encoding with <code>echo</code> and <code>base64</code>:</p> <p>To convert this multi-line script into a single <code>base64</code> string, it's enclosed in quotes and then passed as input to the <code>base64</code> command using a pipe (<code>|</code>). The command used for encoding looks like this:</p> <pre><code>{\n  echo addent -password -p {{USER_NAME}}@{{REALM_NAME}} -k 1 -e aes256-cts\n  echo {{PASSWORD}}\n  echo wkt /var/practicus/{{USER_NAME}}.keytab\n  echo q\n} | ktutil\nchmod 700 /var/practicus/{{USER_NAME}}.keytab\nkinit -V -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h\nservice cron start\n(crontab -l 2&gt;/dev/null; echo '0 */11 * * * kinit -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h') | crontab -\nsh /var/practicus/.practicus/entry.sh\n</code></pre> <p>Running this command outputs the <code>base64</code> encoded string you'll use. This method allows the entire script, including newlines and special characters, to be safely stored and then decoded back to its original form for execution.</p>"},{"location":"automating_kerberos_ticket_renewal_with_cron/#running-the-startup-script-to-create-keytab-and-initial-ticket","title":"Running the Startup Script to Create Keytab and Initial Ticket","text":"<p>In this step, we'll use the provided <code>base64</code> encoded command to create and run a startup script. This script will create the keytab file, obtain your initial Kerberos TGT (Ticket Granting Ticket), and set up the cron job automatically.</p> <p>Critical Security Warning </p> <p>The <code>base64</code> encoded string you'll use contains your Kerberos password in plain text within the script it decodes. This is a significant security risk. You should only use this method for testing or in highly controlled, secure environments. Never pass passwords in this manner in production environments or when dealing with sensitive data. The preferred and secure method is always to use a keytab securely generated and distributed by a Kerberos administrator.</p> <p>Now, execute the following command in your terminal. Before running, replace the placeholders <code>{{USER_NAME}}</code>, <code>{{REALM_NAME}}</code>, and <code>{{PASSWORD}}</code> in the <code>base64</code> string with your actual Kerberos username, realm name, and password, respectively. </p> <pre><code>base64 --decode &lt;&lt;&lt; \"ewogIGVjaG8gYWRkZW50IC1wYXNzd29yZCAtcCB7e1VTRVJfTkFNRX19QHt7UkVBTE1fTkFNRX19IC1rIDEgLWUgYWVzMjU2LWN0cwogIGVjaG8ge3tQQVNTV09SRH19CiAgZWNobyB3a3QgL3Zhci9wcmFjdGljdXMve3tVU0VSX05BTUV9fS5rZXl0YWIKICBlY2hvIHEKfSB8IGt0dXRpbApjaG1vZCA3MDAgL3Zhci9wcmFjdGljdXMve3tVU0VSX05BTUV9fS5rZXl0YWIKa2luaXQgLVYgLWt0IC92YXIvcHJhY3RpY3VzL3t7VVNFUl9OQU1FfX0ua2V5dGFiIHt7VVNFUl9OQU1FfX1Ae3tSRUFMTV9OQU1FfX0gLWwgMTJoCnNlcnZpY2UgY3JvbiBzdGFydAooY3JvbnRhYiAtbCAyPi9kZXYvbnVsbDsgZWNobyAnMCAqLzExICogKiAqIGtpbml0IC1rdCAvdmFyL3ByYWN0aWN1cy97e1VTRVJfTkFNRX19LmtleXRhYiB7e1VTRVJfTkFNRX19QHt7UkVBTE1fTkFNRX19IC1sIDEyaCcpIHwgY3JvbnRhYiAtCnNoIC92YXIvcHJhY3RpY3VzLy5wcmFjdGljdXMvZW50cnkuc2gK\" &gt; /var/practicus/lib-install.sh &amp;&amp; chmod +x /var/practicus/lib-install.sh &amp;&amp; sh /var/practicus/lib-install.sh\n</code></pre> <p>Explanation of this Command Chain</p> <p>This single command line performs a sequence of operations:</p> <ol> <li><code>base64 --decode &lt;&lt;&lt; \"...\"</code>: This part decodes the provided <code>base64</code> encoded string. The decoded string is actually a multi-line shell script containing all the setup steps.</li> <li><code>&gt; /var/practicus/lib-install.sh</code>: The output from the <code>base64 --decode</code> command (which is our decoded shell script) is then redirected and written into a file named <code>/var/practicus/lib-install.sh</code>.</li> <li><code>&amp;&amp; chmod +x /var/practicus/lib-install.sh</code>: The <code>&amp;&amp;</code> operator ensures that this command runs only if the previous command (writing the script to the file) was successful. <code>chmod +x</code> then grants executable permissions to the newly created script file, making it runnable.</li> <li><code>&amp;&amp; sh /var/practicus/lib-install.sh</code>: Again, using <code>&amp;&amp;</code>, this command runs the script we just created and made executable. The <code>sh</code> command executes the script using the system's default shell.</li> </ol>"},{"location":"automating_kerberos_ticket_renewal_with_cron/#monitoring-and-verification","title":"Monitoring and Verification","text":"<p>After the setup is complete, it's crucial to verify that everything is working as expected. These verification steps should be performed directly within the relevant Kubernetes pod (e.g., apphost, modelhost, or worker pod) where your application or process requiring Kerberos authentication is running. </p> <p>Checking Kerberos Ticket Status</p> <p>You can manually check the validity and expiration time of your current Kerberos ticket using the <code>klist</code> command:</p> <pre><code>klist -f\n</code></pre> <p>In the output, you should see Valid starting and Expires times. After the <code>cron</code> job runs, you'll notice that the \"Expires\" time has been extended.</p> <p>Checking Crontab Entry</p> <p>To confirm that the cron job has been added correctly, you can list your <code>crontab</code> entries:</p> <pre><code>crontab -l\n</code></pre> <p>You should see a line similar to <code>0 */11 * * * kinit -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h</code> in the output.</p>"},{"location":"automating_kerberos_ticket_renewal_with_cron/#conclusion","title":"Conclusion","text":"<p>By following these straightforward steps, you've successfully automated the process of creating a Kerberos keytab file, obtaining an initial ticket, and setting up <code>cron</code> to automatically renew your Kerberos tickets. This setup ensures continuous access to your Kerberos-authenticated services, significantly enhancing the reliability of your automated processes and reducing the need for manual intervention.</p>"},{"location":"aws-iam/","title":"AWS IAM policies","text":"<p>Create cloud IAM policies that apply to all of our users to create audit trails for data access and processing.</p>"},{"location":"aws-iam/#administrator-access","title":"Administrator Access","text":"<p>User can attach AdministratorAccess policy to their users via AWS IAM. Practicus can be used easily if the user with AdministratorAccess policy is added in the Settings/AWS Cloud User section of the application.</p> <p></p>"},{"location":"aws-iam/#minimum-privileged-aws-user","title":"Minimum Privileged AWS User","text":"<p>You can create a minimum privileged AWS User from the Settings / Create new cloud user section through the application. When the user is created, the policies to be used for Practicus on AWS are attached to the user.</p> <p></p> <p></p>"},{"location":"cloud/","title":"Cloud","text":"<p>A Cloud Worker is like your computer in the cloud. Various configurations of CPU, memory, storage, and networking capacity for your instances, known as instance types. There are many instance types with different features. You can choose different instance types according to your usage area. You can keep your data on Cloud Worker and work on big data quickly.</p>"},{"location":"cloud/#start-new-cloud-worker","title":"Start New Cloud Worker","text":"<p>You create a new Cloud Worker by choosing the type of instance you want to use, the EBS disk size,Name and the Cloud Worker Version.</p> <p></p> <p>You can stop, start, reset, reboot, replicate and terminate the created Cloud Worker.</p>"},{"location":"cloud/#terminal-cloud-worker","title":"Terminal Cloud Worker","text":"<p>Quickly SSH to the Cloud Worker using the terminal feature.</p> <p></p>"},{"location":"cloud/#cloud-worker-log","title":"Cloud Worker Log","text":"<p>It accesses the logs on the Cloud Worker, it can be saved.</p> <p></p>"},{"location":"cloud/#jupyter-notebook-mlflow-airflow","title":"Jupyter Notebook, MLFlow, Airflow","text":"<p>Provides Jupyter Notebook, MLFlow and Airflow connections with the selected Cloud Worker.</p> <p>Jupyter Notebook allows you to code and see the files in the Cloud Worker.</p> <p>MLFlow allows you to see the registered models and predict using these models.</p> <p>Airflow allows you to create and schedule a new DAG.</p>"},{"location":"code-export/","title":"Code Export","text":"<p>Create production ready data processing code with one-click, embed into Airflow or other orchestration engines.  Choose your preferred data processing engine including Pandas, DASK, RAPIDS (GPU), RAPIDS + DASK (multi-GPU) and SPARK. </p>"},{"location":"code-export/#export-code","title":"Export Code","text":"<p>You can export the operations performed when we do data preparation on the data to the code. If you wish, you can continue your operations over the exported code.</p> <p>You can choose Pandas, DASK, RAPIDS, multi-GPU, Spark as data platform, and you can choose Jupyter Notebook, Python Library and Airflow Library as Code Template.</p> <p>Note : You need a ready Cloud Worker for this operation.</p> <p></p> <p>When the process is complete, you can navigate through the files, continue with the exported code and review the code.</p> <p></p>"},{"location":"data-prep/","title":"Data Preparation use case","text":"<p>There are two main use cases for Practicus AI. Data Preparation and model sharing. </p> <p>Data preparation for ML consumes an estimated 80% to 90% of the time for a Data Scientist today. Practicus AI aims to help data preparation for ML by closing the gap between Excel and Python code. Both data scientists and other supporting personas like business analysts can take advantage of the below functionality, and work together to prepare the data for ML training. </p> <p>Basic data preparation use case</p> <p>1) Export a Pandas DataFrame, NumPy array or a Python list to Excel</p> <pre><code>import practicus\n# export data to an Excel file\npracticus.export_data(my_df, \"my_data.xlsx\")\n</code></pre> <p>2) Open the file in Excel, Google Sheets, LibreOffice or any other Spreadsheet platform to analyze and make changes as usual. </p> <p></p> <p>3) After you are finished updating your data in Excel, you can apply all changes made to create a new data set. </p> <pre><code># import back from Excel, detect all the changes made, and apply to the Data Frame  \nmy_df2 = practicus.apply_changes(my_df, \"my_data.xlsx\") \n\n# practicus auto-generates Python code for you, and applies the updates..\n\n# display the result, which will be the same as what you see in Excel\ndisplay(my_df2)\n</code></pre> <p></p> <p>4) (Optional) Practicus AI will automatically create a data prep (.dp) file containing all detected changes, before generating Python code. You can review this file, remove changes you don't like, or add new ones manually as you wish. Once done, you can apply the updates directly from the .dp file. </p> <p></p> <pre><code># apply changes, but this time directly from the .dp file that you reviewed / updated\nmy_df2 = practicus.apply_changes(my_df, \"my_data.dp\")\n</code></pre> <p>5) (Optional) Rinse and repeat... You can continue the above steps, also working with others in a collaborative environment, to keep generating new versions of Excel files and auto-generated data sets. The detected changes (.dp files) can be updated and archived as needed. Outside of Jupyter notebooks, you can also chain multiple .dp files to create complex data preparation / ML pipelines and later embed these data pipelines to a data engineering platform for production purposes.  Any production grade data integration platform that can run Python code will easily run Practicus AI detected changes at scale.   </p>"},{"location":"delete_specific_trash_folders/","title":"Script: Automated Trash Folder Cleaner (Excluding Shared Path)","text":""},{"location":"delete_specific_trash_folders/#script-purpose","title":"Script Purpose","text":"<p>This script is designed to help you free up disk space by automatically finding and deleting <code>.Trash-1000</code> folders within your <code>/home/ubuntu/</code> directory. It specifically excludes any <code>.Trash-1000</code> folders located under <code>/home/ubuntu/shared/</code>, ensuring shared data remains untouched. For each found folder, it will show its current size, ask for your confirmation, and then permanently delete the entire folder.</p> <p>WARNING: This script uses <code>rm -rf</code>, which is a powerful command. Deleting files or directories with <code>rm -rf</code> is permanent and cannot be undone. Please exercise extreme caution and ensure you understand what the script does before running it.</p>"},{"location":"delete_specific_trash_folders/#understanding-the-trash-1000-folder","title":"Understanding the <code>.Trash-1000</code> Folder","text":"<p>When you delete a file or folder from a location like your project directories while working in a Jupyter Notebook environment (or other graphical file management tools), the system often creates a hidden folder named <code>.Trash-1000</code> in that specific directory. This acts as a local \"Recycle Bin\" for items deleted from that spot, allowing temporary storage before permanent removal. The <code>1000</code> in the name usually refers to your user ID. This script targets and removes these temporary <code>.Trash-1000</code> folders to reclaim disk space.</p>"},{"location":"delete_specific_trash_folders/#how-to-create-and-run-the-script","title":"How to Create and Run the Script","text":"<p>Follow these steps to create the script file and execute it in your terminal.</p>"},{"location":"delete_specific_trash_folders/#1-create-the-script-file","title":"1. Create the Script File","text":"<p>You can create the script file by directly pasting the entire command block below into your terminal. This method uses a \"here document\" to write the multi-line script content into a new file named <code>delete_specific_trash_folders.sh</code>.</p> <pre><code>cat &lt;&lt; 'EOF' &gt; delete_specific_trash_folders.sh\n#!/bin/bash\n\necho \"Script will find all '.Trash-1000' folders under /home/ubuntu/ (excluding /home/ubuntu/shared/) and offer to delete them.\"\necho \"This operation will permanently delete the ENTIRE found '.Trash-1000' folders and cannot be undone!\"\necho \"\" # Empty line for readability\n\n# Path to exclude from search\nEXCLUDE_PATH=\"/home/ubuntu/shared\"\n\n# Find all .Trash-1000 folders under /home/ubuntu/, excluding EXCLUDE_PATH\n# Using -path to exclude the specific directory\nTRASH_FOLDERS=($(find /home/ubuntu -type d -name \".Trash-1000\" -not -path \"${EXCLUDE_PATH}/*\" 2&gt;/dev/null))\n\nif [ ${#TRASH_FOLDERS[@]} -eq 0 ]; then\n    echo \"No '.Trash-1000' folders found under /home/ubuntu/ (excluding '$EXCLUDE_PATH/'). Script finished.\"\n    exit 0\nfi\n\necho \"Found the following '.Trash-1000' folders to process:\"\nfor folder in \"${TRASH_FOLDERS[@]}\"; do\n    echo \"- $folder\"\ndone\necho \"\"\n\n# Loop through each found trash folder\nfor TRASH_FOLDER_FULL_PATH in \"${TRASH_FOLDERS[@]}\"; do\n    echo \"--- Processing: '$TRASH_FOLDER_FULL_PATH' ---\"\n\n    # --- Display disk usage BEFORE deletion ---\n    echo \"--- Disk Usage BEFORE Deletion ---\"\n    if [ -d \"$TRASH_FOLDER_FULL_PATH\" ]; then\n        echo \"Current disk usage of '$TRASH_FOLDER_FULL_PATH':\"\n        du -sh \"$TRASH_FOLDER_FULL_PATH\"\n        echo \"\" # Empty line for readability\n\n        # Get user confirmation for this specific folder\n        read -p \"Do you want to proceed with deleting the ENTIRE folder '$TRASH_FOLDER_FULL_PATH'? (yes/no): \" response\n\n        # Convert response to lowercase\n        response=$(echo \"$response\" | tr '[:upper:]' '[:lower:]')\n\n        if [[ \"$response\" == \"yes\" ]]; then\n            echo \"Starting deletion of: '$TRASH_FOLDER_FULL_PATH'\"\n            # --- Deleting the entire folder ---\n            rm -rf \"$TRASH_FOLDER_FULL_PATH\"\n\n            if [ $? -eq 0 ]; then\n                echo \"Folder successfully deleted: '$TRASH_FOLDER_FULL_PATH'\"\n            else\n                echo \"ERROR: An issue occurred while deleting the folder: '$TRASH_FOLDER_FULL_PATH'.\"\n                echo \"Permissions or another problem might be present. Please check manually.\"\n            fi\n        else\n            echo \"Deletion cancelled for '$TRASH_FOLDER_FULL_PATH'.\"\n        fi\n    else\n        echo \"ERROR: Folder '$TRASH_FOLDER_FULL_PATH' not found or disappeared unexpectedly. Skipping.\"\n    fi\n\n    echo \"\" # Empty line for readability\n\n    # --- Display disk usage AFTER deletion (or cancellation) ---\n    echo \"--- Disk Usage AFTER Operation ---\"\n    if [ -d \"$TRASH_FOLDER_FULL_PATH\" ]; then\n        echo \"Folder '$TRASH_FOLDER_FULL_PATH' still exists (deletion was cancelled or failed):\"\n        du -sh \"$TRASH_FOLDER_FULL_PATH\"\n    else\n        echo \"Folder '$TRASH_FOLDER_FULL_PATH' no longer exists. Space has been reclaimed.\"\n    fi\n    echo \"\" # Empty line for readability\ndone\n\necho \"Script finished.\"\nEOF\n</code></pre>"},{"location":"delete_specific_trash_folders/#2-make-the-script-executable","title":"2. Make the Script Executable","text":"<p>After creating the file, you need to give it permission to run as a program. Open your terminal and run the following command in the directory where you saved <code>delete_specific_trash_folders.sh</code>:</p> <pre><code>chmod +x delete_specific_trash_folders.sh\n</code></pre>"},{"location":"delete_specific_trash_folders/#3-run-the-script","title":"3. Run the Script","text":"<p>Once the script is executable, you can run it from your terminal:</p> <pre><code>./delete_specific_trash_folders.sh\n</code></pre> <p>The script will then proceed to find the relevant <code>.Trash-1000</code> folders (excluding the shared path), display their sizes, and prompt you for confirmation for each one before performing any deletion.</p>"},{"location":"excel-predict/","title":"Predict with Excel","text":"<p>You can also use pure-Excel for prediction! (only at Practicus AI). Export and embed your AI models into Excel or Google  Sheets, attach to an email and allow your users to make predictions completely offline with no app / plugin / macro required. You can use the AutoML models, or export your own custom models. Currently supporting Linear Regression,  Logistic Regression, SVM and Decision Trees.</p>"},{"location":"excel-predict/#prediction","title":"Prediction","text":"<p>After the model is created with the build excel model option, you select the location where you will save the excel model from the dialog that appears.</p> <p></p> <p>You open the saved excel model and make predictions for the column you want. In this example, the target column was revenue and when we filled the temperature column with different values, the revenue column was filled with the predicted values.</p> <p></p>"},{"location":"excel-prep/","title":"Prepare with Excel","text":"<p>You can also use Excel for data preparation! (only at Practicus AI).  Share pure Excel or Google Sheets with others,  so they can analyze the data and make changes, and send it back to you. Practicus AI can then capture those  changes, so you can apply to any data locally or in the cloud, or export to Python code. Please watch the demo for more! </p>"},{"location":"excel-prep/#export-to-excel","title":"Export to Excel","text":"<p>With Export to Excel, you can save your data on which you have done preparation operations as excel anywhere on your local computer.</p> <p>Note : You need ready Cloud Worker for this operation.</p> <p></p>"},{"location":"excel-prep/#edit-in-excel","title":"Edit in Excel","text":"<p>You can delete columns, change their names, perform sort and filter operations on the exported excel, apply excel formulas and perform other data preparation operations. In this example, the CHAS column has been deleted and the name of the NOX column has been changed.</p> <p></p>"},{"location":"excel-prep/#import-from-excel","title":"Import from Excel","text":"<p>After saving your excel file on which you perform operations, you can import it to the application with Import from Excel.</p> <p></p> <p>The changes we made on the data with Excel were noticed. You can delete and edit them if you wish.</p> <p></p> <p>Thus, we have imported our data on Excel into the application with the changes we have made.</p>"},{"location":"explore/","title":"Explore","text":"<p>Up to 100+ times faster sampling. Random sample 1 billion+ rows down to 1 million in only a few seconds.  Switch between data engines if you need (Pandas, DASK, Spark, GPUs with RAPIDS).  Use S3 like a local drive on your laptop by simple copy/paste operations. Query Databases using SQL.</p>"},{"location":"explore/#local-files","title":"Local Files","text":"<p>You can browse local files in the Explore tab and preview the data you want to work with easily.</p> <p></p> <p>You can create a new file, copy and paste the files on Cloud Worker or S3</p>"},{"location":"explore/#cloud-worker-files","title":"Cloud Worker Files","text":"<p>Firstly you have to launch a new Cloud Worker from the Cloud Tab. Then you can navigate the Cloud Worker content. You can preview the file by clicking on it.</p> <p></p>"},{"location":"explore/#create-folder","title":"Create Folder","text":"<p>Let's create a new folder.</p> <p></p> <p></p> <p>A new folder has been created.</p> <p>You can also download and upload files on Cloud Worker and run the necessary scripts for databases.</p>"},{"location":"explore/#upload","title":"Upload","text":"<p>A directory is selected for the upload process.</p> <p></p> <p>You can select the file you want to upload from the file dialog.</p> <p></p> <p>After making your selection, you will be directed to the file transfer tab.</p> <p></p> <p>It starts the process with start transfer and you can close the tab when the process is finished.</p> <p></p> <p>You can see the uploaded file by navigating on Cloud Worker.</p> <p></p>"},{"location":"explore/#download","title":"Download","text":"<p>Select the files and start the download process.</p> <p></p> <p></p> <p>After completing the download via the file transfer tab, you can navigate to local files and find the file.</p> <p></p>"},{"location":"explore/#amazon-s3","title":"Amazon S3","text":"<p>After the required Cloud Config setup process is completed, you can load the buckets with Amazon S3, then select the bucket and navigate the files. Note : For this operation you need ready Cloud Worker.</p> <p></p> <p>You can also perform the features (copy, paste, upload, download, new folder) in the menu bar.</p>"},{"location":"explore/#relational-databases","title":"Relational Databases","text":"<p>You can connect to the database using relational databases, run SQL queries, and continue your operations on the data. Amazon Redshift, Snowflake, PostgreSQL, MySQL, SQLite, Amazon Athena, Hive(Hadoop), SQLServer, Oracle, ElasticSearch, AWSOpenSearch, OtherDB connections are supported. Amazon Athena, Hive(Hadoop), SQLServer, Oracle, ElasticSearch, AWSOpenSearch databases need driver installation.</p> <p>Note: You need a ready Cloud Worker to access Relational Databases.</p> <p></p>"},{"location":"faq/","title":"Faq","text":"<p>This section of the documentation is work in progress..</p>"},{"location":"feedback/","title":"Feedback","text":""},{"location":"feedback/#general-feedback","title":"General Feedback","text":"<p>We would love to hear your feedback!</p> <p>https://practicus.ai/feedback/</p>"},{"location":"feedback/#report-issue","title":"Report issue","text":"<p>Please use the below form to submit any issues you face. We will do our best to fix it as soon as possible and get back to you. </p> <p>https://practicus.ai/report-issue</p>"},{"location":"feedback/#request-new-feature","title":"Request new feature","text":"<p>We only build features that our users request. Please let us know what you need, and we will prioritize accordingly. </p> <p>https://practicus.ai/feature-request</p>"},{"location":"feedback/#other-questions","title":"Other Questions","text":"<p>If you have any questions regarding how to use Practicus AI, please submit your question on one of the popular forums  such as reddit  or stackoverflow. </p> <p>Since we will not get a notification from the public forum, please let us know the question's URL by using the below. We will get back to you as soon as possible.</p> <p>https://practicus.ai/questions</p>"},{"location":"get-started/","title":"Get started","text":"<p>This page has moved to getting-started</p>"},{"location":"getting-started/","title":"Getting Started with Practicus AI","text":"<p>The Practicus AI platform offers multiple tools for working with AI and data intelligence. Regardless of your role, experience level, or objectives, you can choose an entry point that fits your needs.</p>"},{"location":"getting-started/#personas-practicus-ai-tools-tasks","title":"Personas, Practicus AI Tools &amp; Tasks","text":"<p>The diagram below illustrates various user roles, the tools available to them, and the tasks they support:</p> <p></p>"},{"location":"getting-started/#finding-the-right-tool-for-the-job","title":"Finding the Right Tool for the Job","text":"<p>Your choice of tool depends on your goals, role, and technical skill level:</p> <p></p>"},{"location":"getting-started/#where-to-start","title":"Where to Start","text":"<p>Most users begin at Practicus AI Home, typically accessed at an address like <code>https://practicus.your-company.com</code>. If you do not have access, consult your system administrator. If you are not an existing enterprise user, you can experiment offline with the free Practicus AI Studio, or contact your IT team for enterprise installation options.</p> <p>Next, decide whether you prefer to work with code or not.</p>"},{"location":"getting-started/#if-you-code","title":"If You Code","text":"<ol> <li> <p>Create a Worker:    From Practicus AI Home, create one or more Workers\u2014isolated Kubernetes pods configured with the container image, CPU/RAM, GPU, and other resources you need.</p> </li> <li> <p>Use Jupyter Lab or VS Code:    After selecting a Worker, launch Jupyter Lab or VS Code. You can write code, train and deploy ML models, and interact with data. Sample jupyter notebooks are provided in each Worker's home directory for quick exploration.</p> </li> </ol>"},{"location":"getting-started/#if-you-do-not-code","title":"If You Do Not Code","text":"<ol> <li> <p>Access Practicus AI Studio or Workspaces:    From Practicus AI Home, open a browser-based Workspace or download AI Studio, for a no-code/low-code experience. If local installation isn\u2019t possible, online Workspaces already include Practicus AI Studio, office productivity tools, and other useful applications.</p> </li> <li> <p>Explore &amp; Build with No-Code Tools:    Use visual interfaces and guided workflows to analyze data, generate insights, and create AI solutions\u2014no programming required.</p> </li> </ol> <p>Previous: Welcome | Next: Tutorials</p>"},{"location":"images/","title":"Practicus AI Worker Image Requirements","text":""},{"location":"images/#image-practicus-core","title":"Image: Practicus Core","text":"<p>The Practicus Core image is a comprehensive Python environment that includes essential data science libraries such as pandas, numpy, scikit-learn, xgboost, dask, and mlflow, enabling end-to-end workflows for data analysis, big data processing, machine learning, and MLOps.</p> Packages  | Package                 | Version   | |-------------------------|-----------| | ydata-profiling         | 4.16.1    | | bokeh                   | 3.7.3     | | xgboost                 | 3.0.3     | | scikit-learn            | 1.7.1     | | dask[dataframe]         | 2025.7.0  | | distributed             | 2025.7.0  | | dask-ml                 | -         | | mcp                     | 1.12.3    | | mlflow                  | -         | | matplotlib              | -         | | s3fs                    | -         | | boto3                   | -         | | jupyterlab-git          | -         | | pymysql                 | -         | | snowflake-sqlalchemy    | -         | | oracledb                | -         | | pyodbc                  | -         | | pyarrow                 | -         | | pyiceberg               | -         | | polars                  | -         | | neo4j                   | -         | | pika                    | -         | | ruff                    | -         | | uv                      | -         | | shap                    | -         | | openai                  | -         |"},{"location":"images/#image-practicus-minimal","title":"Image: Practicus Minimal","text":"<p>The Practicus Minimal image is a lightweight yet functional Python environment that includes essential libraries such as pandas, pydantic, and sqlalchemy for data processing and database connectivity, along with jupyterlab support for easily conducting data analysis and prototype development workflows.</p> Packages  | Package               | Version | |-----------------------|---------| | pandas                | 2.2.3   | | pydantic              | 2.11.7  | | duckdb                | 1.3.2   | | PyJWT[crypto]         | 2.10.1  | | websocket-client      | 1.8.0   | | sqlalchemy            | 2.0.41  | | aio-pika              | 9.5.5   | | jupyterlab            | 4.4.5   | | jinja2                | -       | | joblib                | -       | | cloudpickle           | -       | | tornado               | -       | | psutil                | -       | | ipywidgets            | -       | | jedi-language-server  | -       | | psycopg2-binary       | -       | | papermill             | -       |"},{"location":"images/#image-practicus-genai","title":"Image: Practicus GenAI","text":"<p>The Practicus GenAI image is optimized for developing large language model (LLM)-based applications, vector database integrations, and interactive AI solutions with libraries such as streamlit, langchain, pymilvus, qdrant-client, and huggingface_hub.</p> Packages  | Package                | Version | |------------------------|---------| | streamlit              | 1.47.1  | | langchain[openai]      | -       | | pymilvus               | 2.5.14  | | qdrant-client          | 1.14.3  | | langchain-community    | 0.3.27  | | huggingface_hub        | 0.34.3  | | psycopg2-binary        | 2.9.10  | | psycopg[binary]        | 3.2.9   | | asyncpg                | 0.30.0  | | sqlmodel               | 0.0.24  | | alembic                | 1.16.4  |"},{"location":"images/#image-practicus-genai-langflow","title":"Image: Practicus GenAI + Langflow","text":"<p>The Practicus GenAI + Langflow image is optimized for easily designing, testing, and running visual flow-based AI applications and LLM integrations using the langflow library.</p> Packages  | Package  | Version | |----------|---------| | langflow | 1.4.3   |"},{"location":"images/#image-practicus-spark","title":"Image: Practicus Spark","text":"<p>The Practicus Spark image is designed for large-scale data processing and analytics using pyspark, with numpy support for efficient numerical computations.</p> Packages  | Package  | Version | |----------|---------| | pyspark  | 3.5.6   | | numpy    | -       |"},{"location":"images/#image-practicus-automl","title":"Image: Practicus Automl","text":"<p>The Practicus AutoML image is optimized for developing automated machine learning workflows, model training and tuning, evaluation, and explainable AI applications using libraries such as pycaret, lightgbm, mlflow, and other supporting packages.</p> Packages  | Package                        | Version | |--------------------------------|---------| | pycaret[models,parallel,tuners]| 3.3.2   | | lightgbm                       | 3.3.5   | | mlflow                         | 2.17.2  | | interpret                      | -       | | fairlearn                      | -       | | prophet                        | -       | | umap-learn                     | -       |"},{"location":"images/#image-practicus-ray","title":"Image: Practicus Ray","text":"<p>The Practicus Ray image is designed for distributed data processing, model training, hyperparameter optimization, and scalable AI application development using libraries such as ray and modin.</p> Packages  | Package                        | Version | |--------------------------------|---------| | ray[default,data,train,tune]   | -       | | modin[ray]                     | -       |"},{"location":"images/#image-practicus-core-gpu","title":"Image: Practicus Core GPU","text":"<p>The Practicus Core GPU image is a comprehensive, GPU-optimized Python environment for advanced data analysis, big data processing, and machine learning, featuring libraries such as transformers, xgboost, scikit-learn, dask, and mlflow, along with tools for visualization, database connectivity, distributed computing, and MLOps workflows.</p> Packages  | Package                 | Version  | |-------------------------|----------| | transformers            | -        | | ydata-profiling         | 4.16.1   | | bokeh                   | 3.7.3    | | xgboost                 | 3.0.3    | | scikit-learn            | 1.7.1    | | dask[dataframe]         | 2025.7.0 | | distributed             | 2025.7.0 | | dask-ml                 | -        | | mcp                     | 1.12.3   | | mlflow                  | -        | | matplotlib              | -        | | s3fs                    | -        | | boto3                   | -        | | jupyterlab-git          | -        | | pymysql                 | -        | | snowflake-sqlalchemy    | -        | | oracledb                | -        | | pyodbc                  | -        | | pyarrow                 | -        | | pyiceberg               | -        | | polars                  | -        | | neo4j                   | -        | | pika                    | -        | | ruff                    | -        | | uv                      | -        | | shap                    | -        | | openai                  | -        |"},{"location":"images/#image-practicus-minimal-gpu","title":"Image: Practicus Minimal GPU","text":"<p>The Practicus Minimal GPU image is optimized for running data analysis, prototype development, and lightweight machine learning workflows on GPUs, featuring essential libraries such as pandas, pydantic, and sqlalchemy, along with jupyterlab support.</p> Packages  | Package               | Version | |-----------------------|---------| | pandas                | 2.2.3   | | pydantic              | 2.11.7  | | duckdb                | 1.3.2   | | PyJWT[crypto]         | 2.10.1  | | websocket-client      | 1.8.0   | | sqlalchemy            | 2.0.42  | | aio-pika              | 9.5.5   | | jupyterlab            | 4.4.5   | | jinja2                | -       | | joblib                | -       | | cloudpickle           | -       | | tornado               | -       | | psutil                | -       | | ipywidgets            | -       | | jedi-language-server  | -       | | psycopg2-binary       | -       | | psycopg[binary]       | -       | | papermill             | -       |"},{"location":"images/#image-practicus-torch-gpu","title":"Image: Practicus Torch GPU","text":"<p>The Practicus Worker GPU + Torch image is optimized for deep learning development and training on GPUs, featuring torch for building and training neural networks and transformers for leveraging state-of-the-art NLP models.</p> Packages  | Package       | Version | |---------------|---------| | torch         | -       | | transformers  | -       |"},{"location":"images/#image-practicus-deepspeed-gpu","title":"Image: Practicus DeepSpeed GPU","text":"<p>The Practicus Worker GPU + DeepSpeed Base image is optimized for large-scale deep learning training and inference on GPUs, featuring DeepSpeed for distributed, memory-efficient training and transformers for working with state-of-the-art NLP models.</p> Packages  | Package       | Version | |---------------|---------| | deepspeed     | -       | | transformers  | -       |"},{"location":"images/#image-practicus-fairscale-gpu","title":"Image: Practicus FairScale GPU","text":"<p>The Practicus Worker GPU + FairScale Base image is optimized for distributed and memory-efficient deep learning training on GPUs, featuring FairScale for advanced parallelization strategies and transformers for working with cutting-edge NLP models.</p> Packages  | Package       | Version | |---------------|---------| | fairscale     | -       | | transformers  | -       |"},{"location":"images/#image-practicus-worker-gpu-whisper","title":"Image: Practicus Worker GPU + Whisper","text":"<p>The Practicus Worker GPU + Whisper image is optimized for speech-to-text and audio transcription tasks on GPUs, featuring openai-whisper for state-of-the-art speech recognition and including ffmpeg for handling a wide range of audio and video formats.</p> Packages  | Package         | Version | |-----------------|---------| | openai-whisper  | -       | | ffmpeg*         | -       |"},{"location":"images/#image-practicus-ray-gpu","title":"Image: Practicus Ray GPU","text":"<p>The Practicus Ray GPU image is designed for distributed data processing, model training, hyperparameter optimization, and scalable AI application development on GPUs using libraries such as ray and modin.</p> Packages  | Package                        | Version | |--------------------------------|---------| | ray[default,data,train,tune]   | -       | | modin[ray]                     | -       |"},{"location":"images/#practicus-ai-modelhost-image-requirements","title":"Practicus AI Modelhost Image Requirements","text":""},{"location":"images/#image-practicus-model-host","title":"Image: Practicus Model Host","text":"<p>The Practicus Model Host image provides the foundational environment for deploying and serving machine learning models, featuring FastAPI and Uvicorn for high-performance API hosting, scikit-learn and xgboost for model execution, and a range of supporting libraries for data processing, caching, messaging, and observability.</p> Packages  | Package                | Version   | |------------------------|-----------| | fastapi                | 0.115.14  | | uvicorn                | 0.34.3    | | PyJWT[crypto]          | 2.10.1    | | lz4                    | 4.4.4     | | aiobotocore            | 2.21.1    | | boto3                  | -         | | prometheus-client      | 0.22.1    | | aiofiles               | 24.1.0    | | pydantic               | &gt;2        | | xgboost                | 2.1.4     | | scikit-learn           | 1.6.1     | | accumulation-tree      | 0.6.4     | | beautifulsoup4         | -         | | httpx                  | -         | | websocket-client       | -         | | requests               | -         | | python-multipart       | -         | | redis[hiredis]         | -         | | confluent-kafka        | -         | | pymemcache             | -         | | joblib                 | -         | | cloudpickle            | -         | | neo4j                  | -         | | pika                   | -         | | pyyaml                 | -         |"},{"location":"images/#image-practicus-model-host-automl","title":"Image: Practicus Model Host AutoML","text":"<p>The Practicus Model Host AutoMLimage is optimized for serving automated machine learning models, featuring PyCaret for streamlined model training, tuning, and deployment workflows.</p> Packages  | Package                | Version | |------------------------|---------| | pycaret[models]        | 3.3.2   |"},{"location":"images/#image-practicus-model-host-gpu","title":"Image: Practicus Model Host GPU","text":"<p>The Practicus Model Hosting GPU Base image is built for deploying and serving GPU-accelerated machine learning models, featuring transformers for working with state-of-the-art deep learning and natural language processing models, along with GPU-specific dependencies for high-performance inference.</p> Packages  | Package       | Version | |---------------|---------| | transformers  | -       |"},{"location":"images/#image-practicus-model-host-gpu-torch","title":"Image: Practicus Model Host GPU + Torch","text":"<p>The Practicus Model Hosting GPU + Torch image is optimized for deploying and serving GPU-accelerated deep learning models, featuring torch for building, training, and running neural networks in production environments.</p> Packages  | Package  | Version | |----------|---------| | torch    | -       |"},{"location":"images/#image-practicus-model-host-gpu-vllm","title":"Image: Practicus Model Host GPU + VLLM","text":"<p>The Practicus Model Host GPU + VLLM image is optimized for high-throughput, low-latency large language model (LLM) inference on GPUs, featuring vllm for efficient model serving with advanced scheduling and memory optimization.</p> Packages  | Package | Version | |---------|---------| | vllm    | 0.9.1   |"},{"location":"images/#image-practicus-model-host-gpu-vllm-tiny","title":"Image: Practicus Model Host GPU + vLLM + Tiny","text":"<p>The Practicus Model Host GPU + vLLM + Tiny image is preloaded with the TinyLlama/TinyLlama-1.1B-Chat-v1.0 model for lightweight, high-speed LLM inference on small or legacy GPUs (e.g., Nvidia T4). It leverages vLLM for efficient serving and includes model files locally to enable fully offline operation.</p> Preloaded Model  | Model Name                                      | Precision | Notes                                      | |-------------------------------------------------|-----------|--------------------------------------------| | TinyLlama/TinyLlama-1.1B-Chat-v1.0              | half      | Optimized for small/legacy GPU deployment  |"},{"location":"images/#image-practicus-model-host-base-litellm","title":"Image: Practicus Model Host Base LiteLLM","text":"<p>The Practicus Model Host Base LiteLLM image is tailored for running the LiteLLM proxy in production, enabling unified LLM routing and spend tracking with Prisma-based persistence and S3-compatible integrations.</p> Packages  | Package          | Version        | |------------------|----------------| | litellm[proxy]   | 1.74.3.post1   | | aiobotocore      | &lt;2.21.1        | | prisma           | 0.11.0         |"},{"location":"images/#image-practicus-model-host-sparkml-base","title":"Image: Practicus Model Host SparkML Base","text":"<p>The Practicus Model Host SparkML Base image is designed for deploying and serving machine learning models built with Apache Spark MLlib, featuring pyspark for large-scale distributed data processing and machine learning.</p> Packages  | Package  | Version | |----------|---------| | pyspark  | 3.5.5   |"},{"location":"images/#practicus-ai-app-image-requirements","title":"Practicus AI App Image Requirements","text":""},{"location":"images/#image-practicus-app-hosting-base","title":"Image: Practicus App Hosting Base","text":"<p>The Practicus App Hosting Base image provides a versatile environment for deploying AI-powered applications, combining FastAPI and Uvicorn for high-performance APIs, Streamlit for interactive frontends, and LangChain with vector database clients for GenAI capabilities, along with broad support for databases, messaging, and caching.</p> Packages  | Package               | Version   | |-----------------------|-----------| | fastapi               | 0.116.1   | | uvicorn               | 0.34.3    | | aiobotocore           | 2.23.2    | | boto3                 | -         | | prometheus-client     | 0.22.1    | | aiofiles              | 24.1.0    | | aio-pika              | 9.5.5     | | jinja2                | -         | | httpx                 | -         | | requests              | -         | | openai                | -         | | streamlit             | 1.47.1    | | pymilvus              | 2.5.14    | | qdrant-client         | 1.14.3    | | langchain-community   | 0.3.27    | | langchain_openai      | 0.3.28    | | mcp                   | 1.12.3    | | psycopg2-binary       | 2.9.10    | | psycopg[binary]       | 3.2.9     | | asyncpg               | 0.30.0    | | sqlmodel              | 0.0.24    | | alembic               | 1.16.4    | | trino                 | 0.335.0   | | sqlglot[rs]           | -         | | beautifulsoup4        | -         | | websocket-client      | -         | | python-multipart      | -         | | redis[hiredis]        | -         | | confluent-kafka       | -         | | pymemcache            | -         | | neo4j                 | -         | | pika                  | -         |"},{"location":"images/#image-practicus-app-hosting-langflow-base","title":"Image: Practicus App Hosting Langflow Base","text":"<p>The Practicus App Hosting Langflow Base image is tailored for building and running visual, flow-based AI applications using Langflow, enabling easy integration and orchestration of LLMs and other AI tools within interactive workflows.</p> Packages  | Package   | Version | |-----------|---------| | langflow  | 1.4.3   |"},{"location":"join/","title":"Join","text":"<p>You can use Practicus AI to join data from very different data sources and even physical locations all around the world. This is not something most traditional databases or data warehouses can do. </p>"},{"location":"join/#how-does-it-work","title":"How does it work?","text":"<p>1) Load data normally. We will call this left worksheet </p> <p>2) (Optional) Make changes to the left worksheet as usual.</p> <p>3) Load data to join with. We will call this the right worksheet</p> <p>4) Join left worksheet to right using a single key column. If you need to use multiple columns to join, you can first concatenate them into a single column i.e. 123-abc-456 </p>"},{"location":"join/#changes-to-the-right-worksheet","title":"Changes to the right worksheet","text":"<p>If you need to make changes to the right worksheet right before the join, you need to export your data to some destination first. If you do not export, the original data will be used. </p> <p>A typical join with changes to the right worksheet could work like this: </p> <p>1) Load the left worksheet, i.e. from Redshift, make changes if you need to</p> <p>2) Load the right worksheet, i.e. from Snowflake</p> <p>3) Make your changes to the right worksheet as usual</p> <p>4) Export the right worksheet data, most likely to an intermediary destination i.e. to your data lake on S3</p> <p>5) Join left worksheet to the right, and the Cloud Worker will use the exported data to read it from the intermediary S3 location. The original source on Snowflake will not be used. </p> <p>If you skip the export step #4, the Cloud Worker would ignore your changes, and read original data directly from Snowflake. </p> <p>To learn more about why Practicus AI joins works like the above, please check the below</p> <p>Modern Data Pipelines </p> <p>section, where we explain atomicity, idempotency and big data scalability concepts of Practicus AI.</p>"},{"location":"join/#join-styles","title":"Join Styles","text":"<p>Practicus AI uses traditional join styles: </p> <p></p> <p>Note: Unless you have a very good reason to do so, please do not use Cartesian join since it will result in left worksheet x right worksheet number of rows. </p>"},{"location":"k8s-setup/","title":"Enterprise Cloud setup guide","text":""},{"location":"k8s-setup/#download-helper-files-recommended","title":"Download Helper files (Recommended)","text":"<p>Before you get started, please feel free to download the practicus_setup.zip file that includes sample configuration (.yaml) files for various cloud, on-prem, or local development environments, and helper scripts to accelerate your setup.</p>"},{"location":"k8s-setup/#overview","title":"Overview","text":"<p>This document will guide you to install Practicus AI Enterprise Cloud Kubernetes backend.</p> <p>There are multiple backend options and Kubernetes is one of them. Please view the detailed comparison table to learn more.</p> <p>Practicus AI Kubernetes backend have some mandatory and optional components.</p>"},{"location":"k8s-setup/#mandatory-components","title":"Mandatory components","text":"<ul> <li>Kubernetes cluster: Cloud (e.g. AWS), on-prem (e.g. OpenShift) or Local (e.g. Docker Desktop)  </li> <li>Kubernetes namespace: Usually named prt-ns</li> <li>Management database: Low traffic database that holds users, connections, and other management components. Can be inside or outside the Kubernetes cluster.</li> <li>Console Deployment: Management console and APIs that the Practicus AI App or SDK communicates with.</li> <li>Istio Service Mesh: Secures, routes and load balances network traffic. You can think of it as an open source and modern NGINX Plus.</li> </ul>"},{"location":"k8s-setup/#optional-components","title":"Optional components","text":"<ul> <li>Optional Services Deployment: Only required if you would like to enable OpenAI GPT and similar AI services.</li> <li>Additional Kubernetes clusters: You can use multiple clusters in different geos to build a flexible data mesh architecture.</li> <li>Additional namespaces: You can also deploy a test environment, usually named prt-ns2 or more.  </li> </ul>"},{"location":"k8s-setup/#dynamic-components","title":"Dynamic components","text":"<ul> <li>Cloud worker pods: These are ephemeral (temporary) pods created and disposed by the management console service. E.g. When a user wants to process large data, perform AutoML etc., the management console creates worker pod(s) on the fly, and disposes after the user is done. The dynamic capacity offered to end users is governed by system admins.</li> </ul>"},{"location":"k8s-setup/#big-picture","title":"Big Picture","text":""},{"location":"k8s-setup/#prerequisites","title":"Prerequisites","text":"<p>Before installing Practicus AI admin console, please make sure you complete the below prerequisite steps.</p> <p>Installation scripts assume you use macOS, Linux or WSL on Windows.  </p>"},{"location":"k8s-setup/#create-or-reuse-a-kubernetes-cluster","title":"Create or reuse a Kubernetes Cluster","text":"<p>Please create a new Kubernetes cluster if you do not already have one. For this document, we will use a new Kubernetes cluster inside Docker Desktop. Min 8GB RAM for Docker engine is recommended. Installation steps are similar for other Kubernetes environments, including cloud ones such as AWS EKS.</p> <p>View Docker Desktop memory settings</p> <p>View Docker Desktop Kubernetes setup guide</p>"},{"location":"k8s-setup/#install-kubectl","title":"Install kubectl","text":"<p>Install kubectl CLI tool to manage Kubernetes clusters</p> Install kubectl for macOS<pre><code>curl -LO \"https://dl.k8s.io/release/v1.27.7/bin/darwin/amd64/kubectl\"\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\nsudo chown root: /usr/local/bin/kubectl\n</code></pre>"},{"location":"k8s-setup/#install-helm","title":"Install Helm","text":"<p>Practicus AI installation is easiest using helm charts.</p> Install Helm<pre><code>curl -fsSL -o get_helm.sh \\\n  https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nrm get_helm.sh\n</code></pre>"},{"location":"k8s-setup/#verify-current-kubectl-context","title":"Verify current kubectl context","text":"<p>Make sure kubectl is pointing to the correct Kubernetes cluster</p> <pre><code>kubectl config current-context\n\n# Switch context to Docker Desktop if required \nkubectl config use-context docker-desktop\n</code></pre>"},{"location":"k8s-setup/#install-istio-service-mesh","title":"Install Istio service mesh","text":"<p>Practicus AI uses Istio to ingest, route traffic, secure and manage the modern data mesh microservices architecture.</p> <p>Istio can be installed on any Kubernetes cluster and designed to run side by side with any number of other production workloads. Istio will not interfere with a namespace unless you ask Istio to do so.</p> <p>Learn more about Istio</p> <p>Note: Some Kubernetes systems come with Istio 'forks' pre-installed, such as Red Hat OpenShift Service Mesh. Practicus AI is designed and tested to work with the original istio only. Istio is designed to be installed and run side-by-side with the forked projects, so you can safely install it on any Kubernetes cluster.  </p> <p>The below script downloads the latest istoctl version, e.g. 1.18.</p> <p>Please update the \"mv istio-... istio\" section below to a newer version if required.</p> Install Istio<pre><code>cd ~ || exit\n\necho \"Downloading Istio\"\nrm -rf istio\ncurl -L https://istio.io/downloadIstio | sh -\nmv istio-1.20.3 istio || \\\n  echo \"*** Istio version is wrong in this script. \\\n        Please update to the version you just downloaded to your home dir ***\"\ncd ~/istio || exit\nexport PATH=$PWD/bin:$PATH\n\necho \"Analyzing Kubernetes for Istio compatibility\"\nistioctl x precheck \n\necho \"Install istio to your kubernetes cluster\"\nistioctl install --set profile=default -y\n\necho \"Recommended: Add istioctl to path\"\n# Add the below line to .zshrc or alike\n# export PATH=~/istio/bin:$PATH\n</code></pre>"},{"location":"k8s-setup/#preparing-a-kubernetes-namespace","title":"Preparing a Kubernetes namespace","text":"<p>Practicus AI Kubernetes backend is designed to run in a namespace and side-by-side with other production workloads.</p> <p>We strongly suggest you use namespaces, even for testing purposes.</p>"},{"location":"k8s-setup/#create-namespace","title":"Create namespace","text":"<p>You can use multiple namespaces for Practicus AI ,and we will use the name convention: prt-ns (e.g. for production), prt-ns2 (for testing) etc.</p> <p>In this document we will only use one namespace, prt-ns.  </p> <pre><code>echo \"Creating a Kubernetes namespace\"\nkubectl create namespace prt-ns\n</code></pre>"},{"location":"k8s-setup/#add-practicusai-helm-repository","title":"Add practicusai helm repository","text":"<p>Practicus AI helm repository will make installing Practicus AI console backend easier.</p> Add practicusai helm repo<pre><code>helm repo add practicusai https://practicusai.github.io/helm\nhelm repo update\necho \"Viewing charts in practicusai repo\"\nhelm search repo practicusai\n</code></pre>"},{"location":"k8s-setup/#create-or-reuse-postgresql-database","title":"Create or reuse PostgreSQL Database","text":"<p>Practicus AI management console uses PostgreSQL database to store some configuration info such user credentials, database connections and more.</p> <p>This is a management database with low transaction requirements even for production purposes.</p>"},{"location":"k8s-setup/#creating-a-production-database","title":"Creating a production database","text":"<p>You can reuse an existing PostgreSQL Server or create a new one using one of the cloud vendors.</p> <p>Please make sure your kubernetes cluster will have access network access to the server.</p> <p>Once the PostgreSQL Server is ready, you can create a new database using a tool such as PgAdmin following the below steps:</p> <ul> <li>Login to PostgreSQL Server</li> <li>Crate a new database, E.g. console</li> <li>Create a new login, E.g. console_user and note its password</li> <li>Right-click the database (console) and go to properties &gt; Security &gt; Privileges &gt; hit + and add the login (e.g. console_user) as Grantee, \"All\" as Privileges.</li> <li>Expand the database items, go to Schemas &gt; public &gt; right click &gt; properties &gt; Security &gt; hit + and add the login (e.g. console_user) as Grantee, \"All\" as privileges.</li> </ul>"},{"location":"k8s-setup/#optional-creating-a-sample-test-database","title":"(Optional) Creating a sample test database","text":"<p>For testing or PoC purposes, you can create a sample PostgreSQL database in your Kubernetes cluster.</p> <p>Important: The sample database should not be used for production purposes.</p> <pre><code>echo \"Creating sample database\"\nhelm install practicus-sampledb practicusai/practicus-sampledb \\\n  --namespace prt-ns\n</code></pre> <p>In order to connect to this database inside the kubernetes cluster you can use the below address:</p> <ul> <li>prt-svc-sampledb.prt-ns.svc.cluster.local</li> </ul> <p>Please note that if you installed the sample database with the above defaults, the rest of the installation will already have the sample database address and credentials set as the default for easier testing.</p>"},{"location":"k8s-setup/#connecting-to-the-sample-database-from-your-laptop","title":"Connecting to the sample database from your laptop","text":"<p>If you ever need to connect the sample database from your laptop using a tool such as PgAdmin, you can open a temporary connection tunnel using kubectl.</p> <pre><code># Get sample db pod name \nSAMPLEDB_POD_NAME=$(kubectl -n prt-ns get pod -l \\\n  app=postgresdb -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Sample db pod name is: $SAMPLEDB_POD_NAME\"\n\necho \"Starting temporary connection tunnel\"\nkubectl -n prt-ns port-forward \"$SAMPLEDB_POD_NAME\" 5432:5432\n</code></pre>"},{"location":"k8s-setup/#deploying-management-console","title":"Deploying Management Console","text":""},{"location":"k8s-setup/#helm-chart-valuesyaml","title":"Helm chart values.yaml","text":"<p>Practicus AI helm chart's come with many default values that you can leave as-is, especially for local dev/test configurations.</p> <p>For all other settings, we suggest you to use values.yaml file</p> <pre><code>mkdir ~/practicus \nmkdir ~/practicus/helm\ncd ~/practicus/helm\ntouch values.yaml\n</code></pre> Sample values.yaml file contents for a local test environment<pre><code>migrate:\n  superUserEmail: \"your_email@your_company.com\"\n  superUserPassword: \"first super admin password\"\n\nenterpriseLicense:\n  email: \"your_email@your_company.com\"\n  key: \"__add_your_key_here__\"\n\ndatabase:\n  engine: POSTGRESQL\n  host: host.docker.internal\n  name: console\n  user: console\n  password: console\n\nadvanced:\n  debugMode: true\n  logLevel: DEBUG\n\nnotification:\n  api_auth_token: \"(optional) _your_email_notification_api_key_\"\n</code></pre> Sample values.yaml file contents for a production environment on AWS, GCE, Azure, OpenShift etc.<pre><code>main:\n  # Dns accessible by app\n  host: practicus.your_company.com\n  # try ssl: false to troubleshoot issues\n  ssl: true\n\nmigrate:\n  superUserEmail: \"your_email@your_company.com\"\n  superUserPassword: \"first super admin password\"\n\nenterpriseLicense:\n  email: \"your_email@your_company.com\"\n  key: \"__add_your_key_here__\"\n\ndatabase:\n  engine: POSTGRESQL\n  host: \"ip address or dns of db\"\n  name: \"db_name\"\n  user: \"db_user\"\n  password: \"db_password\"\n\njwt:\n  # API JWT token issuer, can be any value \n  issuer: iss.my_company.com\n\nnotification:\n  api_auth_token: \"(optional) _your_email_notification_api_key_\"\n</code></pre>"},{"location":"k8s-setup/#ingress-for-aws-eks","title":"Ingress for AWS EKS","text":"<p>This step is not required for a local test setup.</p> <p>For AWS, our helm charts automatically configure Application Load Balancer and SSL certificates.</p> <p>You can simply add the below to values.yaml file.</p> Ingress for AWS EKS<pre><code>aws:\n  albIngress: true\n  # AWS Certificate Manager (ACM) certificate ARN for your desired host address\n  certificateArn: \"arn:aws:acm:__aws_region__:__acct_id___:certificate/___cert_id___\"\n\nistio:\n  # In order to use ALB, Istio gateway host must be \"*\"\n  gatewayHosts:\n  - \"*\"\n</code></pre>"},{"location":"k8s-setup/#ingress-and-other-settings-for-various-kubernetes-systems","title":"Ingress and other settings for various Kubernetes systems","text":"<p>Please check the below documentation to configure Istio and Istio gateway depending on your Kubernetes infrastructure.</p> <ul> <li>Azure</li> <li>Google Cloud</li> <li>OpenShift</li> <li>Oracle Cloud</li> <li>IBM Cloud</li> <li>MicroK8s</li> </ul>"},{"location":"k8s-setup/#configuring-management-database","title":"Configuring management database","text":"<p>Since the management console will immediately try to connect to its database, it makes sense to prepare the database first.</p> <p>The below steps will create a temporary pod that will create or update the necessary tables and populate initial data.</p> <pre><code>cd ~/practicus/helm\n\n# Confirm you are using the correct Kubernetes environment\nkubectl config current-context\n\n# Step 1) Create a temporary pod that will create (or update) the database\n\nhelm install prt-migrate-console-db practicusai/practicus-migrate-console-db \\\n  --namespace prt-ns \\\n  --set advanced.imagePullPolicy=Always \\\n  --values ./values.yaml\n\n# Step 2) View the db migration pod status and logs. \n#   Run it multiple times if pulling the container takes some time.  \n\necho \"DB migration pod status\"\necho \"-----------------------\"\nkubectl get pod -n prt-ns | grep prt-pod-migrate-db\necho \"\"\necho \"Pod logs\"\necho \"--------\"\nkubectl logs --follow prt-pod-migrate-db -n prt-ns\n</code></pre> <p>Once the database migration is completed, you will see success log messages such as the below:</p> <pre><code>Running migrations:\n  Applying contenttypes.0001_initial... OK\n  Applying contenttypes.0002_remove_content_type_name... OK\n  Applying auth.0001_initial... OK\n  Applying auth.0002_alter_permission_name_max_length... OK\n  Applying auth.0003_alter_user_email_max_length... OK\n  Applying auth.0004_alter_user_username_opts... OK\n...\n</code></pre> <pre><code># Step 3) (Recommended) After you confirm that the tables are created,\n#   you can safely terminate the database migration pod using helm.\n#   If you do not, the pod will self-terminate after 10 minutes. \n\nhelm uninstall prt-migrate-console-db --namespace prt-ns \n</code></pre> <p>You can repeat the above 1-3 steps as many times as you need, and for each new version of the management console.</p> <p>If there are no updates to the database schema, the pod will not make any changes.</p>"},{"location":"k8s-setup/#installing-management-console","title":"Installing management console","text":"<p>Practicus AI management console will be the central place for several administrative tasks.</p> Install management console<pre><code>cd ~/practicus/helm\nhelm repo update \n\nhelm install practicus-console practicusai/practicus-console \\\n  --namespace prt-ns \\\n  --values values.yaml\n</code></pre>"},{"location":"k8s-setup/#logging-in-to-management-console","title":"Logging in to management console","text":"<p>You should be able to log in to Practicus AI management console using http://local.practicus.io/console/admin or https://practicus.your_company.com/console/admin</p> <p>Note: local.practicus.io DNS entry points to localhost ip address (127.0.0.1)</p> <p>Your super admin username / password was defined at the top of your values.yaml file. (superUserEmail, superUserPassword)</p>"},{"location":"k8s-setup/#troubleshooting","title":"Troubleshooting","text":"<pre><code># Find the pod name(s)\nkubectl -n prt-ns get pod | grep prt-depl-console-\n\n# View status\nkubectl -n prt-ns describe pod prt-depl-console-...\n\n# View logs\nkubectl -n prt-ns logs --follow prt-depl-console-...\n\n# Analyze using the interactive terminal\nkubectl -n prt-ns exec -it prt-depl-console-... -- /bin/bash  \n</code></pre>"},{"location":"k8s-setup/#upgrading-management-console-to-a-newer-version","title":"Upgrading management console to a newer version","text":"<pre><code>cd ~/practicus/helm\nhelm repo update\n\nhelm upgrade practicus-console practicusai/practicus-console \\\n  --namespace prt-ns \\\n  --values values.yaml\n</code></pre>"},{"location":"k8s-setup/#uninstalling-management-console","title":"Uninstalling management console","text":"<pre><code>helm uninstall practicus-console --namespace=prt-ns\n</code></pre>"},{"location":"k8s-setup/#recommended-start-a-new-cloud-worker-using-practicus-ai-app","title":"(Recommended) Start a new Cloud Worker using Practicus AI App","text":"<ul> <li>Open Practicus AI App</li> <li>Go to settings &gt; click login</li> <li>Enter service address e.g. for local test http://local.practicus.io or https://practicus.your_company.com</li> <li>You can use your super admin user / password</li> <li>Click on either Explore or Cloud at the top bar</li> <li>For Explore: You should see a new \"Practicus AI Service\" (can be renamed later)</li> <li>Click on \"New Worker\", select a size (if on your laptop, select 1 or 2 GB RAM)</li> <li>For Cloud: Select the newly added region (upper right)</li> <li>Click \"Start New\", select a size (if on your laptop, select 1 or 2 GB RAM)</li> </ul> <p>This will start pulling the Cloud Worker image on first use, which can take a while since the Cloud Worker image is ~ 9GB in size.</p> <p>During this time the app will show the Cloud Worker (pod) status as pending. Once the pull is completed the app will notify you. Go to Explore tab, click on \"Worker-x Files\" (x is the counter) and view the local disk content of the pod. This verifies everything is deployed correctly.</p>"},{"location":"k8s-setup/#troubleshooting-cloud-workers","title":"Troubleshooting Cloud Workers","text":"<pre><code># Find the pod name(s)\nkubectl -n prt-ns get pod | grep prt-pod-wn-\n\n# View status\nkubectl -n prt-ns describe pod prt-pod-wn-...\n\n# View logs\nkubectl -n prt-ns logs --follow prt-pod-wn-...\n\n# Analyze using the interactive terminal\nkubectl -n prt-ns exec -it prt-pod-wn-... -- /bin/bash  \n</code></pre> <p>Tip: You can view the status of Cloud Workers for any user and terminate them if needed using the management console. Simply open http://local.practicus.io/console/admin &gt; scroll to Cloud Worker Admin &gt; click on Cloud Worker Consumption Logs. You will see all the active and terminated Cloud Workers. If you click on a log, you will see the Kubernetes pod conditions.</p>"},{"location":"k8s-setup/#management-console-settings","title":"Management console settings","text":"<p>There are several settings on the management console that you can easily change using the admin console page.</p> <p>These changes are stored in the management database, so we strongly suggest you to regularly back up your database.</p> <ul> <li>Groups: We strongly suggest you to create groups before granting rights. E.g.: power users, data scientists, data engineers, citizen data scientists.</li> <li>Users: You can create users and give fine-grained access to admin console elements. Staff users can log in to admin console. Most users should not need this level access, and only use Practicus AI App.</li> <li>Central Configuration: Please view \"Cluster Definitions\" to change your service name and location. E.g. to \"Practicus AI Service\" located in \"Seattle\". When end users login using the App, this is the information they will see while exploring data sources. This information is cached for future use, so the earlier you change the better.</li> <li>Cloud Worker Admin: It is crucial you visit every page on this section and adjust Cloud Worker (pod) capacity settings. You can adjust which group/user should have access to what kind of capacity.  </li> <li>Connection Admin: Users can only use analytical database connections that they add to the system AND the connections you make visible to certain groups / users.</li> <li>SaaS Admin: This section is only used if you activate self-service SaaS through a 3rd party payment gateway. We suggest only the super admin has access to it, and you make this section invisible to all other admin or staff users.</li> </ul>"},{"location":"k8s-setup/#advanced-settings","title":"Advanced settings","text":"<p>Practicus AI helm charts values.yaml files include many advanced settings and explanations as inline comments. Please navigate and alter these settings, upgrade your deployments and validate the state as you see fit.</p> <p>Please see below a sample values.yaml file where you can adjust replica count of a deployment:</p> <pre><code>...\ncapacity:\n  # Console API service replica\n  replicas: 1\n...\n</code></pre>"},{"location":"k8s-setup/#recommended-install-persistent-volume-support","title":"(Recommended) Install persistent volume support","text":"<p>Practicus AI Cloud Workers support 2 types of persistent volumes, personal and shared between users.</p>"},{"location":"k8s-setup/#personal-drives","title":"Personal drives","text":"<p>If enabled, every Cloud Worker gets a drive mounted under ~/my. With the personal drive, a user can persist their files under ~/my folder, so the files are not lost after a Cloud Worker is terminated. This can have some benefits, e.g. persisting jupyter notebooks on Cloud Workers.</p> <p>By default, the personal drive is shared between Cloud Workers using Kubernetes ReadWriteMany (RWX) mode.</p> <p>Please be aware that if you enable personal drives and force ReadWriteOnce (RWO), a user can only use one Cloud Worker at a time and this is not recommended.</p>"},{"location":"k8s-setup/#shared-drives-between-users","title":"Shared drives between users","text":"<p>If enabled, every Cloud Worker gets the shared folder(s) mounted in user home dir e.g. ~/shared/folder/..  </p> <p>You can control which group or individual user has access to which shared folder and the share name.</p>"},{"location":"k8s-setup/#kubernetes-storageclass","title":"Kubernetes StorageClass","text":"<p>In order to dynamically create persistent volumes and to avoid administrative burden, Practicus AI uses Kubernetes storage classes. Please prefer StorageClass read / write many mode.</p> <p>Please be aware that only some Kubernetes storage types, such as NFS, support read / write many mode. Common storage classes such as AWS EBS only allow one Kubernetes pod to mount a drive at a time (read / write once), which is not suitable for sharing use cases.</p> <p>If your storage class does not allow read / write many, you cannot implement shared drives between users. And for personal drives, implementing read / write once will cause users to be able to use one Cloud Worker at a time, which is not recommended.</p> <p>To summarize, please prefer to use NFS or similar style storage systems for Practicus AI persistent volumes.</p>"},{"location":"k8s-setup/#supported-nfs-implementations","title":"Supported NFS implementations","text":"<p>You can use any NFS system, inside or outside your Kubernetes cluster. You can also use NFS as a service, such as AWS EFS with Practicus AI.</p>"},{"location":"k8s-setup/#sample-nfs-server-for-your-local-computer","title":"Sample NFS server for your local computer","text":"<p>Please find below a simple implementation to install a NFS pod on your computer to test it with Practicus AI.</p> Sample NFS server configuration<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: nfs-service\nspec:\n  selector:\n    role: nfs\n  ports:\n    # Open the ports required by the NFS server\n    # Port 2049 for TCP\n    - name: tcp-2049\n      port: 2049\n      protocol: TCP\n    # Port 111 for UDP\n    - name: udp-111\n      port: 111\n      protocol: UDP\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: nfs-server-pod\n  labels:\n    role: nfs\nspec:\n  containers:\n    - name: nfs-server-container\n      image: cpuguy83/nfs-server\n      securityContext:\n        privileged: true\n      args:\n        - /exports\n</code></pre> <p>You can save the above to nfs-server.yaml and run</p> <pre><code>kubectl apply -f nfs-server.yaml\n\n# To delete \nkubectl delete -f nfs-server.yaml\n</code></pre> <p>After you create the NFS pod named nfs-server-pod, please run the below to get its IP address, e.g. 10.0.0.1. you will need this IP address in the below section.</p> <pre><code>kubectl get pod -o wide\n</code></pre> <p>Please note that after you restart your computer, the NFS server IP address might change, and in this case you would have to re-install (or upgrade) the below helm chart to update the IP address.  </p>"},{"location":"k8s-setup/#using-nfs-inside-kubernetes","title":"Using NFS inside Kubernetes","text":"<p>The below will create a provisioner pod and a storage class named prt-sc-primary. You can create as many provisioners and storage classes. These can point to the same or different NFS systems.</p> <pre><code>helm repo add nfs-subdir-external-provisioner \\\n  https://Kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\nhelm repo update\n\nexport NFS_DNS_NAME=\"add NFS server DNS or IP address here\"\n\nhelm install nfs-subdir-external-provisioner \\\n  nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n  --set nfs.server=\"$NFS_DNS_NAME\" \\\n  --set nfs.path=\"/\" \\\n  --set storageClass.accessModes=\"ReadWriteMany\" \\\n  --set storageClass.pathPattern=\"practicus\" \\\n  --set storageClass.onDelete=\"retain\" \\\n  --set storageClass.name=\"prt-sc-primary\"\n\n# To uninstall\nhelm uninstall nfs-subdir-external-provisioner\n</code></pre> <p>To learn more and customize the helm chart, please visit provisioner GitHub page.</p> <p>By using the above helm chart and granting user access, NFS server will have directories such as /practicus/users/john-acme.com that gets mounted to ~/my for a user with email john@acme.com. Only John will have access to this folder.</p> <p>You can also define several shared folders between users E.g. shared/finance which would map to /practicus/shared/finance on the NFS server and gets mounted to ~/shared/finance on Cloud Workers.  </p> <p>The above path structure is an example and be customized flexibly through the NFS system itself, StorageClass provisioner setup, or simply by using the Practicus AI Management Console.</p> <p>Please view Cloud Worker Admin section to customize user or group based persistent drives.</p>"},{"location":"k8s-setup/#using-aws-efs","title":"Using AWS EFS","text":"<p>If you are using AWS EFS, you can use the above provisioner, or as an alternative, you can also use a CSI specific for AWS EKS and AWS EFS. Please view the AWS EFS CSI documentation to learn more. Tip: Even if you decide to use the generic NFS provisioner with AWS EKS, you can still review the CSI page to learn more about security group settings, availability zone optimization etc.</p>"},{"location":"k8s-setup/#optional-creating-sample-object-storage","title":"(Optional) Creating sample object storage","text":"<p>Although it is optional, using object storage systems such as Amazon S3 or compatible for Machine Learning is very common. If you are testing Practicus AI and do not have access to S3 or a compatible store, you can simply use MinIO.</p>"},{"location":"k8s-setup/#sample-object-storage-with-minio","title":"Sample Object Storage with MinIO","text":"<p>You can install MinIO inside your Kubernetes cluster. For demo purposes, we will use a simple Docker container. We will also avoid using the default MinIO S3 port 9000, in case you are also using Practicus AI standalone docker deployment (not K8s). This type of test deployment already uses port 9000.</p> <pre><code>echo \"Creating sample object storage\"\nhelm install practicus-sampleobj practicusai/practicus-sampleobj \\\n  --namespace prt-ns \n</code></pre> <p>After the minio object storage is created, you can connect to minio management console to create buckets, access credentials etc. For these, you need to create a temporary connection tunnel to minio service.</p> <pre><code>SAMPLEOBJ_POD_NAME=$(kubectl -n prt-ns get pod -l \\\n  app=minio -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Sample object store pod name is: $SAMPLEOBJ_POD_NAME\"\n\necho \"Starting temporary connection tunnel\"\nkubectl -n prt-ns port-forward \"$SAMPLEOBJ_POD_NAME\" 9090:9090\n</code></pre> <ul> <li>Login to MinIO Console using http://127.0.0.1:9090 and credentials:</li> <li>User: minioadmin password: minioadmin</li> <li>Click Buckets &gt; Create bucket and create testbucket</li> <li>Click Identity &gt; Users &gt; Create User &gt; select readwrite policy</li> <li>Click Access Keys &gt; Create &gt; Note your access and secret keys</li> <li>Click Object Browser &gt; testbucket &gt; upload a .csv file</li> </ul> <p>You should now see a .csv file in testbucket and created a user, access/secret keys.</p> <p></p> <p>View MinIO installation document</p> <p>To test MinIO or other S3 compatible storage with the Practicus AI app:</p> <ul> <li>Open App &gt; Explore tab &gt; Click on New Connection &gt; Amazon S3</li> <li>Enter your access / secret keys</li> <li>Enter sample object storage endpoint url http://prt-svc-sampleobj.prt-ns.svc.cluster.local</li> <li>Select the bucket you created: testbucket</li> </ul> <p>You can now connect to this object storage to upload/download objects using the Practicus AI app. You will not need to create a connection tunnel to the minio management console to test with the app.</p>"},{"location":"k8s-setup/#optional-install-kubernetes-dashboard","title":"(Optional) Install Kubernetes dashboard","text":"<p>You can visualize and troubleshoot Kubernetes clusters using the dashboard. Please follow the below steps to install and view Kubernetes dashboard on your local development environment. Production setup and viewing will require some additional changes, which are beyond the scope of this document.  </p> <pre><code>echo \"Installing Kubernetes Dashboard\"\nkubectl apply -f \\\n  https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n\necho \"Setting dashboard permissions\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: dashboard-admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dashboard-admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: dashboard-admin-user\n  namespace: kubernetes-dashboard\nEOF\n</code></pre> <p>After the dashboard is installed you can open it using the below commands.</p> <p>Please do not forget to run kubectl proxy in a separate terminal window first, so the web interface is accessible from your browser.</p> <pre><code>echo \"Generating a dashboard access token for 90 days and copying to clipboard\"\nkubectl -n kubernetes-dashboard create token dashboard-admin-user \\\n  --duration=2160h &gt; dashboard-token.txt\n\necho \"Generated token:\"\necho \"\"\ncat dashboard-token.txt\necho \"\"\necho \"\"\npbcopy &lt; dashboard-token.txt\nrm dashboard-token.txt\n\necho \"Opening dashboard at:\"\necho \"http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\"\n\necho \"\"\nopen \"http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\"\necho \"Paste the access token at login page.\"\n\necho \"No login page? Make sure you ran kubectl proxy first\"\n</code></pre> <p></p> <p>Please note that the above steps installed Practicus AI elements to prt-ns namespace. You will have to switch the namespace in the dashboard</p>"},{"location":"k8s-setup/#optional-using-a-private-image-registry","title":"(Optional) Using a private image registry","text":"<p>All Practicus AI container images are customizable, and you can use a private image registry to host your custom images. In this case, your Kubernetes cluster needs to be able to access the private registry. The below is a step-by-step example for AWS ECR private registry.</p> <ul> <li>Create a practicus-private-test private registry repository</li> </ul> <p></p> <ul> <li>Create your Dockerfile.</li> </ul> <p>Important: We strongly recommend you to use virtual environments for your custom python packages and avoid updating global packages.  </p> <pre><code>FROM ghcr.io/practicusai/practicus:24.1.0\n\nRUN echo \"this is a private repo\" &gt; /home/ubuntu/private.txt\n\nRUN echo \"**** Creating Virtual Env ****\" &amp;&amp; \\\n    python3 -m venv /home/ubuntu/.venv/practicus_test --system-site-packages &amp;&amp; \\\n    echo \"**** Installing packages ****\" &amp;&amp; \\\n    /home/ubuntu/.venv/practicus_test/bin/python3 -m pip install some-package &amp;&amp; \\\n    echo \"**** Installing Jupyter Kernel ****\" &amp;&amp; \\\n    python3 -m ipykernel install --user --name practicus_test --display-name \"My virtual environment\"\n</code></pre> <ul> <li>Build and push the image to private repository</li> </ul> <pre><code>aws ecr get-login-password --region us-east-1 | docker login \\\n  --username AWS --password-stdin _your_account_id_.dkr.ecr.us-east-1.amazonaws.com\n\ndocker buildx build -t practicus-private-test:24.1.0 --push .\n\ndocker tag practicus-private-test:24.1.0 \\\n  _your_account_id_.dkr.ecr.us-east-1.amazonaws.com/practicus-private-test:...\n</code></pre> <p>After this step, you should see the image in the AWS ECR console.</p> <ul> <li>Create an access token for the repository, and add as a Kubernetes secret</li> </ul> <pre><code>TOKEN=`aws ecr get-login-password --region us-east-1 | cut -d' ' -f6`\nNAMESPACE=prt-ns\n\nkubectl create secret docker-registry practicus-private-test-secret \\\n  --docker-server=_your_account_id_.dkr.ecr.us-east-1.amazonaws.com/practicus-private-test \\\n  --docker-username=AWS \\\n  --docker-password=$TOKEN \\\n  -n $NAMESPACE\n</code></pre> <p>Note: Some private registry tokens have short lifespans. E.g. AWS ECR default is 12 hours.</p> <ul> <li>Open Practicus AI management console, create a new custom image and add the private registry secret.</li> </ul> <p></p> <ul> <li>The end users that you gave permission to the custom image will be able to launch workers from it.</li> </ul> <p></p> <p>Note: You can also define custom images for other workloads such as model hosting, workspaces, etc.</p>"},{"location":"k8s-setup/#optional-forwarding-dns-for-local-installations","title":"(Optional) Forwarding DNS for local installations","text":"<p>If you are deploying Practicus AI using a local DNS such as local.practicus.io (points to 127.0.0.1) traffic inside the Kubernetes cluster might work since each pod would search for the service locally. This might not be a problem for traffic outside Kubernetes since the local cluster such as Docker Desktop would be listening on 127.0.0.1</p> <p>To solve this problem, we can edit Kubernetes coreDNS setup, so that a local DNS queries would forward to the Istio load balancer.</p> <p>Tip: You can use Kubernetes dashboard UI for the below steps 1-3.</p> <p>1) Locate Istio ingress gateway cluster ip address. E.g. 10.106.28.249 by running the below.</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <p>2) Edit coredns configmap to add the ip address.</p> <pre><code>kubectl edit configmap coredns -n kube-system\n</code></pre> <p>Sample coredns configmap forwarding local.practicus.io to an istio ingress gateway ip address</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: coredns\n... \ndata:\n  Corefile: |\n    .:53 {\n        ...\n    }\n\n    local.practicus.io {\n      hosts {\n        10.106.28.249 local.practicus.io\n      }\n    }\n</code></pre> <p>3) Restart coredns for new changes to be active.</p> <pre><code>kubectl rollout restart deployment/coredns -n kube-system\n</code></pre> <p>4) Test. Start a Practicus AI worker, open a jupyter notebook and run the below code. You should get the istio gateway id address.</p> <pre><code>import socket\nprint(socket.gethostbyname(\"local.practicus.io\"))\n</code></pre>"},{"location":"k8s-setup/#troubleshooting-issues","title":"Troubleshooting issues","text":"<p>Please follow the below steps to troubleshoot some common issues with Kubernetes Dashboard, or equivalent kubectl commands if you do not use the dashboard.</p> <ul> <li>Did the prt-depl-console-... pod start? (Green) If not, view its details.</li> <li>If the pod started, but is not accessible using http://local.practicus.io/console/admin view the pod logs. Click on the pod name &gt; View Logs (upper right)</li> <li>If the logs do not show any errors, Istio sidecar proxy might not be running. Click on the pod name, scroll down to containers and verify there are 2 containers running, prt-cnt-console and istio-proxy.</li> <li>Analyze istio to see if there are any proxy issues detected istioctl analyze -n prt-ns</li> </ul>"},{"location":"k8s-setup/#no-enterprise-key","title":"No enterprise key?","text":"<p>This step is mandatory if you do not have your Enterprise license key.</p> <p>By installing Practicus AI app you will be able to test connectivity to your newly created Kubernetes cluster. You will also have access to your Enterprise license key.</p> <ul> <li>Install the app</li> <li>Go to settings &gt; container section</li> <li>Enter your email to activate your enterprise license</li> <li>View Practicus AI local docker setup guide if you need any help</li> </ul> <p>Once your enterprise license is activated, please open ~/.practicus/core.conf file on your computer, locate the license section, and copy license_key info.</p> <p>Sample license_key inside ~/.practicus/core.conf :</p> <pre><code>[license]\nemail = your_email@your_company.com\nlicense_key = abc12345-1234-1234-12ab-123456789012\nvalid_until = Wed, 01 Jan 2022 00:00:00 GMT\n</code></pre>"},{"location":"k8s-setup/#no-connectivity","title":"No Connectivity?","text":"<p>Most connectivity issues will be a result of mis-configured Istio. To solve these issues we recommend you to create test namespace e.g. istio-test and install the sample Istio app using the below help page.</p> <p>https://istio.io/latest/docs/setup/getting-started/</p> <p>Once you pass this step, deploying Practicus AI will be very similar.</p>"},{"location":"k8s-setup/#support","title":"Support","text":"<p>Need help? Please visit https://helpdesk.practicus.io/ to open a support ticket.</p> <p>Thank you!</p>"},{"location":"logging/","title":"Logging","text":"<p>You can log regular events or audit logs to additional log systems such as CloudWatch, Splunk, Sumo Logic etc. </p> <p>All you need to do is add the necessary handlers to core.conf file on your local computer (rare)  or on Cloud Worker (common). </p>"},{"location":"logging/#cloudwatch","title":"CloudWatch","text":"<p>For CloudWatch, watchtower Python library is already installed on Cloud Worker.  You can update core.conf like the below to start logging.  </p> <pre><code># 1- add CloudWatch handler key\n[handlers]\nkeys = ... ,  cwHandler\n\n# 2- Define handler, select level i.e. DEBUG, INFO and other parameters such as log format\n[handler_cwHandler]\nclass = watchtower.CloudWatchLogHandler\nlevel = DEBUG\nformatter = simpleFormatter\n\n# 3- Add the new handler to any logger  \n[logger_practicus]\nhandlers = ... , cwHandler\n</code></pre> <p>For more information you can check watchtower documentation </p>"},{"location":"logging/#other-log-systems","title":"Other log systems","text":"<p>For log systems other than CloudWatch, you need to install the necessary python logger libraries first. And then you can add the log handler the same way as to CloudWatch</p>"},{"location":"mlops/","title":"Introduction to MLOps","text":"<p>This section requires a Practicus AI Cloud Worker, S3 compatible object storage, and a Kubernetes Cluster. Please visit the introduction to Cloud Workers section of our tutorial to learn more.</p>"},{"location":"mlops/#what-is-mlops","title":"What is MLOps","text":"<ul> <li>MLOps, short for Machine Learning Operations, is a set of practices that automates the machine learning lifecycle from development to deployment to monitoring and maintenance. It bridges the gap between ML engineers and DevOps teams to create a culture of continuous improvement for ML products.</li> </ul>"},{"location":"mlops/#why-is-mlops-important","title":"Why is MLOps Important?","text":"<ul> <li>MLOps is a valuable tool for organizations that want to get more value from their ML investments. By automating and managing the ML lifecycle, MLOps can help organizations to deploy ML models to production faster, improve the quality and reliability of ML models, reduce the risk of ML failures, and increase the ROI of ML investments.</li> </ul>"},{"location":"mlops/#what-is-practicus-ais-mlops-approach","title":"What is Practicus AI's MLOps approach?","text":"<p>Practicus AI MLOps offer a way to deploy and manage AI models effectively. It does this by providing a unified user experience, open-source Cloud Native technology, different deployment methods, dynamic service mesh, fine-grained access control, global APIs, and the ability to modernize legacy systems.</p> <ol> <li>Unified user experience and federated governance: Practicus AI provides a single user interface for managing and consuming AI models, even if they are deployed in multiple locations using different cloud providers and data sources. This makes it easy for business users and developers to interact with your AI models, regardless of their technical expertise.</li> <li>Open-source Cloud Native technology: Practicus AI is built using open-source Cloud Native technology, so you can avoid vendor lock-in. To learn more about cloud native please visit Cloud Native Computing Foundation website</li> <li>Different deployment methods: Practicus AI offers a variety of deployment methods, so you can choose the one that best suits your needs. AutoML makes it easy to build and deploy models without writing any code. Jupyter Notebook allows you to experiment with models and deploy them to production with just a few clicks. And custom code gives you complete control over the deployment process.</li> <li>Dynamic service mesh: Practicus AI uses Kubernetes deployments and Istio technology to create a dynamic service mesh for your AI models. This makes it easy to scale your models up or down as needed, and to manage multiple model versions simultaneously.</li> <li>Fine-grained access control: Practicus AI provides fine-grained access control tokens that allow you to control who can deploy, consume, and manage your AI models. This helps you to protect your models from unauthorized access.</li> <li>Global APIs: Practicus AI allows you to enable global APIs that allow developers to use a single URL to automatically use the closest cloud region, edge location or on-premise deployment. This makes it easy to deploy and consume your AI models globally, with high availability and low latency.</li> <li>Modernize legacy or proprietary models: Practicus AI can be used to modernize legacy or proprietary AI systems by wrapping them with Practicus AI Open MLOps. This allows you to get the benefits of Practicus AI MLOps without having to make changes to your existing code.</li> </ol>"},{"location":"mlops/#deploying-ai-models-using-the-app","title":"Deploying AI Models using the app","text":"<p>There are multiple ways to deploy AI models. In this section we will learn how to deploy the models we create with Practicus AI AutoML. You can also use the SDK or admin web UI to deploy models.</p> <ul> <li>Open Practicus AI App.</li> <li>Click Start Exploring and see this screen.</li> </ul> <p></p> <ul> <li>In this step, tap on the dataset to load the dataset to build the model.</li> <li>Preview your dataset and make optional advanced adjustments, and Load dataset.</li> <li>Click on the model and choose the type of model according to your problem.(E.g. Classification, Clustering, Anomaly Detection...)</li> <li>Select Objective Columns and adjust Advanced settings.</li> </ul> <p></p> <ul> <li>Click OK.</li> <li>Wait for model setup and see the model dialog.</li> </ul> <p></p> <ul> <li>When you see the Model dialog, select Deploy Model (API)</li> </ul> <p></p> <ul> <li>Select the appropriate one from the Prefixes you created in the Admin Console. (We will examine creating them in the admin console tab)</li> <li>Click Select model prefix or model. (Hint: If you register your model on an existing model, it will register as version 2)</li> <li>See your model and specifications under this prefix.</li> </ul> <p></p> <ul> <li>Back to Dataset.</li> <li>Click predict and select the model you deploy.</li> <li>Click OK.</li> <li>You can see the new Predict columns created.</li> </ul> <p>Now that we have learned how to deploy the model we created, let's learn how to manage these operations from the Practicus AI Admin Console.</p>"},{"location":"mlops/#practicus-ai-admin-console-for-ai-model-hosting-administration","title":"Practicus AI Admin Console for AI Model Hosting Administration","text":"<ul> <li>Open Practicus AI Admin Console.</li> </ul>"},{"location":"mlops/#model-deployments","title":"Model Deployments","text":"<ul> <li>Open Practicus AI Admin Console and click AI Model Hosting.</li> </ul> <ul> <li>Click Model Deployments. (Hint: The reason why it is called Model Deployment is that the models you host are created as Deployment in Kubernetes. This has several advantages, and you can do all operations from the Admin Console without updating the YAML file.)</li> </ul> <ul> <li>In the area where you open Model Deployments, you can make configurations related to the model, set the resource to assign to the models and assign auto scaler.</li> </ul> <ul> <li>At the same time, there may be containers that you have prepared with extra packages in this area, you can assign them, and you can add extra libraries to these containers with pip install by writing a startup script.</li> </ul> <ul> <li> <p>You can assign these Deployments on a group and user basis so that you can easily manage the models. You can also authorize these users and groups to run custom code.</p> </li> <li> <p>After completing and saving the necessary operations, you can open Deployments from Kubernetes Dashboard and see the Deployment you created.</p> </li> </ul> <p></p>"},{"location":"mlops/#model-prefixes","title":"Model Prefixes","text":"<ul> <li>Click Model Prefixes.</li> </ul> <p>Prefixes also represent URLs. You can set OpenAPI documentation settings in this area. You can also upload your pre-built models to Practicus AI and manage production settings and super secrets from there.</p> <p></p> <ul> <li>You can set access authorizations on group and user basis to the prefixes you create from this area. You can also set who can run custom code in this prefix.</li> <li>You can also assign a token to this prefix and access the models with this token.</li> </ul> <p></p>"},{"location":"mlops/#models","title":"Models","text":"<ul> <li>The models you create may not work as they used to over time. You can create new models and save these models as new versions and assign these versions as staging and production.</li> <li>At the same time, you can easily switch between the models you have created and saved as staging and/or production.</li> <li>With Practicus AI's dynamic service mesh approach, you can easily route traffic between models. Let's take a look at how this works.</li> </ul> <ul> <li>Click Models.</li> <li>Select Owner.</li> <li>Select Model Prefix.</li> <li>Select Name for model.</li> </ul> <ul> <li>Perform Deployment assignment of models.</li> <li>You can mark Stage as Staging and/or Production.</li> <li>Perform traffic routing between these versions with Traffic Volume Weight%.</li> <li>If you select Cascade prefix access tokens, you can access this model with the access token you defined for the prefix.</li> </ul>"},{"location":"mlops/#model-versions","title":"Model Versions","text":"<ul> <li>Select Model Versions.</li> <li>Click Add Model Version.</li> <li>Choose your existing model.</li> <li>Enter Model Version.</li> <li>Assign Model Deployment.</li> <li>Set Stage status .</li> <li>Assign Traffic Volume Weight%.</li> <li>You can choose the model as a draft if you want. If you choose, this model will not be marked as latest.</li> <li>Click Save.</li> </ul>"},{"location":"mlops/#external-api-access-tokens","title":"External API Access Tokens","text":"<ul> <li>Click External API Access Tokens.</li> <li>Select Add External API Access Token.</li> <li>Select Owner.</li> <li>Set Expiry date. (Hint: For advanced use cases. Python style dictionary to add extra key value pairs to the JWT token.)</li> <li>Select Global id. (Hint: For advanced use cases.  To build highly available and accessible APIs distributed in different geos, you can define tokens with the same global id and use them across the globe. E.g. a mobile app can access a global API 'api.company.com', and the DNS can  route the traffic to 'us.api.company.com' or 'eu.api.company.com' depending on user location OR service availability.)</li> <li>Select Model Prefix External API Access Tokens or Add another Model Prefix External API Access Token.</li> <li>Select Model External API Access Tokens or Add another Model External API Access Token.</li> <li>Click Save.</li> </ul> <p>We have completed the MLOps operations that can be done from the Practicus AI Admin Console.</p> <p>Now you can access our MLOps video from the link below to try these operations and digest the information you have gained:</p> <p>Practicus AI Open MLOps</p>"},{"location":"mlops/#optional-model-documentation","title":"Optional: Model Documentation","text":"<ul> <li>Business users can easily explore individual systems with the interface and access data sources. Technical users can easily access the documentation of the models with Swagger, OpeAPI or Redoc. </li> </ul>"},{"location":"model-api/","title":"For developers","text":"<p>Welcome to Practicus AI Model API documentation. You can use the Practicus AI App or the SDK to build, distribute and deploy AI/ML models and then consume these models with the app or any other REST API compatible system.</p>"},{"location":"model-api/#model-prefixes","title":"Model prefixes","text":"<p>Practicus AI models are logically grouped with the model prefix concept using the https:// [some.address.com] / [some/model/prefix] / [model-name] / [optional-version] / format. E.g. https://practicus.company.com/models/marketing/churn/v6/ can be a model API address where models/marketing is the prefix, churn is the model, and v6 is the optional version.</p>"},{"location":"model-api/#model-documentation","title":"Model Documentation","text":"<p>Models under a prefix can be viewed by using Practicus AI App. If your admin enabled them, you can also view the documentation by visiting ../prefix/redoc/ or ../prefix/docs/ or E.g. https://practicus.company.com/models/marketing/redoc/</p>"},{"location":"model-api/#model-deployment-concept","title":"Model Deployment concept","text":"<p>Models are physically deployed to Kubernetes deployment(s). These have several characteristics including capacity, auto-scaling, additional security etc.</p>"},{"location":"model-api/#authentication","title":"Authentication","text":"<p>Consuming models inside the Practicus AI app does not require additional authentication. For external use, you will need an access token. An admin can create tokens at the model prefix or individual model level.</p>"},{"location":"model-api/#submitting-data-in-batches","title":"Submitting data in batches","text":"<p>Practicus AI app will automatically split large data into mini-batches to improve performance and avoid memory issues.  You are encouraged to do the same and submit data in a volume compatible with your model deployment capacity. E.g. If your model deployment pods have 1 GB RAM, it is advisable to submit data in 250MB or less batch size.    </p>"},{"location":"model-api/#compression","title":"Compression","text":"<p>Practicus AI app automatically compresses requests and responses for the model API. You can also compress using 'lz4', 'zlib', 'deflate', 'gzip' compression algorithms. All you need to do is to compress, and send the algorithm using 'content-encoding' http header, or by simply naming the attached file with the compression extension. E.g. my_data.lz4</p>"},{"location":"model-api/#model-metadata","title":"Model Metadata","text":"<p>You can learn more about any AI model by simply requesting it's meta data using ?get_meta=true query string. E.g. https://practicus.company.com/models/marketing/churn?get_meta=true</p>"},{"location":"model-api/#sample-python-code","title":"Sample Python code","text":"<pre><code>import requests\nheaders = {\n    'authorization': 'Bearer _your_access_token_',\n    'content-type': 'text/csv'\n}\nr = requests.post('https://practicus.company.com/models/marketing/churn/', headers=headers, data=some_csv_data)\nprint('Prediction result: ', r.text)\n</code></pre>"},{"location":"model-api/#getting-session-and-access-api-tokens","title":"Getting session and access API tokens","text":"<p>Please prefer a 'short-lived' session API access token where you can get one programmatically like the below example. If you do not use Practicus AI SDK, you can request a 'long-lived' API access token from a Practicus AI admin as well.  Please note that session tokens will offer increased security due to their short lifetime.  </p> <pre><code>import practicuscore as prt \ntoken = prt.models.get_session_token('https://practicus.company.com/models/marketing/churn/')\n</code></pre>"},{"location":"model-api/#compression-code","title":"Compression code","text":"<p>With or without streaming, you can submit the data after compressing with lz4', 'zlib', 'deflate', 'gzip' algorithms. If you compress the request, the response will also arrive using the same compression algorithm. </p> <pre><code>import lz4 \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv',\n    'content-encoding': 'lz4'\n}\ndata_csv = df.to_csv(index=False)\ncompressed = lz4.frame.compress(data_csv.encode())\nprint(f\"DataFrame compressed from {len(data_csv)} bytes to {len(compressed)} bytes\")\n\nr = requests.post(api_url, headers=headers, data=compressed)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\ndecompressed = lz4.frame.decompress(r.content)\nprint(f\"Result de-compressed from {len(r.content)} bytes to {len(decompressed)} bytes\")\n\npred_df = pd.read_csv(BytesIO(decompressed))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"model/","title":"Model","text":"<p>Build AI models on past data with one-click using Automated Machine Learning (AutoML) and then make predictions  on unseen data. If you are a data scientist, or working with one, export to Jupyter code and share your experimentation  details in a central database (MLflow). Your team will save significant amount of time by avoiding to start coding from  scratch and by collaborating better</p>"},{"location":"model/#create-a-model","title":"Create a Model","text":"<p>You need to have a Cloud Worker ready for the model creation process and start the model creation process with our data on the Cloud Worker.</p> <p></p> <p>We create a model of regression type by selecting the target column revenue using the ice cream data. You can also limit the model with the advanced options section and select build excel for model.</p> <p></p> <p>When the model creation process is finished, some options appear.</p> <p></p> <p>You can load the model in Excel, register it in MLflow, see the details of the experiment with MLflow, you can see the code with Jupyter Notebook.</p> <p></p> <p></p>"},{"location":"modern-data-pipelines/","title":"Modern Data Pipelines","text":"<p>Modern data pipelines often need to implement 2 key concepts: Atomicity and Idempotency to provide consistent and reliable solutions. In this section we will explain what these concepts are and how they are implemented in Practicus AI.    </p>"},{"location":"modern-data-pipelines/#atomicity","title":"Atomicity","text":"<p>Traditional ETL pipelines often processed data in small batches in order to fit in the limited memory of the available hardware. This sometimes created major issues and race conditions, such as a data pipeline running half-way through and failing, leaving the final destination having only half of the data.</p> <p>Sample Non-Atomic Pipeline: </p> <p>Sample Atomic Pipeline:</p> <p></p>"},{"location":"modern-data-pipelines/#idempotency","title":"Idempotency","text":"<p>Idempotence is the concept of allowing a task to run multiple times, producing the same result. For instance, you could press the on button of an idempotent machine multiple times, and it would not change the result.</p> <p>In the below pipeline, if a task had to run multiple times (often due to a failure) and gives the same result, we can say it implements idempotency principle. This wasn't the case in many traditional ETL pipelines.  </p> <p></p>"},{"location":"modern-data-pipelines/#practicus-ai-approach","title":"Practicus AI approach","text":"<p>The default Practicus AI data processing approach is to break down any data pipeline into atomic and idempotent tasks that include 4 key activities.</p>"},{"location":"modern-data-pipelines/#default-anatomy-of-a-practicus-ai-data-pipeline-task","title":"Default anatomy of a Practicus AI data pipeline task","text":"<p>1) Read data from any data source (mandatory, do it once)</p> <p>2) Apply data transformation steps (optional, can be done many times)</p> <p>3) Join to any data by reading it directly from its data source (optional, can do many times)</p> <p>4) Save final data to any destination (mandatory, do it once)</p>"},{"location":"modern-data-pipelines/#default-anatomy-of-a-practicus-ai-data-pipeline","title":"Default anatomy of a Practicus AI data pipeline","text":"<p>Practicus AI creates a modern DAG (directed acyclic graph) which defines the order, parallelism and dependency of as many atomic and idempotent tasks as you need. </p> <p>Let's take a look at the example below:</p> <p>1) Load table1, make some changes and export to table1_new. Keep this worksheet open in the app.</p> <p>2) Do the same for table_2</p> <p>3) Load table_3 and join to table_1 and table_2. Since data in these tables are exported, table_3 will use data from table1_new and table2_new to join.</p> <p>4) Load table4, do not make any changes</p> <p>5) In table3, join again, this time to table4. Since no changes are made and exported, the original data source of table4 will be used</p> <p>6) Export table3 data pipeline code to go to production. </p> <p></p> <p>Your exported Airflow DAG will look like the below:</p> <p></p> <ul> <li>The code that loads table1 and table2, processes the steps, and exports to table1_new and table2_new will work in parallel.</li> <li>If any of these tasks fail, it will retry (default 3 times)</li> <li>If, for some reason, these tasks run successfully again, the resulting tableX_new will not be different (idempotent) </li> <li>When both of these tasks are completed, table3 code will run: loads table3, executes all steps including joining to table1_new, table2_new and table4 in the requested order</li> <li>Once all of the steps and joins for table3 are completed, it will export to final_data (atomicity) </li> <li>Please note that in a traditional pipeline the table1 task could pass in-memory processed data to table3, eliminating the need to export to an intermediary location (table1_new) first. </li> <li>In comparison, Practicus AI generated pipelines require your changes to be exported first. This is due to scalability requirements of modern big data systems.     </li> <li>If you close a worksheet in the app before exporting the deployment code, the exported code will not include the task of that table in the Airflow DAG. </li> <li>I.e. In the above example, if you close table1 before exporting the deployment code, the DAG would become simpler table2 -&gt; table3.  In this case, we assume the task needed to create table1_new will be part of another data pipeline. The new DAG and code for table3 will simply expect table1_new is available and up to date.    </li> <li>Please note all of the above are simply convenience defaults. You can always make changes to your DAG, moving tasks back and forth after code is exported.</li> </ul>"},{"location":"predict/","title":"Predict with App","text":"<p>Predict the unknown with one-click. Search and use models built with any technology / platform. You can use model APIs hosted anywhere, or use models cold stored on your laptop, S3 or other location,  which will become live within seconds. With short startup time and ease of use, your organization\u2019s ML models will reach new users easier</p> <p>This section of the documentation is work in progress..</p>"},{"location":"predict/#prediction","title":"Prediction","text":"<p>You need to have a Cloud Worker ready for the prediction process and start the prediction process with our data on the Cloud Worker.</p> <p></p> <p>In the predict operation, where we will use the registered model, MLflow is selected as the model location(MLflow, Local Computer, S3).</p> <p></p> <p>You can search and select the ice cream model registered as Model URI.</p> <p></p> <p></p> <p>When the prediction is completed, the prediction column is created on the data.</p> <p></p>"},{"location":"prepare/","title":"Prepare with App","text":"<p>Process, clean and prepare your data without any coding. When clicking is not enough, use 200+ Excel compatible formulas.  Add custom Python code using the built-in editor for more complex requirements. Export the final clean data to a file,  data lake or database directly from the app. To build repeatable data pipelines, export to pure Python code and run anywhere you need.</p> <p>This section of the documentation is work in progress..</p>"},{"location":"prepare/#sort","title":"Sort","text":"<p>You can sort the columns you want in ascending or descending order. </p>"},{"location":"prepare/#filter","title":"Filter","text":"<p>You can filter any column using logical operators.</p> <p></p>"},{"location":"prepare/#rename","title":"Rename","text":"<p>You can change the name of the column you choose.</p> <p></p>"},{"location":"prepare/#formula","title":"Formula","text":"<p>You can run and test more than one formula on the column you want.</p> <p></p> <p>When you want to add the formula, the new column on the worksheet will be ready.</p> <p></p>"},{"location":"prepare/#code","title":"Code","text":"<p>In this section, you can run the code yourself and test the code you wrote on the worksheet. When you apply the code, the change on the worksheet is saved.</p> <p></p>"},{"location":"prepare/#other-features","title":"Other Features","text":"<p>In addition, you can easily perform Show/Hide, Move Left , Move Right, Delete , Split, Convert, Missing, Group By, Categorical, One Hot and Join operations on data and columns.</p>"},{"location":"sdk-start/","title":"Home","text":"<p>Basic data preparation use case</p> <p>1) Export a Pandas DataFrame, NumPy array or a Python list to Excel</p> <pre><code>import practicus\n# export data to an Excel file\npracticus.export_data(my_df, \"my_data.xlsx\")\n</code></pre> <p>2) Open the file in Excel, Google Sheets, LibreOffice or any other Spreadsheet platform to analyze and make changes as usual. </p> <p></p> <p>3) After you are finished updating your data in Excel, you can apply all changes made to create a new data set. </p> <pre><code># import back from Excel, detect all the changes made, and apply to the Data Frame  \nmy_df2 = practicus.apply_changes(my_df, \"my_data.xlsx\") \n\n# practicus auto-generates Python code for you, and applies the updates..\n\n# display the result, which will be the same as what you see in Excel\ndisplay(my_df2)\n</code></pre> <p></p> <p>4) (Optional) Practicus AI will automatically create a data prep (.dp) file containing all detected changes, before generating Python code. You can review this file, remove changes you don't like, or add new ones manually as you wish. Once done, you can apply the updates directly from the .dp file. </p> <p></p> <pre><code># apply changes, but this time directly from the .dp file that you reviewed / updated\nmy_df2 = practicus.apply_changes(my_df, \"my_data.dp\")\n</code></pre> <p>5) (Optional) Rinse and repeat... You can continue the above steps, also working with others in a collaborative environment, to keep generating new versions of Excel files and auto-generated data sets. The detected changes (.dp files) can be updated and archived as needed. Outside of Jupyter notebooks, you can also chain multiple .dp files to create complex data preparation / ML pipelines and later embed these data pipelines to a data engineering platform for production purposes.  Any production grade data integration platform that can run Python code will easily run Practicus AI detected changes at scale.   </p>"},{"location":"sdk-start/#aiml-model-sharing","title":"AI/ML Model Sharing","text":"<p>Beyond data preparation, Practicus AI can also be used to export ML models to Excel, which can be used for different purposes. Below you can find some use cases to export your models to Excel. </p> <ul> <li> <p>Practicus AI exported models can help with ML deployment and testing and increase your chances of getting them to production to be used by masses.   </p> </li> <li> <p>The exported models have zero dependency, meaning they only use core Excel functionality and do not depend on 3rd party libraries, products, services, REST APIs etc. You can attach the exported ML model to an email, and the recipient would be able to predict / infer offline without any issues. </p> </li> <li> <p>You can use the exported Excel file to debug your models, since the model representation will be in a very simple form. </p> </li> <li> <p>Model Explainability can be a key blocker for getting ML models to production. Often times, data analysts, business analysts and other business leaders will not allow moving an ML model to production, simply because they do not understand how the model works. Practicus AI exported models in Excel will be significantly easier to consume and understand.</p> </li> </ul> <p>Basic model sharing use case</p> <p>1) Build your ML model as usual. </p> <pre><code># sample Support Vector Machine model\n...\nmy_svm_model.fit(X, Y)\n</code></pre> <p>2) Export to Excel </p> <pre><code>import practicus    \npracticus.export_model(my_svm_model, output_path=\"iris_svm.xlsx\",\n      columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>3) Open the file in Excel, Google Sheets, LibreOffice or others to make predictions, analyze how the model works and make changes as well.</p> <p></p> <p></p> <p></p> <p>4) (Optional) You can use pre-processing pipelines as well. Necessary calculations prior to model prediction will also be exported to Excel as pre-processing steps.   </p> <pre><code># create a pipeline with StandardScaler and LogisticRegression\nmy_pipeline = make_pipeline(\n   StandardScaler(),\n   LogisticRegression())\n\n# train\nmy_pipeline.fit(X, y)\n\n# Export the pre-processing and model calculation to Excel\npracticus.export_model(my_pipeline, output_path=\"model_with_pre_processing.xlsx\",\n                   columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>5) (Optional) You can also export models with Practicus AI using PMML. If you are using R, KNIME, Alteryx or any other ML platform, you can export your models to a .pmml file first (optionally including pre-processing steps as well) and then use the .pmml file with Practicus AI in Python to export it to Excel. The final Excel file will not have any dependencies to your ML platform. </p> <pre><code>practicus.export_model(\"my_model_developed_on_R.pmml\", output_path=\"R_model.xlsx\",\n                   columns=['petal length', 'petal width'], target_name=\"Species\")\n</code></pre>"},{"location":"setup-guide/","title":"Setup Guide","text":"<p>Welcome! It should take a few minutes to set up everything and be ready to go!</p>"},{"location":"setup-guide/#overview","title":"Overview","text":"<p>You can see a simplified view of Practicus AI setup options below.</p> <ul> <li>Practicus AI App is Forever Free and include common analytics and data preparation features.</li> <li>Cloud Workers with Forever Free Tier are optional but highly recommended. They bring in more functionality such as AutoML. You can choose one or more Cloud Worker system.</li> </ul> <p></p>"},{"location":"setup-guide/#install-the-practicus-ai-app","title":"Install the Practicus AI App","text":"<p>Practicus AI App works on your computer and contains the core functionality of our platform.</p> <p>If you haven't already, please install Practicus AI App for Windows, macOS or Linux.  </p> <p>If you are a Python user and prefer to Install the Practicus AI App as a library, please check our Python Setup Guide section below.</p> <p>If you are a programmer and only interested in installing the lightweight Practicus AI SDK (5MB), please only install practicuscore library using the Python Setup Guide section below.</p>"},{"location":"setup-guide/#optional-choose-a-cloud-worker-system-backend","title":"Optional - Choose a Cloud Worker System (backend)","text":"<p>If you prefer the quickest option, you can use our Software as a Service (SaaS), which will be ready in less than a minute. Click here to start Practicus AI SaaS trial</p>"},{"location":"setup-guide/#what-is-a-cloud-worker","title":"What is a Cloud Worker?","text":"<p>Some Practicus AI features such as AutoML, making AI Predictions, Advanced Profiling and production deployment capabilities require a larger setup, so we moved these from the app to a backend (server) system.  </p> <p>You have multiple Cloud Worker options to choose from, and you can find a quick summary on pros and cons of each option below.</p> <p>Please view the detailed comparison table for the pros and cons for each option.</p>"},{"location":"setup-guide/#software-as-a-service","title":"Software as a Service","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings</p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p>"},{"location":"setup-guide/#enterprise-cloud","title":"Enterprise Cloud","text":"<p>You cna think of the Enterprise Cloud as a private Practicus AI SaaS that can run on any cloud, or your private data enter.</p> <p>Configuration is exactly the same as Software as a Service. You only need to change your service address from https://service.practicus.io to your private address that your administrator provided, such as https://practicus.your_company.com</p> <p>Your password will be provided by your administrator. You can also click the Forgot Password button to reset your password.</p> <p></p>"},{"location":"setup-guide/#aws-marketplace","title":"AWS Marketplace","text":"<p>This section explains setting up AWS Cloud Workers.</p> <p></p>"},{"location":"setup-guide/#before-we-begin","title":"Before we begin","text":"<p>Some of our advanced features require cloud capacity.</p> <p>Instead of asking for access to your data, Practicus AI cloud runs 100% in your AWS account, fully isolated. This allows us to offer improved security and absolute privacy.</p> <p>You can activate Practicus AI on your AWS account in a few minutes, and use up to a certain cloud capacity free of charge.  Let's get started.</p>"},{"location":"setup-guide/#1-select-license","title":"1) Select license","text":"<p>Practicus AI cloud offers 3 different licenses.</p>"},{"location":"setup-guide/#free-license","title":"Free License","text":"<p>We offer a forever free cloud tier using 2 vCPUs and 1 GB RAM on AWS with t3.micro cloud instances.</p> <p>Please note that some AWS cloud regions charge roughly 1 cent / hour for t3.micro (see below). If you need to make sure everything your use is absolutely free, please make sure you pick an appropriate AWS cloud region. AWS also offer free tiers for other potential charges like S3 storage and network traffic. To keep everything free, you must experiment responsibly and make sure you do not go beyond these AWS limits.</p> <ul> <li>Please view AWS free tier details to learn more.</li> </ul>"},{"location":"setup-guide/#professional-license","title":"Professional license","text":"<p>When you need larger Practicus AI cloud capacity, you can simply pay-as-you-go (PAYG) hourly without sharing your credit card info, or making any commitments. AWS will charge for the Practicus AI cloud usage hourly, which will show as a separate Practicus AI line item in your monthly AWS cloud invoice.</p> <p>Practicus AI cloud works like electricity, you switch it on when you need it, and only pay for the total number of hours that the lights are on. Our Cloud Workers auto shut down after 90 minutes of inactivity by default, preventing unexpected costs. It works similar to this scenario: you leave your home and forget to turn the lights off. Your lights turn off automatically after 1.5 hours, since there is no motion detected in the house.</p> <p>Please visit practicus.ai/pricing for more info, and example pricing scenarios.</p>"},{"location":"setup-guide/#enterprise-license","title":"Enterprise license","text":"<p>We offer a bring-your-own-license (BYOL) model with benefits such as unlimited use for multiple users, with a fixed fee. </p> <p>If you have an enterprise license, simply open Practicus AI app and go to settings (preferences in macOS), cloud tab, and enter your email to activate your license.</p> <p>Please feel free to contact us to learn more about our enterprise support and licensing.</p> <p>Use your email to activate the enterprise license in app settings: </p>"},{"location":"setup-guide/#2-ready-your-aws-cloud-account","title":"2) Ready your AWS cloud account","text":"<p>This is a one-time task for all users sharing the same AWS account.</p> <p>Please skip this step if you already have an AWS account.</p> <p>Create an Amazon Web Services (AWS) cloud account for free.</p> <p>Please make sure you have created an admin user as well. You can check our AWS account creation guide below for help.</p>"},{"location":"setup-guide/#3-enable-on-aws-marketplace","title":"3) Enable on AWS marketplace","text":"<p>This is a one-time task for all users sharing the same AWS Account.</p> <p>Learn more about AWS marketplace.</p> <p>We have multiple offerings on AWS marketplace. Please click on each in the below list to view the marketplace page explaining the offering, and then click Continue to Subscribe button to enable (see screenshot below). You need at least one AWS marketplace offering enabled to use Practicus AI cloud.</p> <p>Please note that it can take a few minutes for the offering to be active. Once your subscription is active, please do not create a new EC2 instance using AWS cloud console. The next step will take care of configuration.</p>"},{"location":"setup-guide/#free-professional-pay-as-you-go-payg","title":"Free / Professional pay-as-you-go (PAYG)","text":"<ul> <li>Practicus AI - Most common, offers free tier, will give you all the functionality.</li> <li>Practicus AI with GPUs - Accelerated computing for very large data or if you have limited time. Can be 500+ times faster for some operations.</li> </ul>"},{"location":"setup-guide/#enterprise-license-bring-your-own-license-byol","title":"Enterprise License bring-your-own-license (BYOL)","text":"<ul> <li>Practicus AI - Most common enterprise offer</li> <li>Practicus AI with GPUs - Accelerated computing, enterprise offer</li> </ul> Sample view of our AWS marketplace page <p>Please carefully review software, hardware and total cost / hour on our AWS marketplace page. Sample professional license below:</p> <p></p>"},{"location":"setup-guide/#4-activate-your-aws-user-in-the-app","title":"4) Activate your AWS user in the app","text":"<p>You should now have an AWS user Access key ID and Secret access key ready, and the AWS account for this user has at least one Practicus AI AWS marketplace offer enabled.</p> <p>Simply open the Practicus AI app, go to settings (preferences in macOS), cloud tab, click the Activate your AWS user button, choose a default cloud region (you can change this later) and enter your cloud credentials:</p> <p></p> <p>Please note that your cloud credentials are not shared with 3rd parties, including Practicus AI. The app only uses the credentials to communicate with AWS cloud.</p> <p>Before you finalize the cloud settings, we will verify your configuration to check if everything is configured correctly.</p> <p></p> Sample AWS marketplace verification result. You need at least one verified <p>Pro Tip: You can save the cloud configuration info to a file and share with others, so they can open this file with Practicus AI app and automatically configure the cloud setup. Please check the Setup for others section below to learn more.  </p>"},{"location":"setup-guide/#troubleshooting","title":"Troubleshooting","text":"<p>If you could not verify one of the AWS marketplace offers, please check the below as potential reasons:</p> <ul> <li>AWS marketplace activation can take a few minutes. Please make sure you stay on the AWS marketplace page, confirm the activation is completed and go back to the app settings to verify again.  </li> <li>If you use multiple AWS accounts, please make sure you  subscribe using the correct AWS account since it is very easy to mix them up. Simply log-off from your AWS account, click on one of the view buttons inside the app settings to view AWS marketplace page again, login using the correct user, click subscribe, wait for it to be completed, and finally go back to app settings and click verify again.</li> <li>In rare cases, a company admin can disable AWS marketplace usage. If this is the case, please contact your admin, or create a new AWS account.</li> </ul>"},{"location":"setup-guide/#local-container","title":"Local Container","text":"<p>This section explains setting up a local container Cloud Worker.</p>"},{"location":"setup-guide/#1-install-a-container-engine","title":"1) Install a container engine","text":"<p>In order to run a container on your computer you need to first install a container engine.</p> <p>Docker is the most popular option: Install Docker Desktop</p> <p>Although Docker Desktop is free, there has been some licensing changes in the recent years.</p> <p>Podman is a great Docker alternative: Install Podman Desktop</p> <p>Once the installation is completed, simply run Docker or Podman Desktop and confirm the container engine is running.</p> <p>Active Docker Desktop</p> <p></p> <p>Active Podman Desktop</p> <p></p>"},{"location":"setup-guide/#2-pull-download-practicus-ai-container-image","title":"2) Pull (download) Practicus AI container image","text":"<p>Additional Practicus AI software is bundled inside a container image. You need to pull this package on your computer before using it.</p> <ul> <li>Open Practicus AI App settings (preferences in macOS) dialog and navigate to the Container section.</li> <li>If you have a Practicus AI Enterprise license, enter your email to activate and unlock all features. If not, you can use the free tier. Please note that Professional pay-as-you-go license option is not available for local containers. Compare license options.</li> <li>Choose a container engine, Docker or Podman, and confirm in the app the engine is running.</li> <li>Click the Pull (download) Practicus AI container image button</li> </ul> <p></p> <ul> <li>A command prompt window (terminal in macOS) will open to start the pull. This one-time download task can take anywhere between 5 - 20 minutes depending on your internet speed.</li> </ul> <p></p> <ul> <li>Once the container pull is completed, go back to the app and click refresh to view active Practicus AI images on your computer. Confirm you successfully pulled the container image.</li> <li>Click New Cloud Worker button to open Cloud Workers tab.</li> <li>Click Save to close settings.</li> </ul> <p></p> <ul> <li>In the Cloud Workers tab, select local container as Cloud Region.</li> <li>Click Launch New button to start a Cloud Worker.</li> </ul> <p></p> <ul> <li>When navigating cloud data sources in the Explore tab, you can switch between local and Cloud Workers by using the drop-down list at the top right.</li> <li>Practicus AI app also attaches (mounts) container_shared folder, so you can easily copy files back and forth between your file system and the container. Simply open Windows Explorer (Finder in macOS), navigate to: your home folder / practicus / container_shared and copy files. Then navigate to Cloud Worker Files in Explore tab, and the files you copied will be visible under container_shared folder. Click Reload button at the top if you recently copied files.  </li> </ul> <p></p> <p>Issues? If you are facing any issues, please check the container troubleshooting section below.</p>"},{"location":"setup-guide/#references","title":"References","text":""},{"location":"setup-guide/#aws-account-creation","title":"AWS Account Creation","text":"<p>Practicus AI Cloud Workers can start and stop with a click using our app, and they run in your Amazon Web Services (AWS) cloud account in a fully isolated fashion. This way we can offer you 100% privacy.</p> <p>Please follow the below steps to create a free AWS account.</p> <ol> <li>Please click here to visit AWS home page and click Create an AWS account</li> <li>Follow the steps to finish account creation. Please check the AWS help page if you need assistance on creating an account. After this step you will have a root account.</li> <li>Login to AWS management console using your root account.</li> <li>Navigate to IAM (Identity and Access Management), click Users on the left menu and click Add users</li> <li>For User name enter admin, click Access key and Password check boxes (see below screenshot)</li> <li>In Set permissions section select Attach existing policies directly and pick AdministratorAccess (see below screenshot)</li> <li>In the last screen carefully save your Access Key ID, Secret access key and Password (see below screenshot)</li> <li>All done! You can continue with the next step, Enabling on AWS marketplace</li> </ol> <p>Notes:</p> <ul> <li>Although the admin AWS user will be sufficient for Practicus AI Cloud Workers to run, as a security best practice we recommend you to create a least privileged AWS user for day to day usage.  Practicus AI app cloud setup can create this user for you. If you rather prefer to create one manually please make sure the user has access for EC2 and S3 operations.</li> <li>If you plan on having multiple AWS users sharing the same AWS account, you can simply add new users to the appropriate practicus AWS IAM user group that our app creates for you.</li> <li>If you have a local AWS profile (i.e. to be used with AWS CLI) Practicus AI setup can use this directly.</li> </ul> AWS account creation screenshots <p></p> <p></p> <p></p>"},{"location":"setup-guide/#aws-marketplace-reference","title":"AWS Marketplace Reference","text":"<p>Similar to our Windows app being available on Microsoft app store, our cloud workers are available on AWS marketplace. This gives our users \"there's an app for that!\" type experience, but for AI in the cloud.</p> <p>Any time you need to do AI in the cloud, you can just click a button in the Practicus AI app, and we will create the cloud capacity of your choice. And also shut it down with a click when you no longer need it, saving on cost.,</p> <p>For our app to be able to start/stop cloud worker you need to enable (subscribe to) the AWS marketplace offering of your choice.</p> <p>If you use the free tier (t3.micro) with 2 vCPUs and 1 GB RAM, there won't be any software license charges. AWS also offers t3.micro free of charge for eligible customers and regions. For larger capacity, AWS will charge you per hour. i.e. you start a cloud worker, use it for 2 hours and never use it again. Your cloud bill will have a line item for the 2 hours you use. Larger capacity is more expensive and the larger the capacity the bigger discount you will get.  </p>"},{"location":"setup-guide/#setup-for-others","title":"Setup for others","text":"<p>You can save cloud setup information to a file and share with others, so they can simply open the file with Practicus AI app to complete the cloud setup.</p> <p>Practicus AI uses .prt files to save worksheet data and steps. Since .prt files directly open with Practicus AI app, we use the same file extension to configure the app as well.</p> <p>You can simply create a text file, give it a name such as cloud_setup.prt and add setup information like the below:</p> <p>Sample cloud_setup.prt file:</p> <pre><code>[add_license]\nemail = user@mycompany.com\n\n[add_cloud_config]\ncloud_region = us-west-1\naccess_key = ...\nsecret_key = ...\n</code></pre> <p>[add_license] section can be used if you have an enterprise license.</p> <p>[add_cloud_config] section can be used to add an AWS cloud configuration. You can use the cloud_region key to set the default region (can be changed later), and it is optional. access_key (Access Key ID) and secret_key (Secret access key) are mandatory, and can be obtained during AWS IAM user creation.</p>"},{"location":"setup-guide/#container-troubleshooting","title":"Container troubleshooting","text":"<p>If you face any issues using local containers on your computer, please follow the below steps.</p> <p>View Status on Container Engine</p> <p>Open Docker Desktop or Podman Desktop, navigate to containers, and iew it's status. You can delete using these apps, which is the same thing as Terminate in Practicus AI, and create a new one using Launch New button inside Practicus AI App Cloud Workers tab.</p> <p>Starting a Container manually</p> <p>You can use the below command prompt (terminal) command to start a container from an image you downloaded.</p> <pre><code>docker run -p9000:9000 -p5500:5500 -p8585:8585 -p50000-50020:50000-50020 -d \\\n  --mount type=bind,src=$HOME/practicus/container_shared/,dst=/home/ubuntu/container_shared \\\n  --env MAX_PRACTICUS_WORKERS=21 \\\n  --env DEBUG_MODE=False \\\n  --name practicus ghcr.io/practicusai/practicus:22.11.0 \\\n  --env PRACTICUS_ENT_LIC_EMAIL='your-name@your-company.com' \\\n  --env PRACTICUS_ENT_LIC='abcd-1234-abcd-1234-abcd'\n</code></pre> <p>Notes:</p> <ul> <li>Please do not forget to create container_shared folder under your home directory / practicus folder first.</li> <li>No Enterprise License? Please remove PRACTICUS_ENT_LIC_EMAIL and PRACTICUS_ENT_LIC lines.</li> <li>To find your Enterprise license key (PRACTICUS_ENT_LIC), please view your home directory / .practicus / core.conf file and search for text 'license_key'. Please note that .practicus is a hidden folder.</li> <li>Replace 'docker' with podman or another container engine if you are not using docker.</li> <li>Need less or more workers? change MAX_PRACTICUS_WORKERS and also the port range -p50000-50020:50000-50020. E.g. For 10 workers use  MAX_PRACTICUS_WORKERS=10 -p50000-50009:50000-50009</li> <li>If you use network proxies, add them in the below format</li> </ul> <pre><code># ...\n  --env PRACTICUS_ENT_LIC='abcd-1234-abcd-1234-abcd' \\\n  --env HTTP_PROXY='http://192.168.0.1:9090' \\\n  --env HTTPS_PROXY='http://192.168.0.1:9090'  \n</code></pre> <p>Pulling a new container manually</p> <ul> <li>Please visit https://github.com/practicusai/deploy/pkgs/container/practicus and choose a version of Cloud Worker to pull. Use matching yy.mm versions between the App and the container. E.g. If your app is 22.11.2 you can use any 22.11.x Cloud Worker. Ideally use the latest e.g. 22.11.5 instead of 22.11.4. If you use Practicus AI App to pull, it already pulls the latest compatible image.</li> <li>Pull using the command. Prefer to use version tags instead of 'latest'</li> </ul> <pre><code>docker pull ghcr.io/practicusai/practicus:22.11.0\n</code></pre> <p>Hard reset</p> <p>Sometimes previously downloaded images can linger in the cache even if you deleted them. In order to hard reset and delete everything, you can use the below command.</p> <pre><code># Use with caution! This will delete all images and containers\ndocker image prune --force \n</code></pre>"},{"location":"setup-guide/#linux-installation","title":"Linux Installation","text":"<p>Since almost all Linux distros come with Python 3 installed, we do not offer a prepackaged installer.</p> <p>Please check the quick start guide below to see how you can install and run Practicus AI app on Linux. We have extensively tested on Ubuntu, if you encounter any issues please share with us using feedback section.</p>"},{"location":"setup-guide/#python-setup-guide","title":"Python Setup Guide","text":"<p>You can run pip install practicus (Windows/macOS/Linux: Python 3.10+) and then run practicus from the terminal. Or run, python -c \u201cimport practicus; practicus.run()\u201d  (python3 for macOS or Linux).</p> <p>Installing using pip will give you the exact same end-to-end GUI application experience. Similarly, if you download the packaged app you can still code freely when you want to. So, please select any method of installation as you prefer.</p> <p>As for any Python application, we strongly recommend you to use a virtual environment such as venv or conda. Please check the recommended QuickStart scripts on this page to create a virtual env, install Practicus AI and run with peace of mind and in one go.  </p> <p>For server environments or API only usage, you can pip install practicuscore to install the core library by itself without the GUI elements. (Windows/macOS/Linux: Python 3.10+)</p> <p>This is a small library with fewer requirements and no version enforcement for any of its dependencies. It\u2019s designed to run in existing virtual environments without overriding the version requirements of other libraries. Please check the documentation for more details.</p>"},{"location":"setup-guide/#windows-quickstart","title":"Windows QuickStart","text":"<pre><code>:: install \npython -m venv %UserProfile%\\practicus\\venv\n%UserProfile%\\practicus\\venv\\Scripts\\python -m pip install --upgrade practicus\n\n\n:: run\n%UserProfile%\\practicus\\venv\\Scripts\\activate\npracticus\n</code></pre>"},{"location":"setup-guide/#macos-quickstart","title":"macOS QuickStart","text":"<pre><code># install\npython3 -m venv ~/practicus/venv \n~/practicus/venv/bin/python -m pip install --upgrade practicus\n\n\n# run\nsource ~/practicus/venv/bin/activate\npracticus\n</code></pre>"},{"location":"setup-guide/#linux-quickstart","title":"Linux QuickStart","text":"<pre><code># install\npython3 -m venv ~/practicus/venv \n~/practicus/venv/bin/python -m pip install --upgrade practicus\n\n\n# run\nsource ~/practicus/venv/bin/activate\npracticus\n\n# Note: if you get qt.qpa.plugin error please run\nsudo apt-get install '^libxcb.*-dev' libx11-xcb-dev libglu1-mesa-dev libxrender-dev libxi-dev libxkbcommon-dev libxkbcommon-x11-dev\n</code></pre>"},{"location":"setup-guide/#starter-app","title":"Starter app","text":"<p>Instead of running Practicus AI from the command prompt (terminal) you can create shortcuts and run with double click. The below tips assume you installed using the QuickStart tips above.</p> <p>Windows: Navigate to *%UserProfile%\\practicus\\venv\\Scripts* folder and locate practicus.exe, which is essentially a starter for practicus Python library. You can right-click and select pin to start. You can also create a shortcut to this .exe and change its name to Practicus AI and its icon by downloading our icon practicus.ico.</p> <p>macOS: You can download Practicus AI Starter app which is a tiny (100KB) app that starts the Python virtual env in ~/practicus/venv and then starts Practicus AI GUI from practicus Python library. To keep the app in dock please drag &amp; drop the .app file on the dock itself. Right-clicking and choosing \u201ckeep in dock\u201d will not create a shortcut.</p>"},{"location":"share-models/","title":"Sharing AI Models use case","text":"<p>Beyond data preparation, Practicus AI can also be used to export ML models to Excel, which can be used for different purposes. Below you can find some use cases to export your models to Excel. </p> <ul> <li> <p>Practicus AI exported models can help with ML deployment and testing and increase your chances of getting them to production to be used by masses.   </p> </li> <li> <p>The exported models have zero dependency, meaning they only use core Excel functionality and do not depend on 3rd party libraries, products, services, REST APIs etc. You can attach the exported ML model to an email, and the recipient would be able to predict / infer offline without any issues. </p> </li> <li> <p>You can use the exported Excel file to debug your models, since the model representation will be in a very simple form. </p> </li> <li> <p>Model Explainability can be a key blocker for getting ML models to production. Often times, data analysts, business analysts and other business leaders will not allow moving an ML model to production, simply because they do not understand how the model works. Practicus AI exported models in Excel will be significantly easier to consume and understand.</p> </li> </ul> <p>Basic model sharing use case</p> <p>1) Build your ML model as usual. </p> <pre><code># sample Support Vector Machine model\n...\nmy_svm_model.fit(X, Y)\n</code></pre> <p>2) Export to Excel </p> <pre><code>import practicus    \npracticus.export_model(my_svm_model, output_path=\"iris_svm.xlsx\",\n      columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>3) Open the file in Excel, Google Sheets, LibreOffice or others to make predictions, analyze how the model works and make changes as well.</p> <p></p> <p></p> <p></p> <p>4) (Optional) You can use pre-processing pipelines as well. Necessary calculations prior to model prediction will also be exported to Excel as pre-processing steps.   </p> <pre><code># create a pipeline with StandardScaler and LogisticRegression\nmy_pipeline = make_pipeline(\n   StandardScaler(),\n   LogisticRegression())\n\n# train\nmy_pipeline.fit(X, y)\n\n# Export the pre-processing and model calculation to Excel\npracticus.export_model(my_pipeline, output_path=\"model_with_pre_processing.xlsx\",\n                   columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>5) (Optional) You can also export models with Practicus AI using PMML. If you are using R, KNIME, Alteryx or any other ML platform, you can export your models to a .pmml file first (optionally including pre-processing steps as well) and then use the .pmml file with Practicus AI in Python to export it to Excel. The final Excel file will not have any dependencies to your ML platform. </p> <pre><code>practicus.export_model(\"my_model_developed_on_R.pmml\", output_path=\"R_model.xlsx\",\n                   columns=['petal length', 'petal width'], target_name=\"Species\")\n</code></pre>"},{"location":"trial-ent-saas/","title":"Enterprise License and limited SaaS Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your Enterprise license will unlock all advanced features.   </p>"},{"location":"trial-ent-saas/#activating-your-enterprise-license","title":"Activating your Enterprise License","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is not included in your trial. </li> <li>Please click here to start Practicus AI SaaS trial </li> </ul> <p>Enterprise Kubernetes</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p>"},{"location":"trial-ent-saas/#optional-using-your-limited-saas-account","title":"Optional - Using your limited SaaS account","text":"<p>Since we are allocating our online resources to production workloads, your SaaS account is limited to online features such as GPT, but not Cloud Workers. </p> <p>For features such as AutoML, please use Cloud Workers on local containers or AWS marketplace.</p>"},{"location":"trial-ent-saas/#sample-data-exploration-view-utilizing-cost-effective-options","title":"Sample Data Exploration view, utilizing cost-effective options","text":""},{"location":"trial-ent-saas/#using-gpt-and-other-online-only-features-with-your-limited-saas","title":"Using GPT and other \"online only\" features with your limited SaaS","text":"<p>When you use the App + local container or AWS marketplace option, the App intelligently decides when an \"online only\" feature such as GPT is required to use the SaaS. </p> <p>By combining local container or AWS marketplace with the limited SaaS, you will be able to trial all features with no limitations, and in a cost-effective way. </p>"},{"location":"trial-ent-saas/#unlimited-saas-option","title":"Unlimited SaaS option","text":"<p>If you prefer to use our SaaS unlimited, please click here to start Practicus AI SaaS trial </p>"},{"location":"trial-ent-saas/#logging-in-to-your-limited-saas-account","title":"Logging in to your limited SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-ent/","title":"Enterprise License Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your Enterprise license will unlock all advanced features.   </p>"},{"location":"trial-ent/#activating-your-enterprise-license","title":"Activating your Enterprise License","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is not included in your trial. </li> <li>Please click here to start Practicus AI SaaS trial </li> </ul> <p>Enterprise Kubernetes</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-saas-ent/","title":"SaaS and Enterprise License Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Both your SaaS account and the enterprise license will unlock all advanced features.   </p>"},{"location":"trial-saas-ent/#logging-in-to-your-saas-account","title":"Logging in to your SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p>"},{"location":"trial-saas-ent/#optional-activating-your-enterprise-license","title":"Optional - Activating your Enterprise License","text":"<p>If you prefer to use local containers on your laptop offline, or your personal AWS account (GPUs available) instead of Practicus AI SaaS, please activate your enterprise license.</p> <p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is included in your trial. </li> </ul> <p>Enterprise Cloud</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-saas/","title":"Software as a Service (SaaS) Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your SaaS account will unlock all advanced features.   </p>"},{"location":"trial-saas/#logging-in-to-your-saas-account","title":"Logging in to your SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trino_api_guide/","title":"Trino Data Query API using FastAPI","text":"<p>This document describes a Python-based API service built with <code>practicuscore</code> that queries customer data from Trino. It covers the API definition, deployment process, and various methods for testing the deployed API.</p>"},{"location":"trino_api_guide/#1-trino-data-query-api-apishomepy","title":"1. Trino Data Query API (<code>apis/home.py</code>)","text":"<p>This Python code defines an API endpoint that retrieves customer information from a Trino database. It uses Pydantic for data validation and <code>trino</code> client for database connectivity.</p> <pre><code>from datetime import date\nfrom pydantic import BaseModel\nfrom fastapi import HTTPException\nimport practicuscore as prt\nimport trino\nimport trino.auth\n\n# Pydantic model for Customer data\nclass Customer(BaseModel):\n    customer_id: int\n    first_name: str\n    last_name: str\n    email: str\n    phone: str\n    signup_date: date\n\n    # Example for API documentation (Swagger/Redoc)\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": 101,\n                    \"first_name\": \"John\",\n                    \"last_name\": \"Doe\",\n                    \"email\": \"john.doe@example.com\",\n                    \"phone\": \"555-1234\",\n                    \"signup_date\": \"2025-01-01\"\n                }\n            ]\n        }\n    }\n\n# Pydantic model for the API request payload\nclass CustomerIdRequest(BaseModel):\n    customer_id: int\n\n# Pydantic model for the API response\nclass CustomerResponse(BaseModel):\n    customer: Customer\n\n# API endpoint definition using practicuscore decorator\n@prt.api(\"/customer-by-id\")\nasync def get_customer_by_id(payload: CustomerIdRequest, **kwargs) -&gt; CustomerResponse:\n    \"\"\"\n    Retrieves customer details from Trino based on customer_id.\n    \"\"\"\n    try:\n        # Parse and validate the incoming payload\n        parsed_payload = CustomerIdRequest.parse_obj(payload)\n\n        # Establish connection to Trino\n        # Note: 'verify=False' is used here for demonstration,\n        # but in production, proper SSL certificate verification should be configured.\n        # BasicAuthentication is used for simplicity, consider more secure methods like Kerberos if available.\n        conn = trino.dbapi.connect(\n            host=\"{&lt;TRINO_HOST&gt;}\", # e.g., trino.dev.practicus.io\n            port=443,\n            http_scheme=\"http}\",\n            verify=False, # e.g., False (for testing), True (for production with valid certs)\n            auth=trino.auth.BasicAuthentication(\"{&lt;TRINO_USERNAME&gt;}\", \"{&lt;TRINO_PASSWORD&gt;}\"), \n        )\n        cursor = conn.cursor()\n\n        # SQL query to fetch customer data\n        # Using parameterized query (?) for security against SQL injection\n        query = \"SELECT customer_id, first_name, last_name, email, phone, signup_date FROM lakehouse.&lt;schema_name&gt;.&lt;customer_table&gt; WHERE customer_id = ?\"\n        cursor.execute(query, (parsed_payload.customer_id,))\n        row = cursor.fetchone()\n\n        # Handle case where customer is not found\n        if not row:\n            raise HTTPException(status_code=404, detail=\"Customer not found\")\n\n        # Map query results to the Pydantic Customer model\n        columns = [col[0] for col in cursor.description]\n        customer = Customer(**dict(zip(columns, row)))\n\n        # Return the CustomerResponse\n        return CustomerResponse(customer=customer)\n\n    except HTTPException as http_exc:\n        # Re-raise explicit HTTPExceptions\n        raise http_exc\n    except Exception as e:\n        # Catch any other unexpected errors and return a generic 500 error\n        # In a production environment, log the full exception details here for debugging\n        print(f\"An unexpected error occurred: {e}\") # Log the actual error\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n</code></pre> <p>Explanation:</p> <ul> <li>Pydantic Models: <code>Customer</code>, <code>CustomerIdRequest</code>, and <code>CustomerResponse</code> define the structure of data for validation and clear API schema generation.</li> <li><code>@prt.api(\"/customer-by-id\")</code>: This decorator from <code>practicuscore</code> exposes the <code>get_customer_by_id</code> function as an API endpoint accessible at <code>/customer-by-id</code>.</li> <li>Trino Connection: A connection to Trino is established using <code>trino.dbapi.connect()</code>, specifying the host, port, scheme, and authentication. Sensitive credentials and hostnames are replaced with placeholders.</li> <li>Parameterized Query: The SQL query uses a <code>?</code> placeholder for <code>customer_id</code> to prevent SQL injection vulnerabilities.</li> <li>Error Handling: The API handles cases where a customer is not found (404) and general internal server errors (500).</li> </ul>"},{"location":"trino_api_guide/#2-api-deployment-deploy_apppy","title":"2. API Deployment (<code>deploy_app.py</code>)","text":"<p>This script handles the deployment of the application to the Practicus.</p> <pre><code>import practicuscore as prt\n\n# Deployment configuration variables\napp_deployment_key = \"{&lt;YOUR_DEPLOYMENT_KEY&gt;}\" # e.g., \"appdepl-1\"\napp_prefix = \"{&lt;YOUR_APP_PREFIX&gt;}\" # e.g., \"apps\"\n\n# Application metadata\napp_name = \"{&lt;YOUR_APP_NAME&gt;}\" # e.g., \"trino-app-1\"\nvisible_name = \"Trino API App\"\ndescription = \"This API retrieves customer details from Trino using the provided customer_id. It returns basic information such as name, email, phone, and signup date.\"\nicon = \"fa-rocket\" # Font Awesome icon class\n\n# Deploy the application\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None, # Assuming the API code is in a standard location relative to the deployment script\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"Booting UI :\", app_url)\nprint(\"Booting API:\", api_url)\nprint(\"API Docs   :\", api_url + \"redoc/\")\n</code></pre> <p>Explanation:</p> <ul> <li><code>prt.apps.deploy()</code>: This function deploys the API. It requires a <code>deployment_setting_key</code> to specify the deployment environment, a <code>prefix</code> for the URL, an <code>app_name</code>, and metadata like <code>visible_name</code>, <code>description</code>, and <code>icon</code> for UI visibility.</li> <li>The script prints the base URLs for the deployed UI (if any) and the API, including the URL for the auto-generated API documentation (Redoc).</li> </ul>"},{"location":"trino_api_guide/#3-api-testing-internal-test_api_internalpy","title":"3. API Testing - Internal (<code>test_api_internal.py</code>)","text":"<p>This code demonstrates how to test the deployed API using <code>practicuscore</code>'s internal testing utilities.</p> <pre><code>import practicuscore as prt\n\n# Flags to control which parts of the test run\ntest_ui = True # Not applicable for this API-only test, but kept for context\ntest_api = True\n\nif test_api:\n    # Import the response model from your API definition\n    # Assuming 'apis/home.py' defines CustomerResponse\n    from apis.home import CustomerResponse\n\n    # Define the payload for the API call\n    payload = {\"customer_id\": 103}\n\n    # Call the API internally using prt.apps.test_api\n    response: CustomerResponse = prt.apps.test_api(\"/customer-by-id\", payload=payload)\n\n    # Print the full Pydantic response object\n    print(response)\n</code></pre> <p>Explanation:</p> <ul> <li><code>prt.apps.test_api()</code>: This function allows calling deployed API endpoints directly within the <code>practicuscore</code> environment, which is useful for rapid testing during development without making actual HTTP requests.</li> <li>It takes the API path and the payload, and returns the Pydantic response model, enabling strong type checking and easy access to data.</li> </ul>"},{"location":"trino_api_guide/#4-api-testing-external-test_api_externalpy","title":"4. API Testing - External (<code>test_api_external.py</code>)","text":"<p>This script shows how to call the deployed API from an external client using the <code>requests</code> library. It includes token retrieval for authentication.</p> <pre><code>import practicuscore as prt\nimport requests\nimport json # Import json module for pretty printing\n\n# Base URL for the deployed API\napi_base_url = \"https://{&lt;YOUR_PRACTICUS_DOMAIN&gt;}/apps/{&lt;YOUR_APP_NAME&gt;}/api/v1/\" # e.g., [https://dev.practicus.io/apps/trino-app-1/api/v1/](https://practicustest.vodafone.local/apps/trino-app-1/api/v1/)\n\ntoken = None  # Initialize token. Get a new token, or reuse existing if not expired\n# Retrieve a session token from practicuscore for authentication\ntoken = prt.apps.get_session_token(api_url=api_base_url, token=token)\n\n# Construct the full API endpoint URL\napi_url = f\"{api_base_url}customer-by-id/\"\n\nprint(f\"Auth Token: {token}\")\n# Prepare HTTP headers including the Authorization token and content type\nheaders = {\"Authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\nprint(f\"Request Headers: {headers}\")\n\n# Define the payload for the API request\npayload_dict = {\"customer_id\": 108} # Example customer ID\n\nprint(f\"\\nSending JSON payload to: {api_url}\")\nprint(f\"Payload: {json.dumps(payload_dict, indent=2)}\") # Pretty print payload\n\ntry:\n    # Send a POST request to the API endpoint\n    resp = requests.post(api_url, json=payload_dict, headers=headers)\n    resp.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n\n    # Parse the JSON response\n    response_data = resp.json()\n\n    print(\"\\n--- API Raw Response JSON ---\")\n    print(json.dumps(response_data, indent=2)) # Pretty print raw JSON response\n\n    # Access and print formatted customer details from the raw JSON dictionary\n    if \"customer\" in response_data:\n        customer_data = response_data[\"customer\"]\n        print(\"\\n---------- Formatted Customer Details ----------\")\n        print(\n            f\"{customer_data.get('signup_date')} tarihinde kay\u0131t olan {customer_data.get('first_name')} \"\n            f\"{customer_data.get('last_name')} adl\u0131 m\u00fc\u015fterinin mail adresi {customer_data.get('email')} \"\n            f\"olup telefonu ise {customer_data.get('phone')} \u015feklindedir.\"\n        )\n    else:\n        print(\"Error: 'customer' key not found in response.\")\n\nexcept requests.exceptions.RequestException as e:\n    # Handle HTTP request-specific errors\n    print(f\"\\n--- HTTP Request Failed ---\")\n    print(f\"Error: {e}\")\n    if hasattr(e, 'response') and e.response is not None:\n        print(\"Raw response body (if any):\")\n        print(e.response.text)\n    else:\n        print(\"No response body received.\")\n\nexcept Exception as e:\n    # Handle any other unexpected errors during response processing\n    print(f\"\\n--- Error Processing Response ---\")\n    print(f\"An unexpected error occurred: {e}\")\n</code></pre> <p>Explanation:</p> <ul> <li><code>requests</code> Library: Used to send HTTP POST requests to the deployed API.</li> <li>Authentication: <code>prt.apps.get_session_token()</code> is used to securely obtain an authentication token, which is then included in the <code>Authorization</code> header.</li> <li>Error Handling: Includes <code>try-except</code> blocks to catch network errors (<code>requests.exceptions.RequestException</code>) and other general exceptions, providing informative error messages.</li> <li>The <code>json.dumps(..., indent=2)</code> is used to pretty-print JSON responses for better readability in the console output.</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Practicus AI offers two primary types of tutorials\u2014one designed for technical users who prefer to code and another suited for those who don\u2019t code, using AI Studio\u2019s more intuitive, no-code/low-code capabilities.</p>"},{"location":"tutorials/#technical-tutorial-if-you-code","title":"Technical Tutorial (if you code)","text":"<p>You can start by reading the technical tutorial right here in the documentation\u2014simply open the Technical Tutorial</p>"},{"location":"tutorials/#for-a-hands-on-coding-experience-recommended","title":"For a hands-on coding experience (recommended)","text":"<p>If you\u2019d like a hands-on experience, follow these steps to run the tutorial notebooks in your Practicus AI environment:</p> <ol> <li> <p>Create a Worker:    From Practicus AI Home, create a Worker.</p> </li> <li> <p>Open Jupyter Lab or VS Code:    Open Jupyter Lab or VS Code on the worker you just created.</p> </li> <li> <p>View the <code>samples/notebooks</code> Directory:    Here you\u2019ll find the tutorial notebooks that match exactly to what you read in the technical tutorial.</p> </li> <li> <p>View the SDK Docs:    Check out the SDK documentation for additional reference and guidance.</p> </li> </ol>"},{"location":"tutorials/#ai-studio-tutorial-if-you-do-not-code","title":"AI Studio Tutorial (if you do not code)","text":"<p>If coding isn\u2019t your focus, the AI Studio tutorial is ideal:</p> <ol> <li> <p>Access Practicus AI Studio or Workspaces:    From Practicus AI Home, open a browser-based Workspace or download AI Studio. </p> </li> <li> <p>Follow the tutorial steps    Open AI Studio tutorial and start following the tutorial steps.</p> </li> </ol>"},{"location":"tutorials/#operations-tutorial","title":"Operations Tutorial","text":"<p>If your primary focus is MLOps, DataOps, SecOps, or DevOps, we recommend that you first skim the Technical Tutorial to gain a solid understanding of the Practicus AI platform fundamentals. Once you\u2019re familiar with the basics, proceed to the Operations tutorial.</p> <p>Previous: Tutorials | Next: Choose one of the below.</p> <ul> <li>Technical Tutorial (if you code)</li> <li>AI Studio tutorial (if you do not code)</li> <li>Operations tutorial </li> </ul>"},{"location":"ai-studio-tutorial/Experiment-Service/","title":"Introduction to Advanced Model, Experiment, and Artifact Management in Practicus AI with MLFlow","text":"<p>The MLFlow integration of the Practicus AI platform provides a solution that significantly simplifies and optimizes the management process of your machine learning projects. </p> <p>This guide explains how to use model creation, management of experiments and other features MLFlow offers.</p>"},{"location":"ai-studio-tutorial/Experiment-Service/#access-to-mlflow-interface","title":"Access to MLFlow Interface:","text":"<p>Within Practicus AI, to access MLFlow:</p> <ul> <li>Open Explore tab</li> <li>Select the MLFlow service defined for you in the working region</li> <li>You can see the models and experiments saved in MLFlow here</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/Experiment-Service/#saving-a-created-model-in-mlflow","title":"Saving a created model in MLFLow","text":"<ul> <li>Click on Explore tab</li> <li>Make sure your Worker is already selected upper left</li> <li>Click Worker Files to load content </li> <li>Expand samples folder and click on ice_cream.csv</li> <li>Click Load </li> <li>Click on the Model button</li> <li>Click Advance</li> <li>Choose Log Exp. Service as MLFlow Primary</li> </ul> <ul> <li>Click OK</li> <li>When the model is created, a plugin will arrive and set the incoming plugin like this</li> </ul> <ul> <li>Click OK</li> </ul>"},{"location":"ai-studio-tutorial/Experiment-Service/#models-experiments-and-artifacts","title":"Models, Experiments and Artifacts","text":"<ul> <li>Open the opened MLFLow service in the browser from the tab above</li> <li>Find the session you created and open the session</li> <li>Here you can see the prt format file, the json containing the model's metadata and the pickle</li> <li>Click Parameters</li> </ul> <ul> <li>Back to Session</li> <li>Find the session you created and click on the '+' sign under table</li> </ul> <ul> <li>Select the first model under session here</li> </ul> <ul> <li>Click on Metrics and see the error metrics saved in MLFlow:</li> </ul> <ul> <li>Scroll to the bottom of the page and access the model artifacts</li> </ul> <p>Scroll to the bottom of the page and access the model artifacts</p> <p></p>"},{"location":"ai-studio-tutorial/Experiment-Service/#sending-an-experiment-from-notebook-to-mlflow","title":"Sending an Experiment from Notebook to MLFlow","text":"<ul> <li>Back to Notebook opened after the model</li> </ul> <ul> <li>Run step by step and create exp in step 3</li> </ul> <ul> <li>Update setup params and run the cell</li> </ul> <ul> <li>Now you can setup experiments</li> </ul> <ul> <li> <p>Run the rest of the steps</p> </li> <li> <p>Save setup </p> </li> </ul> <p></p> <ul> <li>Open MLFlow in the browser</li> </ul> <p></p> <ul> <li>Click on the new Experiment and open it</li> <li>See the changes here</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/Workflow/","title":"Introduction to Workflow Automation","text":"<p>The Workflow tab is a feature in the Practicus AI platform that allows users to automate their workflow. </p> <p>This feature offers a wide range of functionality from code generation to execution of steps on the desired engine.</p> <p>In the scenario we will consider, we will make simple preview steps in the interface and examine their transformation into workflows</p> <p>Hint: From this tab we will only cover the use of Workflow Automation. For other options, you can check the Airflow tab.</p>"},{"location":"ai-studio-tutorial/Workflow/#1-access-to-workflow-tab","title":"1. Access to Workflow Tab","text":"<ul> <li>Open Explore tab</li> <li>Select Cloud Worker Files and open the file below </li> <li>Explore &gt; samples &gt; select and open any csv file</li> <li>You will see 3 different options for Deploy.</li> </ul>"},{"location":"ai-studio-tutorial/Workflow/#2-workflow-creation","title":"2. Workflow Creation","text":"<ul> <li>Open Explore tab</li> <li>Select Cloud Worker Files and open the file below </li> <li>Explore &gt; samples &gt; insurance.csv</li> <li>Click filter and set charges &lt; Less than 40000 and add then click OK</li> <li>Then select sex and click one hot from the prepare tab.</li> <li>Then select sex and click delete from the prepare tab.</li> <li>Click Steps and see following steps:</li> </ul> <ul> <li>In a moment we will translate these steps into workflows</li> <li>Click Deploy and Select Workflow Automation</li> </ul> <p>Hint: This step allows the created workflow to be automated and run on the specified engine. The \"Deploy\" option turns your workflow into a practical application and allows it to run automatically with the schedule and settings you specify.</p> <p>You can set Deployment name, Schedule interval, Cloud Region, and Workflow Service</p> <ul> <li> <p>Click Advanced and select Data processing engine as SPARK </p> </li> <li> <p>Click OK</p> </li> </ul> <p></p>"},{"location":"ai-studio-tutorial/Workflow/#3-generated-codes","title":"3. Generated codes","text":"<p>After deploying the workflow, the generated scripts are opened. In fact, these codes generated by Practicus IA are located in the user's git repository. </p> <p></p> <p></p> <ul> <li>Here you can see that 2 .py files are generated. </li> <li>One of them contains the Airflow DAG and the other one contains our preprocess steps. </li> <li>After generating the code, users can play with the code and redeploy it.</li> </ul>"},{"location":"ai-studio-tutorial/Workflow/#4-airflow-ui-on-app-and-browser","title":"4. Airflow UI on App and Browser","text":"<ul> <li>Open Explore and choose Workflow Service</li> <li>See Airflow UI on app </li> <li>Then click your DAG </li> <li>Then click browser </li> <li>Then click your DAG and trigger</li> <li>Firstly you need to set configuration  </li> <li>Then trigger your DAG and see status your dag</li> </ul>"},{"location":"ai-studio-tutorial/chatgpt/","title":"Analyzing and preparing data with GPT","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"ai-studio-tutorial/chatgpt/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; insurance.csv</li> </ul>"},{"location":"ai-studio-tutorial/chatgpt/#using-gpt-to-generate-code","title":"Using GPT to generate code","text":"<p>To generate code, you should provide GPT with a prompt that describes the code you want to generate. The prompt should be written in English and include the desired functionality of the code snippet. ChatGPT will then generate a code snippet that matches the description.</p> <p></p> <ul> <li>When you select GPT, you will see the below dialog.</li> <li>Explain to the GPT what you want to do.</li> </ul> <p></p> <ul> <li>If you want to see the code generated by GPT, you can click Advanced. In addition, you can set other options.</li> </ul> <p></p> <ul> <li>Click Apply GPT and see the final dataset.</li> </ul> <p>Let's create a slightly more advanced query with GPT.</p> <ul> <li>Click Test GPT then click Advanced</li> </ul> <p></p> <ul> <li>Please make specific comments in advanced queries so that GPT generates the code properly.</li> </ul> <p></p> <ul> <li>Click Apply GPT and see the final dataset with new variables.</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/chatgpt/#optionalhow-can-i-generate-the-optimal-gpt-query","title":"(Optional)How can I generate the optimal GPT query?","text":"<p>Prompt engineering is the process of crafting effective input prompts to elicit the desired output from large language models (LLMs). LLMs are trained on massive datasets of text and code, and can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way. However, they need to be prompted correctly in order to produce the desired output.</p> <p>Here is a brief overview of the steps involved in prompt engineering:</p> <p>1) Identify the desired output.</p> <p>2) Craft a prompt. </p> <ul> <li>The prompt should be written in plain language that is easy to understand. It should also be as specific as possible, so that the LLMs knows what you want it to generate.</li> </ul> <p>3) Test the prompt.</p> <ul> <li>Once you have crafted a prompt, you need to test it to make sure that it works.</li> </ul> <p>4) Refine the prompt. </p> <ul> <li>If the LLMs does not generate the desired output, you need to refine the prompt. This may involve making the prompt more specific, or providing more examples.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/code/","title":"Extensibility with Python Coding","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/collaboration/","title":"Collaboration","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/data-analysis-intro/","title":"Introduction to Data Analysis","text":"<p>This section only requires Practicus AI app and can work offline.</p>"},{"location":"ai-studio-tutorial/data-analysis-intro/#working-with-local-files","title":"Working with local files","text":"<ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on Local Files</li> <li>Navigate to the samples directory and open ice_cream.csv : </li> <li>home &gt; practicus &gt; samples &gt; data &gt; ice_cream.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p>This simple dataset shows how much revenue an ice cream shop generates, based on the outside temperature (Celsius) </p>"},{"location":"ai-studio-tutorial/data-analysis-intro/#visualizing-data","title":"Visualizing Data","text":"<ul> <li>Click on Analyze and select Graph</li> </ul> <ul> <li>Select Temperature for X axis and Revenue for Y, click ok</li> </ul> <ul> <li>You will see a new tab opens up and a graph is plotted</li> </ul> <ul> <li>Move your mouse over the blue line, and you will see the coordinates changing at the top right</li> <li>Click on zoom, move your mouse to any spot, left-click and hold to draw a rectangle. You will zoom into that area</li> <li>Click on Pan, left-click and hold a spot and move around</li> </ul> <p>Now let's use a more exciting data set:</p> <ul> <li>Go back to the Explore tab and load the file below:</li> <li>home &gt; practicus &gt; samples &gt; data &gt; insurance.csv</li> </ul> <p></p> <p>This dataset is about the insurance charges of U.S. individuals based on demographics such as, age, sex, bmi .. </p> <ul> <li>Click on the charges column name to select it </li> <li>You see a mini-histogram on the upper right with basic quartile information</li> <li>Min, 25%, 50% (median), Avg (Average / mean), 75%, and Max</li> <li>Move your mouse over a distribution (shown as blue lines) on the histogram, and a small pop-up will show you the data range, how many samples are there, and the total percent of the samples in that distribution. </li> </ul> <p></p> <p>Now let's open a larger histogram for a closer look:</p> <ul> <li>Click on Analyze &gt; Graph</li> <li>Select Histogram for style </li> <li>Select charges, click ok</li> </ul> <p></p> <p>You will see the below histogram </p> <p></p>"},{"location":"ai-studio-tutorial/data-analysis-intro/#visualizing-outliers","title":"Visualizing Outliers","text":"<p>Now let's analyze to see the outliers in our data set.</p> <ul> <li>Click Analyze &gt; Graph</li> <li>Select boxplot style, charges and click ok</li> </ul> <p></p> <p>You will see the boxplot graph visualizing outliers. </p> <p></p> <p>The above tells us that some individuals pay significantly more insurance  charges compared to the rest. E.g. $60,000 which is more than 5x the median (50%). </p> <p>Please note: Since Q1 - 1.5 x IQR is -$10,768, overall sample minimum $1,121 is used as boxplot min. This is common in skewed data.</p> <p>Sometimes outliers are due to data errors, and we will see how to remove these in the next section. And sometimes we still remove them even if they are correct to improve AI model quality. We will also discuss this later.</p>"},{"location":"ai-studio-tutorial/data-analysis-intro/#group-by-to-summarize-data","title":"Group by to Summarize Data","text":"<p>Since our insurance data also has demographic information such as region, we can summarize (aggregate) based on how we wish to break down our data. </p> <ul> <li>Select Analyze &gt; Group By</li> </ul> <p></p> <ul> <li>Select region and then sex for the Group by section</li> <li>Select charges - Mean (Average), charges - Median (50%), charges - Std. Dev (Standard Deviation), age - Mean(Average), bmi - Variance for the summarize section</li> <li>Click ok </li> </ul> <p></p> <p>You will see the selected charges summaries for region and sex break-down. There is no limit, you can break down for as many columns as you need.</p> <p></p> <p>Now let's create a more advanced multi-layer graph:</p> <ul> <li>Select Analyze &gt; Graph</li> <li>Click on Advanced Options</li> <li>Select region for X, charges_mean for Y</li> <li>Click Add Layer</li> <li>Select region for X, charges_median for Y</li> <li>Click Add Layer again</li> <li>Click ok</li> </ul> <p></p> <p>You will see the mean and median for different U.S. regions. </p> <p></p> <p>Let's say we want to email this graph to someone:</p> <ul> <li>Click on Save</li> <li>Select a file name. e.g. insurance.png</li> </ul> <p></p> <p>You will get a graphics file saved on your computer.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/data-prep-intro/","title":"Introduction to Data Preparation","text":"<p>This section only requires Practicus AI app and can work offline.</p> <p>Let's start by loading boston.csv. We will ignore the meaning of this dataset since we will only use it to manipulate values.</p> <p></p> <p>Click on Prepare button to view some common data preparation actions</p> <p></p> <p></p>"},{"location":"ai-studio-tutorial/data-prep-intro/#sorting-data","title":"Sorting Data","text":"<ul> <li>Click on CRIM column</li> <li>Hit Ctrl (Cmd for macOS) + down arrow to sort ascending</li> <li>Hit Ctrl (Cmd for macOS) + up arrow to sort descending</li> </ul> <p>You can also open the advanced sort menu by clicking Prepare &gt; Sort</p>"},{"location":"ai-studio-tutorial/data-prep-intro/#filtering-data","title":"Filtering Data","text":"<p>There are several ways to filter data: </p> <ul> <li>Click on RM column to view the mini-histogram</li> <li>Click on the left most distribution to view filter menu</li> <li>Select Filter to keep &gt;= 4.605</li> </ul> <p></p> <ul> <li>Click on a cell value in INDUS column, e.g. 6.2</li> <li>Select Filter to keep &lt;</li> </ul> <p>This will remove all INDUS values greater than 6.2</p> <p></p>"},{"location":"ai-studio-tutorial/data-prep-intro/#updating-wrong-values","title":"Updating Wrong Values","text":"<ul> <li>Click on any cell with 12.5 in ZN column  </li> <li>Select Change Values</li> </ul> <ul> <li>Enter 100, click ok</li> </ul> <p>You will see that ALL cells with the value 12.5 in ZN column will be updated to 100</p> <p></p> <p>Please note that Practicus AI does not allow you to update the value of an individual cell only. All updates need to be rule-based. The reason for this is to be able to create production data pipelines. E.g. what you design can be used on fresh data every night, automatically. Individual cell updates do not work for this scenario.</p>"},{"location":"ai-studio-tutorial/data-prep-intro/#formulas","title":"Formulas","text":"<p>Practicus AI supports 200+ Excel compatible functions to write formulas. If you can use formulas in Excel, you can in Practicus AI.</p> <ul> <li>Click on the Formula button to open up the formula designer</li> </ul> <p></p> <ul> <li>Select ISODD function under Math section to find odd numbers  </li> <li>You will be asked to choose a column, select RAD column as Number </li> <li>Click Add</li> </ul> <p>You can use the designer to build the formula or type by hand. You can also create them in Excel and copy / paste. Unlike Excel, you do not use cell references e.g. A5, D8, but column names directly like the below:</p> <p></p> <ul> <li>Leave the column name as suggested: ISODD_RAD</li> <li>Click ok to run the formula </li> <li>You will see a new column named ISODD_RAD added to the dataset</li> <li>Click on ISODD_RAD column to select and hit Ctrl (Cmd in macOS) + left arrow key to move the column to left. Keep doing it until it is next to RAD column</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/data-prep-intro/#advanced-filter","title":"Advanced filter","text":"<ul> <li>Click on INDUS column name to select and then Prepare &gt; Filter</li> <li>Advanced filter designer will open and INDUS column already selected </li> <li>Select &lt;= Less Than and Equal as criteria </li> <li>Click on ... and choose 6.91 select ok </li> <li>Click on Add</li> <li>Now, select our newly created column ISODD_RAD instead of INDUS</li> <li>Leave Is True and click Add</li> </ul> <p>You will see a filter statement is getting created like the below. You can use brackets and keywords such as (, ), and, or, to build the filter you need.  </p> <p></p> <p>After applying your filter you will see the new data set.</p> <ul> <li>Click on INDUS column</li> <li>Hit Ctrl (or Cmd) + up arrow to sort descending</li> <li>Hit Ctrl (or Cmd) + right arrow to move the column next to RAD</li> </ul> <p>You can view the columns we are working on listed together, like the below </p> <p></p>"},{"location":"ai-studio-tutorial/data-prep-intro/#viewing-and-updating-steps","title":"Viewing and updating Steps","text":"<p>You can make mistakes while manipulating data, and it is possible to fix these without starting from scratch.</p> <ul> <li>Click on Steps button</li> </ul> <p></p> <p>You will view the current steps so far</p> <ul> <li>Select the filter step</li> <li>Hit Edit step button</li> <li>Hint: Practicus AI detects the types of variables and does type conversion automatically. You can see this in steps 2 and 3.</li> </ul> <p></p> <ul> <li>Change the filter value from 6.91 to 5</li> <li>Click ok</li> </ul> <p></p> <p>You will see the updated step in green</p> <ul> <li>Click ok to make the change</li> </ul> <p></p> <p>You will see that INDUS column is now less than 5</p> <p></p> <p>Please note that updating steps will reset the column order, such as moving columns left / right or hiding them.</p> <p>Instead of opening the Steps dialog you can quickly undo / redo as well:</p> <ul> <li>Hit Ctrl (or Cmd) + Z to undo a step. </li> <li>Do it a few more times, you will see data is updated automatically </li> <li>Now, hit Ctrl (or Cmd) + Y few times to redo the steps you undid</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/data-prep/","title":"Data Preparation","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/data-profiling/","title":"Data Profiling","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Profiling your data is an extremely powerful way to get a good understanding of your data distribution and correlations.</p> <ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; insurance.csv</li> <li>Select Analyze &gt; Profile </li> </ul> <p></p> <p>After a few minutes, you will see the data profile completed.</p> <p>You will see several statistical calculations about this data. </p> <p></p> <p>When you click Alerts, you will see various alerts, such as high correlation.</p> <p></p> <p>Click Variables on the up-right tab, and Select Variables as Age</p> <p></p> <p>When you click Correlations on the up-right tab, you will see a correlation heatmap.</p> <ul> <li>If you want to see the correlation matrix, you can click Table.</li> </ul> <p></p> <p>In this example, all correlations are positive, so they are displayed with blue. Negative correlations are displayed in red. E.g. If we had a column about how fast someone can run, age column would probably have a negative correlation. I.e. if someone is younger, they would run faster.</p>"},{"location":"ai-studio-tutorial/data-profiling/#profiling-for-difference","title":"Profiling for difference","text":"<p>You can make changes to your data and then create a profile to compare with the original.</p> <ul> <li>Go back to the insurance dataset </li> <li>Under smoker column click on a cell that has the no value </li> <li>Select Filter to keep cell value</li> <li>Select Analyze &gt; Profile again</li> <li>You will be asked if you would like to compare with the original data set, select yes</li> </ul> <p></p> <p>You will now see all the statistics for the original and current data and can compare them side-by-side. </p> <p></p> <p>Click Variables on the up-right tab, and Select Variables as Age</p> <ul> <li>See the descriptive statistics of the comparison of the old version of the age variable with the new version. </li> <li>In addition, you can click More Details to see statistics, histograms, etc.</li> <li></li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/deploy/","title":"Deployment to Production","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/develop_ml_models/","title":"Develop Machine Learning Models with Jupyter Notebooks","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>This section will provide information on how technical and non-technical users can easily intervene in the code and improve the machine learning models they build.</p> <p>You have set up the model and everything is fine. You can either complete the model at this point and save it, or you can easily intervene in the generated Jupyter Notebook code to improve the model. </p> <ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; titanic.csv</li> <li>Select Model &gt; Predictive Objective </li> <li>Choose Objective Column as Survived and Technique should be Classifications</li> <li>Click OK</li> <li>After the model build is completed you will see a dialog</li> <li>Select Open Jupyter Notebook to experiment further</li> </ul>"},{"location":"ai-studio-tutorial/develop_ml_models/#scale-and-transform","title":"Scale and Transform","text":"<p>In the Scale and Transform section, you can rescale the values of numeric columns in the dataset without distorting differences in the ranges of values or losing information.</p> <p></p> <p>Parameters for Normalization in shortly:</p> <ul> <li>You must first set normalize to True</li> <li>You can choose z-score, minmax, maxabs, or robust methods for normalize</li> </ul> <p>Parameters for Feature Transform in shortly: </p> <ul> <li>You must first set normalize to True</li> <li>You can choose yeo-johnson or quantile methods.</li> <li>For the Target Transform you can choose yeo-johnson or quantile methods</li> </ul> <p></p> <p>If you want to have deeper knowledge for Scale and Transform, you can review this link</p>"},{"location":"ai-studio-tutorial/develop_ml_models/#feature-engineering","title":"Feature Engineering","text":"<p>In the Feature Engineering section, you can automatically try new variables and have them improve the accuracy of the model. You can also apply rare encoding to data with low frequency</p> <p></p> <p>Parameters for Polynomial Features in shortly:</p> <ul> <li>You must first set polynomial_features to True</li> <li>polynomial_degree should be int</li> </ul> <p>Parameters for Bin Numeric Features in shortly:</p> <ul> <li>bin_numeric_features should be list</li> </ul> <p>Parameters for Combine Rare Levels in shortly:</p> <ul> <li>rare_to_value: float, default=None</li> <li>rare_value: default='rare'</li> </ul> <p></p> <p>If you want to have deeper knowledge for Feature Engineering, you can review this link</p>"},{"location":"ai-studio-tutorial/develop_ml_models/#feature-selection","title":"Feature Selection","text":"<p>In the Feature Selection section, you can set which variables to include in the model and exclude some variables based on the relationships between the variables.</p> <p></p> <p>Parameters for Remove Multicollinearity in shortly:</p> <ul> <li>You must first set remove_multicollinearity to True</li> <li>multicollinearity_threshold should be float</li> </ul> <p>Parameters for Principal Component Analysis in shortly:</p> <ul> <li>You must first set pca to True</li> <li>pca_method should be linear, kernel, or incremental</li> <li>pca_components should be None, int, float, mle</li> <li>Hint: Minka\u2019s MLE is used to guess the dimension (ony for pca_method='linear')</li> </ul> <p>Parameters for Ignore Low Variance in shortly:</p> <ul> <li>low_variance_threshold should be float or None.</li> </ul> <p>If you want to have deeper knowledge for Feature Selection, you can review this link</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/end-advanced/","title":"End of Advanced Topics","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/end-basic/","title":"End of Basic Tutorial","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/engines/","title":"Data Processing Engines","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/excel-prep/","title":"Excel Prep","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/explore/","title":"Exploring Cloud Data Sources","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Once you have a Practicus AI Cloud Worker running and ready, you can access and explore different data sources on the cloud. </p>"},{"location":"ai-studio-tutorial/explore/#cloud-worker-files","title":"Cloud Worker Files","text":"<p>Every Cloud Worker has some local storage that you can use to quickly upload/download to use cloud-only features. </p> <ul> <li>Click on Explore</li> <li>Make sure your Cloud Worker is already selected upper left</li> <li>Click Cloud Worker Files to load content </li> <li>Expand samples folder and click on boston.csv</li> <li>Click Load </li> </ul> <p></p> <p>Since this data is loaded using a Cloud Worker, a copy of it is already on the Cloud Worker. </p> <p>Congrats, now you can use all the Practicus AI features, including advanced AI ones such as AutoML.</p> <p></p> <p>For curious minds: When you make changes to your data using the app, you will see the results instantly on the app, and in parallel the changes are applied on the cloud. This avoids wait time.</p>"},{"location":"ai-studio-tutorial/explore/#uploading-files","title":"Uploading Files","text":"<p>There are multiple ways to quickly upload your data to the cloud, so you can take advantage of cloud-only Practicus AI features or use external services such as Jupyter. </p> <ul> <li>Click on Explore </li> <li>Select Cloud Worker Files</li> <li>Click on New Folder</li> <li>Enter a name, such as my_data</li> <li>Select your newly created folder, my_data </li> <li>Click on Upload button</li> </ul> <p></p> <ul> <li>Select one of our local data samples</li> <li>Home &gt; practicus &gt; samples &gt; data &gt; boston.csv</li> <li>A file Transfer tab will open, click Start Transfer</li> <li>Close after done </li> <li>Go back to Explore tab, click Reload </li> </ul> <p>You will see that your data is now uploaded to the local disk of the Cloud Worker</p> <p></p> <p>Tips:</p> <ul> <li>You can upload / download entire directories </li> <li>Your data is compressed during upload / download, saving on bandwidth and time</li> <li>You can also use Copy / Paste between data sources to quickly move data around</li> </ul>"},{"location":"ai-studio-tutorial/explore/#quick-upload","title":"Quick Upload","text":"<p>Sometimes you will not know in advance if you are going to use a cloud-only feature or not, and start working offline. </p> <p>Practicus AI offers a quick way for you to start offline and continue in the cloud later:</p> <ul> <li>Open Explore tab</li> <li>Click on Local Files and load the below file </li> <li>Home &gt; practicus &gt; samples &gt; data &gt; boston.csv </li> <li>Select CRIM column, hit delete or backspace to delete the column</li> <li>Click on Model button (AutoML) </li> </ul> <p>You will see a note telling this is a cloud-only feature and asks if you would like to upload your data to the cloud.</p> <ul> <li>Select Yes</li> </ul> <p></p> <ul> <li>Select a Cloud Worker, such as the one you created in the previous section</li> <li>Click Ok</li> </ul> <p></p> <p>Your data will be uploaded AND all your steps (such as deleting CRIM column) will be applied. </p> <ul> <li>Click Model button again to verify it now works</li> <li>Click cancel, we will visit AutoML later</li> </ul>"},{"location":"ai-studio-tutorial/explore/#working-with-s3","title":"Working with S3","text":"<p>S3 is a very common location for Big Data and AI/ML workloads. Your AWS account might already have access to some S3 buckets (locations)  </p> <ul> <li>Open Explore tab</li> <li>Click on S3</li> <li>Select Load Buckets under Bucket</li> <li>In as moment, you will see all the S3 buckets you have access to under Bucket</li> </ul> <p></p> <p>Notes: </p> <ul> <li>If you need to use a bucket under another AWS account, you can select I will enter my credentials under Cloud Config to manually enter security credentials</li> <li>If you would like to learn how to create a bucket please visit the AWS guide</li> <li>If you would like to experiment using public S3 buckets, please check this registry</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/gpu/","title":"GPU Acceleration","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/model/","title":"Modeling with AutoML","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <ul> <li>You can use Practicus AI for both supervised and unsupervised learning.</li> <li>Hint: In supervised learning, your dataset is labeled, and you know what you want to predict.  In unsupervised learning, your dataset is unlabeled, and you don't know what to do. But with Practicus AI, you can switch from unsupervised to supervised learning.</li> </ul>"},{"location":"ai-studio-tutorial/model/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; insurance.csv and Load </li> <li>Click Model button</li> </ul> <ul> <li>View the below optional sections, and then click ok to build the model</li> </ul>"},{"location":"ai-studio-tutorial/model/#optional-building-excel-model","title":"(Optional) Building Excel Model","text":"<p>By default, models you build can be consumed using Practicus AI app or any other AI system. If you would like to build an Excel model, please do the following. Please note that this feature is not available for the free cloud tier.</p> <ul> <li>In the Model dialog click on Advanced Options</li> <li>Select Build Excel model and enter for how many rows, such as 1500</li> </ul>"},{"location":"ai-studio-tutorial/model/#optional-explaining-models","title":"(Optional) Explaining Models","text":"<p>If you would like to build graphics that explain how your model works, please do the following. Please note that the free tier cloud capacity can take a long time to build these visualizations.  </p> <ul> <li>In the Model dialog click on Advanced Options</li> <li> <p>Select Explain</p> </li> <li> <p>Click ok to start building the model</p> </li> <li> <p>Hint: The Select Top % features by importance setting only includes the most important variables in the model. If two variables are highly correlated, then the model can already predict the target variable with one variable, so the other variable is not included.</p> </li> </ul> <p>If you choose the optional sections, model dialog should look like the below:</p> <p></p> <p>You should see a progress bar at the bottom building the model. </p> <p></p> <p>For a fresh Cloud Worker with regular size (2 CPUs) the first time you build this model it should take 5-10 minutes to be completed. Subsequent model runs will take less time. Larger Cloud Workers with more capacity build models faster and more accurately.  </p> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select all the options</li> <li>Click ok</li> </ul> <p></p> <p>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</p> <p></p> <p>If you requested to build an Excel model, you will be asked if you want to download. </p> <p></p> <p>We will make predictions in the next section using these models, or models that other built. </p>"},{"location":"ai-studio-tutorial/model/#optional-reviewing-model-experiment-details","title":"(Optional) Reviewing Model Experiment Details","text":"<p>If you chose to explain how the model works: </p> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Navigate to relevant graphics, for instance Feature Importance</li> </ul> <p></p> <p>The above tells us that an individual not being a smoker (smoker = 0), their bmi, and age are the most defining features to predict the insurance charge they will pay.</p> <p>Note: You can always come back to this screen later by opening Cloud Workers tab, clicking on MLflow button and finding the experiment you are interested with. </p>"},{"location":"ai-studio-tutorial/model/#optional-downloading-model-experiment-files","title":"(Optional) Downloading model experiment files","text":"<p>You can always download model related files, including Excel models, Python binary models, Jupyter notebooks, model build detailed logs, and other artifacts by going back to Explore tab and visiting Home &gt; models</p> <p>You can then select the model experiment you are interested, and click download  </p> <p></p>"},{"location":"ai-studio-tutorial/model/#optional-saving-models-to-a-central-database","title":"(Optional) Saving Models to a central database","text":"<p>In the above steps, the model we built are stored on a Cloud Worker and will disappear if we delete the Cloud Worker without downloading the model first. This is usually ok for an ad-hoc experimentation. A better alternative can be to configure a central MLflow database, so your models are visible to others, and vice versa, you will be easily find theirs. We will visit this topic later.    </p>"},{"location":"ai-studio-tutorial/model/#modelling-with-time-series","title":"Modelling with Time Series","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; airline.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select Explain </li> </ul> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select Predict on Current data and entry Forecast Horizon. It sets how many periods ahead you want to forecast in the forecast horizon.</li> <li>Click ok</li> </ul> <p></p> <p>Select Visualize after predicting and see below graph.</p> <ul> <li>The orange color in the graph is the 12-month forecast result.</li> </ul> <p></p> <ul> <li>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/model/#modelling-with-anomaly-detection","title":"Modelling with Anomaly Detection","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; unusual.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select all the options</li> <li>Click ok</li> </ul> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select all the options</li> </ul> <p></p> <ul> <li>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</li> </ul> <p></p> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Click t-SNE(3d) Dimension Plot</li> </ul> <p>You can hover over this 3D visualization and analyze the anomaly results.</p> <p></p>"},{"location":"ai-studio-tutorial/model/#modelling-with-segmentationclustering","title":"Modelling with Segmentation(Clustering)","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; customer.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select all the options</li> <li>Click ok</li> </ul> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Click Elbow Plot</li> <li>See the optimal number of clusters</li> <li>Hint: The elbow method is a heuristic used to determine the optimum number of clusters in a dataset. </li> </ul> <ul> <li>Click Distribution Plot</li> <li>You can hover over this Bar chart</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/next-steps/","title":"Next Steps","text":"<p>Congrats on completing Practicus AI Studio tutorial!</p> <p>There are still many topics that we haven't covered in our tutorial, and we will be adding sections about these in the near future. So please be on the lookout.</p> <p>Please feel free to experiment with these features yourself, since many of them are very intuitive to use.</p> <p>We have several videos where you can learn about Practicus AI. Please view them on the Demo Videos section.</p> <p>Some topics we haven't covered are:</p>"},{"location":"ai-studio-tutorial/next-steps/#advanced-data-preparation","title":"Advanced Data Preparation","text":"<ul> <li>Joins: You can join any data, from different data sources. E.g. You can join data on S3 to data on a relational database. Please view the join help page in our documentation to learn more. </li> <li>Handling missing values</li> <li>Machine learning related tasks</li> </ul>"},{"location":"ai-studio-tutorial/next-steps/#excel-prep","title":"Excel Prep","text":"<p>If you work with someone who doesn't use Practicus AI, you can share any data over Excel / Google Sheets with them, so they can make changes and send back to you. You can then detect their changes and apply as steps to any data.</p> <p>Please view Excel prep help page to learn more</p>"},{"location":"ai-studio-tutorial/next-steps/#python-coding","title":"Python Coding","text":"<p>You can use the built-in Python editor in Practicus AI app to extend for any functionality. This feature doesn't require the cloud and the app comes with all the relevant libraries pre-installed such as pandas, and requests for API calls.</p>"},{"location":"ai-studio-tutorial/next-steps/#deployment-to-production","title":"Deployment to production","text":"<p>Once you finish preparing your data, building models, making predictions, and other tasks, you can automate what you have designed, so it can run automatically on a defined schedule such as every night or every week.</p> <p>Please view the following pages to learn more:</p> <ul> <li>Code Export (Deploy)</li> <li>Airflow Integration</li> <li>Modern Data Pipelines</li> </ul>"},{"location":"ai-studio-tutorial/next-steps/#observability","title":"Observability","text":"<p>Please view logging help page to learn more.</p>"},{"location":"ai-studio-tutorial/next-steps/#gpu-acceleration","title":"GPU Acceleration","text":"<p>You can choose to use GPU powered Cloud Workers by simply selecting Accelerated Computing family while launching a Cloud Worker. </p>"},{"location":"ai-studio-tutorial/next-steps/#data-processing-engines","title":"Data Processing Engines","text":"<p>If you are a technical user, you can choose an engine of your choice, including pandas, DASK, RAPIDS, RAPIDS+DASK and SPARK. Simply click the Advanced button before loading data. </p>"},{"location":"ai-studio-tutorial/next-steps/#advanced-sampling","title":"Advanced Sampling","text":"<p>If you work with very large data, you can load a different sample size on the Cloud Worker. E.g. original data source can be 1 billion rows, you can read 200 million on the cloud and 1 million on the app. Simply click the Advanced button before loading data.      </p> <p>This concludes our tutorial. Thank you for reading!</p> <p>&lt; Previous</p>"},{"location":"ai-studio-tutorial/observability/","title":"Observability","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/plot/","title":"Introduction to Data Visualization","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Plotting datasets visually aids in data exploration, revealing patterns, and relationships. Thus, it is very important for decision-making, storytelling and insight generation. For that Practicus AI give you Plot service which plots the data in worker as well as in app.</p> <p>Let's have a look of Plot basics by loading salary.csv.  We will ignore the meaning of this dataset since we will only use it to explain basics of plot.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open salary.csv :</li> <li>samples &gt; salary.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>After loading the data set, click on Plot button to start ploting service.</p> <p></p>"},{"location":"ai-studio-tutorial/plot/#basics-of-plot","title":"Basics of Plot","text":"<p>The first thing we will see at Plot tab is going to be Data Source, from this menu we can select the data sheet which we want to visualize.</p> <ul> <li>Click Data Source drop down menu</li> <li>select salary</li> </ul> <p>After choosing the data sheet then we could chose graphic from Graphic drop down menu which we want use while working on visualizing.</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> </ul> <p></p> <p>After choosing the graphic style we want to work with we will see the options listed down below:</p> <ul> <li>Sampling: This option refers to a subset of data set selected from a larger dataset to represent its characteristics. Smaller samples in large data sets can be plotted more quickly, enhancing the efficiency of exploratory data analysis.</li> <li>X Coordinate: This option refers to the horizontal axis of the plot, representing the column(s) of the data set. Within Bar and H Bar graphic styles axis could get string columns as well as numerical columns.</li> <li>Y Cooridnate: This option refers to the vertical axis of the plot, also representing column(s).</li> <li>Color: This option refers to color which will be the filling of shapes within selected graphic style.</li> <li>Size: This option refers to size of the shapes within selected graphic style,  with the exception of the Bar and H Bar graphic styles, where size refers to the spacing between bars.</li> </ul> <p>Let's have a quick look to these options with a simple examle.</p> <ul> <li>Click to  X Coordinate drop down menu and select YearsExperience</li> <li>Click to  Y Coordinate drop down menu and select Salary</li> <li>Click to Add Layer</li> </ul> <p></p> <p>In the end we will have the plot down below:</p> <p></p>"},{"location":"ai-studio-tutorial/plot/#advanced-techniques-in-plot","title":"Advanced Techniques in Plot","text":"<p>In this section we will try to have look to more advance techniques we can use in Plot such as adding multiple layer of visualizing, dynamic size and color, transparency, tooltip and usage of Geo Map graphic style.</p>"},{"location":"ai-studio-tutorial/plot/#dynamic-size-color","title":"Dynamic Size &amp; Color","text":"<p>One of the most illustrative datasets for demonstrating dynamic size and color options is the Titanic dataset. Let's load it into one of our worker environments.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open titanic.csv :</li> <li>samples &gt; titanic.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>The Titanic dataset is a popular dataset used in machine learning and data analysis. It contains information about passengers aboard the RMS Titanic, including whether they survived or not. Within this data set we will use Circle graphic style from Plot and columns of pclass, fare, age and survived. Let's describe what these columns means for better understanding.</p> <ul> <li>Pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).</li> <li>Fare: Passenger fare.</li> <li>Age: Passenger's age in years.</li> <li>Survived: Indicates whether the passenger survived or not (0 = No, 1 = Yes).</li> </ul> <p>Let's start our plotting journey,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select titanic</li> <li>Click Graphic drop down menu</li> <li>Select Circle</li> <li>Click to Advanced</li> </ul> <p></p> <p>After open up advanced section you will see the options of Dynamic Size and Dynamic Color. Dynamic size and color in a circle plot refer to adjusting the size and color of circles based on additional variables, beyond the x and y coordinates. Let's have look with the example of titanic data set.</p> <ul> <li>Click to  X Coordinate drop down menu and select age</li> <li>Click to  Y Coordinate drop down menu and select fare</li> <li>Click to Dynamic size drop down menu and select pclass</li> <li>Click to Dynamic color drop down menu and select survive</li> <li>Click to Add Layer</li> </ul> <p>We can deduce from the analysis that passengers with smaller data points (indicating lower values of \"Pclass\") paid higher fares and had a better chance of survival. Moreover, it's evident that passengers with lower ages (on the X-axis) had a higher likelihood of surviving.</p>"},{"location":"ai-studio-tutorial/plot/#analyze-over-multiple-layer","title":"Analyze over multiple layer","text":"<p>One of the most illustrative datasets for demonstrating multiple layer anlyze is the Iris dataset. Let's load it into one of our worker environments.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open iris.csv :</li> <li>samples &gt; iris.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>The Iris dataset is a popular dataset in machine learning and statistics, often used for classification tasks. It consists of 150 samples of iris flowers, each belonging to one of three species: Setosa, Versicolor, or Virginica. Within this data set we will use both Bar and Circle graphic style from Plot. The dataset comprises four features, each representing measurements of the length and width of both the petals and sepals of flowers.</p> <p>Before start let's use label encode and group by on the species and for better visualisation:</p> <ul> <li>Click Snippets</li> <li>Click Advanced</li> <li>Locate and select Label encoder</li> <li>Select species from Text columns drop down menu</li> <li>Click +</li> <li>Click OK</li> </ul> <p></p> <ul> <li>Click Prepare</li> <li>Click Group By</li> <li>Select species from Group by drop down menu</li> <li>Select sepal_length and Mean (Average) from Summarize drop down menus</li> <li>Select sepal_windth and Mean (Average) from Summarize drop down menus</li> <li>Select petal_length and Mean (Average) from Summarize drop down menus</li> <li>Select petal_windth and Mean (Average) from Summarize drop down menus</li> <li>Click OK</li> </ul> <p></p> <p>In the end we should have the table down below:</p> <p></p> <p>Let's start plotting for multiple layer analyze,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select iris</li> </ul> <p>For first layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Bars</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select sepal_length_mean</li> <li>Click to Advanced</li> <li>Select greenish color from Color drop down menu</li> <li>Enter a value of 50 for the Transparency % input</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For second layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select sepal_windth_mean</li> <li>Click to Advanced</li> <li>Select a darker greenish color from Color drop down menu</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For third layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Bars</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select petal_length_mean</li> <li>Click to Advanced</li> <li>Select blueish color from Color drop down menu</li> <li>\u00canter a value of 50 for the Transparency % input</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For fourth layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select petal_windth_mean</li> <li>Click to Advanced</li> <li>Select a darker blueish color from Color drop down menu</li> <li>Click to Add Layer</li> </ul> <p></p> <p>In the end we sould have a plot like down below:</p> <p></p> <p>As we hover over the bars and lines, data point values will be displayed. Additionally, on the right side of plot, there are options available for zooming in, zooming out, and saving the plot.</p> <p>Observing this multi-layer graph, it becomes evident that both sepal length and petal length play a crucial role in distinguishing between classes. Similarly, the same differentiation can be observed for petal width.</p>"},{"location":"ai-studio-tutorial/plot/#geo-map-tutorial","title":"Geo-map Tutorial","text":"<p>To use the Geo-map feature of Plot, the initial requirement is to define the Google Maps API either through the admin console or within the application itself. If you don't know how to retrieve a Google Maps API key you can check Google's documentetion.</p> <p>Defining a Google Maps API over admin console:</p> <ul> <li>Open Admin Console of Practicus AI</li> <li>Expand (Click) Definitions from left menu</li> <li>Click Cluster Definitions</li> <li>Click GOOGLE_MAPS_API_KEY from table</li> </ul> <p></p> <ul> <li>Enter your key to Value input</li> <li>(Optional) Enter a description to Description input</li> <li>Click Save</li> </ul> <p></p> <p>Defining a Google Maps API within application:</p> <ul> <li>Click Settings frop top menu</li> <li>Click Other tab from opened window</li> <li>Enter your Google Maps API to Personal API Key at down below</li> <li>Click Save</li> </ul> <p></p> <p>After assigning the Google Map API we could have a look to Geo-Map by using car_insurance dataset. This dataset contains information about the insurance company's past customers who have purchased health insurance.  The objective is to use this dataset to train a predictive model that can determine whether these past customers would also be interested in purchasing vehicle insurance from the same company.</p> <p>The features can be listed as:</p> <p>id: A unique identifier assigned to each customer. Gender: The gender of the customer. Age: The age of the customer. Driving_License: Indicates whether the customer possesses a driving license. Region_Code: A distinct code assigned to denote the region of the customer. Previously_Insured: Indicates whether the customer already holds vehicle insurance. Vehicle_Age: The age of the vehicle. Vehicle_Damage: Indicates whether the customer's vehicle has been damaged in the past. Annual_Premium: The yearly premium amount that the customer is required to pay. Policy_Sales_Channel: An anonymized code representing the outreach channel used to contact the customer, including different agents, mail, phone, and in-person visits. Vintage: The duration, in days, for which the customer has been associated with the company. Response: Indicates customer interest. 1 indicates interest, while 0 signifies no interest.</p> <p>Let's load the dataset to our worker:</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open airports.csv :</li> <li>samples &gt; car_insurance.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>Before start let's group the data on the Regeion_Code column for better visualisation:</p> <ul> <li>Click Prepare</li> <li>Click Group By</li> <li>Select Regeion_Code from Group by drop down menu</li> <li>Select Response and sum from Summarize drop down menus</li> <li>Select Previously_Insured and sum from Summarize drop down menus</li> <li>Select Lat and max from Summarize drop down menus</li> <li>Select Lon and max from Summarize drop down menus</li> <li>Click OK</li> </ul> <p></p> <p>Let's start our plotting journey,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select car_insurance</li> <li>Click Graphic drop down menu</li> <li>Select Geo Map</li> </ul> <p>After selecting the \"Geo Map\" graphic style, we observe four distinct options that set it apart from other graphic styles:</p> <ul> <li>Latitude: Indicates distance north or south of the Equator.</li> <li>Longitude: Specifies distance east or west of the Prime Meridian.</li> <li>Map Type: Indicates Google Maps styles.</li> <li>Zoom: Provides an approximation of the number of miles/kilometers that fit into the area represented by the plot.</li> </ul> <p>Let's try to visualize the relation between Response and Previously_Insured on Google Maps by plotting data from the car_insurance dataset:</p> <ul> <li>Select Lon_max from Longitude drop down menu</li> <li>Select Lat_max from Latitude drop down menu</li> <li>Enter 1500 to Zoom input</li> <li>Click Advanced</li> <li>Select Response_sum from Dynamic Size drop down menu</li> <li>Select Previously_Insured_sum from Dynamic Color drop down menu</li> <li>Click Add Layer</li> </ul> <p></p> <p>Let's say we want to email this plot to someone:</p> <ul> <li>Click on Save from menu at right side</li> <li>Select a file name. e.g. us_flight.png</li> </ul> <p></p> <p>You will get a graphic file saved on your computer.</p>"},{"location":"ai-studio-tutorial/predict/","title":"Making Predictions","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"ai-studio-tutorial/predict/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; insurance.csv and Load </li> <li>Click Predict button</li> </ul>"},{"location":"ai-studio-tutorial/predict/#ai-model-locations","title":"AI Model locations","text":"<p>The AI model you or some other person built can be found on a variety of locations or databases. </p> <p>Practicus AI currently support predicting models in the below locations:</p> <ul> <li>MLflow model database</li> <li>Cloud Worker</li> <li>Your computer</li> <li>S3</li> <li>An API endpoint (for programmers)</li> </ul> <p>In this tutorial we will show MLflow usage. </p>"},{"location":"ai-studio-tutorial/predict/#predicting-using-mlflow-database","title":"Predicting using MLflow database","text":"<ul> <li>To locate the model we just built click on the ... to search for it</li> </ul> <ul> <li> <p>In the search dialog that opens, type insurance and hit enter</p> </li> <li> <p>You will find the AI model you built in the previous section of this tutorial.</p> </li> </ul> <p>Cannot find model? It is probably because you are not using a central MLflow database yet, and you built the AI model using a different Cloud Worker. Please check the using a central MLflow database section below to learn more. For now, please read insurance.csv using the same Cloud Worker that you built the AI model with.  </p> <p></p> <ul> <li>Select the model and click ok</li> </ul> <p>You will see the model location (URI) on screen. Although you can click ok to make the  predictions now, we strongly suggest you load the model details (metadata) first. </p> <ul> <li>Click Load to load model details</li> </ul> <p></p> <ul> <li>If you prefer, make the dialog larger</li> <li>Rename predicted column name to charges predicted since we already have a column named charges. If you don't, predicted values will overwrite the original ones.</li> <li>Confirm the features (columns) requested by the model match the ones that we provide</li> </ul> <p>Note: Sometimes the feature names will not match. E.g. AI model could need a feature named children, and our dataset could have a feature named child. Practicus AI uses a simple AI algorithm to match the features names, but it is possible there is a mistake. It is always a good practice to confirm the matching is correct. </p> <ul> <li>Click ok to predict </li> </ul> <p></p> <p>Your predictions will be made.</p> <p></p>"},{"location":"ai-studio-tutorial/predict/#analyzing-model-error","title":"Analyzing model error","text":"<p>Your AI model will almost always have a margin of error. We can estimate the error we expect this model will make. </p> <ul> <li>Click on Formula button </li> <li>Add the below formula, and name the new column Model Error</li> </ul> <p><pre><code>ABS(col[charges] - col[charges predicted])\n</code></pre> We used ABS() function to calculate absolute value E.g. |-1| = 1 so error is always a positive value. </p> <ul> <li>Click ok </li> </ul> <p></p> <p>You will see the errors our model makes. </p> <ul> <li>Click on Model Error column</li> <li>Move your mouse over the mini-histogram</li> </ul> <p>You will see that in ~75% of the cases, we expect that our model will make an error less than ~$2200</p> <p></p> <ul> <li>Move your mouse to the right on the mini-histogram to see in what % of the cases our model makes not so good predictions</li> <li>Click on one of the bad prediction bars on the mini-histogram, and select Filter to Keep &gt;= ...</li> </ul> <p></p> <p>You will see all the cases where our model did not do a good job.</p> <ul> <li>Select Model Errors column </li> <li>Hit Ctrl (or Cmd) + Up Arrow to sort descending</li> </ul> <p>You will now see the worst predictions located at the top. When you select a column, bottom right hand of the screen shows some basic statistics in addition to the mini-histogram. You will see that in 127 cases our model had some room for improvement.</p> <p></p> <p>This might be a good time to analyze the individuals of the not so good predictions, and see if there is anything we can do to get our data in a better shape. Often, better data will lead to better models.</p> <p>You can use the same analytics tasks to visualize and profile this data to see if there is a pattern you can detect. In some cases, your data will have errors, and fixing these will improve the model. In some other cases, you will find out that you are simply missing key features. E.g. for the insurance example, existing health problems could very well be a key indicator for the charges, and we miss this data. </p> <p>Sadly, there is no standard way to improve AI models. Sometimes a technical machine learning coder can greatly improve a model by tweaking parameters. And in some other cases a domain experts' professional experience to optimize the data will be the answer. For some tough problems, you will most likely need both of these personas do their magic, and collaborate effectively. </p>"},{"location":"ai-studio-tutorial/predict/#optional-understanding-ground-truth","title":"(Optional) Understanding Ground Truth","text":"<p>In most real life scenarios, you will make predictions for unknown situations based on what we already know, the ground truth. </p> <p>Then, \"life will happen\", and you will observe the real outcome. </p> <p>For instance, you can make predictions on which customer you \"might\" lose. This is called customer churn prediction AI use case. 6 months later, you will see that (unfortunately) you really lost some customers. And you will have more up-to-date ground truth. </p> <p>With Practicus AI, you can make regular predictions, for instance weekly or nightly, by deploying your models to production. We will discuss how to do this later. </p> <p>Practicus AI allows you to store your predictions in a database, so you can compare them later with the ground truth life brings, This will give you the real errors your models make, in comparison to the estimated errors we discussed in the previous section.   </p>"},{"location":"ai-studio-tutorial/predict/#optional-understanding-model-drift","title":"(Optional) Understanding Model Drift","text":"<p>If you choose to store your prediction results regularly, and then compare these to the ground truth, AND you do this on a regular basis you will have a good feeling of how your models perform. In some cases, a model that was doing great a while back will start to perform not so good. This is called the model drift.</p> <p>With Practicus AI, you can implement systems to automatically detect which of models start drifting. We will discuss this topic later. </p>"},{"location":"ai-studio-tutorial/predict/#optional-using-a-central-mlflow-database","title":"(Optional) Using a central MLflow Database","text":"<p>In this tutorial we used a local MLflow database on a Cloud Worker, which will disappear when you terminate it. This will be perfectly fine if you are just experimenting. For other cases, you can copy your models from the Cloud Worker manually. Or even better, you can store them in a central database at first place. </p> <p>To store models in a central database and share with others, do the following:  - Create or reuse a cloud database, such as MySQL or Postgresql  - Create an S3 bucket to store model artifacts. These are binary files, which can be very large, so it is not a good idea to store them in a relational database  - Open Practicus AI app, go to settings &gt; MLflow - Enter the connection string of the database - Enter S3 bucket</p> <p>Sample MLflow setup </p> <p></p> <p>When you have a central MLflow database configured, your Cloud Workers will automatically switch to using this database, instead of the local ones. </p> <p>With a central model database you can:</p> <ul> <li>Consume other users models to make predictions</li> <li>View other users experiment results </li> <li>View model explanation visuals </li> <li>Predict using models that are not built with Practicus AI </li> <li>Share Practicus AI models with other systems, so they can make predictions without Practicus AI </li> </ul> <p>Central model databases can greatly democratize how AI models are built, and how they are consumed. </p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/snippets/","title":"Snippets","text":""},{"location":"ai-studio-tutorial/snippets/#what-is-snippets","title":"What is Snippets?","text":"<p>Snippets are blocks of code written in Python, prepared for technical and non-technical users. These snippets are designed to perform specific and frequently encountered programming tasks so that users can manage their data processing faster and more efficiently. Snippets are especially ideal for tasks such as data manipulation because these code blocks eliminate the need for users to rewrite code every time.</p>"},{"location":"ai-studio-tutorial/snippets/#functionality-and-benefits-of-snippets","title":"Functionality and Benefits of Snippets","text":"<ul> <li>Snippets provides standardized and optimized code solutions, allowing users to quickly apply frequently needed operations on data sets with a few clicks. This saves time and minimizes coding errors.</li> </ul>"},{"location":"ai-studio-tutorial/snippets/#advantages-of-using-snippets","title":"Advantages of Using Snippets","text":"<ul> <li> <p>Time Efficiency: Saves time by reducing the need to write code for repetitive tasks.</p> </li> <li> <p>Accessibility: Snippets are open source. Users can share the snippets they create with their teammates or all Practicus AI users.</p> </li> <li> <p>Standardization: Data processing operations are applied in a consistent and repeatable way for each user, ensuring continuity across projects.</p> </li> </ul>"},{"location":"ai-studio-tutorial/snippets/#using-existing-snippets","title":"Using Existing Snippets","text":"<ul> <li> <p>Hint: By default, snippets are located in users' hidden .practicus folder. In this tutorial you will see more than one snippet folder. This is because I have also put the snippets in the other Practicus folder. The advantage is that you can create and test new snippets there and easily customize existing snippets. </p> </li> <li> <p>Open Explore tab </p> </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; airline.csv</li> <li>Click Prepare</li> <li>Select period column and click convert then choose Date Time</li> <li>Click OK</li> <li>Then select Snippets</li> <li>Open the Date time folder and choose Year</li> <li>Then choose Datetime column as period</li> <li>Then set the result name to Year and click on the test, then see the preview.</li> </ul> <p></p> <ul> <li>Then Click OK and see the new variable created</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/snippets/#using-advanced-snippets","title":"Using Advanced Snippets","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; insurance.csv</li> <li>Select Snippets</li> <li>Open the Advanced folder and select Normalize</li> <li>Then choose Number columns as age, bmi, and charges</li> <li>Then set Normalize option as Z-Score Normalization</li> </ul> <ul> <li>Then Click OK and see the new variable created with columnname + _normalized</li> </ul>"},{"location":"ai-studio-tutorial/snippets/#advanced-section-for-technical-users","title":"Advanced Section for Technical Users","text":""},{"location":"ai-studio-tutorial/snippets/#snippets-for-spark-engine","title":"Snippets for SPARK Engine","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; insurance.csv</li> <li>Click customize sampling at the bottom left </li> <li>Click Advanced and set Data Engine as SPARK </li> </ul> <ul> <li>Then click OK and Load</li> <li>Select Snippets</li> <li>Open the Statistical folder and select Corr</li> <li>Then choose Number columns as age, bmi, and charges</li> <li>Then set Corr method option as Pearson (other options kendall and spearman)</li> </ul> <ul> <li>Then Click OK and see that the correlation table is formed </li> </ul> <ul> <li>Hint: Not all snippets work on SPARK Engine. To see which engines the snippets work in, you can look at the Snippets Code at the bottom of the screen when you select a snippet.</li> </ul>"},{"location":"ai-studio-tutorial/snippets/#creating-new-snippets","title":"Creating New Snippets","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Open any csv file</li> <li>Select Snippets and click New at the bottom</li> <li>Then, you will see a template for generating snippets, and in this template you will have access to a detailed explanation of how to create snippets.</li> </ul>"},{"location":"ai-studio-tutorial/sql/","title":"Working with SQL","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"ai-studio-tutorial/sql/#using-sql-with-a-sql-database","title":"Using SQL with a SQL database","text":"<p>This is quite straightforward, if the database you connect to support SQL, you can simply start by authoring a SQL statement. </p> <p>Practicus AI Cloud Workers come with a simple music artists database.</p> <ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select SQLite on the left menu </li> <li>Click Run Query</li> </ul> <p>You will see the result of a sample SQL, feel free to experiment and rerun the SQL</p> <p></p> <p>As you will see later in this tutorial, Practicus AI also allows you to run SQL on the result of your previous SQL, as many times as you need. On a SQL database you would need to use temporary tables to do so, which is a relatively advanced topic.</p>"},{"location":"ai-studio-tutorial/sql/#using-sql-on-any-data","title":"Using SQL on any data","text":"<p>Practicus AI allows you to use SQL without a SQL database. Let's demonstrate using an Excel file. You can do the same on other non-SQL data, such as S3. </p> <ul> <li>Open Explore tab</li> <li>Select Local Files</li> <li>Load Home &gt; practicus &gt; samples &gt; data &gt; worker_aws.xlsx</li> </ul> <p></p> <ul> <li>Click on SQL Query button</li> </ul> <p></p> <ul> <li>After click on the Query tab, the dialog will open. Select Yes.</li> </ul> <p></p> <p>Since SQL is an advanced feature, it will require a Cloud Worker to run. You will be asked if you would like to quickly upload to a Cloud Worker. Click Yes, select a Cloud Worker, and now your Excel file will be on the cloud. </p> <p>Click on SQL Query button again. This time the SQL query editor will be displayed.</p> <ul> <li>Type the below SQL </li> </ul> <pre><code>SELECT \"name\", \"total_price\", \"bang_for_buck\" \nFROM \"node_types\" \nWHERE \"total_price\" &lt; 5\nORDER BY \"bang_for_buck\" DESC\n</code></pre> <p></p> <p>Tip: double-clicking on a column name adds its name to the query editor, so you can write SQL faster. If you select some text before double-clicking, your selected text is replaced with the column name.</p> <ul> <li>Click on Test SQL button</li> </ul> <p>Note: Your first SQL on a particular Cloud Worker (cold run) will take a little longer to run. Subsequent SQL queries will run instantly. </p> <ul> <li>(Optional) Experiment further with the SQL, click</li> <li>When ready, click Apply SQL button</li> </ul> <p>You will get the result of the SQL back in the worksheet.  </p>"},{"location":"ai-studio-tutorial/sql/#optional-visualize-sql-result","title":"(Optional) Visualize SQL result","text":"<ul> <li>Click Analyze &gt; Graph</li> <li>Select bang_for_buck for X and total_price for Y</li> <li>Click ok</li> </ul> <p>You will see the estimated total cost (Practicus AI license cost + Cloud infrastructure cost), and how much cloud capacity value you would expect to get (bang for buck) visualized. </p> <p></p> <p>Note: If you have Practicus AI Enterprise license, your software is already paid for. So this graph would not make any sense.  This is only meaningful for the professional pay-as-you-go license type.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/start/","title":"Practicus AI Studio Tutorial","text":"<p>Welcome to Practicus AI Studio hands-on tutorial. </p>"},{"location":"ai-studio-tutorial/start/#topics-covered-in-this-tutorial","title":"Topics covered in this tutorial","text":"<p>This tutorial will help you get started with the below topics using Practicus AI</p> <ul> <li>Explore and Analyze Big Data</li> <li>Build AI models using AutoML </li> <li>Make predictions using AI models </li> <li>Data preparation to manipulate data interactively </li> </ul>"},{"location":"ai-studio-tutorial/start/#how-can-this-tutorial-help-you","title":"How can this tutorial help you?","text":""},{"location":"ai-studio-tutorial/start/#non-technical-users","title":"Non-technical Users","text":"<p>You will learn to be self-sufficient for most AI and Big Data tasks. If things get complicated, you will also learn to effectively collaborate with technical-users.</p>"},{"location":"ai-studio-tutorial/start/#semi-technical-users","title":"Semi-technical Users","text":"<p>Can you write SQL or Excel formulas? In addition to the above, you will also learn to extend the functionality quite a bit.</p>"},{"location":"ai-studio-tutorial/start/#data-scientists","title":"Data Scientists","text":"<p>If you are ok to click for some tasks, this tutorial will teach you to accelerate plenty, especially for data prep.  If you are a die-hard coder, you can use Practicus AI to get support from others, especially domain experts, for data discovery, preparation, quality, and model consumption. </p>"},{"location":"ai-studio-tutorial/start/#data-ml-engineers","title":"Data / ML Engineers","text":"<p>You can use Practicus AI as a modern and interactive ETL tool. You will also get clean production ready code for all the tasks (Data + AutoML + Predictions) that others perform, technical or not.</p>"},{"location":"ai-studio-tutorial/start/#big-picture-summary","title":"Big Picture Summary","text":"<p>The below is a summary of what this tutorial will cover and for whom. Please check our documentation for other topics. </p> <p></p>"},{"location":"ai-studio-tutorial/start/#before-we-begin","title":"Before we begin","text":"<p>If you are viewing this tutorial inside the Practicus AI app, you can right-click the tutorial tab and open it inside your browser as well. This can be useful if you would like to view the tutorial side-by-side with the app.  </p> <p></p> <p>Let's get started! </p> <p>Next &gt;</p>"},{"location":"ai-studio-tutorial/worker-node-intro/","title":"Introduction to Practicus AI Cloud Workers","text":"<p>Some advanced Practicus AI features require to use software in addition to Practicus AI APP or the developer SDK. In this section we will learn how to use Practicus AI Cloud Workers.</p>"},{"location":"ai-studio-tutorial/worker-node-intro/#what-is-a-cloud-worker","title":"What is a Cloud Worker?","text":"<p>Some Practicus AI features such as AutoML, making AI Predictions, Advanced Profiling and production deployment capabilities require a larger setup, so we moved these from the app to a backend (server) system.  </p>"},{"location":"ai-studio-tutorial/worker-node-intro/#setup-cloud-worker","title":"Setup Cloud Worker","text":"<p>Please check the Setup Guide to learn how to configure Practicus AI Cloud Workers.</p> <p>You can use the free cloud tier for this tutorial, or use containers on your computer as well.</p>"},{"location":"ai-studio-tutorial/worker-node-intro/#launching-a-new-cloud-worker","title":"Launching a new Cloud Worker","text":"<ul> <li>Click on the Cloud button to open the Cloud Workers tab</li> <li>Make sure the selected local for your computer, or the optimal AWS Cloud Region. The closest region geographically will usually give you the best internet network performance</li> <li>Click Start New</li> </ul> <ul> <li>Pick the Cloud Worker Size of your Cloud Worker</li> <li>Click ok to Start new Cloud Worker</li> </ul> <p>The default size will be enough for most tasks. You can also choose the free cloud tier.</p> <p></p> <p>In a few seconds you will see your Cloud Worker is launching, and in 1-2 minutes you will get a message saying your Cloud Worker is ready.</p> <p></p>"},{"location":"ai-studio-tutorial/worker-node-intro/#stopping-a-cloud-worker","title":"Stopping a Cloud Worker","text":"<p>If you use local container Cloud Workers you have less to worry about stopping them.  </p>"},{"location":"ai-studio-tutorial/worker-node-intro/#cloud-workers","title":"Cloud Workers","text":"<p>Similar to electricity, water, or other utilities, your cloud vendor (AWS) will charge you a fee for the hours your Cloud Worker is running. Although Practicus AI Cloud Workers automatically shut down after 90 minutes, it would be a practical approach to shut down your Cloud Workers manually when you are done for the day.</p> <p>For this, you can simply select a clod node and click on the Stop button. The next day, you can select the stopped Cloud Worker, click Start and continue where you are left.</p> <p>Tip: It is usually not a good idea to frequently stop / start instances. Please prefer to stop if your break is at least a few hours for optimal cost and wait time.</p>"},{"location":"ai-studio-tutorial/worker-node-intro/#terminating-a-cloud-worker","title":"Terminating a Cloud Worker","text":"<p>Practicus AI Cloud Workers are designed to be disposable, also called ephemeral. You can choose a Cloud Worker and click Terminate to simply delete everything related to it.</p> <p>Please be careful that if you choose to store data on the local disk of your Cloud Worker, this will also get lost after termination. In this case, you can prefer to copy your data manually, or simply click the Replicate button before terminating a Cloud Worker.</p>"},{"location":"ai-studio-tutorial/worker-node-intro/#optional-using-jupyter-lab","title":"(Optional) Using Jupyter Lab","text":"<p>For technical users.</p> <p>Every Cloud Worker comes with some external services preconfigured, such as Jupyter Lab, Mlflow, Airflow.  </p> <ul> <li>Select a Cloud Worker that is running and ready</li> <li>Click on Jupyter button</li> </ul> <p>This will start the Jupyter Lab service and view inside the app. You can also right-click tab name and select Open in browser to view the notebook on your default browser.</p> <p></p> <p>Notes:</p> <ul> <li>If you shut down the app, the secure connection tunnel to the Cloud Worker notebook service will be lost even if the Cloud Worker continues to run.</li> <li>There are two separate Conda kernels configured for your notebook server. Big data one will have common libraries and data engines, such as DASK, RAPIDS (if you have GPUs) and Spark installed. The ML one, as the name suggests, will have ML related libraries such as scikit-learn, Xgboost, Pycaret ..</li> </ul>"},{"location":"ai-studio-tutorial/worker-node-intro/#optional-using-the-terminal","title":"(Optional) Using the Terminal","text":"<p>For technical users.</p> <p>You can choose a Cloud Worker and click the Terminal button to instantly open up the terminal. You have sudo (super-user) access and this can be a very powerful and flexible way to customize your Cloud Worker.</p> <p></p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/add-on-services/","title":"Add-On Services","text":""},{"location":"ops-tutorial/add-on-services/#overview","title":"Overview","text":"<p>This document outlines how to manage and configure Add-On Services within the Practicus AI platform. It includes steps for adding new services, defining service types, and assigning connections for database, object storage, and Git repositories.</p>"},{"location":"ops-tutorial/add-on-services/#add-add-on-service","title":"Add Add-On Service","text":"<p>To add a new Add-On Service, follow these steps:</p> <ul> <li> <p>Navigate to Add-On Services &gt; Add Add-On Service.</p> </li> <li> <p>Fill in the required fields:</p> </li> <li>Key: Enter a unique key for the service.</li> <li>Name: Provide a user-friendly name for the service.</li> <li>Add-On Service Type: Select the type of service from the dropdown menu.</li> <li> <p>URL: Enter the primary URL of the service.</p> </li> <li> <p></p> </li> <li> <p>Optional fields:</p> </li> <li>Enterprise SSO App: If applicable, select an SSO app to authenticate users with Practicus AI login credentials.</li> <li>Database Connection: Choose a database connection if the service requires database access.</li> <li>Object Storage Connection: Select an object storage connection for data storage.</li> <li>Git Repo Connection: Specify a Git repository connection if the service integrates with a repository.</li> </ul> <p></p> <ul> <li>Click Save, or choose Save and add another to configure additional services.</li> </ul>"},{"location":"ops-tutorial/add-on-services/#add-add-on-service-type","title":"Add Add-On Service Type","text":"<p>To define a new service type, follow these steps:</p> <ul> <li> <p>Navigate to Add-On Services &gt; Add-On Service Types &gt; Add Add-On Service Type.</p> </li> <li> <p>Fill in the fields:</p> </li> <li>Key: Enter a unique identifier for the service type.</li> <li>Name: Provide a descriptive name for the service type.</li> </ul> <p></p> <ul> <li>Click Save, or choose Save and add another to configure additional service types.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/ai_assistants/","title":"AI Assistants","text":""},{"location":"ops-tutorial/ai_assistants/#ai-assistants","title":"AI Assistants","text":"<p>This document explains how to create and configure AI Assistants within the Practicus AI platform. It includes step-by-step instructions for adding new AI Assistants, configuring their connection to underlying APIs or models, and adjusting important inference-related settings (e.g., temperature, max tokens).  </p>"},{"location":"ops-tutorial/ai_assistants/#overview","title":"Overview","text":"<p>An AI Assistant is a configurable endpoint that allows you to integrate different underlying model providers or APIs (such as Practicus AI Model API, Practicus AI App API, or external APIs like OpenAI). By defining an AI Assistant, you centralize the settings and authentication tokens necessary for users to interact with your models or AI services.</p>"},{"location":"ops-tutorial/ai_assistants/#accessing-ai-assistants","title":"Accessing AI Assistants","text":"<ol> <li>In the left-hand navigation menu, expand the Generative AI section.  </li> <li>Click AI Assistants to open the list of existing AI Assistants.  </li> <li> <p>From the AI Assistants overview page, you can view, edit, or create new assistants.  </p> </li> <li> <p>Key: Unique identifier for each AI Assistant.  </p> </li> <li>Name: Human-readable name.  </li> <li>API interface: Indicates the selected connection method (e.g., OpenAI, Auto).  </li> <li>Model API, App API, Custom API: The configured back-end for the AI.  </li> <li> <p>Sort order: The position in which the assistant is displayed in the user interface.</p> <p></p> </li> </ol>"},{"location":"ops-tutorial/ai_assistants/#adding-a-new-ai-assistant","title":"Adding a New AI Assistant","text":""},{"location":"ops-tutorial/ai_assistants/#step-1-open-the-add-ai-assistant-form","title":"Step 1: Open the \"Add AI Assistant\" Form","text":"<ul> <li>Click the + (Add) button in the top-right corner (or select Add AI Assistant if available).  </li> </ul> <p>You will be redirected to the Add AI Assistant page.  </p>"},{"location":"ops-tutorial/ai_assistants/#step-2-fill-out-basic-information","title":"Step 2: Fill Out Basic Information","text":"<ul> <li>Key (Required):  </li> <li>A unique identifier using lowercase letters, digits, or dashes (e.g., <code>assistant-1</code>).  </li> <li>Name (Recommended):  </li> <li>A short, descriptive name for the AI Assistant.  </li> <li>Useful if multiple models or endpoints are being used.  </li> <li>Description (Optional):  </li> <li>A short summary describing the assistant\u2019s behavior, capabilities, or specialized domain.</li> </ul>"},{"location":"ops-tutorial/ai_assistants/#step-3-select-the-interface-and-apis","title":"Step 3: Select the Interface and APIs","text":"<ul> <li>API interface:  </li> <li>Determines which type of endpoint this AI Assistant will connect to.  </li> <li>Options might include <code>Auto</code> (system decides automatically), <code>OpenAI</code>, or other integrated providers.  </li> <li>Model API:  </li> <li>If you want to connect to a Practicus AI Model API, select it here.  </li> <li>If set, the assistant will use user permissions for that specific model.  </li> <li>App API:  </li> <li>If you want to connect to a Practicus AI App API, select it here.  </li> <li>If set, the assistant will use user permissions for that specific app.  </li> <li>Custom API:  </li> <li>Use this field to set a base URL for a non-Practicus API endpoint (e.g., OpenAI\u2019s <code>https://api.openai.com/v1</code>).  </li> </ul>"},{"location":"ops-tutorial/ai_assistants/#step-4-configure-access-tokens-and-parameters","title":"Step 4: Configure Access Tokens and Parameters","text":"<ul> <li>Custom token:  </li> <li>Specify a custom token if the external API requires one (e.g., an API key).  </li> <li>If left blank, a token will be generated automatically for Practicus AI models or apps.  </li> <li>For external APIs (like OpenAI), you must provide your own token.  </li> <li>Model name:  </li> <li>Indicates which model to use if the endpoint can serve multiple models (e.g., <code>llama-3-8b</code>).  </li> <li>Leave blank to use a default or single model.  </li> <li>Temperature:  </li> <li>Controls the randomness of AI-generated text.  </li> <li>Higher values produce more varied output; lower values produce more deterministic output.  </li> <li>Max tokens:  </li> <li>Sets a limit on the number of tokens in the generated response.  </li> <li>Custom config:  </li> <li>An optional JSON field where you can supply additional configuration parameters, including required headers for certain APIs (e.g., <code>OpenAI-Organization</code>).  </li> </ul>"},{"location":"ops-tutorial/ai_assistants/#step-5-sort-order","title":"Step 5: Sort Order","text":"<ul> <li>Sort order (Required):  </li> <li>Determines the display order of AI Assistants in the user interface.  </li> <li>Higher numbers appear later in the list.</li> </ul>"},{"location":"ops-tutorial/ai_assistants/#step-6-save-your-new-ai-assistant","title":"Step 6: Save Your New AI Assistant","text":"<p>At the bottom of the form, you have several options: - Save and add another: Save the current Assistant and start configuring a new one immediately. - Save and continue editing: Save the current Assistant but remain on the same page for further modifications. - Save: Save the current Assistant and return to the AI Assistants list.</p>"},{"location":"ops-tutorial/ai_assistants/#managing-existing-ai-assistants","title":"Managing Existing AI Assistants","text":"<ol> <li>From the AI Assistants list, select the checkbox next to an Assistant or click its name to view details.  </li> <li>Use Select action (if multiple are selected) or the edit icon to make changes to the selected Assistant.  </li> <li>Update the fields (e.g., API interface, tokens, model parameters) as needed.  </li> <li>Click Save to confirm any changes.</li> </ol> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/app-deployment/","title":"Application (App) Deployment","text":""},{"location":"ops-tutorial/app-deployment/#deployment-settings","title":"Deployment Settings","text":""},{"location":"ops-tutorial/app-deployment/#overview","title":"Overview","text":"<p>Deployment settings in the Practicus AI platform define configurations for deploying applications. These settings include worker types, object stores, scaling parameters, and observability options.</p>"},{"location":"ops-tutorial/app-deployment/#adding-a-deployment-setting","title":"Adding a Deployment Setting","text":"<p>To add a new deployment setting, click the Add Deployment Setting button. The following fields are required:</p> <ul> <li>Key: Internal name for the deployment setting.</li> <li>Name: User-friendly name.</li> <li>App Object Store: The object store where app files are located.</li> <li>Worker Type: Specifies the worker size (e.g., <code>2X-Small (1.0GB)</code>).</li> <li>Default Replica: Number of initial replicas for deployment.</li> <li>Auto Scaled: Enables or disables dynamic scaling.</li> <li>Min Replica and Max Replica: Define scaling boundaries.</li> <li>Enable Observability: Toggles observability settings for metrics collection.</li> <li>Log Level: Defines the verbosity of logs (e.g., <code>Default</code>).</li> </ul> <p> </p> <p>Click Save to finalize the configuration.</p>"},{"location":"ops-tutorial/app-deployment/#app-prefixes","title":"App Prefixes","text":""},{"location":"ops-tutorial/app-deployment/#overview_1","title":"Overview","text":"<p>App prefixes group applications under a shared path and configuration. These prefixes simplify app management and API accessibility.</p>"},{"location":"ops-tutorial/app-deployment/#adding-an-app-prefix","title":"Adding an App Prefix","text":"<p>To add a new app prefix, click the Add App Prefix button. Configure the following fields:</p> <ul> <li>Key: Unique identifier for the prefix.</li> <li>Prefix: Path under which all apps are grouped (e.g., <code>apps/finance</code>).</li> <li>Visible Name: Optional user-friendly name for the prefix.</li> <li>Description: Brief description of the apps under the prefix.</li> <li>Icon: Optional Font Awesome icon for visual identification.</li> <li>Sort Order: Determines display order (higher values appear first).</li> </ul> <p></p> <p>Click Save to complete.</p>"},{"location":"ops-tutorial/app-deployment/#application-versions","title":"Application Versions","text":""},{"location":"ops-tutorial/app-deployment/#overview_2","title":"Overview","text":"<p>Application versions allow users to manage multiple iterations of the same app, ensuring flexibility and version control.</p>"},{"location":"ops-tutorial/app-deployment/#managing-versions","title":"Managing Versions","text":"<p>The App Versions page lists all application versions, including their deployment configurations. You can edit or remove versions as needed.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/create-user-group/","title":"Managing Users and Groups","text":"<p>This section requires access to the Practicus AI Admin Console. Please make sure you have the necessary permissions to manage users and groups.</p>"},{"location":"ops-tutorial/create-user-group/#creating-a-user-create-user","title":"Creating a User (Create User)","text":""},{"location":"ops-tutorial/create-user-group/#adding-a-new-user","title":"Adding a New User","text":"<ul> <li>Open the Users tab under Users &amp; Groups in the left-hand navigation menu.  </li> <li>Click the + (Add User) button in the top-right corner.  </li> </ul> <ul> <li>Fill out the following details in the form:  </li> </ul> <ul> <li>Click Save to create the user.</li> </ul>"},{"location":"ops-tutorial/create-user-group/#assigning-permissions-optional","title":"Assigning Permissions (Optional)","text":"<p>After creating the user, you may need to assign roles or permissions:</p> <ul> <li>Locate the newly created user in the Users list and click on their name.  </li> <li> <p>Use the Permissions section to assign groups or specific roles: </p> </li> <li> <p>Click Save after making changes.</p> </li> </ul>"},{"location":"ops-tutorial/create-user-group/#tips-for-managing-users","title":"Tips for Managing Users","text":"<ul> <li>Group Assignments: Assign users to groups for role-based access control.  </li> <li>Permission Updates: Regularly review permissions to ensure compliance with organizational policies.</li> </ul> <p>By following these steps, you can effectively manage user creation and permissions in the Practicus AI Admin Console.</p>"},{"location":"ops-tutorial/create-user-group/#adding-a-group-create-group","title":"Adding a Group (Create Group)","text":""},{"location":"ops-tutorial/create-user-group/#adding-a-new-group","title":"Adding a New Group","text":"<ul> <li> <p>Open the Groups tab under Users &amp; Groups in the left-hand navigation menu.  </p> </li> <li> <p>Click the + (Add Group) button in the top-right corner.  </p> </li> <li> <p>In the Add Group form, provide the following details:  </p> </li> <li>Name: Enter a unique name for the group (e.g., \"Default\", \"Developers\").  </li> <li> <p>Permissions: Use the selector to assign available permissions to the group:  </p> <ul> <li>Select desired permissions from the Available permissions list.  </li> <li>Use the arrow button to move selected permissions to the Chosen permissions list. </li> </ul> </li> <li> <p>Click Save to create the group.</p> </li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/custom-images/","title":"Custom Container Images","text":"<p>This section provides guidance on managing and inserting container images into the Practicus AI platform. These images can be configured for various services like Cloud Workers, App Hosting, Model Hosting, and Workspaces.</p>"},{"location":"ops-tutorial/custom-images/#viewing-existing-container-images","title":"Viewing Existing Container Images","text":"<ul> <li>Navigate to Infrastructure &gt; Container Images in the left-hand navigation menu.  </li> <li>The list displays available container images with the following details:</li> <li>Image: The name of the container image.</li> <li>Service Type: Specifies where the image is used (e.g., Cloud Worker, App Hosting, Model Hosting).</li> <li>Version: The version of the image.</li> <li>Minimum and Recommended Versions: Ensures compatibility with the Practicus AI platform.</li> <li>Image Pull Policy: Specifies how the image is pulled (e.g., Always, Namespace default).    </li> </ul>"},{"location":"ops-tutorial/custom-images/#adding-a-new-container-image","title":"Adding a New Container Image","text":"<ul> <li>Click the + (Add Container Image) button in the top-right corner.</li> <li>Fill out the required fields in the Add Container Image form:</li> <li>Image: Provide the full image address (e.g., <code>registry.practicus.io/practicus/llm-cpu</code>).</li> <li>Service Type: Select the applicable service type (e.g., Cloud Worker, App Hosting).</li> <li>Version: Specify the version of the container image.</li> <li>Name and Description: Provide a short name and optional description for easy identification. </li> </ul>"},{"location":"ops-tutorial/custom-images/#additional-configuration","title":"Additional Configuration","text":"<ul> <li>Configure the following optional fields:</li> <li>Minimum and Recommended Versions: Define version constraints for compatibility.</li> <li>Image Pull Policy: Choose the policy for pulling the container image (e.g., Always).</li> <li> <p>Private Registry Details: Provide credentials and secrets if the image is stored in a private registry:</p> <ul> <li>Secret Name: Kubernetes secret for private registry access.</li> <li>Username and Password: Credentials for authentication.</li> </ul> </li> <li> <p>(Optional) Add a Startup Script to be executed when the container starts. </p> </li> <li> <p>Click Save to finalize the image insertion.</p> </li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/enterprise-sso/","title":"Enterprise SSO","text":"<p>This section describes how to manage applications and configure Single Sign-On (SSO) settings in the Enterprise SSO module.</p>"},{"location":"ops-tutorial/enterprise-sso/#managing-applications","title":"Managing Applications","text":""},{"location":"ops-tutorial/enterprise-sso/#viewing-applications","title":"Viewing Applications","text":"<p>To view the list of existing applications configured for Enterprise SSO:</p> <ul> <li>Navigate to Enterprise SSO in the sidebar menu.</li> <li>Select Applications.</li> <li>A table of applications is displayed with the following columns:</li> <li>Id: Unique identifier of the application.</li> <li>Name: Application name.</li> <li>User: Associated user or administrator for the application.</li> <li>Client Type: Indicates whether the client is Confidential or Public.</li> <li>Authorization Grant Type: Type of OAuth 2.0 grant used by the application (e.g., Authorization Code).    </li> </ul>"},{"location":"ops-tutorial/enterprise-sso/#adding-a-new-application","title":"Adding a New Application","text":"<p>To add a new application:</p> <ul> <li>Click the + button in the top-right corner.</li> <li>Fill in the following fields:</li> <li>Client Id: Automatically generated unique identifier for the client.</li> <li>User: Search and assign a user to the application (optional).</li> <li>Redirect URIs: Specify allowed URIs for redirecting after successful login.</li> <li>Post Logout Redirect URIs: Specify allowed URIs for redirecting after logout.</li> <li> <p>Client Type: Choose between:</p> <ul> <li>Confidential</li> <li>Public </li> </ul> </li> <li> <p>Authorization Grant Type: Select the appropriate grant type:</p> <ul> <li>Authorization Code</li> <li>Implicit</li> <li>Resource Owner Password-Based</li> <li>Client Credentials</li> <li>OpenID Connect Hybrid</li> </ul> </li> <li>Client Secret: Automatically generated secret for confidential clients. Ensure to copy this if it is a new secret.</li> <li>Name: Provide a descriptive name for the application.</li> <li>Skip Authorization: Optionally enable this to bypass authorization prompts.</li> <li>Algorithm: Specify OIDC support algorithm if applicable.</li> <li> <p>Allowed Origins: Add origins allowed to access the application.    </p> </li> <li> <p>Click Save to finalize the application creation or choose other saving options:</p> </li> <li>Save and add another: Save and immediately start configuring a new application.</li> <li>Save and continue editing: Save but remain on the same configuration screen.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/git-config/","title":"Select System Git Repository","text":"<p>This document describes the settings and configuration for managing system Git repositories within Practicus AI.</p>"},{"location":"ops-tutorial/git-config/#overview","title":"Overview","text":"<p>The System Git Repositories section allows you to manage Git repositories for system-level operations such as Airflow DAGs or other shared resources.</p>"},{"location":"ops-tutorial/git-config/#system-git-repository-list","title":"System Git Repository List","text":"<p>The list displays all existing Git repositories, including details such as:</p> <p></p>"},{"location":"ops-tutorial/git-config/#adding-a-new-system-git-repository","title":"Adding a New System Git Repository","text":"<p>To add a new Git repository, click the Add System Git Repository button and fill in the following fields:</p> <ul> <li>Key: (Required) Unique name for the repository.</li> <li>Http(s) URL: URL for the repository (e.g., <code>https://github.com/your_account/repo_name.git</code>).</li> <li>Username: Optional Git username for authentication.</li> <li>Token: Optional password or token for authentication.</li> <li>SSH URL: Optional SSH address for the repository (e.g., <code>git@github.com:your_account/repo_name.git</code>).</li> <li>Branch: (Required) Specify the branch name (e.g., <code>main</code>).</li> <li>Committer Email: Optional email to override the default email used in commits.</li> </ul> <p></p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/mfa/","title":"MFA (Multi-Factor Authentication)","text":"<p>This document explains how to enable, configure, and manage Multi-Factor Authentication (MFA) in the Practicus AI platform. It also covers the relevant settings in the <code>values.yaml</code> file and how to handle potential issues that might arise.</p>"},{"location":"ops-tutorial/mfa/#overview","title":"Overview","text":"<p>MFA adds an extra layer of security by requiring users to present a secondary authentication factor (for example, a TOTP code generated by Google Authenticator or Authy) in addition to their regular credentials. This significantly reduces the risk of unauthorized access, even if passwords are compromised.</p>"},{"location":"ops-tutorial/mfa/#configuring-mfa-in-valuesyaml","title":"Configuring MFA in <code>values.yaml</code>","text":"<p>The main MFA-related settings are defined under <code>userLogin.mfa</code> in your <code>values.yaml</code> file:</p> <pre><code>userLogin:\n  mfa:\n    enabled: true\n    mandatory: false\n    mandatoryForSysAdmins: false\n\n  rememberMe:\n    enabled: true\n    days: 30\n    disabledForSysAdmins: false\n</code></pre> <ul> <li>enabled </li> <li>If <code>true</code>, MFA functionality is available in the system.  </li> <li> <p>If <code>false</code>, MFA is disabled.</p> </li> <li> <p>mandatory </p> </li> <li>If <code>true</code>, all users are required to enroll in MFA. Any user without MFA setup will be prompted to configure it before they can log in.  </li> <li> <p>If <code>false</code>, MFA enrollment remains optional.</p> </li> <li> <p>mandatoryForSysAdmins </p> </li> <li>If <code>true</code>, system administrators (admin/staff) must use MFA.  </li> <li>If <code>false</code>, administrators are treated like regular users for MFA requirements (either optional or mandatory, based on the global setting).</li> </ul>"},{"location":"ops-tutorial/mfa/#remember-me","title":"Remember Me","text":"<p>Below the MFA configuration, you\u2019ll find <code>rememberMe</code> settings, which control how long users can stay logged in without re-entering their credentials:</p> <ul> <li>enabled: If set to <code>true</code>, users can select a \u201cRemember Me\u201d option at login.  </li> <li>days: The number of days users can remain logged in before the system asks for re-authentication.  </li> <li>disabledForSysAdmins: If <code>true</code>, system administrators will not be able to use the \u201cRemember Me\u201d feature.</li> </ul>"},{"location":"ops-tutorial/mfa/#users-and-mfa-status","title":"Users and MFA Status","text":"<p>Under Users &amp; Groups &gt; Users, you can see a list of all users. Commonly, there are columns like MFA and MFA fernet active:</p> <ul> <li>MFA: Indicates whether the user has MFA configured (<code>True</code>) or not (<code>False</code>).  </li> <li>MFA fernet active: Shows whether the user\u2019s MFA data is encrypted with the current active key (<code>True</code>) or not (<code>False</code>).</li> </ul> <p>From this page, you can also perform bulk actions using the Select action menu at the top or bottom. These actions include:</p> <ul> <li>Remove MFA setup: Resets the selected user\u2019s MFA configuration. The user will have to set it up again when they log in next time.  </li> <li>Migrate MFA fernet key to active: If the system was previously using a rolling key for encryption, this action re-encrypts the user\u2019s MFA data using the current active key.</li> </ul> <p></p>"},{"location":"ops-tutorial/mfa/#secretstore-and-fernet-keys","title":"secretStore and Fernet Keys","text":"<p>In the same <code>values.yaml</code>, you will find <code>secretStore</code> settings:</p> <pre><code>secretStore:\n  activeKey: \"\"\n  rollingKey: \"\"\n</code></pre> <ul> <li>activeKey: The encryption key used for all MFA data stored in the system.  </li> <li>rollingKey: A key used for migration (for decrypting data that was encrypted with a previous key).  </li> </ul> <p>Add this key to either <code>activeKey</code> or <code>rollingKey</code> in your <code>values.yaml</code>. After updating, any new MFA data is encrypted with the activeKey, and existing data can be migrated from the rollingKey using the Migrate MFA fernet key to active action in the Users section.</p>"},{"location":"ops-tutorial/mfa/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ops-tutorial/mfa/#nobody-can-log-in-including-admins","title":"Nobody Can Log In (Including Admins)","text":"<p>If MFA is mandatory and an administrator loses access to their MFA device, it can lock everyone out. In this situation, you can manually reset the MFA secret for the relevant user in your database to allow a password-only login again:</p> <ol> <li>Connect to the Database </li> <li> <p>Use the credentials specified for Practicus AI\u2019s database (e.g., PostgreSQL).</p> </li> <li> <p>Remove or Clear the MFA Secret </p> </li> <li> <p>Sign In as Admin and Reconfigure MFA </p> </li> <li>After resetting the MFA secret, the admin user can log in without MFA.  </li> <li>Review or update your MFA settings in <code>values.yaml</code> or via the MFA settings page to prevent similar lockouts.</li> </ol> <p>Important: Always create a backup before making direct changes to the database.  </p> <p>&lt; Previous</p>"},{"location":"ops-tutorial/model-deployment/","title":"Model Deployment Settings","text":""},{"location":"ops-tutorial/model-deployment/#overview","title":"Overview","text":"<p>This document provides details for configuring model deployment settings within the Practicus AI platform. It focuses on essential fields and configurations required to create, modify, and manage model deployments efficiently.</p>"},{"location":"ops-tutorial/model-deployment/#adding-a-model-deployment","title":"Adding a Model Deployment","text":""},{"location":"ops-tutorial/model-deployment/#key-fields","title":"Key Fields","text":"<ul> <li>Key (Required): Unique identifier for the deployment.</li> <li>Name (Required): Human-readable name for the deployment.</li> <li>Model Object Store (Required): The storage system containing the model files.</li> <li>Worker Type (Required): Defines the capacity (e.g., <code>Small</code>, <code>Large</code>) of the worker used for the deployment.</li> <li>Default Replica (Required): Specifies the default number of pods to run.</li> <li>Auto Scaled: Enables dynamic scaling of pod counts based on workload.</li> <li>Min Replica: Minimum number of pods when auto-scaling is enabled.</li> <li>Max Replica: Maximum number of pods when auto-scaling is enabled.</li> <li>Enable Observability: Activates metrics collection for external systems like Prometheus.</li> <li>Log Level: Sets the granularity of logs (e.g., <code>DEBUG</code>, <code>INFO</code>).</li> </ul>"},{"location":"ops-tutorial/model-deployment/#advanced-options","title":"Advanced Options","text":"<ul> <li>Node Selector: Assigns deployments to specific Kubernetes nodes using labels.</li> <li>Custom Image: Allows selecting or defining a custom container image.</li> <li>Startup Script: Shell commands executed before starting the API endpoint.</li> <li>Traffic Log Object Store: Specifies where request data and prediction logs are stored.</li> <li>Deployment Group Accesses: Defines groups with access permissions.</li> <li>Deployment User Accesses: Specifies individual user access rights.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#steps-to-add-a-deployment","title":"Steps to Add a Deployment","text":"<ul> <li>Navigate to ML Model Hosting &gt; Model Deployments.</li> <li>Click Add Model Deployment.</li> <li>Fill in the required fields under the \"Add Model Deployment\" form.</li> <li>Specify advanced configurations, if necessary.</li> <li>Save the deployment using one of the options:</li> <li>Save and add another: Save and open a new form.</li> <li>Save and continue editing: Save and remain on the current form.</li> <li>Save: Save and return to the main list.     </li> </ul>"},{"location":"ops-tutorial/model-deployment/#managing-deployment-settings","title":"Managing Deployment Settings","text":""},{"location":"ops-tutorial/model-deployment/#viewing-deployment-settings","title":"Viewing Deployment Settings","text":"<ul> <li>Navigate to the list of deployments in ML Model Hosting &gt; Model Deployments.</li> <li>Select a deployment to view or modify its settings.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#key-information-displayed","title":"Key Information Displayed","text":"<ul> <li>Model Object Store</li> <li>Worker Type</li> <li>Default Replica Count</li> <li>Observability Settings</li> <li>Traffic Log Object Store</li> </ul>"},{"location":"ops-tutorial/model-deployment/#modifying-deployments","title":"Modifying Deployments","text":"<ul> <li>Select the deployment to modify from the list.</li> <li>Update necessary fields.</li> <li>Save the changes using the appropriate option.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#observability-settings","title":"Observability Settings","text":""},{"location":"ops-tutorial/model-deployment/#core-metrics","title":"Core Metrics","text":"<ul> <li>Enables tracking of essential metrics like request count and total prediction time.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#model-drift-detection","title":"Model Drift Detection","text":"<ul> <li>Activates comparisons between predicted results and ground truth to detect model drift.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#logging","title":"Logging","text":"<ul> <li>Prediction Percentiles: Logs percentiles of prediction results for analysis.</li> <li>Custom Metrics: Allows defining custom metrics in Python.</li> <li>Traffic Logging: Logs request and prediction data in a compatible format.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#advanced-logging-options","title":"Advanced Logging Options","text":"<ul> <li>Log Batch Rows: Sets the number of rows per log batch.</li> <li>Log Batch Minutes: Defines time intervals for flushing logs.  </li> </ul>"},{"location":"ops-tutorial/model-deployment/#model-deployment-best-practices","title":"Model Deployment Best Practices","text":"<ul> <li>Use auto-scaling for deployments with variable workloads.</li> <li>Enable observability for monitoring model performance and drift.</li> <li>Leverage custom images for deployments requiring specialized environments.</li> <li>Use traffic log object store for centralized logging and analysis.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/object-storage/","title":"Object Storage","text":"<p>This document outlines the configuration process for adding and managing system object storages within Practicus AI.</p>"},{"location":"ops-tutorial/object-storage/#overview","title":"Overview","text":"<p>The System Object Storages section enables you to integrate external object storage services (e.g., AWS S3) for managing and accessing data objects, such as model files and other resources.</p>"},{"location":"ops-tutorial/object-storage/#adding-a-new-object-storage","title":"Adding a New Object Storage","text":"<p>To add a new object storage, click the Add System Object Storage button and fill in the following fields:</p> <ul> <li>Key: (Required) A unique name for the object storage used for administrative purposes.</li> <li>Endpoint URL: (Required) The URL of the object storage service. For AWS S3, use the format <code>https://s3.{aws_region}.amazonaws.com</code> where <code>{aws_region}</code> is the region (e.g., <code>us-east-1</code>).</li> <li>Bucket Name: (Required) The name of the object storage bucket where data will be stored. Models will be stored under the path <code>[endpoint_url]/[bucket_name]/models/[model_host_key]/[model_key]/[version]/[model_files]</code>.</li> <li>Prefix: (Optional) A prefix path to use before accessing objects. For example, if the prefix is <code>some/prefix</code>, objects will be accessed using <code>https://[endpoint]/[bucket]/some/prefix/object.data</code>.</li> <li>Access Key ID: (Required) The access key for the object storage service.</li> <li>Secret Access Key: (Required) The secret key for the object storage service. </li> </ul>"},{"location":"ops-tutorial/object-storage/#actions","title":"Actions","text":"<ul> <li>Save: Saves the new object storage configuration.</li> <li>Save and Add Another: Saves the configuration and opens a new form to add another object storage.</li> <li>Save and Continue Editing: Saves the configuration but keeps the form open for further adjustments.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/resources-management/","title":"Resource Management","text":"<p>This section focuses on managing worker sizes, consumption logs, group limits, and user limits to optimize resource utilization in the Practicus AI platform.</p>"},{"location":"ops-tutorial/resources-management/#worker-sizes","title":"Worker Sizes","text":"<p>The Worker Sizes section allows you to configure and view the available compute resources for tasks. You can define worker sizes with specific configurations, such as CPU cores, memory, GPU availability, and security contexts.</p>"},{"location":"ops-tutorial/resources-management/#adding-a-worker-size","title":"Adding a Worker Size","text":"<ul> <li>Navigate to Infrastructure &gt; Worker Sizes in the left-hand navigation menu.  </li> <li>Click the + (Add Worker Size) button in the top-right corner.  </li> <li>Fill out the required fields in the form, such as:</li> <li>Name: Unique identifier for the worker size.</li> <li>CPU cores and memory: Specify resource allocation.</li> <li>GPU and VRAM: Assign GPU resources if applicable.</li> <li>Click Save to add the worker size. </li> </ul>"},{"location":"ops-tutorial/resources-management/#consumption-logs","title":"Consumption Logs","text":"<p>The Consumption Logs section provides an overview of resource usage, including detailed logs of worker activities. - Fields Available:   - User: Indicates the user utilizing the resource.   - Instance ID: Unique ID of the running worker.   - Size: Worker size (e.g., Small, Medium).   - CPU cores, memory, GPUs: Resource allocation.   - Stop Time and Duration: Logs the completion time and total runtime.  </p> <p>Use this section to monitor resource consumption and analyze trends.</p> <p></p>"},{"location":"ops-tutorial/resources-management/#consumption-summary","title":"Consumption Summary","text":"<p>The Consumption Summary provides an aggregated view of resource usage by users over specific periods (daily, weekly, monthly). - Key Metrics:   - Memory and VRAM usage: Tracks active and total consumption for CPU and GPU resources.   - User-based usage: Detailed summary per user.</p> <p>This feature helps identify heavy resource consumers and optimize allocations.</p>"},{"location":"ops-tutorial/resources-management/#group-limits","title":"Group Limits","text":"<p>The Group Limits section allows you to set resource usage caps for groups. - Steps to Add Group Limits:   - Navigate to Infrastructure &gt; Group Limits.   - Click the + (Add Group Limit) button.   - Specify the following fields:      - Group: Select the group to configure.      - Memory GB active: Define the active memory capacity for the group.      - Daily, weekly, and monthly limits: Set resource caps.      - VRAM limits: Allocate GPU-specific resources if needed.   - Save the changes to apply the limits. </p>"},{"location":"ops-tutorial/resources-management/#user-limits","title":"User Limits","text":"<p>The User Limits section allows you to define individual resource usage caps per user. - Steps to Add User Limits:   - Navigate to Infrastructure &gt; User Limits.   - Click the + (Add User Limit) button.   - Configure the following:      - User: Select the target user.      - Memory and VRAM limits: Define active and total usage thresholds.      - Daily, weekly, and monthly limits: Set specific caps for resources.   - Save the changes to enforce the user limit.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/start/","title":"Practicus AI Operations Tutorial","text":"<p>In this demo we will focus on the operational side of Practicus AI. This tutorial will guide you through how to manage, monitor, and maintain the platform in production at scale. Whether you're running Practicus AI on a public cloud Kubernetes environment such as AWS EKS, Azure AKS, Google GKE, or on-premises solutions like Red Hat OpenShift or Rancher, understanding these operational best practices ensures a stable, scalable system.</p>"},{"location":"ops-tutorial/start/#overview","title":"Overview","text":"<ul> <li> <p>Cloud-Native &amp; On-Prem Flexibility   Practicus AI is fully cloud-native and can be deployed across various Kubernetes-based environments\u2014from major cloud providers to on-premises clusters. </p> </li> <li> <p>Observability &amp; Monitoring   Ability to track logs, metrics, events, and errors across your Practicus AI deployments. By leveraging add-on services like Grafana, you can create real-time dashboards and alerts to ensure continuous uptime and optimal performance.</p> </li> <li> <p>Workflow &amp; Scheduling   Airflow integration provides a robust solution for scheduling and automating complex data pipelines. In an enterprise setting, these workflows often involve cross-team or cross-department coordination\u2014this tutorial shows you how to manage and monitor such tasks seamlessly.</p> </li> <li> <p>Security &amp; Compliance   As part of day-2 operations, you\u2019ll need to ensure that your deployments adhere to security best practices. This includes understanding Kubernetes namespace isolation, role-based access control (RBAC), and any compliance measures your organization must meet.</p> </li> </ul> <p>Next &gt;</p>"},{"location":"ops-tutorial/storage/","title":"Storage (My/Shared Folders)","text":"<p>This section outlines how to manage personal and shared storage configurations in the Practicus AI platform. You can configure storage for individual users or groups, ensuring proper allocation and permissions.</p>"},{"location":"ops-tutorial/storage/#group-storage","title":"Group Storage","text":""},{"location":"ops-tutorial/storage/#viewing-group-storage","title":"Viewing Group Storage","text":"<ul> <li>Navigate to Infrastructure &gt; Group Storage in the left-hand navigation menu.</li> <li>View existing group storage configurations:</li> <li>Storage Class Name: Defines the type of storage being used.</li> <li>Access Mode: Specifies the level of access (e.g., Read Write Many).</li> <li>Storage Prefix: Indicates the directory path assigned to the group (e.g., <code>users/</code>).</li> </ul>"},{"location":"ops-tutorial/storage/#adding-group-storage","title":"Adding Group Storage","text":"<ul> <li>Click + (Add Personal Storage) in the top-right corner.</li> <li>Configure the following fields:</li> <li>Group: Select the group for which the storage is being configured.</li> <li>Storage Class Name: Define the Kubernetes storage class.</li> <li>Access Mode: Choose access level (e.g., Read Write Many).</li> <li>Storage Prefix: Specify the path prefix for the storage.</li> <li>Priority: Set the priority for the group (higher values take precedence).</li> <li>Size (MB): Optionally set a size limit for the storage.</li> <li>Click Save to apply changes. </li> </ul>"},{"location":"ops-tutorial/storage/#group-shared-storage","title":"Group Shared Storage","text":""},{"location":"ops-tutorial/storage/#viewing-group-shared-storage","title":"Viewing Group Shared Storage","text":"<ul> <li>Navigate to Infrastructure &gt; Group Shared Storage in the left-hand navigation menu.</li> <li>View shared storage configurations for groups:</li> <li>Group: Indicates the group using the shared storage.</li> <li>Service Type: Shows whether storage is for Cloud Worker or Workspace.</li> <li>Storage Class Name: Defines the storage class.</li> <li>Share Name: Path of the shared directory (e.g., <code>shared/partner</code>).</li> <li>Access Mode: Specifies access permissions (e.g., Read Write Many).</li> </ul>"},{"location":"ops-tutorial/storage/#adding-shared-storage-for-groups","title":"Adding Shared Storage for Groups","text":"<ul> <li>Click + (Add Shared Storage) in the top-right corner.</li> <li>Fill in the following details:</li> <li>Group: Select the group to configure shared storage for.</li> <li>Service Type: Choose the service type (e.g., Cloud Worker).</li> <li>Storage Class Name: Specify the Kubernetes storage class.</li> <li>Share Name: Define the directory name for shared storage.</li> <li>Access Mode: Set the access level (e.g., Read Write Many).</li> <li>Size (MB): Optionally limit the size of the shared storage.</li> <li>Click Save to finalize the configuration. </li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"technical-tutorial/distributed-computing/introduction/","title":"Distributed Computing with Practicus AI","text":"<p>Distributed computing enables you to leverage multiple machines or compute nodes to process data, train models, or execute tasks in parallel, achieving scalability, efficiency, and speed. This paradigm is crucial for modern workloads such as large-scale data processing, distributed training of machine learning models, and real-time analytics.</p> <p>Practicus AI simplifies distributed computing by offering a flexible adapter-based system. This approach abstracts the complexities of managing distributed jobs while remaining extensible to suit a wide range of use cases.</p>"},{"location":"technical-tutorial/distributed-computing/introduction/#adapter-system","title":"Adapter System","text":"<p>Practicus AI's adapter system acts as a bridge between the platform and various distributed frameworks. It ensures seamless integration and management of distributed jobs, such as:</p> <ul> <li>Data Processing Jobs:  </li> <li>Apache Spark: Process large-scale datasets using distributed Spark jobs, enabling parallel computation across multiple nodes.  </li> <li> <p>Dask: Execute lightweight, distributed computations for real-time analytics or complex workflows.  </p> </li> <li> <p>Distributed Training for Machine Learning:  </p> </li> <li>DeepSpeed: Efficiently fine-tune large language models (LLMs) using distributed GPU resources.  </li> <li>FairScale: Scale your training workflows with advanced memory optimization and model parallelism.  </li> <li> <p>Horovod: Run distributed training jobs across heterogeneous clusters.  </p> </li> <li> <p>Extensibility for Custom Needs:   Practicus AI allows customers to build and integrate custom adapters to extend its capabilities. Whether you\u2019re using a proprietary framework or need to incorporate specialized tools, the adapter system provides the flexibility to tailor distributed computing to your organization\u2019s unique requirements.</p> </li> </ul>"},{"location":"technical-tutorial/distributed-computing/introduction/#key-benefits","title":"Key Benefits","text":"<p>Practicus AI\u2019s distributed computing platform provides the following advantages:</p> <ol> <li> <p>Unified Interface: Manage distributed jobs across various frameworks using a consistent SDK. Whether it\u2019s a Spark job or a DeepSpeed training task, the process remains intuitive and unified.</p> </li> <li> <p>Scalability: Automatically scale compute resources based on workload demands, enabling efficient processing of massive datasets or training of complex machine learning models.</p> </li> <li> <p>Isolation and Control: Each distributed job runs within isolated Kubernetes pods, ensuring secure and reliable execution.</p> </li> <li> <p>Extensibility: The adapter-based system supports existing frameworks and can be extended to accommodate custom tools or workflows.</p> </li> </ol>"},{"location":"technical-tutorial/distributed-computing/introduction/#use-cases","title":"Use Cases","text":"<ul> <li>Data Pipelines: Create distributed pipelines to process petabytes of data with Spark or Dask, optimizing performance through parallelism and resource allocation.</li> <li>Model Training: Fine-tune large models like GPT or BERT using DeepSpeed, FairScale, or custom frameworks, ensuring efficient memory utilization and scaling across GPU clusters.</li> <li>Custom Workflows: Build proprietary adapters for niche frameworks or processes, integrating them seamlessly with Practicus AI\u2019s distributed system</li> </ul> <p>Previous: Build | Next: Spark &gt; Interactive &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/start-cluster/","title":"Customizing Distributed Clusters with Custom Adaptors","text":"<p>This example demonstrates how to customize Practicus AI distributed cluster or job engine by creating a subclass of one of the existing classes and overriding its methods and properties as needed.</p>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>worker_size = None\nworker_count = None\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/start-cluster/#copy-the-contents-of-this-folder-to-mycustom_adaptor","title":"Copy the contents of this folder to ~/my/custom_adaptor","text":"<pre><code>import os\n\nassert os.path.exists(\"/home/ubuntu/my/custom_adaptor/my_adaptor.py\"), (\n    \"Please copy the contents of this folder to ~/my/custom_adaptor\"\n)\n</code></pre> <pre><code># Understanding methods and properties to override\n# Let's customize SparkAdaptor\nfrom practicuscore.dist_job import SparkAdaptor\n\nprint(\"SparkAdaptor methods and properties:\")\nprint(dir(SparkAdaptor))\n\n# Please view my_adaptor.py for an example overriding coordinator and agent startup command.\n</code></pre> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    worker_count=worker_count,\n    # Let's change job_type to custom\n    job_type=prt.DistJobType.custom,\n    # Job directory must have the .py file of our custom adaptor\n    job_dir=\"/home/ubuntu/my/custom_adaptor\",\n    # MySparkAdaptor class in my_adaptor.py\n    custom_adaptor=\"my_adaptor.MySparkAdaptor\",\n)\n\n# Let's define worker features of the cluster\nworker_config = prt.WorkerConfig(\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n    # Turn on debug logging so we can troubleshoot custom adaptor issues.\n    log_level=\"DEBUG\",\n)\n\n# Creating the coordinator (master) worker\n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Spark cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next notebook in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator\n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/start-cluster/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/custom-adaptor/start-cluster/#my_adaptorpy","title":"my_adaptor.py","text":"<pre><code>import practicuscore as prt\nfrom practicuscore.dist_job import SparkAdaptor\n\n\nclass MySparkAdaptor(SparkAdaptor):\n    @property\n    def _run_cluster_coordinator_command(self) -&gt; str:\n        old_command = super()._run_cluster_coordinator_command\n\n        # Change the command as needed\n        new_command = old_command + \" # add your changes here\"\n\n        return new_command\n\n    @property\n    def _run_cluster_agent_command(self) -&gt; str:\n        old_command = super()._run_cluster_agent_command\n\n        new_command = old_command + \" # add your changes here\"\n\n        return new_command\n</code></pre> <p>Previous: Use Cluster | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/","title":"Use Cluster","text":""},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/#using-the-interactive-spark-cluster-client","title":"Using the interactive Spark Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Spark cluster we created, and execute simple Spark operations.</li> <li>Please run this example on the <code>Spark Coordinator (master)</code>.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/#custom-cluster-setup","title":"Custom Cluster Setup","text":"<p>Since you are working with a custom cluster, the usual method (<code>spark = prt.distributed.get_client()</code>) does not automatically create a Spark session. You have two options:</p>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/#option-1-manually-create-a-spark-session-relatively-harder","title":"Option 1: Manually Create a Spark Session (relatively harder)","text":"<p>In this option, you manually create a Spark session by specifying the master URL, port, and any additional configurations.</p> <pre><code>master_addr = f\"prt-svc-wn-{coordinator_instance_id}\"\nmaster_port = coordinator_port\nspark_master_url = f\"spark://{master_addr}:{master_port}\"\nconf = ...  # other configuration settings\nspark = SparkSession.builder \\\n    .appName(\"my-spark-app\") \\\n    .master(spark_master_url) \\\n    .config(conf=conf) \\\n    .getOrCreate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/#option-2-patch-the-job-type-back-to-spark-relatively-easier","title":"Option 2: Patch the Job Type Back to Spark (relatively easier)","text":"<p>When starting the cluster, set the distributed <code>job_type</code> to <code>custom</code>. Once the Spark cluster is running, you can switch the <code>job_type</code> back to <code>spark</code>.</p> <p>To do this: 1. Load the distributed configuration (which is base64 encoded) from the OS environment. 2. Update the configuration. 3. Write the updated configuration back.</p> <p>Note: The change in the OS environment is temporary and will only persist for the current notebook kernel.</p> <pre><code>import os\nimport base64\nimport json\n\ndistributed_conf_dict_b64 = os.getenv(\"PRT_DISTRIBUTED_CONF\", None)\ndistributed_conf_str = base64.b64decode(distributed_conf_dict_b64.encode(\"utf-8\")).decode(\"utf-8\")\ndistributed_conf = json.loads(distributed_conf_str)\n\nprint(\"Current distributed job configuration:\")\nprint(distributed_conf)\n\n# Let's patch job_type\ndistributed_conf[\"job_type\"] = \"spark\"\n\nprint(\"Patched distributed job configuration:\")\nprint(distributed_conf)\n\ndistributed_conf_str = json.dumps(distributed_conf)\ndistributed_conf_dict_b64 = base64.b64encode(distributed_conf_str.encode(\"utf-8\")).decode(\"utf-8\")\n\n# And save it back to OS environment temporarily\nos.environ[\"PRT_DISTRIBUTED_CONF\"] = distributed_conf_dict_b64\n</code></pre> <pre><code>import practicuscore as prt\n\n# Our Spark session code will work as usual\nspark = prt.distributed.get_client()\n</code></pre> <pre><code># And execute some code\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Perform a transformation\ndf_filtered = df.filter(df.Age &gt; 30)\n\n# Show results\ndf_filtered.show()\n</code></pre> <pre><code># Let's end the session\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/#troubleshooting","title":"Troubleshooting","text":"<p>If you\u2019re experiencing issues with an interactive cluster that doesn\u2019t run job/train.py, please follow these steps:</p> <ol> <li> <p>Agent Count Mismatch:    If the number of distributed agents shown by <code>prt.distributed.get_client()</code> is less than what you expected, wait a moment and then run <code>get_client()</code> again. This is usually because the agents have not yet joined the cluster.    Note: Batch jobs automatically wait for agents to join.</p> </li> <li> <p>Viewing Logs:    To view logs, navigate to the <code>~/my/.distributed</code> folder.</p> </li> </ol>"},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/custom-adaptor/use-cluster/#my_adaptorpy","title":"my_adaptor.py","text":"<pre><code>import practicuscore as prt\nfrom practicuscore.dist_job import SparkAdaptor\n\n\nclass MySparkAdaptor(SparkAdaptor):\n    @property\n    def _run_cluster_coordinator_command(self) -&gt; str:\n        old_command = super()._run_cluster_coordinator_command\n\n        # Change the command as needed\n        new_command = old_command + \" # add your changes here\"\n\n        return new_command\n\n    @property\n    def _run_cluster_agent_command(self) -&gt; str:\n        old_command = super()._run_cluster_agent_command\n\n        new_command = old_command + \" # add your changes here\"\n\n        return new_command\n</code></pre> <p>Previous: Start Cluster | Next: Unified DevOps &gt; Introduction</p>"},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/","title":"Executing batch jobs in Dask Cluster","text":"<p>In this example we will: - Create a Dask cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"dask\" under your \"~/my\" folder</li> <li>And copy job.py under this folder</li> </ul> <pre><code>worker_size = None\nworker_count = None\nlog_level = \"DEBUG\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert log_level, \"Please enter your log_level.\"\n</code></pre> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/dask\"\n\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.dask,\n    job_dir=job_dir,\n    py_file=\"job.py\",\n    worker_count=worker_count,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n    log_level=log_level,\n)\n\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n)\n</code></pre> <pre><code># You can view the logs during or after the job is completed\n# To view coordinator (master) set rank = 0\nrank = 0\n# To view other workers set rank = 1,2, ..\n\nprt.distributed.view_log(job_dir=job_dir, job_id=coordinator_worker.job_id, rank=rank)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/#wrapping-up","title":"Wrapping up","text":"<ul> <li>Once the job is completed, you can view the results in <code>~/my/dask/result.csv/</code></li> <li>Please note that result.csv is a folder that can contain <code>parts of the processed file</code> by each worker (Dask executors)</li> <li>Also note that you do not need to terminate the cluster since it has a 'py_file' to execute, which defaults <code>terminate_on_completion</code> parameter to True.</li> <li>You can change terminate_on_completion to False to keep the cluster running after the job is completed to troubleshoot issues.</li> <li>You can view other <code>prt.DistJobConfig</code> properties to customize the cluster</li> </ul>"},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt\nimport dask.dataframe as dd\n\n# Let's get a Dask session\nprint(\"Getting Dask session\")\ndask = prt.distributed.get_client()\n\nprint(\"Reading diamond data\")\ndf = dd.read_csv(\"/home/ubuntu/samples/data/diamond.csv\")\n\nprint(\"Calculating\")\ndf[\"New Price\"] = df[\"Price\"] * 0.8\n\nprint(\"Since Dask is a lazy execution engine,\")\nprint(\" actual calculations will happen when you call compute() or save.\")\n\nprint(\"Saving\")\ndf.to_csv(\"/home/ubuntu/my/dask/result.csv\")\n\n# Note: the save location must be accessible by all workers\n# A good place to save for distributed processing is object storage\n</code></pre> <p>Previous: Use Cluster | Next: Distributed Training &gt; XGBoost</p>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/","title":"Distributed XGBoost with Dask for Scalable Machine Learning","text":"<p>This example showcases the use of Dask for distributed computing with XGBoost, enabling efficient training on large datasets. We cover:</p> <ul> <li>Training an XGBoost model on a Dask cluster.</li> <li>Saving the trained model to disk.</li> <li>Loading the saved model and making predictions on new data.</li> </ul> <pre><code>worker_size = None\nworker_count = None\nmodel_path = \"model.ubj\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert model_path, \"Please enter your model_path.\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Let's start with creating an interactive Dask cluster\n# Note: you can also run this as a batch job.\n# To learn more, please view the batch section of this guide.\n\nif prt.distributed.running_on_a_cluster():\n    print(\"You are already running this code on a distributed cluster. No need to create a new one..\")\nelse:\n    print(\"Starting a new distributed Dask cluster.\")\n    distributed_config = prt.DistJobConfig(\n        job_type=prt.DistJobType.dask,\n        worker_count=worker_count,\n    )\n    worker_config = prt.WorkerConfig(\n        worker_size=worker_size,\n        distributed_config=distributed_config,\n    )\n    coordinator_worker = prt.create_worker(\n        worker_config=worker_config,\n    )\n\n    # Let's login to the cluster coordinator\n    notebook_url = coordinator_worker.open_notebook()\n\n    print(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#execute-on-dask-cluster","title":"Execute on Dask Cluster","text":"<ul> <li>If you just created a new cluster, please open the new browser tab to login to the Distributed Dask coordinator, and continue with the below steps..</li> <li>If you are already on the cluster, you can continue with the below steps..</li> </ul>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#optional-viewing-training-details-on-the-dask-dashboard","title":"(Optional) Viewing training details on the Dask dashboard","text":"<p>If you would like to view training details, please login the Dask dashboard with the below.</p> <pre><code>import practicuscore as prt\n\ndashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code>import practicuscore as prt\n\n# Let's get a Dask session\nclient = prt.distributed.get_client()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#training-with-xgboost-on-dask-cluster","title":"Training with XGBoost on Dask Cluster","text":"<pre><code># Check most recent docs:\n# https://xgboost.readthedocs.io/en/stable/tutorials/dask.html\n\nfrom xgboost import dask as dxgb\n\nimport dask.array as da\nimport dask.distributed\n\nnum_obs = 1e5\nnum_features = 20\nX = da.random.random(size=(num_obs, num_features), chunks=(1000, num_features))\ny = da.random.random(size=(num_obs, 1), chunks=(1000, 1))\n\ndtrain = dxgb.DaskDMatrix(client, X, y)\n# or\n# dtrain = dxgb.DaskQuantileDMatrix(client, X, y)\n\noutput = dxgb.train(\n    client,\n    {\"verbosity\": 2, \"tree_method\": \"hist\", \"objective\": \"reg:squarederror\"},\n    dtrain,\n    num_boost_round=4,\n    evals=[(dtrain, \"train\")],\n)\nprint(\"Model trained successfully\")\n\nprediction = dxgb.predict(client, output, X)\nprint(\"Predictions made successfully\")\n\nprediction = dxgb.inplace_predict(client, output, X)\nprint(\"Predictions made successfully using inplace version\")\n\noutput[\"booster\"].save_model(model_path)\nprint(f\"Model saved to {model_path}\")\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#load-and-train-with-the-xgboost-model","title":"Load and train with the XGBoost model","text":"<pre><code>import xgboost as xgb\n\nprint(\"Loading Model and Predicting\")\n\n# Load the saved model\nloaded_bst = xgb.Booster()\nloaded_bst.load_model(model_path)\nprint(\"Model loaded successfully\")\n\n# Generate a *new* random dataset (important: different from training/testing)\nX_new = da.random.random(size=(num_obs, num_features), chunks=(1000, num_features))  # New data!\nX_new_computed = client.compute(X_new).result()  # Important to compute before creating DMatrix\ndnew = xgb.DMatrix(X_new_computed)\n\n# Make predictions using the loaded model\nnew_preds = loaded_bst.predict(dnew)\nprint(\"New predictions made successfully\")\n\n# Print some predictions (convert to NumPy array for easier printing)\nprint(\"First 10 New Predictions:\")\nprint(new_preds[:10])\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#deploying-model-as-an-api","title":"Deploying model as an API","text":"<p>Note: If you would like to deploy the XGBoost model as an API, please visit the modeling basics section.</p> <pre><code># Cleanup\ntry:\n    # if code is running where you started the cluster\n    coordinator_worker.terminate()\nexcept:\n    # Or else, let's terminate self, which will also terminate the cluster.\n    prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Batch Job | Next: DeepSpeed &gt; Basics &gt; Intro To DeepSpeed</p>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/start-cluster/","title":"Starting an interactive Dask Cluster","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Dask cluster, and execute simple Dask operations.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>worker_size = None\nworker_count = None\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.dask,\n    worker_count=worker_count,\n)\n\n# Let's define worker features of the cluster\nworker_config = prt.WorkerConfig(\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker\n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Dask cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next example in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator\n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Batch Job | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/use-cluster/","title":"Using the interactive Dask Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Dask cluster we created, and execute simple Dask operations.</li> <li>Please run this example on the <code>Dask Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's get a Dask session\nclient = prt.distributed.get_client()\n</code></pre> <pre><code># And execute some code\nimport dask.array as da\n\nprint(\"Starting calculation.\")\n\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\nresult = (x + x.T).mean(axis=0).compute()\n\nprint(\"Completed calculation. Results:\", result)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/use-cluster/#dask-dashboard","title":"Dask Dashboard","text":"<p>Practicus AI Dask offers an interactive dashboard where you can view execution details. Let's open the dashboard.</p> <pre><code>dashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code># Let's execute the same code\nimport dask.array as da\n\nprint(\"Starting calculation.\")\n\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\nresult = (x + x.T).mean(axis=0).compute()\n\nprint(\"Completed calculation. Results:\", result)\n</code></pre> <p>Now you should see in real-time the execution details in a view similar to the below.</p> <p></p>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/use-cluster/#troubleshooting","title":"Troubleshooting","text":"<p>If you\u2019re experiencing issues with an interactive cluster that doesn\u2019t run job/train.py, please follow these steps:</p> <ol> <li> <p>Agent Count Mismatch:    If the number of distributed agents shown by <code>prt.distributed.get_client()</code> is less than what you expected, wait a moment and then run <code>get_client()</code> again. This is usually because the agents have not yet joined the cluster.    Note: Batch jobs automatically wait for agents to join.</p> </li> <li> <p>Viewing Logs:    To view logs, navigate to the <code>~/my/.distributed</code> folder.</p> </li> </ol> <p>Previous: Start Cluster | Next: Batch Job &gt; Batch Job</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/","title":"Distributed DeepSpeed Training","text":"<p>This example demonstrates the process of setting up distributed workers for distributed training using Practicus AI DeepSpeed.</p> <p>Focuses: - Configuring and launching distributed workers using Practicus AI. - Monitoring and logging distributed job performance and resource usage. - Terminating the distributed job after completion.</p> <p>The train.py script and ds_config.json configuration files are used to define the model fine-tuning process.</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#importing-libraries-and-configuring-distributed-job","title":"Importing Libraries and Configuring Distributed Job","text":"<p>This step imports the required libraries, including <code>practicuscore</code>, and sets up the configurations for a distributed job. The key elements are: - <code>job_dir</code>: Directory containing DeepSpeed configuration files and the training script. - <code>DistJobConfig</code>: Defines distributed job parameters such as worker count and termination policy. - <code>WorkerConfig</code>: Specifies worker parameters, including the Docker image, worker size, and startup script.</p> <p>The configuration prepares a coordinator worker that initializes a distributed job.</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"deepspeed\" under your \"~/my\" folder.</li> <li>And copy <code>train.py</code> and <code>ds_config.json</code> under this folder.</li> </ul> <pre><code>worker_size = \"L-GPU\"\nworker_count = None\nlog_level = \"DEBUG\"\nworker_image = \"ghcr.io/practicusai/practicus-gpu-deepspeed\"\nterminate_on_completion = False\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert log_level, \"Please enter your log_level.\"\nassert worker_image, \"Please enter your worker_image.\"\nassert terminate_on_completion, \"Please enter your terminate_on_completion (True or False).\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# DeepSpeed job directory must have default files ds_config.json and train.py (can be renamed)\njob_dir = \"~/my/deepspeed\"\n\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.deepspeed,\n    job_dir=job_dir,\n    worker_count=worker_count,\n    terminate_on_completion=False,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_image=worker_image, worker_size=worker_size, log_level=log_level, distributed_config=distributed_config\n)\n\ncoordinator_worker = prt.create_worker(worker_config)\n\njob_id = coordinator_worker.job_id\n\nassert job_id, \"Could not create distributed job\"\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#monitoring-distributed-job","title":"Monitoring Distributed Job","text":"<p>The <code>live_view</code> and <code>view_log</code> utilities from the Practicus SDK are used to monitor the progress of the distributed job. This provides details such as: - Job ID, start time, worker states, and GPU utilization. - Resource allocation for each worker in the distributed cluster.</p> <p>It helps in tracking the real-time status of the distributed job.</p> <pre><code># Live resource allocation\nprt.distributed.live_view(job_dir, job_id)\n</code></pre> <pre><code># For master logs, you can check Rank-0 logs:\nprt.distributed.view_log(job_dir, job_id, rank=0)\n</code></pre> <pre><code># For pair logs, you must specify pair IDs, e.g. 1:\nprt.distributed.view_log(job_dir, job_id, rank=1)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#terminating-distributed-job-cluster","title":"Terminating Distributed Job Cluster","text":"<p>The distributed job cluster and all associated workers are terminated using the <code>terminate</code> method of the coordinator worker.</p> <pre><code>coordinator_worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#ds_configjson","title":"ds_config.json","text":"<pre><code>{\n    \"train_batch_size\": 4,\n    \"gradient_accumulation_steps\": 2,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\n            \"lr\": 0.001\n        }\n    },\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 1\n    }\n}\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#trainpy","title":"train.py","text":"<pre><code>import torch\nimport torch.nn as nn\nimport deepspeed\nimport torch.distributed as dist\nimport os\nimport json\n\n\n# Define a dummy large neural network\nclass LargeModel(nn.Module):\n    def __init__(self):\n        super(LargeModel, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(8192, 4096),\n            nn.ReLU(),\n            nn.Linear(4096, 2048),\n            nn.ReLU(),\n            nn.Linear(2048, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n\n# Function to check FP16\ndef is_fp16_enabled(config_path=\"ds_config.json\"):\n    with open(config_path) as f:\n        config = json.load(f)\n    return config.get(\"fp16\", {}).get(\"enabled\", False)\n\n\n# Main training function\ndef train():\n    # Distributed setup\n    dist.init_process_group(\n        backend=\"nccl\",\n        init_method=f\"tcp://{os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}\",\n        rank=int(os.environ[\"RANK\"]),\n        world_size=int(os.environ[\"WORLD_SIZE\"]),\n    )\n\n    device = torch.device(f\"cuda:0\")\n    model = LargeModel().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Initialize DeepSpeed\n    model, optimizer, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=\"ds_config.json\")\n\n    # Training loop\n    for epoch in range(5):\n        optimizer.zero_grad()\n\n        # Creation of dummy train data\n        data = torch.randn(32768, 8192).to(device)\n        target = torch.randn(32768, 1).to(device)\n\n        # Convert data and target to FP16 if enabled\n        if is_fp16_enabled(\"ds_config.json\"):\n            data, target = data.half(), target.half()\n\n        # Log memory usage and loss\n        loss = nn.MSELoss()(model(data), target)\n        model.backward(loss)\n        model.step()\n\n        print(\n            f\"[GPU {os.environ['RANK']}] Epoch {epoch}, Loss: {loss.item()}, \"\n            f\"Allocated VRAM: {torch.cuda.memory_allocated(device) / 1e9:.2f} GB\"\n        )\n\n    # Clean cache\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    train()\n</code></pre> <p>Previous: XGBoost | Next: LLM Fine Tuning &gt; Llms With DeepSpeed</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/","title":"Distributed LLM Fine-Tuning with DeepSpeed","text":"<p>Fine-tuning large language models (LLMs) often requires distributed computing to efficiently handle the computational demands of large datasets and model sizes. Practicus AI, combined with the DeepSpeed library, provides a streamlined platform for distributed training and fine-tuning of LLMs. This notebook demonstrates the end-to-end process of dataset preparation, worker configuration, distributed job execution, and model testing using Practicus AI.</p> <p>Key highlights include: - Preparing datasets and downloading pre-trained models. - Setting up distributed workers using DeepSpeed. - Monitoring and logging distributed jobs. - Testing the fine-tuned model against its base version.</p> <p>With Practicus AI's flexible distributed computing framework, you can efficiently manage resource-intensive tasks like LLM fine-tuning, ensuring scalability, idempotency, and atomicity across the entire workflow.</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#dataset-preparation-and-model-download","title":"Dataset Preparation and Model Download","text":"<p>The first step in fine-tuning involves preparing the dataset and downloading the pre-trained model. These steps include:</p> <ol> <li>Loading and saving the fine-tuning dataset.</li> <li>Authenticating with Hugging Face to securely access model repositories.</li> <li>Downloading a pre-trained LLM (e.g., LLaMA-3B-Instruct) from the Hugging Face Hub for fine-tuning.</li> </ol> <pre><code>worker_size = \"L-GPU\"\nworker_count = None\nlog_level = \"DEBUG\"\nworker_image = \"ghcr.io/practicusai/practicus-gpu-deepspeed\"\nterminate_on_completion = False\nstartup_script = \"pip install accelerate trl peft datasets\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert log_level, \"Please enter your log_level.\"\nassert worker_image, \"Please enter your worker_image.\"\nassert terminate_on_completion, \"Please enter your terminate_on_completion (True or False).\"\nassert startup_script, \"Please enter your startup_script.\"\n</code></pre> <pre><code>import pandas as pd\n\nurl = \"https://raw.githubusercontent.com/practicusai/sample-data/refs/heads/main/customer_support/Customer_Support_Dataset.csv\"\ndf = pd.read_csv(url, index_col=0)\n\ndf.head()\n</code></pre> <pre><code>df.to_csv(\"Customer_Support_Dataset.csv\")\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#hugging-face-authentication","title":"Hugging Face Authentication","text":"<p>To download models from Hugging Face, you need to authenticate using your API token. This ensures secure access to both public and private repositories.</p> <pre><code>from huggingface_hub.hf_api import HfFolder\n\ntry:\n    HfFolder.save_token(\"...\")  # Replace with your Hugging Face API token\n\nexcept:\n    print(\"Hugging face token is wrong.\")\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#download-pre-trained-model","title":"Download Pre-Trained Model","text":"<p>The pre-trained LLM is downloaded to a local directory for fine-tuning. Replace <code>local_dir</code> and <code>REPO_ID</code> with the desired directory and model ID, respectively.</p> <pre><code>from huggingface_hub import snapshot_download\n\ntry:\n    local_dir = \"...\"  # Example: /home/ubuntu/my/llm_fine_tune/llama-3B-instruct\n    REPO_ID = \"...\"  # Example: meta-llama/Llama-3.2-3B-Instruct\n\n    snapshot_download(repo_id=REPO_ID, local_dir=local_dir)\n\nexcept:\n    print(\"Snapshot didn't found.\")\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#building-workers-for-distributed-llm-fine-tuning","title":"Building Workers for Distributed LLM Fine-Tuning","text":"<p>This section demonstrates how to configure and launch distributed workers for fine-tuning an LLM using DeepSpeed. The main focus areas are: - Configuring distributed job parameters (e.g., worker count, job directory). - Setting up workers with appropriate Docker images and resource configurations. - Initializing the distributed cluster for fine-tuning.</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a folder <code>~/my/deepspeed/llm_fine_tune</code>.</li> <li>Copy <code>train.py</code> and <code>ds_config.json</code> into this directory.</li> </ul> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/deepspeed/llm_fine_tune\"\nworker_count = 2\n\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.deepspeed,\n    job_dir=job_dir,\n    worker_count=worker_count,\n    terminate_on_completion=False,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_image=worker_image,\n    worker_size=worker_size,\n    log_level=log_level,\n    distributed_config=distributed_config,\n    startup_script=startup_script,\n)\n\ncoordinator_worker = prt.create_worker(worker_config)\njob_id = coordinator_worker.job_id\nassert job_id, \"Could not create distributed job\"\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#monitoring-distributed-job","title":"Monitoring Distributed Job","text":"<p>Use the <code>live_view</code> and <code>view_log</code> utilities to monitor the job's progress. These tools provide real-time insights into worker status, GPU utilization, and job logs.</p> <pre><code>prt.distributed.live_view(job_dir, job_id)\n</code></pre> <pre><code>prt.distributed.view_log(job_dir, job_id, rank=0)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#terminating-distributed-job-cluster","title":"Terminating Distributed Job Cluster","text":"<p>Once the job completes, terminate the distributed cluster and release resources.</p> <pre><code>coordinator_worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#testing-the-fine-tuned-model","title":"Testing the Fine-Tuned Model","text":"<p>This section demonstrates how to load and test a fine-tuned LLM. The steps include: - Installing required libraries. - Loading the fine-tuned model and tokenizer. - Comparing the fine-tuned model\u2019s outputs with those from the base model.</p> <pre><code>! pip install peft accelerate\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#comparing-fine-tuned-model-and-base-model","title":"Comparing Fine-Tuned Model and Base Model","text":"<p>The fine-tuned model and tokenizer are loaded from the <code>output_dir</code>. The <code>device_map='auto'</code> parameter ensures efficient resource allocation, distributing the model across available GPUs. A conversation-like input is passed to both models, and their outputs are compared.</p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load the fine-tuned model\nmodel_name_or_path = \"./output_dir\"  # Path to the saved fine-tuned model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, device_map=\"auto\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n</code></pre> <pre><code># Define chat messages\nmessages = [{\"role\": \"user\", \"content\": \"want assistance to cancel purchase 554\"}]\n\n# Apply chat template\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Tokenize input text\ninputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n\n# Generate model response\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\n# Decode model output\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract response content\nprint(text.split(\"assistant\")[1])\n</code></pre> <pre><code># Load the base model\nmodel_name_or_path = \"./llama-3B-instruct\"  # Path to the saved fine-tuned model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, device_map=\"auto\")\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n</code></pre> <pre><code># Define chat messages\nmessages = [{\"role\": \"user\", \"content\": \"want assistance to cancel purchase 554\"}]\n\n# Apply chat template\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Tokenize input text\ninputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n\n# Generate model response\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\n# Decode model output\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract response content\nprint(text.split(\"assistant\")[1])\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#ds_configjson","title":"ds_config.json","text":"<pre><code>{\n    \"train_batch_size\": 16,\n    \"gradient_accumulation_steps\": 1,\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"reduce_bucket_size\": 4194304,\n        \"stage3_prefetch_bucket_size\": 3774873,\n        \"stage3_param_persistence_threshold\": 20480,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#trainpy","title":"train.py","text":"<pre><code>import os\nimport pandas as pd\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport torch\nfrom transformers import (\n    set_seed,\n    TrainingArguments,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n)\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\nfrom datasets import Dataset\n\n\nBASE_DIR = \"/home/ubuntu/my/deepspeed/llm_fine_tune\"\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: str = field(\n        default=os.path.join(BASE_DIR, \"llama-3B-instruct\"),  # Set default model path here\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n    )\n    lora_alpha: Optional[int] = field(default=16)\n    lora_dropout: Optional[float] = field(default=0.1)\n    lora_r: Optional[int] = field(default=64)\n    lora_target_modules: Optional[str] = field(\n        default=\"q_proj,k_proj,v_proj,o_proj,down_proj,up_proj,gate_proj\",\n        metadata={\"help\": \"Comma-separated list of target modules to apply LoRA layers to.\"},\n    )\n    use_flash_attn: Optional[bool] = field(default=False, metadata={\"help\": \"Enables Flash attention for training.\"})\n\n\n@dataclass\nclass DataTrainingArguments:\n    dataset_path: str = field(\n        default=os.path.join(BASE_DIR, \"Customer_Support_Dataset.csv\"),\n        metadata={\"help\": \"Path to the CSV file containing training data.\"},\n    )\n    input_field: str = field(default=\"instruction\", metadata={\"help\": \"Field name for user input in the dataset.\"})\n    target_field: str = field(default=\"response\", metadata={\"help\": \"Field name for target responses in the dataset.\"})\n    max_seq_length: int = field(default=180, metadata={\"help\": \"Maximum sequence length for tokenization.\"})\n\n\ndef create_and_prepare_model(args, data_args):\n    # LoRA configuration\n    peft_config = LoraConfig(\n        lora_alpha=args.lora_alpha,\n        lora_dropout=args.lora_dropout,\n        r=args.lora_r,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=args.lora_target_modules.split(\",\"),\n    )\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        attn_implementation=\"flash_attention_2\" if args.use_flash_attn else \"eager\",\n    )\n    model.resize_token_embeddings(len(tokenizer))\n\n    return model, peft_config, tokenizer\n\n\ndef preprocess_data(df, tokenizer, data_args):\n    def tokenize_function(row):\n        inputs = tokenizer(\n            row[data_args.input_field], truncation=True, padding=\"max_length\", max_length=data_args.max_seq_length\n        )\n        targets = tokenizer(\n            row[data_args.target_field], truncation=True, padding=\"max_length\", max_length=data_args.max_seq_length\n        )\n        return {\n            \"input_ids\": inputs[\"input_ids\"],\n            \"attention_mask\": inputs[\"attention_mask\"],\n            \"labels\": targets[\"input_ids\"],\n        }\n\n    # Adding instruction to training dataset\n    instruction = \"\"\"You are a top-rated customer service agent named John. \n    Be polite to customers and answer all their questions.\"\"\"\n\n    df[data_args.input_field] = df[data_args.input_field].apply(lambda x: instruction + str(x))\n\n    # Convert pandas DataFrame to Hugging Face Dataset\n    dataset = Dataset.from_pandas(df)\n\n    # Apply tokenization\n    tokenized_dataset = dataset.map(tokenize_function, batched=False)\n\n    return tokenized_dataset\n\n\ndef main():\n    # Define training arguments internally\n    training_args = TrainingArguments(\n        output_dir=\"output_dir\",  # Directory where the model and logs will be saved\n        overwrite_output_dir=True,\n        do_train=True,\n        per_device_train_batch_size=8,\n        num_train_epochs=3,\n        logging_dir=\"logs\",\n        logging_strategy=\"steps\",\n        logging_steps=500,\n        save_strategy=\"steps\",\n        save_steps=500,\n        save_total_limit=2,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        load_best_model_at_end=True,\n        seed=42,\n        deepspeed=\"ds_config.json\",  # Path to your DeepSpeed config file\n        fp16=True,\n    )\n\n    # Model and data arguments\n    model_args = ModelArguments()\n    data_args = DataTrainingArguments()\n\n    # Set seed for reproducibility\n    set_seed(training_args.seed)\n\n    # Load dataset\n    train_dataset = pd.read_csv(data_args.dataset_path)\n\n    # Taking sample of the dataset\n    train_dataset = train_dataset.sample(frac=1.0, random_state=65).reset_index(drop=True)\n    train_dataset = train_dataset.iloc[:10]\n\n    # Prepare model and tokenizer\n    model, peft_config, tokenizer = create_and_prepare_model(model_args, data_args)\n\n    # Tokenize data\n    tokenized_data = preprocess_data(train_dataset, tokenizer, data_args)\n\n    # Trainer setup\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=tokenized_data,\n        eval_dataset=tokenized_data,\n        peft_config=peft_config,\n        max_seq_length=512,\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Save the final model\n    trainer.save_model()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Intro To DeepSpeed | Next: Ray &gt; Interactive &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/","title":"Executing batch jobs in Ray Cluster","text":"<p>In this example we will: - Create a Ray cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"ray\" under your \"~/my\" folder</li> <li>And copy job.py under this folder</li> </ul> <pre><code>worker_size = None\nworker_count = None\nlog_level = \"DEBUG\"\nworker_image = \"practicus-ray\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert log_level, \"Please enter your log_level.\"\nassert worker_image, \"Please enter your worker_image.\"\n</code></pre> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/ray\"\n\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.ray,\n    job_dir=job_dir,\n    py_file=\"job.py\",\n    worker_count=worker_count,\n)\n\nworker_config = prt.WorkerConfig(\n    # Please note that Ray requires a specific worker image\n    worker_image=worker_image,\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n    log_level=log_level,\n)\n\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n)\n</code></pre> <pre><code># You can view the logs during or after the job is completed\n# To view coordinator (master) set rank = 0\nrank = 0\n# To view other workers set rank = 1,2, ..\n\nprt.distributed.view_log(job_dir=job_dir, job_id=coordinator_worker.job_id, rank=rank)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/#wrapping-up","title":"Wrapping up","text":"<ul> <li>Once the job is completed, you can view the results in <code>~/my/ray/result.csv/</code></li> <li>Please note that result.csv is a folder that can contain <code>parts of the processed file</code> by each worker (Ray executors)</li> <li>Also note that you do not need to terminate the cluster since it has a 'py_file' to execute, which defaults <code>terminate_on_completion</code> parameter to True.</li> <li>You can change terminate_on_completion to False to keep the cluster running after the job is completed to troubleshoot issues.</li> <li>You can view other <code>prt.DistJobConfig</code> properties to customize the cluster</li> </ul>"},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt\n\nray = prt.distributed.get_client()\n\n\n@ray.remote\ndef square(x):\n    return x * x\n\n\ndef calculate():\n    numbers = [i for i in range(10)]\n    futures = [square.remote(i) for i in numbers]\n    results = ray.get(futures)\n    print(\"Distributed square results of\", numbers, \"is\", results)\n\n\nif __name__ == \"__main__\":\n    calculate()\n    ray.shutdown()\n</code></pre> <p>Previous: Use Cluster | Next: Modin &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/start-cluster/","title":"Starting an interactive Ray Cluster","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Ray cluster, and execute simple Ray operations.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>worker_size = None\nworker_count = None\nworker_image = \"practicus-ray\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert worker_image, \"Please enter your worker_image.\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.ray,\n    worker_count=worker_count,\n)\n\n# Let's define worker features of the cluster\nworker_config = prt.WorkerConfig(\n    # Please note that Ray requires a specific worker image\n    worker_image=worker_image,\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker\n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Ray cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next example in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator\n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Llms With DeepSpeed | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/use-cluster/","title":"Using the interactive Ray Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Ray cluster we created, and execute simple Ray operations.</li> <li>Please run this example on the <code>Ray Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's get a Ray session.\n# this is similar to running `import ray` and then `ray.init()`\nray = prt.distributed.get_client()\n</code></pre> <pre><code>@ray.remote\ndef square(x):\n    return x * x\n\n\ndef calculate():\n    numbers = [i for i in range(10)]\n    futures = [square.remote(i) for i in numbers]\n    results = ray.get(futures)\n    print(\"Distributed square results of\", numbers, \"is\", results)\n\n\ncalculate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/use-cluster/#ray-dashboard","title":"Ray Dashboard","text":"<p>Practicus AI Ray offers an interactive dashboard where you can view execution details. Let's open the dashboard.</p> <pre><code>dashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code>@ray.remote\ndef square(x):\n    return x * x\n\n\ndef calculate():\n    numbers = [i for i in range(10)]\n    futures = [square.remote(i) for i in numbers]\n    results = ray.get(futures)\n    print(\"Distributed square results of\", numbers, \"is\", results)\n\n\ncalculate()\n</code></pre> <p>Now you should see in real-time the execution details in a view similar to the below. You can click the Job tab for useful information.</p> <p></p> <pre><code># Let's close the session\nray.shutdown()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/use-cluster/#troubleshooting","title":"Troubleshooting","text":"<p>If you\u2019re experiencing issues with an interactive cluster that doesn\u2019t run job/train.py, please follow these steps:</p> <ol> <li> <p>Agent Count Mismatch:    If the number of distributed agents shown by <code>prt.distributed.get_client()</code> is less than what you expected, wait a moment and then run <code>get_client()</code> again. This is usually because the agents have not yet joined the cluster.    Note: Batch jobs automatically wait for agents to join.</p> </li> <li> <p>Viewing Logs:    To view logs, navigate to the <code>~/my/.distributed</code> folder.</p> </li> </ol> <p>Previous: Start Cluster | Next: Batch Job &gt; Batch Job</p>"},{"location":"technical-tutorial/distributed-computing/ray/modin/start-cluster/","title":"Distributed Data processing with Modin and Ray","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Ray cluster, and execute simple modin + Ray operations.</li> <li>Although the example is interactive, you can apply the same for batch jobs as well.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/ray/modin/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>worker_size = None\nworker_count = None\nworker_image = \"practicus-ray\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert worker_image, \"Please enter your worker_image.\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.ray,\n    worker_count=worker_count,\n)\n\n# Let's define worker features of the cluster\nworker_config = prt.WorkerConfig(\n    # Please note that Ray requires a specific worker image\n    worker_image=worker_image,\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker\n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Ray cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/modin/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next example in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator\n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Batch Job | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/modin/use-cluster/","title":"Using the interactive Ray Cluster for Modin","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Ray cluster we created, and execute modin + Ray operations.</li> <li>Please run this example on the <code>Ray Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's get a Ray session.\n# this is similar to running `import ray` and then `ray.init()`\nray = prt.distributed.get_client()\n</code></pre> <pre><code># Modin aims to be a drop-in replacement for pandas\n# import pandas as pd\nimport modin.pandas as pd\n\ndf = pd.read_csv(\"/home/ubuntu/samples/data/airline.csv\")\n\nprint(\"DataFrame type is:\", type(df))\n\ndf[\"passengers\"] = df[\"passengers\"] * 2\n\ndf\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/modin/use-cluster/#ray-dashboard","title":"Ray Dashboard","text":"<p>Practicus AI Ray offers an interactive dashboard where you can view execution details. Let's open the dashboard.</p> <pre><code>dashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code>df[\"passengers\"] = df[\"passengers\"] * 2\n\ndf\n</code></pre> <pre><code># Let's close the session\nray.shutdown()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/modin/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Vllm &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/start-cluster/","title":"Distributed vLLM with Ray","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Ray cluster, and execute simple vLLM + Ray operations.</li> <li>Although the example is interactive, you can apply the same for batch jobs as well.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>worker_size = None\nworker_count = None\nworker_image = \"practicus-gpu-ray\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert worker_image, \"Please enter your worker_image.\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.ray,\n    worker_count=worker_count,\n)\n\n# Let's define worker features of the cluster\nworker_config = prt.WorkerConfig(\n    # Please note that this example requires GPUs\n    # Please note that Ray requires a specific worker image\n    worker_image=worker_image,\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker\n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Ray cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next example in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator\n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Use Cluster | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/use-cluster/","title":"Using the interactive Ray Cluster for vLLM","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Ray cluster we created, and execute vLLM + Ray operations.</li> <li>Please run this example on the <code>Ray Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's get a Ray session.\n# this is similar to running `import ray` and then `ray.init()`\nray = prt.distributed.get_client()\n</code></pre> <pre><code>from vllm import LLM, SamplingParams\n\nprompts = [\n    \"Mexico is famous for \",\n    \"The largest country in the world is \",\n]\n\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\nllm = LLM(model=\"facebook/opt-125m\")\nresponses = llm.generate(prompts, sampling_params)\n\nfor response in responses:\n    print(response.outputs[0].text)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/use-cluster/#ray-dashboard","title":"Ray Dashboard","text":"<p>Practicus AI Ray offers an interactive dashboard where you can view execution details. Let's open the dashboard.</p> <pre><code>dashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code># Let's close the session\nray.shutdown()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/use-cluster/#troubleshooting","title":"Troubleshooting","text":"<p>If you\u2019re experiencing issues with an interactive cluster that doesn\u2019t run job/train.py, please follow these steps:</p> <ol> <li> <p>Agent Count Mismatch:    If the number of distributed agents shown by <code>prt.distributed.get_client()</code> is less than what you expected, wait a moment and then run <code>get_client()</code> again. This is usually because the agents have not yet joined the cluster.    Note: Batch jobs automatically wait for agents to join.</p> </li> <li> <p>Viewing Logs:    To view logs, navigate to the <code>~/my/.distributed</code> folder.</p> </li> </ol> <p>Previous: Start Cluster | Next: Custom Adaptor &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/","title":"Batch Job","text":""},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#starting-a-batch-job-on-auto-scaled-spark-cluster","title":"Starting a Batch Job on auto-scaled Spark Cluster","text":"<p>This example demonstrates how to set up and run an auto-scaled batch job in Practicus AI. Instead of launching a fixed-size environment, we will create a batch job with the ability to automatically scale its compute resources based on demand.</p>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#important-note-on-worker-container-image","title":"Important note on worker container image","text":"<ul> <li>Unlike standard Spark cluster, auto-scaled Spark cluster executors have a separate type of container image <code>ghcr.io/practicusai/practicus-spark</code></li> <li>This means packages accessible to coordinator worker might not be accessible to the executors.</li> <li>To install packages please install to both the coordinator and the executor images and create custom container images.</li> <li>While creating the spark client, you can then pass arguments to specify which executor image to use.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#important-note-on-privileged-access","title":"Important note on privileged access","text":"<ul> <li>For auto-scaled Spark to work, <code>you will need additional privileges</code> on the Kubernetes cluster.</li> <li>Please ask your admin to grant you access to worker size definitions with privileged access before you continue with this example.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#finding-an-auto-scaled-privileged-worker-size","title":"Finding an Auto-Scaled (Privileged) Worker Size","text":"<p>Let's identify a worker size that supports auto-scaling and includes the required privileged capabilities for running batch jobs.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\n</code></pre> <pre><code>auto_dist_worker_size = None\n</code></pre> <p>If you don't know your auto-distributed (privileged) workers you can check them out by using the SDK like down below:</p> <pre><code>worker_size_list = region.worker_size_list\ndisplay(worker_size_list.to_pandas())  # Check auto_distributed col.\n</code></pre> <pre><code>assert auto_dist_worker_size, \"Please select an auto-distributed (privileged) worker sizes.\"\n</code></pre> <pre><code># Configure distributed job settings\n# This example uses Spark with auto-scaling capabilities.\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.spark,\n    auto_distributed=True,  # Enables automatic scaling of the Spark cluster\n    initial_count=1,  # Start with 1 executor plus the coordinator (2 workers total)\n    max_count=4,  # Allow the cluster to scale up to 4 additional executors if needed\n)\n\n# Define the worker configuration\n# Ensure that the chosen worker size includes privileged access\n# to support auto-scaling.\nworker_config = prt.WorkerConfig(\n    worker_size=auto_dist_worker_size,\n    distributed_config=distributed_config,\n)\n\n# Note: We are not creating a coordinator worker interactively here\n# since this setup is intended for batch tasks rather than interactive sessions.\n# Example: coordinator_worker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Running the batch job:\n# - Starts a worker\n# - Submits 'job.py' to run on the cluster\n# - 'job.py' creates a Spark session and triggers cluster creation\n#   with multiple executors as defined above.\n# - Monitors execution and prints progress\n# - On completion, terminates the Spark session and executors\nworker, success = prt.run_task(\n    file_name=\"job.py\",\n    worker_config=worker_config,\n    terminate_on_completion=False,  # Leave the cluster running until we decide to terminate\n)\n</code></pre> <pre><code>if success:\n    print(\"Job is successful, terminating cluster.\")\n    worker.terminate()\nelse:\n    print(\"Job failed, opening notebook on coordinator to analyze.\")\n    worker.open_notebook()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt\n\nprint(\"Requesting a Spark session...\")\nspark = prt.distributed.get_client()\n\n# Create a sample DataFrame\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\nprint(\"Creating DataFrame...\")\ndf = spark.createDataFrame(data, columns)\n\nprint(\"Applying filter: Age &gt; 30\")\ndf_filtered = df.filter(df.Age &gt; 30)\n\nprint(\"Filtered results:\")\ndf_filtered.show()\n\n# Note:\n# Auto-scaled Spark executors are different from standard Practicus AI workers.\n# They use a specialized container image and do not have direct access to\n# `~/my` or `~/shared` directories.\n# For saving results, consider using a data lake or object storage.\n</code></pre> <p>Previous: Use Cluster | Next: Dask &gt; Interactive &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/","title":"Starting an auto-scaled Spark Cluster","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Spark auto-scaled cluster, and execute simple Spark operations. </li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/#important-note-on-worker-container-image","title":"Important note on worker container image","text":"<ul> <li>Unlike standard Spark cluster, auto-scaled Spark cluster executors have a separate type of container image ghcr.io/practicusai/practicus-spark</li> <li>This means packages accessible to coordinator worker might not be accessible to the executors.</li> <li>To install packages please install to both the coordinator and the executor images and create custom container images.</li> <li>While creating the spark client, you can then pass arguments to specify which executor image to use.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/#important-note-on-privileged-access","title":"Important note on privileged access","text":"<ul> <li>For auto-scaled Spark to work, <code>you will need additional privileges</code> on the Kubernetes cluster.</li> <li>Please ask your admin to grant you access to worker size definitions with privileged access before you continue with this example.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/#finding-an-auto-scaled-privileged-worker-size","title":"Finding an Auto-Scaled (Privileged) Worker Size","text":"<p>Let's identify a worker size that supports auto-scaling and includes the required privileged capabilities for running batch jobs.</p> <pre><code>auto_dist_worker_size = None\ninitial_count = None\nmax_count = None\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\n</code></pre> <p>If you don't know your auto-distributed (privileged) workers you can check them out by using the SDK like down below:</p> <pre><code>worker_size_list = region.worker_size_list\ndisplay(worker_size_list.to_pandas())  # Check auto_distributed col.\n</code></pre> <pre><code>assert auto_dist_worker_size, \"Please select auto-distributed (privileged) worker size.\"\nassert initial_count, \"Please select initial_count.\"\nassert max_count, \"Please select max_count.\"\n</code></pre> <pre><code># Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.spark,\n    # ** The below changes the default cluster behavior **\n    auto_distributed=True,\n    # Set the initial size.\n    # These are 'additional` executors to coordinator,\n    # E.g. the below will create a cluster of 2 workers.\n    initial_count=initial_count,\n    # Optional: set a maximum to auto-scale to, if needed.\n    # E.g. with the below, the cluster can scale up to 5 workers\n    max_count=max_count,\n)\n\n# Let's define worker features of the cluster\nworker_config = prt.WorkerConfig(\n    worker_image=\"ghcr.io/practicusai/practicus-spark-worker:25.5.4\",\n    # Please make sure to use a worker size with\n    #   privileged access.\n    worker_size=auto_dist_worker_size,\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker:\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n# - The above will NOT create the executors instantly.\n# - You will only create one worker.\n# - Additional executors will be created when needed.\n</code></pre> <pre><code># Since this is an interactive Spark cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next notebook in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator\n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Batch Job | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/use-cluster/","title":"Using the interactive Spark Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Spark cluster we created, and execute simple Spark operations.</li> <li>Please run this example on the <code>Spark Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's get a Spark session\nspark = prt.distributed.get_client()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/use-cluster/#behind-the-scenes","title":"Behind the scenes","text":"<ul> <li>After the above code, new Spark executors will start running.</li> <li>This is specific to auto-scaled Spark only and not the dfault behavior.</li> </ul> <pre><code># And execute some code\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Perform a transformation\ndf_filtered = df.filter(df.Age &gt; 30)\n\n# Show results\ndf_filtered.show()\n</code></pre> <pre><code># Explicitly delete spark session\nprt.engines.delete_spark_session()\n# Unlike the standard Spark cluster, the below won't work for auto-scaled.\n# spark.stop()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <pre><code>coordinator_worker.terminate()\n</code></pre> <ul> <li>Or, terminate \"self\" and children workers with the below:</li> </ul> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Batch &gt; Batch Job</p>"},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/","title":"Executing batch jobs in Spark Cluster","text":"<p>In this example we will: - Create a Spark cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"spark\" under your \"~/my\" folder</li> <li>And copy job.py under this folder</li> </ul> <pre><code>worker_size = None\nworker_count = None\nlog_level = \"DEBUG\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert log_level, \"Please enter your log_level.\"\n</code></pre> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/spark\"\n\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.spark,\n    job_dir=job_dir,\n    py_file=\"job.py\",\n    worker_count=worker_count,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"ghcr.io/practicusai/practicus-spark-worker:25.5.4\",\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n    log_level=log_level,\n)\n\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n)\n</code></pre> <pre><code># You can view the logs during or after the job is completed\n# To view coordinator (master) set rank = 0\nrank = 0\n# To view other workers set rank = 1,2, ..\n\nprt.distributed.view_log(job_dir=job_dir, job_id=coordinator_worker.job_id, rank=rank)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/#wrapping-up","title":"Wrapping up","text":"<ul> <li>Once the job is completed, you can view the results in <code>~/my/spark/result.csv/</code></li> <li>Please note that result.csv is a folder that contains <code>parts of the processed file</code> by each worker (Spark executors)</li> <li>Also note that you do not need to terminate the cluster since it has a 'py_file' to execute, which defaults <code>terminate_on_completion</code> parameter to True.</li> <li>You can change terminate_on_completion to False to keep the cluster running after the job is completed to troubleshoot issues.</li> <li>You can view other <code>prt.DistJobConfig</code> properties to customize the cluster</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt\n\n# Let's get a Spark session\nprint(\"Getting Spark session\")\nspark = prt.distributed.get_client()\n\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\nprint(\"Creating DataFrame\")\ndf = spark.createDataFrame(data, columns)\n\nprint(\"Calculating\")\ndf_filtered = df.filter(df.Age &gt; 30)\n\nprint(\"Writing to csv\")\ndf_filtered.write.csv(\"/home/ubuntu/my/spark/result.csv\", header=True, mode=\"overwrite\")\n</code></pre> <p>Previous: Use Cluster | Next: Auto Scaled &gt; Interactive &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/start-cluster/","title":"Starting an interactive Spark Cluster","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Spark cluster, and execute simple Spark operations.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>worker_size = None\nworker_count = None\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.spark,\n    worker_count=worker_count,\n)\n\n# Let's define worker features of the cluster\nworker_config = prt.WorkerConfig(\n    worker_image=\"ghcr.io/practicusai/practicus-spark-worker:25.5.4\",\n    worker_size=worker_size,\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker\n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Spark cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next notebook in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator\n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Introduction | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/use-cluster/","title":"Using the interactive Spark Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Spark cluster we created, and execute simple Spark operations.</li> <li>Please run this example on the <code>Spark Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's get a Spark session\nspark = prt.distributed.get_client()\n</code></pre> <pre><code># And execute some code\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Perform a transformation\ndf_filtered = df.filter(df.Age &gt; 30)\n\n# Show results\ndf_filtered.show()\n</code></pre> <pre><code># Let's end the session\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/use-cluster/#troubleshooting","title":"Troubleshooting","text":"<p>If you\u2019re experiencing issues with an interactive cluster that doesn\u2019t run job/train.py, please follow these steps:</p> <ol> <li> <p>Agent Count Mismatch:    If the number of distributed agents shown by <code>prt.distributed.get_client()</code> is less than what you expected, wait a moment and then run <code>get_client()</code> again. This is usually because the agents have not yet joined the cluster.    Note: Batch jobs automatically wait for agents to join.</p> </li> <li> <p>Viewing Logs:    To view logs, navigate to the <code>~/my/.distributed</code> folder.</p> </li> </ol> <p>Previous: Start Cluster | Next: Batch Job &gt; Batch Job</p>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/","title":"Trino, Iceberg, and API Integration","text":"<p>This notebook guides you through setting up a <code>customer_table</code> within a Trino Lakehouse environment, injecting sample data, deploying an API endpoint using <code>practicuscore</code> to query this table, and finally demonstrating how to interact with the deployed API.</p>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#trino-and-iceberg","title":"Trino and Iceberg","text":""},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#establishing-trino-connection","title":"Establishing Trino Connection","text":"<p>Before interacting with Trino, we need to establish a connection. This cell sets up the Trino connection.</p> <pre><code>import trino\n\nexternal_access = True\n\nif external_access:\n    host = '&lt;TRINO_EXTERNAL_HOST&gt;' # Replace with your actual Trino external host\n    port = 443\n    verify = True\n    # In this scenario, https traffic will be terminated at the load balancer \n    # and Trino will receive http traffic\nelse:\n    host = '&lt;TRINO_INTERNAL_HOST&gt;' # Replace with your actual Trino internal host\n    port = 8443 # In this scenario, Trino must receive https traffic\n\n# verify = \"ca_cert.pem\"\n# Disabling SSL verification might not work in some cases\n# Either use a global Certificate Authority (CA), or provide the ca_cert.pem\n# verify = False\n\n# Create a connection to Trino\nconn = trino.dbapi.connect(\n    host=host,\n    port=port,\n    http_scheme='https',\n    verify=verify, \n    auth=trino.auth.BasicAuthentication(\"&lt;USER_NAME&gt;\", \"&lt;PASSWORD&gt;\") # Basic authentication\n)\n</code></pre>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#creating-schema-and-table","title":"Creating Schema and Table","text":"<p>Now, we'll create the necessary schema (namespace) and the <code>customer_table</code> within your specified Lakehouse catalog. The <code>IF NOT EXISTS</code> clause prevents errors if the schema or table already exists.</p> <p>Note: The <code>LAKEHOUSE_CATALOG</code> and <code>S3_BUCKET_NAME</code> values are typically sourced from your Practicus Analytics environment's deployment <code>YAML</code> configurations (e.g., <code>prt-ns-analytics</code>).</p> <pre><code>LAKEHOUSE_CATALOG = 'lakehouse' # Your Trino Lakehouse catalog name (e.g., 'lakehouse'). Refer to your YAML configs.\nSCHEMA_NAME = 'customers' # The schema (namespace) name you want to use (e.g., 'customers', 'sales', 'test')\nTABLE_NAME = 'customer_table' # The name of the table to be created\nS3_BUCKET_NAME = 'trino' # The name of your S3-compatible storage bucket (e.g., 'my-data-lake-bucket'). Refer to your YAML configs.\n</code></pre> <pre><code>cursor = conn.cursor()\n\nif conn and cursor:\n    try:\n        # Create schema without semicolon\n        query_create_schema = f\"CREATE SCHEMA IF NOT EXISTS {LAKEHOUSE_CATALOG}.{SCHEMA_NAME}\"\n        cursor.execute(query_create_schema)\n        print(f\"Schema '{LAKEHOUSE_CATALOG}.{SCHEMA_NAME}' ensured to exist.\")\n\n        # Create table without semicolon\n        query_create_table = f\"\"\"CREATE TABLE IF NOT EXISTS {LAKEHOUSE_CATALOG}.{SCHEMA_NAME}.{TABLE_NAME} (\n            customer_id BIGINT,\n            first_name VARCHAR,\n            last_name VARCHAR,\n            email VARCHAR,\n            phone VARCHAR,\n            signup_date DATE\n        )\n        WITH (\n            format = 'PARQUET',\n            location = 's3a://{S3_BUCKET_NAME}/{SCHEMA_NAME}/{TABLE_NAME}'\n        )\"\"\"\n        cursor.execute(query_create_table)\n        print(f\"Table '{LAKEHOUSE_CATALOG}.{SCHEMA_NAME}.{TABLE_NAME}' created successfully (or already exists).\")\n\n        # Show the table DDL\n        query_show_table = f\"SHOW CREATE TABLE {LAKEHOUSE_CATALOG}.{SCHEMA_NAME}.{TABLE_NAME}\"\n        cursor.execute(query_show_table)\n        ddl_result = cursor.fetchone()\n        print(\"\\n--- SHOW CREATE TABLE Output ---\")\n        print(ddl_result[0])\n\n        # Optional: also describe the table\n        query_describe_table = f\"DESCRIBE {LAKEHOUSE_CATALOG}.{SCHEMA_NAME}.{TABLE_NAME}\"\n        cursor.execute(query_describe_table)\n        describe_result = cursor.fetchall()\n        print(\"\\n--- DESCRIBE TABLE Output ---\")\n        for row in describe_result:\n            print(row)\n\n    except Exception as e:\n        print(f\"Error during schema/table creation: {e}\")\nelse:\n    print(\"Trino connection not established. Cannot create schema/table.\")\n\nconn.close()\n</code></pre> <pre><code>Schema 'lakehouse.customers' ensured to exist.\nTable 'lakehouse.customers.customer_table' created successfully (or already exists).\n\n--- SHOW CREATE TABLE Output ---\nCREATE TABLE lakehouse.customers.customer_table (\n   customer_id bigint,\n   first_name varchar,\n   last_name varchar,\n   email varchar,\n   phone varchar,\n   signup_date date\n)\nWITH (\n   format = 'PARQUET',\n   format_version = 2,\n   location = 's3a://trino/customers/customer_table',\n   max_commit_retry = 4\n)\n\n--- DESCRIBE TABLE Output ---\n['customer_id', 'bigint', '', '']\n['first_name', 'varchar', '', '']\n['last_name', 'varchar', '', '']\n['email', 'varchar', '', '']\n['phone', 'varchar', '', '']\n['signup_date', 'date', '', '']\n</code></pre>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#injecting-sample-data","title":"Injecting Sample Data","text":"<p>With the table created, let's inject a comprehensive set of sample customer data. This multi-row <code>INSERT</code> statement efficiently populates the table for testing purposes.</p> <pre><code>if conn and cursor:\n    try:\n        # SQL DML to insert multiple rows of sample data\n        query_insert_data = f\"\"\"INSERT INTO {LAKEHOUSE_CATALOG}.{SCHEMA_NAME}.{TABLE_NAME} VALUES\n        (101, 'John', 'Doe', 'john.doe@example.com', '555-1234', DATE '2025-01-01'),\n        (102, 'Jane', 'Smith', 'jane.smith@example.com', '555-5678', DATE '2025-01-02'),\n        (103, 'Robert', 'Johnson', 'robert.johnson@example.com', '555-8765', DATE '2025-01-03'),\n        (104, 'Emily', 'Williams', 'emily.williams@example.com', '555-4321', DATE '2025-01-04'),\n        (105, 'Michael', 'Brown', 'michael.brown@example.com', '555-3456', DATE '2025-01-05'),\n        (106, 'Emma', 'Davis', 'emma.davis@example.com', '555-7890', DATE '2025-01-06'),\n        (107, 'Daniel', 'Garcia', 'daniel.garcia@example.com', '555-6543', DATE '2025-01-07'),\n        (108, 'Olivia', 'Martinez', 'olivia.martinez@example.com', '555-2109', DATE '2025-01-08'),\n        (109, 'James', 'Lopez', 'james.lopez@example.com', '555-8901', DATE '2025-01-09'),\n        (110, 'Sophia', 'Hernandez', 'sophia.hernan@example.com', '555-0987', DATE '2025-01-10'),\n        (111, 'Liam', 'Young', 'liam.young@example.com', '555-7777', DATE '2025-01-11'),\n        (112, 'Ava', 'Lee', 'ava.lee@example.com', '555-9999', DATE '2025-01-12'),\n        (113, 'Ethan', 'Gonzalez', 'ethan.gonzalez@example.com', '555-1230', DATE '2025-01-13'),\n        (114, 'Mia', 'Nelson', 'mia.nelson@example.com', '555-5670', DATE '2025-01-14'),\n        (115, 'William', 'Carter', 'william.carter@example.com', '555-4320', DATE '2025-01-15'),\n        (116, 'Isabella', 'Mitchell', 'isabella.mitch@example.com', '555-3450', DATE '2025-01-16'),\n        (117, 'Alexander', 'Perez', 'alexander.perez@example.com', '555-7895', DATE '2025-01-17'),\n        (118, 'Charlotte', 'Roberts', 'charlotte.r@example.com', '555-6545', DATE '2025-01-18'),\n        (119, 'Benjamin', 'Turner', 'benjamin.turner@example.com', '555-2105', DATE '2025-01-19'),\n        (120, 'Amelia', 'Phillips', 'amelia.p@example.com', '555-8905', DATE '2025-01-20')\"\"\"\n        cursor.execute(query_insert_data)\n        print(f\"Sample data inserted successfully into '{LAKEHOUSE_CATALOG}.{SCHEMA_NAME}.{TABLE_NAME}'.\")\n    except Exception as e:\n        print(f\"Error inserting sample data: {e}\")\nelse:\n    print(\"Trino connection not established. Cannot insert data.\")\n</code></pre> <pre><code>Sample data inserted successfully into 'lakehouse.customers.customer_table'.\n</code></pre>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#querying-sample-data","title":"Querying Sample Data","text":"<p>The following code snippet executes a query to retrieve and display a limited set of records from the specified table based on a given condition.</p> <pre><code>cursor = conn.cursor()\nquery_select = f\"\"\"SELECT * FROM {LAKEHOUSE_CATALOG}.{SCHEMA_NAME}.{TABLE_NAME} WHERE signup_date &gt; DATE '2025-01-15' LIMIT 10\"\"\"\ncursor.execute(query_select)\nresults = cursor.fetchall()\n\nprint(f\"\\n--- First 10 Rows from {TABLE_NAME} ---\")\nfor row in results:\n    print(row)\nconn.close()\n</code></pre> <pre><code>--- First 10 Rows from customer_table ---\n[116, 'Isabella', 'Mitchell', 'isabella.mitch@example.com', '555-3450', datetime.date(2025, 1, 16)]\n[117, 'Alexander', 'Perez', 'alexander.perez@example.com', '555-7895', datetime.date(2025, 1, 17)]\n[118, 'Charlotte', 'Roberts', 'charlotte.r@example.com', '555-6545', datetime.date(2025, 1, 18)]\n[119, 'Benjamin', 'Turner', 'benjamin.turner@example.com', '555-2105', datetime.date(2025, 1, 19)]\n[120, 'Amelia', 'Phillips', 'amelia.p@example.com', '555-8905', datetime.date(2025, 1, 20)]\n</code></pre>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#api","title":"API","text":""},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#definition-and-deployment","title":"Definition and Deployment","text":"<p>This section defines our API endpoint, which acts as the interface to query our Trino table. We'll then use <code>practicuscore</code> to deploy this API.</p>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#api-endpoint-definition-and-file-creation","title":"API Endpoint Definition and File Creation","text":"<p>Here are the steps to manually add the API code: - In your current working directory, create a new folder named <code>apis</code>. - Inside the <code>apis</code> folder, create a new file called <code>trino_api.py</code>. - Open <code>trino_api.py</code> with a text editor of your choice. - Copy the entire provided API code and paste it into <code>trino_api.py</code>. - The placeholders <code>LAKEHOUSE_CATALOG</code>, <code>SCHEMA_NAME</code>, and <code>TABLE_NAME</code> in the query string must be replaced by the user with the actual catalog, schema, and table names defined earlier in their environment or notebook cells to ensure the query works correctly. - Save and close the file.</p> <pre><code>from datetime import date\nfrom pydantic import BaseModel\nimport practicuscore as prt\nimport trino\nimport trino.auth\n\nclass Customer(BaseModel):\n    customer_id: int\n    first_name: str\n    last_name: str\n    email: str\n    phone: str\n    signup_date: date\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": 101,\n                    \"first_name\": \"John\",\n                    \"last_name\": \"Doe\",\n                    \"email\": \"john.doe@example.com\",\n                    \"phone\": \"555-1234\",\n                    \"signup_date\": \"2025-01-01\"\n                }\n            ]\n        }\n    }\n\nclass CustomerIdRequest(BaseModel):\n    customer_id: int\n\nclass CustomerResponse(BaseModel):\n    customer: Customer\n\n@prt.api(\"/customer-by-id\")\nasync def get_customer_by_id(payload: CustomerIdRequest, **kwargs) -&gt; CustomerResponse:\n    try:\n        parsed_payload = CustomerIdRequest.parse_obj(payload)\n\n        conn = trino.dbapi.connect(\n            host=\"&lt;TRINO_EXTERNAL_HOST&gt;\",  # Replace with your actual Trino external host\n            port=443,\n            http_scheme=\"https\",\n            verify=False,\n            auth=trino.auth.BasicAuthentication(\"&lt;USER_NAME&gt;\", \"&lt;PASSWORD&gt;\")  # Basic authentication,\n        )\n        cursor = conn.cursor()\n\n        query = \"SELECT customer_id, first_name, last_name, email, phone, signup_date FROM {LAKEHOUSE_CATALOG}.{SCHEMA_NAME}.{TABLE_NAME} WHERE customer_id = ?\"\n        cursor.execute(query, (parsed_payload.customer_id,))\n        row = cursor.fetchone()\n\n        if not row:\n            raise HTTPException(status_code=404, detail=\"Customer not found\")\n\n        columns = [col[0] for col in cursor.description]\n        customer = Customer(**dict(zip(columns, row)))\n\n        return CustomerResponse(customer=customer)\n\n    except HTTPException as http_exc:\n        raise http_exc\n    except Exception as e:\n        # For production, you'd typically log the full error here\n        raise HTTPException(status_code=500, detail=\"Internal server error\")\n</code></pre>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#internal-test-of-api","title":"Internal Test of API","text":"<p>This Python code snippet demonstrates a basic interaction with a hypothetical practicuscore API to retrieve customer information.</p> <pre><code>from apis.trino_api import CustomerResponse\nimport practicuscore as prt\n\npayload = {\"customer_id\": 103}\nresponse: CustomerResponse = prt.apps.test_api(\"/customer-by-id\", payload=payload)\n\nprint(response)\n</code></pre> <pre><code>customer=Customer(customer_id=103, first_name='Robert', last_name='Johnson', email='robert.johnson@example.com', phone='555-8765', signup_date=datetime.date(2025, 1, 3))\n</code></pre>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#api-deployment","title":"API Deployment","text":"<p>This section describes the Python code responsible for defining and deploying an API using the <code>practicuscore</code> framework.</p> <pre><code>app_deployment_key = \"appdepl-1\"\napp_prefix = \"apps\"\n\napp_name = \"trino-app\"\nvisible_name = \"Trino API App\"\ndescription = \"This API retrieves customer details from Trino using the provided customer_id. It returns basic information such as name, email, phone, and signup date.\"\nicon = \"fa-rocket\"\n\nimport practicuscore as prt \n\n# Deploy the API\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # Use None to deploy from the current directory\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"\\n--- API Deployment Details ---\")\nprint(\"Booting UI  :\", app_url)\nprint(\"Booting API :\", api_url)\nprint(\"API Docs    :\", api_url + \"redoc/\")\n</code></pre>"},{"location":"technical-tutorial/extras/big-data-analytics/trino/trino-iceberg-API-integration/#querying-data-via-the-deployed-api","title":"Querying Data via the Deployed API","text":"<p>Finally, we'll make an actual <code>HTTP</code> request to the deployed API to retrieve customer data. This demonstrates the end-to-end functionality.</p> <pre><code>import practicuscore as prt \n\ntoken = None  # Get a new token, or reuse existing, if not expired\ntoken = prt.apps.get_session_token(api_url=api_url, token=token)\nprint(api_url)\ncustomer_by_id_api_url = f\"{api_url}customer-by-id/\"\nprint(customer_by_id_api_url)\nheaders = {\"Authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n</code></pre> <pre><code>import requests\n\npayload_dict = {\"customer_id\": 108}\n\ntry:\n    resp = requests.post(customer_by_id_api_url, json=payload_dict, headers=headers)\n    resp.raise_for_status() # Raise for HTTP errors\n\n    # Work directly with the raw JSON dictionary\n    response_data = resp.json()\n\n    print(\"\\n--- API Raw Response JSON ---\")\n    print(response_data)\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"\\n--- HTTP Request Failed ---\")\n    print(f\"Error: {e}\")\n    print(\"Raw response body (if any):\")\n    print(resp.text)\n\nexcept Exception as e:\n    print(f\"\\n--- Error Processing Response ---\")\n    print(f\"An unexpected error occurred: {e}\")\n</code></pre> <pre><code>--- API Raw Response JSON ---\n{'customer': {'customer_id': 108, 'first_name': 'Olivia', 'last_name': 'Martinez', 'email': 'olivia.martinez@example.com', 'phone': '555-2109', 'signup_date': '2025-01-08'}}\n</code></pre> <p>This interactive notebook provides a complete walk-through from setting up your Trino environment to deploying and consuming a data API. </p> <p>Previous: Spark Object Storage</p>"},{"location":"technical-tutorial/extras/data-analysis/eda/analyze/","title":"EDA Sample","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.style as plt_styl\n\nimport warnings\n\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", 30)\npd.set_option(\"display.width\", 150)\npd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\nwarnings.simplefilter(action=\"ignore\")\n</code></pre> <pre><code>data_set_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/insurance.csv\"}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n\nproc = worker.load(data_set_conn, engine=\"AUTO\")\n\ndf = proc.get_df_copy()\ndisplay(df)\n</code></pre> <pre><code>df.info()\n</code></pre> <pre><code>def grab_col_names(dataframe, cat_th=10, car_th=25, show_date=False):\n    date_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"datetime64[ns]\"]\n    cat_cols = dataframe.select_dtypes([\"object\", \"category\"]).columns.tolist()\n    num_but_cat = [\n        col for col in dataframe.select_dtypes([\"float\", \"integer\"]).columns if dataframe[col].nunique() &lt; cat_th\n    ]\n    cat_but_car = [\n        col for col in dataframe.select_dtypes([\"object\", \"category\"]).columns if dataframe[col].nunique() &gt; car_th\n    ]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    num_cols = dataframe.select_dtypes([\"float\", \"integer\"]).columns\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f\"date_cols: {len(date_cols)}\")\n    print(f\"cat_cols: {len(cat_cols)}\")\n    print(f\"num_cols: {len(num_cols)}\")\n    print(f\"cat_but_car: {len(cat_but_car)}\")\n    print(f\"num_but_cat: {len(num_but_cat)}\")\n\n    if show_date == True:\n        return date_cols, cat_cols, cat_but_car, num_cols, num_but_cat\n    else:\n        return cat_cols, cat_but_car, num_cols, num_but_cat\n</code></pre> <pre><code>grab_col_names(df)\n</code></pre> <pre><code>cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df)\n</code></pre> <pre><code>df.head()\n</code></pre> <pre><code>df[(df[\"region\"] == 3)]\n</code></pre> <pre><code>print(cat_cols)\n</code></pre> <pre><code>def cat_analyzer(dataframe, variable, target=None):\n    print(variable)\n    if target == None:\n        print(\n            pd.DataFrame(\n                {\n                    \"COUNT\": dataframe[variable].value_counts(),\n                    \"RATIO\": dataframe[variable].value_counts() / len(dataframe),\n                }\n            ),\n            end=\"\\n\\n\\n\",\n        )\n    else:\n        temp = dataframe[dataframe[target].isnull() == False]\n        print(\n            pd.DataFrame(\n                {\n                    \"COUNT\": dataframe[variable].value_counts(),\n                    \"RATIO\": dataframe[variable].value_counts() / len(dataframe),\n                    \"TARGET_COUNT\": dataframe.groupby(variable)[target].count(),\n                    \"TARGET_MEAN\": temp.groupby(variable)[target].mean(),\n                    \"TARGET_MEDIAN\": temp.groupby(variable)[target].median(),\n                    \"TARGET_STD\": temp.groupby(variable)[target].std(),\n                }\n            ),\n            end=\"\\n\\n\\n\",\n        )\n</code></pre> <pre><code>cat_analyzer(df, \"region\")\n</code></pre> <pre><code>df[num_cols].hist(figsize=(25, 20), bins=15);\n</code></pre> <pre><code>df[num_cols].describe([0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.95, 0.99]).T.drop([\"count\"], axis=1)\n</code></pre> <pre><code>def outliers_threshold(dataframe, column):\n    q1 = dataframe[column].quantile(0.05)\n    q3 = dataframe[column].quantile(0.95)\n    inter_quartile_range = q3 - q1\n    low = q1 - 1.5 * inter_quartile_range\n    up = q3 + 1.5 * inter_quartile_range\n    return low, up\n\n\ndef grab_outlier(dataframe, column, index=False):\n    low, up = outliers_threshold(dataframe, column)\n    if dataframe[(dataframe[column] &lt; low) | (dataframe[column] &gt; up)].shape[0] &lt; 10:\n        print(dataframe[(dataframe[column] &lt; low) | (dataframe[column] &gt; up)][[column]])\n    else:\n        print(dataframe[(dataframe[column] &lt; low) | (dataframe[column] &gt; up)][[column]])\n    if index:\n        outlier_index = dataframe[(dataframe[column] &lt; low) | (dataframe[column] &gt; up)].index.tolist()\n        return outlier_index\n\n\ndef replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outliers_threshold(dataframe, col_name)\n    if low_limit &gt; 0:\n        dataframe.loc[(dataframe[col_name] &lt; low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] &gt; up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] &gt; up_limit), col_name] = up_limit\n</code></pre> <pre><code>df[df[\"age\"] &lt;= 64][\"age\"].plot(kind=\"box\")\n</code></pre> <pre><code>for col in num_cols:\n    print(\n        \"********************************************************************* {} *****************************************************************************\".format(\n            col.upper()\n        )\n    )\n    grab_outlier(df, col, True)\n    replace_with_thresholds(df, col)\n    print(\n        \"****************************************************************************************************************************************************************\",\n        end=\"\\n\\n\\n\\n\\n\",\n    )\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>cat_cols\n</code></pre> <pre><code>df.head()\n</code></pre> <pre><code>plt.figure(figsize=(30, 20))\ncorr_matrix = df.select_dtypes(include=[\"int64\", \"int32\", \"float64\"]).corr()\nsns.heatmap(corr_matrix, annot=True, cmap=\"Reds\")\nplt.title(\"Correlation Heatmap\")\n</code></pre> <pre><code>def cat_summary(dataframe, x_col, plot=False, rotation=45):\n    display(\n        pd.DataFrame(\n            {x_col: dataframe[x_col].value_counts(), \"Ratio\": 100 * dataframe[x_col].value_counts() / len(dataframe)}\n        )\n    )\n\n    if plot:\n        count = dataframe.groupby(x_col).size().sum()\n        dataframe_grouped = (\n            dataframe.groupby(x_col).size().reset_index(name=\"counts\").sort_values(\"counts\", ascending=False)\n        )\n        num_bars = len(dataframe_grouped[x_col].unique())\n        colors = plt.cm.Set3(np.linspace(0, 1, num_bars))\n        fig, ax = plt.subplots(figsize=(8, 5))\n\n        x_pos = range(len(dataframe_grouped[x_col]))\n\n        ax.bar(x_pos, dataframe_grouped[\"counts\"], color=colors)\n        ax.set_xlabel(x_col)\n        ax.set_ylabel(\"Count\")\n        ax.set_title(f\"Distribution by {x_col}\")\n\n        ax.set_xticks(x_pos)\n        ax.set_xticklabels(dataframe_grouped[x_col], rotation=rotation)\n\n        for i, value in enumerate(dataframe_grouped[\"counts\"]):\n            ax.annotate(\n                \"{:.1%}\".format(value / count), (i, value), textcoords=\"offset points\", xytext=(0, 10), ha=\"center\"\n            )\n\n        plt.show()\n</code></pre> <pre><code>for col in cat_cols:\n    cat_summary(df, col, plot=True)\n</code></pre> <pre><code>proc.kill()\n</code></pre> <p>Previous: Multiple Layers | Next: Data Processing &gt; Pre Process Data &gt; Preprocess</p>"},{"location":"technical-tutorial/extras/data-analysis/plot/introduction/","title":"Introduction to plotting with Bokeh","text":"<p>In this example we are going to give you a brief tutorial on how to use color and size feature of glyphs (a.k.a graphs and plots) dynamically by assigning features of dataset to color and size parameters.</p> <ul> <li>How to create figure</li> <li>How to create and edit circle plots </li> <li>How to add dynamic explanations over glyphs</li> </ul>"},{"location":"technical-tutorial/extras/data-analysis/plot/introduction/#before-you-begin","title":"Before you begin","text":"<ul> <li>At the time of this writing, Bokeh only support <code>Jupyter Lab</code> and not <code>VS Code</code></li> <li>View issue details</li> <li>Please make sure you run this example in Jupyter Lab, or verify on Bokeh repo that the issue is resolved.</li> </ul> <pre><code>from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import LinearColorMapper, ColumnDataSource, ColorBar, HoverTool\nfrom bokeh.transform import transform\nimport practicuscore as prt\n</code></pre> <p>Here's a breakdown of each bokeh function we've imported:</p> <ol> <li>bokeh.plotting:<ul> <li>figure: This module provides a high-level interface for creating Bokeh plots. It includes functions for creating and customizing figures, such as setting titles, axis labels, plot size, and other visual properties.</li> <li>show: This function displays a Bokeh plot in the current environment, such as a browser or a Jupyter notebook.</li> <li>output_notebook: This function configures Bokeh to display plots directly within Jupyter notebooks.</li> </ul> </li> <li>bokeh.models:<ul> <li>ColumnDataSource: This module contains a collection of classes and functions representing various components of a Bokeh plot, such as glyphs (shapes representing data points), axes, grids, annotations, and tools. ColumnDataSource is a fundamental data structure in Bokeh that holds the data to be plotted and allows for efficient updating and sharing of data between different plot elements.</li> <li>HoverTool: This module provides a tool for adding interactive hover tooltips to Bokeh plots. It allows users to display additional information about data points when the mouse cursor hovers over them.</li> <li>LinearColorMapper: A mapper that maps numerical data to colors in a linear manner. It's often used to color glyphs based on a continuous range of data values.</li> <li>ColorBar: A color bar that provides a visual representation of the mapping between data values and colors, often used with LinearColorMapper</li> </ul> </li> <li>bokeh.transform:<ul> <li>transform: This function is used to apply a transformation to the data in a column of a ColumnDataSource. It takes two arguments, <ul> <li>column_name: The name of the column in the ColumnDataSource to transform.</li> <li>transform_expression: A JavaScript expression defining the transformation to apply to the data. This expression can involve mathematical operations, functions, or other JavaScript constructs.</li> </ul> </li> </ul> </li> </ol> <pre><code>worker = prt.get_local_worker()\n</code></pre> <p>One of the most illustrative datasets for demonstrating dynamic size and color is the Titanic dataset.</p> <p>The Titanic dataset is a popular dataset used in machine learning and data analysis. It contains information about passengers aboard the RMS Titanic, including whether they survived or not. Within this data set we will use columns of pclass, fare, age and survived. Let's describe what these columns means for better understanding.</p> <ul> <li>Pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).</li> <li>Fare: Passenger fare.</li> <li>Age: Passenger's age in years.</li> <li>Survived: Indicates whether the passenger survived or not (0 = No, 1 = Yes).</li> </ul> <p>Let's load it into one of our worker environments.</p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"sample_size\": 1180,\n    \"file_path\": \"/home/ubuntu/samples/data/titanic.csv\",\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine=\"AUTO\")\n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>To display Bokeh plots inline in a classic Jupyter notebook, use the output_notebook() function from bokeh.io.</p> <pre><code>output_notebook()\n</code></pre> <p>We need to create a data structure that holds the data to be plotted to use Bokeh more efficiently. For this we will use ColumnDataSource() function of Bokeh.</p> <pre><code>source = ColumnDataSource(df)\n</code></pre> <p>We need to create a mapper of colors to use color feature as dynamically. We will use LinearColorMapper() built-in function of Bokeh to create our mapper.</p> <pre><code>color_mapper = LinearColorMapper(palette=\"Sunset11\", low=df[\"survived\"].min(), high=df[\"survived\"].max())\n</code></pre> <p>Here's a breakdown of the parameters:</p> <ul> <li>palette: This parameter specifies the color palette to use for mapping the data values. In this case, it's set to 'Sunset11', which is refers to a predefined color palette named 'Sunset11' of Bokeh. This palette consists of 11 distinct colors arranged in a gradient from light to dark or from one color to another. You can look at Bokeh's documentation to see more options.</li> <li>low: This parameter sets the lowest data value in the range of values to be mapped to colors. It's typically set to the minimum value of the data being mapped. In this case, it's set to df['survived'].min(), indicating that the lowest value in the 'survived' column of the DataFrame (df) will be mapped to the lowest color in the palette.</li> <li>high: This parameter sets the highest data value in the range of values to be mapped to colors. It's typically set to the maximum value of the data being mapped. Here, it's set to df['survived'].max(), indicating that the highest value in the 'survived' column of the DataFrame (df) will be mapped to the highest color in the palette.</li> </ul> <p>Let's create our figure to do some visualization.</p> <pre><code>p = figure(title=\"Analysis Over Survivors\", x_axis_label=\"age\", y_axis_label=\"fare\", width=600, height=400)\n</code></pre> <p>Here's an explanation of each parameter in the figure function call:</p> <ul> <li>title: Sets the title of the plot. In this case, it's set to \"Analysis Over Survivors\". The title is displayed at the top of the plot.</li> <li>x_axis_label: Specifies the label for the x-axis. It provides information about the data represented on the x-axis. In this case, it's set to 'age', indicating that the x-axis represents age of travellers.</li> <li>y_axis_label: Specifies the label for the y-axis. Similar to x_axis_label, it provides information about the data represented on the y-axis. Here, it's set to 'fare', indicating that the y-axis represents paid fares of travellers.</li> <li>width: Sets the width of the plot in pixels. In this case, it's set to 400 pixels, determining the horizontal size of the plot.</li> <li>height: Sets the height of the plot in pixels. Here, it's set to 300 pixels, determining the vertical size of the plot.</li> </ul> <p>Let's continue with our first plotting of circle (circle plotting)!</p> <pre><code>circle = p.circle(\n    x=\"age\", y=\"fare\", radius=\"pclass\", color=transform(\"survived\", color_mapper), alpha=0.5, source=source\n)\nshow(p)\n</code></pre> <p>Here's an explanation of each parameter:</p> <ul> <li>p: This is the figure object where the circles will be added. It seems like p is previously defined as a figure with certain settings like title, axis labels, etc.</li> <li>circle: This variable stores the result of the p.circle function call, representing the circles added to the plot.</li> <li>x: This parameter specifies the x-coordinates of the circles. It's mapped to the 'age' column in the data source (source), indicating the position of each circle along the x-axis.</li> <li>y: This parameter specifies the y-coordinates of the circles. It's mapped to the 'fare' column in the data source (source), indicating the position of each circle along the y-axis.</li> <li>radius: This parameter specifies the radius of the circles. It's mapped to the 'pclass' column in the data source (source), indicating the radius of each circle.</li> <li>color: This parameter specifies the color of the circles. Here, it's set to transform('survived', color_mapper).</li> <li>transform('survived', color_mapper): This function applies the color mapping defined by the LinearColorMapper object (color_mapper) to the 'survived' column in the data source (source). The color of each circle will be determined by the value in the 'survived' column, mapped to colors based on the color_mapper.</li> <li>alpha: This parameter sets the transparency of the circles. It's set to 0.5, making the circles partially transparent.</li> <li>source: This parameter specifies the data source from which the circles will pull their data. It's set to source, which is likely a ColumnDataSource object containing the data needed to plot the  circles.</li> </ul> <p>At this point we should add a bar which describes the meaning of colors. We could do this by using ColorBar() feature of Bokeh.</p> <pre><code>color_bar = ColorBar(color_mapper=color_mapper, padding=3, ticker=p.xaxis.ticker, formatter=p.xaxis.formatter)\n\np.add_layout(color_bar, \"right\")\nshow(p)\n</code></pre> <p>Here's an explanation of each parameter:</p> <ul> <li>color_mapper: This parameter specifies the LinearColorMapper object (color_mapper) that defines the mapping between data values and colors. The color bar will use this mapper to display the range of colors corresponding to the range of data values.</li> <li>padding: This parameter sets the padding (in pixels) between the color bar and other elements of the plot. It's set to 3 pixels in this case, providing some space around the color bar.</li> <li>ticker: This parameter specifies the ticker to use for labeling the color bar axis. It's set to p.xaxis.ticker, which likely means that the same ticker used for the x-axis of the plot (p) will be used for the color bar axis.</li> <li>formatter: This parameter specifies the formatter to use for formatting the tick labels on the color bar axis. It's set to p.xaxis.formatter, meaning that the same formatter used for the x-axis of the plot (p) will be used for the color bar axis.</li> <li>p.add_layout: This method adds a layout element to the plot (p). Here, we're adding the color bar to the plot.</li> <li>color_bar: This is the ColorBar object that we created earlier, representing the color bar to be added to the plot.</li> <li>'right': This parameter specifies the location where the color bar will be added relative to the plot. Here, it's set to 'right', indicating that the color bar will be placed to the right of the plot.</li> </ul> <p>We still missing something, it would be a cool feature if we could see the values of data points. Actually, we could use HoverTool() bokeh to do that!</p> <pre><code>tips = [(\"Fare\", \"@fare\"), (\"Age\", \"@age\"), (\"Survived\", \"@survived\"), (\"Pclass\", \"@pclass\")]\n\np.add_tools(HoverTool(tooltips=tips))\nshow(p)\n</code></pre> <p>Right now, when we hover over plots we could see the values of data points.</p> <p>Let's break down the code we have used:</p> <ul> <li>tips: This is a list of tuples, where each tuple contains two elements. The first element of each tuple represents the label for the tooltip, and the second element represents the data field from the data source (source) to be displayed in the tooltip. For example, Fare is the label for the tooltip, and @fare instructs Bokeh to display the value of the fare column from the data source (source) when hovering over a data point</li> <li>p.add_tools: This method adds tools to the plot (p). Here, we're adding the HoverTool to enable hover tooltips.</li> <li>HoverTool: This is a tool provided by Bokeh for adding hover functionality to plots. It displays additional information when the mouse cursor hovers over a data point.</li> <li>tooltips=tips: This parameter of the HoverTool constructor specifies the tooltips to be displayed when hovering over data points. We pass the tips list, which contains the tooltip labels and data fields.</li> </ul> <p>That was the end. You can always checkout our other notebooks about plotting or documentation of bokeh to see more!</p> <pre><code>proc.kill()\n</code></pre> <p>Previous: Memory Chabot | Next: Multiple Layers</p>"},{"location":"technical-tutorial/extras/data-analysis/plot/multiple-layers/","title":"Plotting with Multiple Layers","text":"<p>In this example we are going to give you a brief tutorial of how to use multiple graphics on one figure by using bokeh.</p> <p>We are going to cover these topics: - How to create and edit a figure - How to process multiple glyphs (graphs) over a figure - How to add dynamic explanations over glyphs - How to create Bar and Line plot</p>"},{"location":"technical-tutorial/extras/data-analysis/plot/multiple-layers/#before-you-begin","title":"Before you begin","text":"<p>At the time of this writing, Bokeh only support <code>Jupyter Lab</code> and not <code>VS Code</code> View issue details Please make sure you run this example in Jupyter Lab, or verify on Bokeh repo that the issue is resolved.</p> <p>Let's begin by importing our libraries</p> <pre><code>from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import ColumnDataSource, HoverTool\nimport practicuscore as prt\n</code></pre> <p>Here's a breakdown of each bokeh function we've imported:</p> <ol> <li>bokeh.plotting<ul> <li>figure: Creates a new Bokeh plot with customizable options such as plot size, title, axis labels, etc.</li> <li>show: Displays a Bokeh plot in the current environment, such as a browser or a Jupyter notebook.</li> <li>output_notebook: Configures Bokeh to display plots directly in Jupyter notebooks.</li> </ul> </li> <li>bokeh.models<ul> <li>ColumnDataSource: A data structure that holds the data to be plotted and facilitates efficient updating and sharing of data between different plot elements.</li> <li>HoverTool: A tool that provides interactive hover tooltips, displaying additional information about data points when the mouse cursor hovers over them</li> </ul> </li> </ol> <pre><code>worker = prt.get_local_worker()\n</code></pre> <p>One of the most illustrative datasets for demonstrating multiple layer analyze is the Iris dataset.</p> <p>The Iris dataset is a popular dataset in machine learning and statistics, often used for classification tasks. It consists of 150 samples of iris flowers, each belonging to one of three species: Setosa, Versicolor, or Virginica. Within this data set we will use both Bar and Circle graphic style from Plot. The dataset comprises four features, each representing measurements of the length and width of both the petals and sepals of flowers.</p> <p>Let's load it into one of our worker environments.</p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"sample_size\": 150,\n    \"file_path\": \"/home/ubuntu/samples/data/iris.csv\",\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine=\"AUTO\")\n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Before starting data analyzing with bokeh, we need to do some preprocess on Iris dataset</p> <pre><code>means = df.groupby(\"species\").mean().reset_index()\n</code></pre> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\nmeans[\"species_encoded\"] = label_encoder.fit_transform(means[\"species\"])\n</code></pre> <pre><code>means\n</code></pre> <p>To display Bokeh plots inline in a classic Jupyter notebook, use the output_notebook() function from bokeh.io.</p> <pre><code>output_notebook()\n</code></pre> <p>We need to create a data structure that holds the data to be plotted to use Bokeh more efficiently. For this we will use ColumnDataSource() function of Bokeh.</p> <pre><code>source = ColumnDataSource(means)\n</code></pre> <p>Let's create our figure to do some visualisation.</p> <pre><code>p = figure(title=\"Analysis Over Species\", x_axis_label=\"Species\", y_axis_label=\"Features\", width=400, height=300)\n</code></pre> <p>Here's an explanation of each parameter in the figure function call:</p> <ul> <li>title: Sets the title of the plot. In this case, it's set to \"Analysis Over Species\". The title is displayed at the top of the plot.</li> <li>x_axis_label: Specifies the label for the x-axis. It provides information about the data represented on the x-axis. In this case, it's set to 'Species', indicating that the x-axis represents different species.</li> <li>y_axis_label: Specifies the label for the y-axis. Similar to x_axis_label, it provides information about the data represented on the y-axis. Here, it's set to 'Features', indicating that the y-axis represents various features.</li> <li>width: Sets the width of the plot in pixels. In this case, it's set to 400 pixels, determining the horizontal size of the plot.</li> <li>height: Sets the height of the plot in pixels. Here, it's set to 300 pixels, determining the vertical size of the plot.</li> </ul> <p>Let's continue with our first plotting of vbar (vertical bar plotting)!</p> <pre><code>first_layer = p.vbar(\n    x=\"species_encoded\",\n    top=\"sepal_length\",\n    width=0.9,\n    line_color=\"green\",\n    fill_color=\"lime\",\n    fill_alpha=0.5,\n    legend_label=\"Sepal Length\",\n    source=source,\n)\nshow(p)\n</code></pre> <p>Here's an explanation of each vbar parameters:</p> <ul> <li>p: This is the figure object where the plot will be added.</li> <li>vbar: This is the glyph function used to create vertical bar glyphs (rectangles) on the plot.</li> <li>x: This parameter specifies the x-coordinates of the bars.</li> <li>top: This parameter specifies the top edge of each bar.</li> <li>width: This parameter determines the width of the bars. Here, it's set to 0.9, indicating that the bars will have a width of 0.9 units along the x-axis.</li> <li>line_color: This parameter sets the color of the outline of the bars. It's set to 'green', giving the bars a green outline.</li> <li>fill_color: This parameter sets the fill color of the bars. It's set to 'lime', giving the bars a lime green color.</li> <li>fill_alpha: This parameter sets the transparency of the fill color. It's set to 0.5, making the bars partially transparent.</li> <li>legend_label: This parameter sets the label for the legend entry corresponding to this glyph. It's set to \"Sepal Length\", which will be displayed in the legend.</li> <li>source: This parameter specifies the data source from which the glyph will pull its data. It's set to source, which is defined previously by using ColumnDataSource().</li> </ul> <p>Overall, this line of code creates a vertical bar plot of sepal lengths for different species, with customization for appearance and legend labeling, and it adds this plot as a layer to the existing figure p. You can check out bokeh documentation of colors for more coloring options.</p> <p>Let's add another vbar, a second layer, which will show case petal_length.</p> <pre><code>second_layer = p.vbar(\n    x=\"species_encoded\",\n    top=\"petal_length\",\n    width=0.9,\n    line_color=\"blue\",\n    fill_color=\"lightskyblue\",\n    fill_alpha=0.5,\n    legend_label=\"Petal Length\",\n    source=source,\n)\nshow(p)\n</code></pre> <p>After adding our second layer it started to be too crowded for such a small figure frame, let's expand it and we could also use some more explanatory labels for axis.</p> <pre><code>p.width = 800\np.height = 600\np.xaxis.axis_label = \"Flower Species\"\np.yaxis.axis_label = \"Flower Features\"\n\nshow(p)\n</code></pre> <p>Let's add some plots about our flowers width, but if we add more bars it will make the figure too confusing to read. Therefore, let's add line plots to visualize width features of our flowers.</p> <pre><code>third_layer = p.line(\n    x=\"species_encoded\",\n    y=\"sepal_width\",\n    line_width=4,\n    line_color=\"darkolivegreen\",\n    legend_label=\"Sepal Width\",\n    source=source,\n)\nshow(p)\n</code></pre> <p>Here's an explanation of each line parameters:</p> <ul> <li>p: This is the figure object where the line plot will be added.</li> <li>line: This is the glyph function used to create line glyphs on the plot.</li> <li>x: This parameter specifies the x-coordinates of the line.</li> <li>y: This parameter specifies the y-coordinates of the line.</li> <li>line_width: This parameter determines the width of the line. It's set to 4, indicating that the line will be drawn with a width of 4 units.</li> <li>line_color: This parameter sets the color of the line. It's set to 'darkolivegreen', giving the line a dark olive green color.</li> <li>legend_label: This parameter sets the label for the legend entry corresponding to this glyph. It's set to \"Sepal Width\", which will be displayed in the legend.</li> <li>source: This parameter specifies the data source from which the glyph will pull its data.  It's set to source, which is defined previously by using ColumnDataSource().</li> </ul> <p>Overall, this line of code creates a line plot of sepal widths for different species, with customization for appearance and legend labeling, and it adds this plot as a layer to the existing figure p.</p> <p>Let's add our fourth and last plot!</p> <pre><code>fourth_layer = p.line(\n    x=\"species_encoded\", y=\"petal_width\", line_width=4, line_color=\"darkblue\", legend_label=\"Petal Width\", source=source\n)\nshow(p)\n</code></pre> <p>After adding all layer we still missing something, it would be a cool feature if we could see the values of data points. Actually, we could use HoverTool() bokeh to do that!</p> <pre><code>tips = [\n    (\"Sepal Length\", \"@sepal_length\"),\n    (\"Petal Length\", \"@petal_length\"),\n    (\"Sepal Width\", \"@sepal_width\"),\n    (\"Petal Width\", \"@petal_width\"),\n]\n\np.add_tools(HoverTool(tooltips=tips))\nshow(p)\n</code></pre> <p>Right now, when we hover over plots we could see the values of data points.</p> <p>Let's break down the code we have used:</p> <ul> <li>tips: This is a list of tuples, where each tuple contains two elements. The first element of each tuple represents the label for the tooltip, and the second element represents the data field from the data source (source) to be displayed in the tooltip. For example, Sepal Length is the label for the tooltip, and @sepal_length instructs Bokeh to display the value of the sepal_length column from the data source (source) when hovering over a data point</li> <li>p.add_tools: This method adds tools to the plot (p). Here, we're adding the HoverTool to enable hover tooltips.</li> <li>HoverTool: This is a tool provided by Bokeh for adding hover functionality to plots. It displays additional information when the mouse cursor hovers over a data point.</li> <li>tooltips=tips: This parameter of the HoverTool constructor specifies the tooltips to be displayed when hovering over data points. We pass the tips list, which contains the tooltip labels and data fields.</li> </ul> <p>Last but not least, we could remove layers by using their visible parameter:</p> <pre><code>first_layer.visible = False\nshow(p)\n</code></pre> <pre><code>first_layer.visible = True\nshow(p)\n</code></pre> <p>That was the end. You can always checkout our other notebooks about plotting or documentation of bokeh to see more!</p> <pre><code>proc.kill()\n</code></pre> <p>Previous: Introduction | Next: Eda &gt; Analyze</p>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/","title":"Data Processing","text":""},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#scenario-pre-process-steps-by-using-sdk","title":"Scenario: Pre-process steps by using SDK","text":"<p>In this example, we'll showcase how to apply pre-process steps by using our SDK, practicuscore.</p> <ol> <li> <p>Loading the \"income\" dataset</p> </li> <li> <p>Profiling the dataset</p> </li> <li> <p>Applying pre-process steps:</p> <ul> <li>Suppressing outliers by using snippets</li> <li>Applying one hot encoding</li> <li>Applying label encoding</li> <li>Re-naming columns</li> <li>Deleting columns</li> <li>Applying standardization by using snippets</li> </ul> </li> </ol>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#step-1-loading-the-dataset","title":"Step-1: Loading the Dataset","text":"<pre><code>dataset_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/income.csv\"}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(dataset_conn)\n\nproc.show_head()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#step-2-profiling-the-dataset","title":"Step-2: Profiling the dataset","text":"<p>\"ydata_profiling\" Python library is a powerful tool for data analysts and data scientists to analyze data sets quickly and effectively. </p> <p>The ProfileReport method of this library performs a thorough inspection of a data frame and generates a detailed profile report. This report provides comprehensive summaries of the dataset's overall statistics, missing values, distributions, correlations and other important information. With the report, users can quickly identify potential problems and patterns in the data set, which greatly speeds up and simplifies the data cleaning and pre-processing phases.</p> <pre><code>from ydata_profiling import ProfileReport\n</code></pre> <pre><code>df_raw = proc.get_df_copy()\n</code></pre> <pre><code>ProfileReport(df_raw)\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#step-3-pre-process","title":"Step-3: Pre-process","text":""},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#31-handling-with-missing-values","title":"3.1: Handling with missing values","text":"<p>The 'handle_missing' method of the SDK can be utilized to fill or drop missing values on target columns.</p> <ul> <li>technique: The method which used in handling missing value. It could take the values down below:<ul> <li>'delete': drops the rows with missing values</li> <li>'custom': filling the missing values with a custom value</li> <li>'minimum': filling the missing values with minunmum value of column</li> <li>'maximum': filling the missing values with maximum value of column</li> <li>'average': filling the missing values with average value of column</li> </ul> </li> <li>column_list: List of targeted columns (columns with missing values)</li> <li>custom_value: The value which will be used in filling columns, if not using 'custom' method leave it to be 'None'</li> </ul> <pre><code>proc.handle_missing(technique=\"minimum\", column_list=[\"workclass\"], custom_value=\"None\")\nproc.handle_missing(technique=\"custom\", column_list=[\"native-country\"], custom_value=\"unknown\")\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#32-suppressing-of-outliers-by-using-snippets","title":"3.2: Suppressing of outliers by using snippets","text":"<p>Snippets are built-in python functions prepared by Practicus AI but, also you can build your own snippets for your company (for more information please visit https://docs.practicus.ai/tutorial)</p> <p>To utilize snippets effectively, ensure that you create and open a folder named 'snippets' within your working directory. Then, place the snippet files into this designated folder.</p> <p>Every snippets has parameters which are optional or mandatory to run. You can checkout the parameters within the snippet code.</p> <p>E.g. the paramaters within 'suppress_outliers' can be listed as: - outlier_float_col_list: list[str] | None (List of numeric columns to check for outliers. If left empty, applies to all numeric columns.), - q1_percentile: float = 0.25 (Custom percentile for Q1, takes 0.25 as default value), - q3_percentile: float = 0.75 (Custom percentile for Q3, takes 0.75 as default value), - result_col_suffix: str | None = \"no_outlier\" (suffix for the new column where the suppressed data will be stored, takes \"no_outlier\" as default), - result_col_prefix: str | None = None (Prefix for the new column where the suppressed data will be stored.),</p> <pre><code>proc.run_snippet(\n    \"suppress_outliers\", outlier_float_col_list=[\"capital-gain\", \"capital-loss\"], q1_percentile=0.05, q3_percentile=0.95\n)\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#33-one-hot-encoding","title":"3.3: One-hot Encoding","text":"<p>The 'one_hot' method of the SDK can be utilized to apply one-hot encoding to the selected column.</p> <ul> <li>column_name: The name of the current column to be one-hot encoded.</li> <li>column_prefix: A prefix to use for the new one-hot encoded columns.</li> </ul> <pre><code>cat_col_list = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"native-country\"]\n</code></pre> <pre><code>for col in cat_col_list:\n    proc.one_hot(column_name=col, column_prefix=col)\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#34-label-encoding","title":"3.4: Label Encoding","text":"<p>The 'categorical_map' method of the SDK can be utilized to apply label encoding to the selected column.</p> <ul> <li>column_name: The name of the current column to be one-hot encoded.</li> <li>column_prefix: A prefix to use for the new one-hot encoded columns.</li> </ul> <pre><code>proc.categorical_map(column_name=\"sex\", column_suffix=\"cat\")\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#35-re-naming-columns","title":"3.5: Re-naming Columns","text":"<p>The 'rename_column' method of the SDK can be utilized to rename columns.</p> <pre><code>proc.rename_column(\"hours-per-week\", \"hours_per_week\")\n</code></pre> <pre><code>proc.rename_column(\"capital-loss\", \"capital_loss\")\n</code></pre> <pre><code>proc.rename_column(\"capital-gain\", \"capital_gain\")\n</code></pre> <pre><code>proc.rename_column(\"education-num\", \"education_num\")\n</code></pre> <pre><code>proc.rename_column(\"income &gt;50K\", \"income_50K\")\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#36-deleting-columns","title":"3.6: Deleting Columns","text":"<p>The 'delete_columns' method of the SDK can be utilized to delete columns.</p> <pre><code>proc.delete_columns([\"sex\", \"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"native-country\"])\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#37-standardization-of-numerical-columns","title":"3.7: Standardization of numerical columns","text":"<p>The 'normalize.py' snippet of the SDK can be utilized to apply standardization to numeric columns.</p> <pre><code>proc.run_snippet(\n    \"normalize\",\n    numeric_col_list=[\"age\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"],\n    normalization_option=\"Min-Max Normalization\",\n    result=None,\n)\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#38-logging-the-pre-process","title":"3.8: Logging the pre-process","text":"<p>The 'wait_until_done' and 'show_logs' methods of the SDK can be utilized to check and log the pre-process steps.</p> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#39-exporting-the-data-set-into-pandas-dataframe","title":"3.9: Exporting the data set into pandas dataframe","text":"<p>The 'get_df_copy' methods of the SDK can be utilized to export the dataset as pandas dataframe to continue working with dataset on different aspect of Data Science.</p> <pre><code>df_processed = proc.get_df_copy()\n</code></pre> <pre><code>df_processed.head()\n</code></pre> <pre><code>ProfileReport(df_processed)\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#snippetsimpute_missing_knnpy","title":"snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(\n    df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform\n):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(\n                f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\"\n            )\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = [\"pandas\"]\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#snippetsnormalizepy","title":"snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(\n    df,\n    numeric_col_list: list[str] | None = None,\n    normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE,\n    result: list[str] | None = None,\n):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = (\n            col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        )\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#snippetssuppress_outlierspy","title":"snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n    df,\n    outlier_float_col_list: list[str] | None,\n    q1_percentile: float = 0.25,\n    q3_percentile: float = 0.75,\n    result_col_suffix: str | None = \"no_outlier\",\n    result_col_prefix: str | None = None,\n):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f\"{col}_{result_col_suffix}\"\n        elif result_col_prefix:\n            new_col_name = f\"{result_col_prefix}_{col}\"\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre> <p>Previous: Analyze | Next: Process Data &gt; Insurance</p>"},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/","title":"Insurance With Remote Worker","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/#insurance-sample-sdk-usage-with-remote-worker","title":"Insurance Sample SDK Usage with Remote Worker","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/#scenario-process-on-the-notebook","title":"Scenario: Process on the Notebook","text":"<ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Create json for worker and connection.(Json generation code coming soon)</p> </li> <li>The Worker json file must contain the following configurations:</li> <li> <p>Service url, worker_size, worker_image, email, and refresh_token</p> </li> <li> <p>The Connection json file must contain the following configurations:</p> </li> <li> <p>Connection_type, ws_uuid, ws_name, and file_path</p> </li> <li> <p>Encoding categorical variables</p> </li> <li> <p>Delete the originals of the columns you encoded</p> </li> <li> <p>Run the process and kill processing when finished</p> </li> </ol> <p>Create a new worker with practicuscore method of \"create_worker\" and use this new worker for your operations</p> <pre><code>worker_conf = {\n    \"worker_size\": \"Medium\",\n    \"worker_image\": \"practicus\",\n    # \"service_url\": \"\",\n    # \"email\": \"\",\n    # \"refresh_token\": \"**entry_your_token**\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.create_worker(worker_conf)\n</code></pre> <ul> <li>To access the dataset you need to work with connection configuration dictionary</li> <li>Also, you can choose a different Engine than Advance in the Deploy phase</li> </ul> <pre><code># configuration of connection\n\ndata_set_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/insurance.csv\"}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine=\"AUTO\")\n</code></pre> <p>Data prep with Practicus ai SDK</p> <pre><code>proc.categorical_map(column_name=\"sex\", column_suffix=\"category\")\n</code></pre> <pre><code>proc.categorical_map(column_name=\"smoker\", column_suffix=\"category\")\n</code></pre> <pre><code>proc.categorical_map(column_name=\"region\", column_suffix=\"category\")\n</code></pre> <pre><code>proc.delete_columns([\"region\", \"smoker\", \"sex\"])\n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Finish the process</p> <pre><code>proc.kill()\n</code></pre> <pre><code>worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/#you-can-also-prepare-this-code-directly-in-pipeline","title":"You can also prepare this code directly in pipeline:","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/#if-you-make-the-process-functional-and-do-it-with-with-you-dont-need-to-kill-the-worker-when-the-process-is-finished-the-worker-is-automatically-killed-when-this-process-is-finished","title":"If you make the process functional and do it with with, you don't need to kill the worker when the process is finished. The worker is automatically killed when this process is finished","text":"<pre><code>with prt.create_worker(worker_conf) as worker:\n    with worker.load(data_set_conn) as proc:\n        (proc.categorical_map(column_name=\"sex\", column_suffix=\"category\"),)\n        (proc.categorical_map(column_name=\"smoker\", column_suffix=\"category\"),)\n        (proc.categorical_map(column_name=\"region\", column_suffix=\"category\"),)\n        proc.delete_columns([\"region\", \"smoker\", \"sex\"])\n        proc.wait_until_done(timeout_min=600)\n        proc.show_logs()\n</code></pre> <p>Previous: Insurance | Next: Spark Custom Config</p>"},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/","title":"Insurance","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/#insurance-sample-sdk-usage-with-local-worker","title":"Insurance Sample SDK Usage with Local Worker","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/#scenario-process-on-the-interface-deploy-and-access-the-process","title":"Scenario: Process on the interface, deploy and access the process","text":"<ol> <li> <p>Open insurance.csv</p> </li> <li> <p>Encoding categorical variables</p> </li> <li> <p>Delete the originals of the columns you encoded</p> </li> <li> <p>Navigating the Deploy button, choosing Jupyter Notebook Option:</p> </li> <li> <p>Clicking Advanced and selecting view code and include security token options:</p> </li> <li> <p>Seeing your connection and worker json created and your code ready:</p> </li> </ol> <p>Connect to your active worker with get local worker</p> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n</code></pre> <ul> <li>To access the dataset you need to work with connection configuration dictionary</li> <li>Also, you can choose a different Engine than Advance in the Deploy phase</li> </ul> <pre><code># configuration of connection\n\ndata_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"ws_uuid\": \"d9b92183-8832-4fd1-8187-ac741ff6aab0\",\n    \"ws_name\": \"insurance\",\n    \"file_path\": \"/home/ubuntu/samples/data/insurance.csv\",\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn)\n</code></pre> <p>Data preparation with Practicus AI SDK</p> <pre><code>proc.categorical_map(column_name=\"sex\", column_suffix=\"category\")\n</code></pre> <pre><code>proc.categorical_map(column_name=\"smoker\", column_suffix=\"category\")\n</code></pre> <pre><code>proc.categorical_map(column_name=\"region\", column_suffix=\"category\")\n</code></pre> <pre><code>proc.delete_columns([\"region\", \"smoker\", \"sex\"])\n</code></pre> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Finish the process</p> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/#you-can-also-prepare-this-code-directly-in-pipeline","title":"You can also prepare this code directly in pipeline:","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/#if-you-make-the-process-functional-and-do-it-with-with-you-dont-need-to-kill-the-worker-when-the-process-is-finished-the-worker-is-automatically-killed-when-this-process-is-finished","title":"If you make the process functional and do it with with, you don't need to kill the worker when the process is finished. The worker is automatically killed when this process is finished","text":"<pre><code># Running the below will terminate the worker\n# with prt.get_local_worker() as worker:\n\n# E.g. the below would run the task, and then kill the process first, and then the worker.\n# with prt.get_local_worker() as worker:\n#     with worker.load(data_set_conn) as proc:\n#         proc.categorical_map(column_name='sex', column_suffix='category'),\n#         proc.categorical_map(column_name='smoker', column_suffix='category'),\n#         proc.categorical_map(column_name='region', column_suffix='category'),\n#         proc.delete_columns(['region', 'smoker', 'sex'])\n#         proc.wait_until_done()\n#         proc.show_logs()\n</code></pre> <p>Previous: Preprocess | Next: Insurance With Remote Worker</p>"},{"location":"technical-tutorial/extras/data-processing/process-data/spark-custom-config/","title":"Spark Custom Config","text":"<pre><code># Defining Parameters\ns3_access = None\ns3_secret = None\ns3_bucket_uri = None  # E.g \"s3a://sample-bucket/boston.csv\"\n</code></pre> <pre><code>assert s3_access, \"Please enter an access key\"\nassert s3_secret, \"Please enter an secret key\"\nassert s3_bucket_uri, \"Please enter s3 bucket uri\"\n</code></pre> <pre><code># For AWS S3\ns3_endpoint = \"s3.amazonaws.com\"\n# For others, e.g. Minio\n# s3_endpoint = \"http://prt-svc-sampleobj.prt-ns.svc.cluster.local\"\n</code></pre> <pre><code>extra_spark_conf = {\n    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n    \"spark.hadoop.fs.s3a.access.key\": s3_access,\n    \"spark.hadoop.fs.s3a.secret.key\": s3_secret,\n    \"spark.hadoop.fs.s3a.endpoint\": s3_endpoint,\n}\n\nimport practicuscore as prt\n\nspark = prt.engines.get_spark_session(extra_spark_conf=extra_spark_conf)\n\ndf = spark.read.csv(s3_bucket_uri)\ndf.head()\n</code></pre> <p>Previous: Insurance With Remote Worker | Next: Spark Object Storage</p>"},{"location":"technical-tutorial/extras/data-processing/process-data/spark-object-storage/","title":"Spark Object Storage","text":"<pre><code># Defining Parameters\naws_region = None\naws_access_key_id = None\naws_secret_access_key = None\nendpoint_url = None  # example \"http://prt-svc-sampleobj.prt-ns.svc.cluster.local\",\ns3_bucket_uri = None  # E.g \"s3a://sample-bucket/boston.csv\"\n</code></pre> <pre><code>assert aws_region, \"Please enter a aws_region\"\nassert aws_access_key_id, \"Please enter a aws_access_key_id\"\nassert aws_secret_access_key, \"Please enter a aws_secret_access_key\"\nassert endpoint_url, \"Please enter a endpoint_url\"\nassert s3_bucket_uri, \"Please enter s3 bucket uri\"\n</code></pre> <pre><code># For AWS S3\nconnection = {\n    \"connection_type\": \"S3\",\n    \"aws_region\": aws_region,\n    \"aws_access_key_id\": aws_access_key_id,\n    \"aws_secret_access_key\": aws_secret_access_key,\n    # Optional\n    # \"aws_session_token\", \"...\"\n}\n</code></pre> <pre><code># For others, e.g. Minio\nconnection = {\n    \"connection_type\": \"S3\",\n    \"endpoint_url\": endpoint_url,\n    \"aws_access_key_id\": aws_access_key_id,\n    \"aws_secret_access_key\": aws_secret_access_key,\n}\n</code></pre> <pre><code>import practicuscore as prt\n\n# Create a Spark session\nspark = prt.engines.get_spark_session(connection)\n\n# If you are using distributed Spark, you should now have the Spark cluster up &amp; running.\n</code></pre> <pre><code>df = spark.read.csv(s3_bucket_uri)\ndf.head()\n</code></pre> <pre><code># Optional: delete Spark Session\nprt.engines.delete_spark_session(spark)\n\n# If you are using distributed Spark, you should now have the Spark cluster terminated.\n# You can also terminate your worker, which will automatically terminate the child Spark Cluster.\n</code></pre> <p>Previous: Spark Custom Config | Next: Big Data Analytics &gt; Trino &gt; Trino Iceberg API Integration</p>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/","title":"LangChain LLM Model: RAG Model for Banking","text":"<p>This notebook demonstrates the development of a Retrieval-Augmented Generation (RAG) model for banking-related queries.  The RAG model retrieves relevant contextual data and generates meaningful answers using a language model.  We utilize LangChain, Transformers, and other related libraries to build the model.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None  # Example url -&gt; 'company.practicus.com'\nembedding_model_path = None\nmodel_name = None\nmodel_prefix = None\n\nvector_store = None\nif vector_store == \"MilvusDB\":\n    milvus_uri = None  # Milvus connection url, E.g. 'company.practicus.milvus.com'\n</code></pre> <pre><code>assert host, \"Please enter your host url\"\nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert model_name, \"Please enter your embedding model_name.\"\n\n# You can use one of ChromaDB or MilvusDB as vector store\nassert model_prefix, \"Please enter your embedding model_prefix.\"\n\nassert vector_store in [\"ChromaDB\", \"MilvusDB\"], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"\nif vector_store == \"MilvusDB\":\n    assert \"milvus_uri\", \"Please enter your milvus connection uri\"\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#preparing-data","title":"Preparing Data","text":"<pre><code>import os\nimport requests\n\n# Create the GitHub API URL\nurl = \"https://api.github.com/repos/practicusai/sample-data/contents/FAQ_Sample?ref=main\"\n\n# Call the API\nresponse = requests.get(url)\nif response.status_code == 200:\n    files = response.json()  # Get response in JSON format\n\n    for file in files:\n        file_url = file[\"download_url\"]\n        file_name = file[\"name\"]\n\n        # Download files\n        file_response = requests.get(file_url)\n        if file_response.status_code == 200:\n            with open(file_name, \"wb\") as f:\n                f.write(file_response.content)\n            print(f\"'{file_name}' successfully downloaded.\")\n        else:\n            print(f\"'{file_name}' file failed to download.\")\nelse:\n    print(f\"Failed to retrieve data from API, HTTP status: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#libraries-installation","title":"Libraries Installation","text":"<p>Ensure the necessary libraries are installed using the following commands:</p> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb pypdf\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#importing-required-libraries","title":"Importing Required Libraries","text":"<p>The notebook imports libraries such as Transformers for text processing, LangChain for building components like text splitters and vector stores, and additional utilities for embedding generation and document processing.</p> <pre><code>from transformers import pipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#define-llm-api-function","title":"Define LLM API Function","text":"<p>A function is defined to interact with the ChatPracticus API, sending input prompts and receiving language model-generated responses.  The API URL and token are required to authenticate and access the service.</p> <pre><code>def call_llm_api(inputs, api_url, api_token):\n    # We need to give input to 'generate_response'. This function will use our 'api_token' and 'endpoint_url' and return the response.\n\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n    # response = chat.invoke(\"What is Capital of France?\")  # This also works\n\n    return response.content\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#load-and-split-pdf-files","title":"Load and Split PDF Files","text":"<p>PDF documents are loaded and split into manageable text chunks using LangChain's <code>CharacterTextSplitter</code>.  This enables the processing of large documents for retrieval tasks.</p> <pre><code>def load_and_split_pdfs(pdf_files, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load all pdf files and split with using the 'seperator'.\n\n    :param pdf_files: A list of paths to the PDF files to be processed.\n    :param chunk_size: The maximum number of characters in each text chunk.\n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = (\n        CharacterTextSplitter(  # langchain method used to separate documents, there are different methods as well\n            separator=\"  \\n \\n \\n \\n\",  # Defines the separator used to split the text.\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            length_function=len,\n            is_separator_regex=False,\n        )\n    )\n\n    for pdf_file in pdf_files:\n        loader = PyPDFLoader(pdf_file)  # pdf loader compatible with langchain\n        documents = loader.load_and_split()\n        split_docs = text_splitter.split_documents(documents)\n        all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#define-pdf-files-and-process-them","title":"Define PDF Files and Process Them","text":"<p>A list of PDF files is specified, and the <code>load_and_split_pdfs</code> function processes these files into text chunks for downstream retrieval and analysis.</p> <pre><code>pdf_list = [\"faq1.pdf\", \"faq2.pdf\", \"faq3.pdf\", \"faq4.pdf\", \"faq5.pdf\"]\n\ntext_chunks = load_and_split_pdfs(pdf_list)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#create-chroma-vector-store","title":"Create Chroma Vector Store","text":"<p>The function creates a Chroma vector store, which encodes the text chunks into embeddings using a pre-trained model.  The vector store allows similarity-based document retrieval.</p> <pre><code>if vector_store == \"ChromaDB\":\n    # Generate embeddings and create ChromaDB vector store\n    def create_chroma_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n            encode_kwargs={\"normalize_embeddings\": False},\n        )\n\n        # This method creates a vector store from the provided documents (chunks) and embeddings.\n        vectorstore_pdf = Chroma.from_documents(\n            collection_name=\"langchain_example\", documents=chunks, embedding=embeddings\n        )\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to\n        # the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_pdf = vectorstore_pdf.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n        return retriever_pdf\n\n    retriever_pdf = create_chroma_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#optional-create-milvus-db","title":"(OPTIONAL) Create Milvus DB","text":"<p>This function creates a vector store in Milvus by generating embeddings for text chunks using a HuggingFace pre-trained model. It connects to the Milvus database, stores the embeddings in a specified collection, and ensures any old collections with the same name are dropped. The resulting vector store is converted into a retriever for similarity-based searches, retrieving the top 2 relevant documents for a query.</p> <pre><code>from langchain_milvus import Milvus\nfrom pymilvus import connections\n\nif vector_store == \"MilvusDB\":\n\n    def create_milvus_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n            encode_kwargs={\"normalize_embeddings\": False},\n        )\n\n        connections.connect(\"default\", host=milvus_uri, port=\"19530\")  # Connection to milvus db\n\n        vectorstore = Milvus.from_documents(\n            documents=chunks,\n            embedding=embeddings,\n            collection_name=\"langchain_example\",  # Name for created vector table\n            connection_args={\n                \"uri\": f\"https://{milvus_uri}:19530\"  # Connection configuration to milvus db\n            },\n            drop_old=True,  # Drop the old Milvus collection if it exists\n        )\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents\n        # to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_pdf = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n        return retriever_pdf\n\n    retriever_pdf = create_milvus_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#format-document-chunks","title":"Format Document Chunks","text":"<p>A utility function combines document chunks into a single string format.  This formatted text is passed to the language model for querying.</p> <pre><code>def format_docs(docs):\n    # Retrieves the content of each document in the `docs` list and joins the content of all documents into a single string, with each document's content separated by two newline characters.\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#query-pdf-using-llm","title":"Query PDF Using LLM","text":"<p>This function combines document retrieval, prompt construction, and LLM inference to answer a given query.  The RAG model retrieves context from the vector store, formats it, and queries the LLM.</p> <pre><code># Query the PDF using the API-based LLM\ndef query_pdf(retriever, question, api_url, api_token):\n    \"\"\"\n    this function is used for returning response by using all of the chains we defined above\n\n    :param retriever : An instance of a retriever used to fetch relevant documents.\n    :param question : The question to be asked about the PDF content.\n    \"\"\"\n\n    prompt_template = PromptTemplate(  # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=(  # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        ),\n    )\n\n    docs = retriever.get_relevant_documents(\n        question\n    )  # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs)  # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question)  # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split(\"Answer:\")[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#practicus-integration","title":"Practicus Integration","text":"<p>Imports the Practicus library, which is used for managing API configurations and token generation for LLM queries.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = None  # Get a new token, or reuse existing, if not expired\ntoken = prt.models.get_session_token(api_url=api_url, token=token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#execute-a-sample-query","title":"Execute a Sample Query","text":"<p>Demonstrates querying the RAG model with a sample question about banking services.  The system retrieves relevant context and generates an answer using the integrated LLM.</p> <pre><code># Example query\nanswer = query_pdf(\n    retriever=retriever_pdf,\n    question=\"My transaction was interrupted at an ATM, where should I apply?\",\n    api_url=api_url,\n    api_token=token,\n)\nprint(answer)\n</code></pre> <p>Previous: Prtchatbot | Next: AI Assistants &gt; AI Assistants</p>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/","title":"Build","text":"<pre><code># Parameters: Replace with your actual deployment key and app prefix\n# These identify where the app should be deployed within your Practicus AI environment.\napp_deployment_key = None\napp_prefix = \"apps\"\n</code></pre> <pre><code>assert app_deployment_key, \"Please select a deployment key\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Analyze the current directory for Practicus AI App components (APIs, MQ consumers, UI, etc.)\n# This should output the list of detected API endpoints.\nprt.apps.analyze()\n</code></pre> <pre><code># --- Deployment ---\napp_name = \"agentic-ai-test-sales\"\nvisible_name = \"Agentic AI Test SALES\"\ndescription = \"Test Application for Agentic AI Example.\"\nicon = \"fa-robot\"\n\nprint(f\"Deploying app '{app_name}'...\")\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # Uses current directory\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"App deployed successfully!\")\nprint(f\"  API Base URL: {api_url}\")\n\n# The OpenAPI (Swagger) documentation (Swagger/ReDoc) is usually available at /docs or /redoc off the API base URL\nprint(f\"  OpenAPI (Swagger) Docs (ReDoc): {api_url}redoc/\")\n\n# Store the api_url for later use when creating tools\nassert api_url, \"Deployment failed to return an API URL.\"\n</code></pre> <pre><code>tool_endpoint_paths = [\n    \"bootstrap-data/\",\n    \"analyze-sales-trends/\",\n    \"detect-regional-drop/\",\n    \"top-products-insight/\",\n    \"predict-growth-opportunities/\",\n    \"generate-campaign-idea/\",\n    \"suggest-target-audience/\",\n    \"generate-marketing-slogan/\",\n    \"sentiment-test-slogan/\",\n    \"validate-campaign-json/\",\n    \"generate-social-posts/\",\n    \"generate-strategic-summary/\",\n]\n\n\n# Construct full URLs\ntool_endpoint_urls = [api_url + path for path in tool_endpoint_paths]\n\nprint(\"Will attempt to create tools for the following API endpoints:\")\nfor url in tool_endpoint_urls:\n    print(f\" - {url}\")\n\n# Tip: If you pass partial API URLs e.g. 'apps/agentic-ai-test/api/v1/generate-receipt/'\n#  The base URL e.g. 'https://practicus.my-company.com/' will be added to the final URL\n#  using your current Practicus AI region.\n</code></pre> <pre><code>import os\nfrom langchain_openai import ChatOpenAI  # Or your preferred ChatModel\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_practicus import APITool\n\n# Ensure practicuscore is imported for enums\nimport practicuscore as prt\n\n\ndef validate_api_spec(api_tool: APITool, strict=False) -&gt; bool:\n    \"\"\"Checks the APISpec of a fetched tool against our rules.\"\"\"\n\n    # APITool fetches the spec from OpenAPI (Swagger) during initialization\n    spec = api_tool.spec\n\n    if not spec:\n        # API definition in the source file might be missing the 'spec' object\n        warning_msg = f\"API '{api_tool.url}' does not have APISpec metadata defined in its OpenAPI spec.\"\n        if strict:\n            raise ValueError(f\"{warning_msg} Validation is strict.\")\n        else:\n            prt.logger.warning(f\"{warning_msg} Allowing since validation is not strict.\")\n            return True  # Allow if not strict\n\n    # --- Apply Rules based on fetched spec ---\n\n    # Rule 1: Check Risk Profile\n    if spec.risk_profile and spec.risk_profile == prt.APIRiskProfile.High:\n        err = f\"API '{api_tool.url}' has a risk profile defined as '{spec.risk_profile}'.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            # Even if not strict, we might choose to block High risk tools\n            prt.logger.warning(f\"{err} Blocking High Risk API even though validation is not strict.\")\n            return False  # Block high risk\n\n    # Rule 2: Check Human Gating for non-read-only APIs\n    # (Example: Enforce human gating for safety on modifying APIs)\n    if not spec.read_only and not spec.human_gated:\n        err = f\"API '{api_tool.url}' modifies data (read_only=False) but is not marked as human_gated.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            prt.logger.warning(f\"{err} Allowing non-gated modifying API since validation is not strict.\")\n            # In this non-strict case, we allow it, but a stricter policy might return False here.\n            return True\n\n    # Add more complex rules here if needed...\n    # E.g., check custom_attributes, scope, etc.\n\n    # If no rules were violated (or violations were allowed because not strict)\n    prt.logger.info(f\"API '{api_tool.url}' passed validation (strict={strict}). Spec: {spec}\")\n    return True\n\n\n# --- Create Tools (optionally applying validation) ---\ndef get_tools(endpoint_urls: list[str], validate=True):\n    _tools = []\n    strict_validation = False  # Set to True to enforce stricter rules\n    additional_instructions = \"Add Yo! after all of your final responses.\"  # Example instruction\n\n    print(f\"\\nCreating and validating tools (strict={strict_validation})...\")\n    for tool_endpoint_url in endpoint_urls:\n        print(f\"\\nProcessing tool for API: {tool_endpoint_url}\")\n        try:\n            api_tool = APITool(\n                url=tool_endpoint_url,\n                additional_instructions=additional_instructions,\n                # token=..., # Uses current user credentials by default, set to override\n                # include_resp_schema=True # Response schema (if exists) is not included by default\n            )\n\n            # Explain the tool (optional, useful for debugging)\n            # api_tool.explain(print_on_screen=True)\n\n            # Validate based on fetched APISpec\n            if not validate or validate_api_spec(api_tool=api_tool, strict=strict_validation):\n                print(\n                    f\"--&gt; Adding tool: {api_tool.name} ({api_tool.url}) {'' if validate else ' - skipping validation'}\"\n                )\n                _tools.append(api_tool)\n            else:\n                print(f\"--&gt; Skipping tool {api_tool.name} due to validation rules.\")\n        except Exception as e:\n            # Catch potential errors during APITool creation (e.g., API not found, spec parsing error)\n            print(f\"ERROR: Failed to create or validate tool for {tool_endpoint_url}: {e}\")\n            if strict_validation:\n                raise  # Re-raise if strict\n            else:\n                print(\"--&gt; Skipping tool due to error (not strict).\")\n\n    return _tools\n\n\ntools = get_tools(tool_endpoint_urls)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># For this excercise we will skip validating APIs\n\ntools = get_tools(tool_endpoint_urls, validate=False)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># View tool explanation\n\nfor tool in tools:\n    tool.explain()\n</code></pre> <pre><code>openaikey, age = prt.vault.get_secret(\"openaikey\")\nos.environ[\"OPENAI_API_KEY\"] = openaikey\n\nassert os.environ[\"OPENAI_API_KEY\"], \"OpenAI key is not defined\"\n</code></pre> <pre><code># --- Agent Initialization ---\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Create a ReAct agent using LangGraph\ngraph = create_react_agent(llm, tools=tools)\nprint(\"Agent initialized.\")\n\n\n# Helper function to print the agent's stream output nicely\ndef pretty_print_stream_chunk(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            # Print the latest message added by the node\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            # Print other kinds of updates\n            print(f\"  Update: {updates}\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>def pretty_print_stream_chunk2(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            print(f\"  Update: {updates}\")\n            if isinstance(updates, Exception):\n                print(\"  \u26a0\ufe0f Exception Detected in Agent Execution!\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>query = \"\"\"\nAnalyze my sales trends and suggest where I should focus to grow. Then create a campaign idea for it.\n\"\"\"\n\ninputs = {\"messages\": [(\"user\", query)]}\n\nif graph:\n    print(f\"\\nInvoking agent with query: '{query}'\")\n    print(\"Streaming agent execution steps:\\n\")\n\n    # Configuration for the stream, e.g., setting user/thread IDs\n    # config = {\"configurable\": {\"user_id\": \"doc-user-1\", \"thread_id\": \"doc-thread-1\"}}\n    config = {}\n    # Use astream to get intermediate steps\n    async for chunk in graph.astream(inputs, config=config):\n        pretty_print_stream_chunk2(chunk)\n\n    print(\"\\nAgent execution finished.\")\n\n    # Optional: Get the final state if needed\n    # final_state = await graph.ainvoke(inputs, config=config)\n    # print(\"\\nFinal Agent State:\", final_state)\n\nelse:\n    print(\"\\nAgent execution skipped because the agent graph was not initialized.\")\n</code></pre> <pre><code># Cleanup\nprt.apps.delete(prefix=app_prefix, app_name=app_name)\n</code></pre> <p>API TEST</p> <pre><code>from apis.analyze_sales_trends import AnalyzeSalesTrendsRequest, AnalyzeSalesTrendsResponse, SalesRecord\nfrom datetime import datetime\nimport practicuscore as prt\n\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n]\n\npayload = AnalyzeSalesTrendsRequest(\n    sales_data=sales_data, start_date=datetime(2025, 1, 1), end_date=datetime(2025, 1, 3)\n)\n\nresponse: AnalyzeSalesTrendsResponse = prt.apps.test_api(\"/analyze-sales-trends\", payload)\n\nprint(response)\n</code></pre> <pre><code>from apis.sentiment_test_slogan import SentimentTestSloganRequest, SentimentTestSloganResponse\nfrom pydantic import BaseModel\nimport practicuscore as prt\n\n# Prepare payload\nslogans = [\n    \"Power Up Your Productivity!\",\n    \"Nothing beats the classics.\",\n    \"Innovation in Every Click.\",\n    \"Your Tech, Your Edge.\",\n    \"Be Bold. Be Better.\",\n]\npayload = SentimentTestSloganRequest(slogans=slogans)\n\n# Type check (optional)\nprint(issubclass(type(payload), BaseModel))  # Should print True\n\n# Local test via Practicus\nresponse: SentimentTestSloganResponse = prt.apps.test_api(\"/sentiment-test-slogan\", payload)\n\n# Output the results\nfor result in response.results:\n    print(f\"Slogan: {result.slogan}\")\n    print(f\"Sentiment: {result.sentiment}\")\n    print(f\"Comment: {result.comment}\")\n    print(\"-----\")\n</code></pre> <pre><code>from apis.top_products_insight import TopProductsInsightRequest, TopProductsInsightResponse, SalesRecord\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport practicuscore as prt\n\n# \u00d6rnek sat\u0131\u015f verisi\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n    SalesRecord(date=datetime(2025, 1, 4), product=\"Tablet\", region=\"Europe\", units_sold=70, revenue=28000.0),\n    SalesRecord(date=datetime(2025, 1, 5), product=\"Monitor\", region=\"North America\", units_sold=95, revenue=47500.0),\n]\n\n# Test payload\npayload = TopProductsInsightRequest(sales_data=sales_data, top_n=3)\n\n# API test\ntry:\n    response: TopProductsInsightResponse = prt.apps.test_api(\"/top-products-insight\", payload)\n\n    # Sonu\u00e7lar\u0131 yazd\u0131r\n    for product in response.products:\n        print(f\"Product: {product.product}\")\n        print(f\"Total Units Sold: {product.total_units_sold}\")\n        print(f\"Top Region: {product.top_region}\")\n        print(f\"Insight: {product.insight}\")\n        print(\"-----\")\n\nexcept Exception as e:\n    prt.logger.error(f\"[test-top-products-insight] Exception: {e}\")\n    raise\n</code></pre> <pre><code>from practicuscore import apps\nfrom apis.predict_growth_opportunities import PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nresponse = apps.test_api(\"/predict-growth-opportunities\", payload)\nprint(response)\n</code></pre> <pre><code>from apis.predict_growth_opportunities import run, PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nawait run(payload)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apisanalyze_sales_trendspy","title":"apis/analyze_sales_trends.py","text":"<pre><code># apis/analyze_sales_trends.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Literal\nfrom datetime import datetime, timezone\n\n\nclass SalesTrendSummary(BaseModel):\n    trend_direction: Literal[\"increasing\", \"decreasing\", \"stable\"]\n    \"\"\"Overall direction of sales trend\"\"\"\n\n    peak_day: str\n    \"\"\"Day with the highest sales, in YYYY-MM-DD format\"\"\"\n\n\nclass SalesRecord(BaseModel):\n    date: datetime\n    \"\"\"The date of the sales transaction in UTC format.\"\"\"\n\n    product: str\n    \"\"\"The name of the product sold.\"\"\"\n\n    region: str\n    \"\"\"The geographical region where the sale occurred.\"\"\"\n\n    units_sold: int\n    \"\"\"The number of product units sold.\"\"\"\n\n    revenue: float\n    \"\"\"The total revenue generated from this sale.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"date\": \"2024-12-01T00:00:00Z\",\n                    \"product\": \"Laptop\",\n                    \"region\": \"Asia\",\n                    \"units_sold\": 120,\n                    \"revenue\": 120000.0\n                }\n            ]\n        }\n    }\n\n\nclass AnalyzeSalesTrendsRequest(BaseModel):\n    sales_data: List[SalesRecord]\n    \"\"\"List of sales records to analyze.\"\"\"\n\n    start_date: datetime\n    \"\"\"The start date of the period to be analyzed, in UTC.\"\"\"\n\n    end_date: datetime\n    \"\"\"The end date of the period to be analyzed, in UTC.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"sales_data\": [\n                        {\n                            \"date\": \"2024-12-01T00:00:00Z\",\n                            \"product\": \"Laptop\",\n                            \"region\": \"Asia\",\n                            \"units_sold\": 120,\n                            \"revenue\": 120000.0\n                        }\n                    ],\n                    \"start_date\": \"2024-12-01T00:00:00Z\",\n                    \"end_date\": \"2024-12-31T00:00:00Z\"\n                }\n            ]\n        }\n    }\n\n\nclass AnalyzeSalesTrendsResponse(BaseModel):\n    total_units: int\n    \"\"\"Total number of units sold in the analyzed period.\"\"\"\n\n    total_revenue: float\n    \"\"\"Total revenue generated in the analyzed period.\"\"\"\n\n    daily_average_units: float\n    \"\"\"Average number of units sold per day.\"\"\"\n\n    trend_summary: SalesTrendSummary\n    \"\"\"Summary of trend direction and peak day.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"total_units\": 30500,\n                    \"total_revenue\": 15200000.0,\n                    \"daily_average_units\": 492.06,\n                    \"trend_summary\": {\n                        \"trend_direction\": \"increasing\",\n                        \"peak_day\": \"2025-01-10\"\n                    }\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/analyze-sales-trends\", spec=api_spec)\nasync def run(payload: AnalyzeSalesTrendsRequest, **kwargs) -&gt; AnalyzeSalesTrendsResponse:\n    \"\"\"\n    Analyze overall sales performance for a selected date range V19.\n    Provides total units sold, revenue, daily averages, trend direction and peak day.\n    \"\"\"\n\n    try:\n        def make_utc(dt: datetime) -&gt; datetime:\n            if dt.tzinfo is None:\n                return dt.replace(tzinfo=timezone.utc)\n            return dt.astimezone(timezone.utc)\n\n        start_date = make_utc(payload.start_date)\n        end_date = make_utc(payload.end_date)\n\n        filtered = [\n            record for record in payload.sales_data\n            if start_date &lt;= make_utc(record.date) &lt;= end_date\n        ]\n\n        if not filtered:\n            return AnalyzeSalesTrendsResponse(\n                total_units=0,\n                total_revenue=0.0,\n                daily_average_units=0.0,\n                trend_summary=SalesTrendSummary(\n                    trend_direction=\"stable\",\n                    peak_day=\"N/A\"\n                )\n            )\n\n        filtered.sort(key=lambda x: x.date)\n\n        total_units = sum(r.units_sold for r in filtered)\n        total_revenue = sum(r.revenue for r in filtered)\n        num_days = (end_date - start_date).days + 1\n        daily_avg = total_units / num_days if num_days else 0\n\n        first_avg = sum(r.units_sold for r in filtered[:5]) / 5\n        last_avg = sum(r.units_sold for r in filtered[-5:]) / 5\n\n        if last_avg &gt; first_avg * 1.1:\n            trend = \"increasing\"\n        elif last_avg &lt; first_avg * 0.9:\n            trend = \"decreasing\"\n        else:\n            trend = \"stable\"\n\n        day_totals = {}\n        for r in filtered:\n            dt = make_utc(r.date)\n            day_totals.setdefault(dt, 0)\n            day_totals[dt] += r.units_sold\n        peak_day = max(day_totals, key=day_totals.get).strftime(\"%Y-%m-%d\")\n\n        return AnalyzeSalesTrendsResponse(\n            total_units=total_units,\n            total_revenue=total_revenue,\n            daily_average_units=round(daily_avg, 2),\n            trend_summary={\n                \"trend_direction\": trend,\n                \"peak_day\": peak_day\n            }\n        )\n\n    except Exception as e:\n        prt.logger.error(f\"[analyze-sales-trends] Exception: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apisbootstrap_datapy","title":"apis/bootstrap_data.py","text":"<pre><code># apis/bootstrap_data.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nfrom datetime import datetime, timedelta\nimport random\n\nclass SalesRecord(BaseModel):\n    date: datetime\n    \"\"\"Date of the sale.\"\"\"\n\n    product: str\n    \"\"\"Product name.\"\"\"\n\n    region: str\n    \"\"\"Sales region.\"\"\"\n\n    units_sold: int\n    \"\"\"Number of units sold.\"\"\"\n\n    revenue: float\n    \"\"\"Revenue from the sale.\"\"\"\n\nclass BootstrapDataRequest(BaseModel):\n    \"\"\"Empty request for bootstrap data loading.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [{}]\n        }\n    }\n\n\nclass BootstrapDataResponse(BaseModel):\n    sales_data: List[SalesRecord]\n    \"\"\"Predefined static list of 100 sales records.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True\n    }\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low\n)\n\n\n@prt.api(\"/bootstrap-data\", spec=api_spec)\nasync def run(payload: BootstrapDataRequest, **kwargs) -&gt; BootstrapDataResponse:\n    \"\"\"\n    Load a static dataset of 100 predefined sales records into the agent's context.\n    This tool should be invoked at the start of any agent flow.\n    \"\"\"\n    products = [\"Laptop\", \"Monitor\", \"Tablet\"]\n    regions = [\"Asia\", \"Europe\", \"North America\"]\n    base_date = datetime(2025, 1, 1)\n\n    static_data = []\n\n    random.seed(42) \n\n    for i in range(10):  \n        product = random.choice(products)\n        region = random.choice(regions)\n        date = base_date + timedelta(days=i % 30)\n        units_sold = random.randint(20, 150)\n        unit_price = {\n            \"Laptop\": 1000,\n            \"Monitor\": 500,\n            \"Tablet\": 400\n        }[product]\n        revenue = units_sold * unit_price\n\n        static_data.append(SalesRecord(\n            date=date,\n            product=product,\n            region=region,\n            units_sold=units_sold,\n            revenue=revenue\n        ))\n\n    return BootstrapDataResponse(sales_data=static_data)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apisdetect_regional_droppy","title":"apis/detect_regional_drop.py","text":"<pre><code># apis/detect_regional_drop.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nfrom datetime import datetime\nfrom collections import defaultdict\n\n\nclass SalesRecord(BaseModel):\n    date: datetime\n    \"\"\"The UTC date when the sale was recorded.\"\"\"\n\n    product: str\n    \"\"\"The name of the product sold.\"\"\"\n\n    region: str\n    \"\"\"The geographical region where the product was sold.\"\"\"\n\n    units_sold: int\n    \"\"\"The number of units sold in this transaction.\"\"\"\n\n    revenue: float\n    \"\"\"The total revenue generated from the sale.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"date\": \"2024-12-01T00:00:00Z\",\n                    \"product\": \"Monitor\",\n                    \"region\": \"Asia\",\n                    \"units_sold\": 150,\n                    \"revenue\": 45000.0\n                }\n            ]\n        }\n    }\n\n\nclass DetectRegionalDropRequest(BaseModel):\n    sales_data: List[SalesRecord]\n    \"\"\"List of sales transactions to analyze.\"\"\"\n\n    start_date: datetime\n    \"\"\"Start date of the analysis window (UTC).\"\"\"\n\n    end_date: datetime\n    \"\"\"End date of the analysis window (UTC).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"sales_data\": [\n                        {\n                            \"date\": \"2024-12-01T00:00:00Z\",\n                            \"product\": \"Monitor\",\n                            \"region\": \"Asia\",\n                            \"units_sold\": 150,\n                            \"revenue\": 45000.0\n                        }\n                    ],\n                    \"start_date\": \"2024-12-01T00:00:00Z\",\n                    \"end_date\": \"2025-02-28T00:00:00Z\"\n                }\n            ]\n        }\n    }\n\n\nclass RegionalDrop(BaseModel):\n    region: str\n    \"\"\"Region where the sales drop was detected.\"\"\"\n\n    product: str\n    \"\"\"Product that showed a decrease in sales.\"\"\"\n\n    drop_percentage: float\n    \"\"\"Percentage of decline in units sold between the two periods.\"\"\"\n\n    comment: Optional[str]\n    \"\"\"Optional explanation or context about the decline.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"region\": \"Asia\",\n                    \"product\": \"Monitor\",\n                    \"drop_percentage\": 42.5,\n                    \"comment\": \"Sales declined sharply compared to the previous period.\"\n                }\n            ]\n        }\n    }\n\n\nclass DetectRegionalDropResponse(BaseModel):\n    drops: List[RegionalDrop]\n    \"\"\"List of regional product drops with percentage and commentary.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"drops\": [\n                        {\n                            \"region\": \"Asia\",\n                            \"product\": \"Monitor\",\n                            \"drop_percentage\": 42.5,\n                            \"comment\": \"Sales declined sharply compared to the previous period.\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n\n@prt.api(\"/detect-regional-drop\", spec=api_spec)\nasync def run(payload: DetectRegionalDropRequest, **kwargs) -&gt; DetectRegionalDropResponse:\n    \"\"\"\n    Detect regional drops in product sales over a selected date range.\n    Compares first and second half of the period to find significant declines.\n    \"\"\"\n    try:\n\n        from datetime import timezone\n\n        def make_utc(dt: datetime) -&gt; datetime:\n            if dt.tzinfo is None:\n                return dt.replace(tzinfo=timezone.utc)\n            return dt.astimezone(timezone.utc)\n\n\n        start_date = make_utc(payload.start_date)\n        end_date = make_utc(payload.end_date)\n        mid_point = start_date + (end_date - start_date) / 2\n\n        current_period = defaultdict(list)\n        previous_period = defaultdict(list)\n\n        for r in payload.sales_data:\n            r_date = make_utc(r.date)\n            if start_date &lt;= r_date &lt;= mid_point:\n                previous_period[(r.region, r.product)].append(r.units_sold)\n            elif mid_point &lt; r_date &lt;= end_date:\n                current_period[(r.region, r.product)].append(r.units_sold)\n\n        drops = []\n        for key in previous_period:\n            prev_avg = sum(previous_period[key]) / len(previous_period[key]) if previous_period[key] else 0\n            curr_avg = sum(current_period.get(key, [])) / len(current_period.get(key, [])) if current_period.get(key) else 0\n\n            if prev_avg &gt; 0 and curr_avg &lt; prev_avg * 0.85:\n                drop_pct = round((prev_avg - curr_avg) / prev_avg * 100, 2)\n                drops.append(RegionalDrop(\n                    region=key[0],\n                    product=key[1],\n                    drop_percentage=drop_pct,\n                    comment=\"Sales declined sharply compared to the previous period.\"\n                ))\n\n        return DetectRegionalDropResponse(drops=drops)\n\n    except Exception as e:\n        prt.logger.error(f\"[detect-regional-drop] Exception: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apisgenerate_campaign_ideapy","title":"apis/generate_campaign_idea.py","text":"<pre><code># apis/generate_campaign_idea.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass GrowthOpportunity(BaseModel):\n    product: str\n    \"\"\"The product that represents a growth opportunity.\"\"\"\n\n    region: str\n    \"\"\"The region where this product shows growth potential.\"\"\"\n\n    reason: str\n    \"\"\"Explanation of why this product-region combination is promising.\"\"\"\n\n    confidence: str\n    \"\"\"Confidence level of the opportunity (e.g., high, medium, low).\"\"\"\n\n\nclass GenerateCampaignIdeaRequest(BaseModel):\n    goal: str\n    \"\"\"The business goal in natural language (e.g., increase monitor sales in Asia by 20%).\"\"\"\n\n    opportunities: List[GrowthOpportunity]\n    \"\"\"A list of previously identified product-region growth opportunities.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"goal\": \"Increase monitor sales in Asia by 20% in Q1\",\n                    \"opportunities\": [\n                        {\n                            \"product\": \"Monitor\",\n                            \"region\": \"Asia\",\n                            \"reason\": \"Strong market potential with recent positive sales trend.\",\n                            \"confidence\": \"high\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\nclass GenerateCampaignIdeaResponse(BaseModel):\n    campaign_name: str\n    \"\"\"A short and catchy name for the proposed marketing campaign.\"\"\"\n\n    description: str\n    \"\"\"A one-paragraph summary of what the campaign aims to achieve.\"\"\"\n\n    strategy: str\n    \"\"\"A suggested strategic approach to achieve the campaign's goal.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"campaign_name\": \"Focus Asia 20\",\n                    \"description\": \"A regional campaign to boost monitor sales by highlighting productivity and affordability.\",\n                    \"strategy\": \"Use targeted ads on professional networks and bundled discounts for remote workers.\"\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=False,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/generate-campaign-idea\", spec=api_spec)\nasync def run(payload: GenerateCampaignIdeaRequest, **kwargs) -&gt; GenerateCampaignIdeaResponse:\n    \"\"\"\n    Generate a marketing campaign plan based on a defined business goal and a list of growth opportunities.\n\n    Use this tool when a user provides a clear objective and market insights,\n    and needs help forming a compelling campaign concept.\n\n    The input includes a business goal and opportunity details (product, region, reason, confidence).\n    The output includes a campaign name, high-level description, and suggested strategy.\n    This tool supports decision-making for sales and marketing initiatives.\n    \"\"\"\n\n    goal = payload.goal\n    product_names = [op.product for op in payload.opportunities]\n    region_names = [op.region for op in payload.opportunities]\n\n    campaign_name = f\"{product_names[0]} Boost in {region_names[0]}\"\n    description = f\"This campaign focuses on growing {product_names[0]} sales in {region_names[0]}, supporting the goal: '{goal}'.\"\n    strategy = f\"Leverage digital ads and local influencers in {region_names[0]}. Highlight features that align with productivity and affordability.\"\n\n    return GenerateCampaignIdeaResponse(\n        campaign_name=campaign_name,\n        description=description,\n        strategy=strategy\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apisgenerate_marketing_sloganpy","title":"apis/generate_marketing_slogan.py","text":"<pre><code># apis/generate_marketing_slogan.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass GenerateCampaignIdeaResponse(BaseModel):\n    campaign_name: str\n    \"\"\"Name of the campaign being promoted.\"\"\"\n\n    description: str\n    \"\"\"Detailed description explaining the campaign purpose.\"\"\"\n\n    strategy: str\n    \"\"\"Suggested strategy to execute the campaign.\"\"\"\n\n\nclass GenerateMarketingSloganRequest(BaseModel):\n    campaign: GenerateCampaignIdeaResponse\n    \"\"\"Full campaign details (name, description, and strategy) used to generate slogans.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"campaign\": {\n                        \"campaign_name\": \"Focus Asia 20\",\n                        \"description\": \"A regional campaign to boost monitor sales by highlighting productivity.\",\n                        \"strategy\": \"Leverage social media influencers and promote bundled deals.\"\n                    }\n                }\n            ]\n        }\n    }\n\n\nclass GenerateMarketingSloganResponse(BaseModel):\n    slogans: List[str]\n    \"\"\"List of suggested marketing slogans tailored to the campaign.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"slogans\": [\n                        \"Power Up Your Productivity!\",\n                        \"The Monitor That Works As Hard As You Do.\",\n                        \"Boost Your Vision, Boost Your Goals.\"\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/generate-marketing-slogan\", spec=api_spec)\nasync def run(payload: GenerateMarketingSloganRequest, **kwargs) -&gt; GenerateMarketingSloganResponse:\n    \"\"\"\n    Generate catchy marketing slogans from a campaign description and strategy.\n\n    Use this tool when a campaign is already defined and you need creative slogan ideas.\n    The input must contain a structured campaign object including name, description, and strategy.\n    The output is a list of short slogans designed to align with the campaign tone and goal.\n\n    Ideal for marketing teams looking to quickly brainstorm branding lines.\n    \"\"\"\n\n    description = payload.campaign.description.lower()\n\n    if \"productivity\" in description:\n        slogans = [\n            \"Power Up Your Productivity!\",\n            \"Work Smarter, See Sharper.\",\n            \"Boost Your Workflow with Every Pixel.\"\n        ]\n    elif \"deal\" in description:\n        slogans = [\n            \"Big Value, Bigger Vision.\",\n            \"Bundles That Mean Business.\",\n            \"Smart Tech. Smarter Price.\"\n        ]\n    else:\n        slogans = [\n            \"Your Tech, Your Edge.\",\n            \"Be Bold. Be Better.\",\n            \"Innovation in Every Click.\"\n        ]\n\n    return GenerateMarketingSloganResponse(slogans=slogans)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apisgenerate_social_postspy","title":"apis/generate_social_posts.py","text":"<pre><code># apis/generate_social_posts.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\n\n\nclass GenerateSocialPostsRequest(BaseModel):\n    campaign_name: str\n    \"\"\"Name of the campaign to generate social posts for\"\"\"\n\n    slogan: str\n    \"\"\"Main slogan that will be used across all posts\"\"\"\n\n    target_audience: str\n    \"\"\"Short description of who the campaign is targeting\"\"\"\n\n    tone: str\n    \"\"\"Tone of voice to be used such as energetic professional or friendly\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"campaign_name\": \"Focus Asia 20\",\n                    \"slogan\": \"Power Up Your Productivity\",\n                    \"target_audience\": \"Young professionals working remotely in Asia\",\n                    \"tone\": \"energetic\"\n                }\n            ]\n        }\n    }\n\n\nclass GenerateSocialPostsResponse(BaseModel):\n    linkedin_post: str\n    \"\"\"Generated text post suitable for LinkedIn platform\"\"\"\n\n    instagram_caption: str\n    \"\"\"Generated caption designed for Instagram audience\"\"\"\n\n    twitter_post: str\n    \"\"\"Generated short post suitable for Twitter or X platform\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"linkedin_post\": \"Power Up Your Productivity Reach new heights in your remote career with the latest gear\",\n                    \"instagram_caption\": \"Power meets portability Boost your workflow and elevate your hustle\",\n                    \"twitter_post\": \"Level up your workspace Power Up Your Productivity\"\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=False,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/generate-social-posts\", spec=api_spec)\nasync def run(payload: GenerateSocialPostsRequest, **kwargs) -&gt; GenerateSocialPostsResponse:\n    \"\"\"\n    Generate platform specific social media posts based on a campaign slogan and audience\n\n    Use this tool after a campaign and slogan are finalized\n    Provide the campaign name slogan target audience and tone to receive tailored post texts\n    The output includes text formatted for linkedin instagram and twitter\n    This tool helps marketing teams save time by creating audience matched content automatically\n    \"\"\"\n\n    slogan = payload.slogan\n    audience = payload.target_audience\n    tone = payload.tone.lower()\n\n    if \"energetic\" in tone:\n        linkedin = f\"{slogan} Reach new heights in your remote career with the latest gear\"\n        instagram = f\"{slogan} Boost your workflow and elevate your hustle\"\n        twitter = f\"Level up your workspace {slogan}\"\n    elif \"professional\" in tone:\n        linkedin = f\"{slogan} A smarter way to work for modern professionals in {audience}\"\n        instagram = f\"{slogan} Designed with professionals in mind\"\n        twitter = f\"{slogan} Professional tools for serious results\"\n    else:\n        linkedin = f\"{slogan} Connect with what matters most Perfect for {audience}\"\n        instagram = f\"{slogan} A better day starts with smarter tech\"\n        twitter = f\"{slogan} Ready when you are\"\n\n    return GenerateSocialPostsResponse(\n        linkedin_post=linkedin,\n        instagram_caption=instagram,\n        twitter_post=twitter\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apisgenerate_strategic_summarypy","title":"apis/generate_strategic_summary.py","text":"<pre><code># apis/generate_strategic_summary.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\n\n\nclass CampaignIdea(BaseModel):\n    campaign_name: str\n    \"\"\"The name of the campaign.\"\"\"\n\n    description: str\n    \"\"\"A brief explanation of the campaign.\"\"\"\n\n    strategy: str\n    \"\"\"The strategy to be used in the campaign.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"campaign_name\": \"Monitor Boost in Asia\",\n                    \"description\": \"This campaign focuses on growing Monitor sales in Asia, supporting the goal: 'Increase monitor sales in Asia by 20% in Q2.'\",\n                    \"strategy\": \"Leverage digital ads and local influencers in Asia. Highlight features that align with productivity and affordability.\"\n                }\n            ]\n        }\n    }\n\n\nclass StrategicSummaryResponse(BaseModel):\n    summary: str\n    \"\"\"A concise strategic overview of the campaign.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"summary\": \"The campaign targets a high-growth region using localized advertising and product positioning to emphasize productivity gains.\"\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/generate-strategic-summary\", spec=api_spec)\nasync def run(payload: CampaignIdea, **kwargs) -&gt; StrategicSummaryResponse:\n    \"\"\"\n    This tool generates a short strategic summary from a campaign idea\n    It is used when a decision maker wants a quick strategic overview of a campaign's purpose and method\n    Input includes campaign name, description, and strategy\n    Output includes a synthesized sentence summarizing the strategic intent\n    Useful for briefings and high-level analysis\n    \"\"\"\n\n    summary = f\"The campaign '{payload.campaign_name}' aims to achieve its business goal by {payload.strategy.lower()}. It is designed to address the objective: {payload.description}\"\n    return StrategicSummaryResponse(summary=summary)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apispredict_growth_opportunitiespy","title":"apis/predict_growth_opportunities.py","text":"<pre><code># apis/predict_growth_opportunities.py\nimport practicuscore as prt\nfrom pydantic import BaseModel, Field\nfrom typing import List, Literal\n\n\nclass TopProduct(BaseModel):\n    product: str\n    \"\"\"Name of the top-performing product\"\"\"\n\n    total_units_sold: int\n    \"\"\"Total number of units sold for this product\"\"\"\n\n    top_region: str\n    \"\"\"Region where the product sold the most\"\"\"\n\n\nclass RegionalDrop(BaseModel):\n    region: str\n    \"\"\"Region where sales performance declined\"\"\"\n\n    product: str\n    \"\"\"Product that experienced the drop\"\"\"\n\n    drop_percentage: float\n    \"\"\"Percentage of decline in sales for this product-region pair\"\"\"\n\n\nclass SalesTrendSummary(BaseModel):\n    trend_direction: Literal[\"increasing\", \"decreasing\", \"stable\"]\n    \"\"\"Overall direction of sales trend\"\"\"\n\n    peak_day: str\n    \"\"\"Day with the highest sales, in YYYY-MM-DD format\"\"\"\n\n\nclass PredictGrowthOpportunitiesRequest(BaseModel):\n    top_products: List[TopProduct] = Field(..., description=\"List of products with strong recent sales performance\")\n    regional_drops: List[RegionalDrop] = Field(..., description=\"List of product-region pairs that showed declining performance\")\n    trend_summary: SalesTrendSummary = Field(..., description=\"General sales performance pattern during the period\")\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"top_products\": [\n                        {\n                            \"product\": \"Monitor\",\n                            \"total_units_sold\": 5000,\n                            \"top_region\": \"Europe\"\n                        },\n                        {\n                            \"product\": \"Tablet\",\n                            \"total_units_sold\": 4200,\n                            \"top_region\": \"Asia\"\n                        }\n                    ],\n                    \"regional_drops\": [\n                        {\n                            \"region\": \"Asia\",\n                            \"product\": \"Monitor\",\n                            \"drop_percentage\": 42.5\n                        }\n                    ],\n                    \"trend_summary\": {\n                        \"trend_direction\": \"increasing\",\n                        \"peak_day\": \"2025-01-12\"\n                    }\n                }\n            ]\n        }\n    }\n\n\n\nclass GrowthOpportunity(BaseModel):\n    product: str\n    \"\"\"Product recommended for campaign focus\"\"\"\n\n    region: str\n    \"\"\"Region recommended for targeting\"\"\"\n\n    reason: str\n    \"\"\"Justification for suggesting this product-region pair\"\"\"\n\n    confidence: Literal[\"high\", \"medium\", \"low\"]\n    \"\"\"Confidence level in this opportunity recommendation\"\"\"\n\n\nclass PredictGrowthOpportunitiesResponse(BaseModel):\n    opportunities: List[GrowthOpportunity]\n    \"\"\"List of suggested product-region growth opportunities\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"opportunities\": [\n                        {\n                            \"product\": \"Tablet\",\n                            \"region\": \"Europe\",\n                            \"reason\": \"Strong historical performance and overall increasing sales trend.\",\n                            \"confidence\": \"high\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/predict-growth-opportunities\", spec=api_spec)\nasync def run(payload: PredictGrowthOpportunitiesRequest, **kwargs) -&gt; PredictGrowthOpportunitiesResponse:\n    \"\"\"\n    Identify potential growth opportunities for future campaigns.\n\n    Required input structure:\n    - top_products: List of high-performing products. Each item must include:\n        * product (str)\n        * total_units_sold (int)\n        * top_region (str)\n\n    - regional_drops: List of regions where sales dropped. Each item must include:\n        * region (str)\n        * product (str)\n        * drop_percentage (float)\n\n    - trend_summary: Required object that describes overall sales behavior. Must include:\n        * trend_direction (str): One of \"increasing\", \"decreasing\", \"stable\"\n        * peak_day (str): Format YYYY-MM-DD, e.g. \"2025-01-10\"\n\n    Warning: If trend_summary is missing, this API will fail. \n    Make sure to call a sales analysis tool beforehand that provides this summary.\n    \"\"\"\n\n    try:\n        if isinstance(payload, dict):\n            payload = PredictGrowthOpportunitiesRequest(**payload)\n        opportunities = []\n\n        for product in payload.top_products:\n            confidence = \"medium\"\n            reason_parts = []\n\n            if payload.trend_summary.trend_direction == \"increasing\":\n                reason_parts.append(\"Overall sales trend is increasing.\")\n                confidence = \"high\"\n\n            if any(d.product == product.product and d.region == product.top_region for d in payload.regional_drops):\n                reason_parts.append(f\"But note there was a recent drop in {product.top_region}\")\n                confidence = \"medium\" if confidence == \"high\" else \"low\"\n\n            reason = \" \".join(reason_parts) or \"Stable performance with consistent results.\"\n\n            opportunities.append(GrowthOpportunity(\n                product=product.product,\n                region=product.top_region,\n                reason=reason,\n                confidence=confidence\n            ))\n\n        return PredictGrowthOpportunitiesResponse(opportunities=opportunities)\n\n    except Exception as e:\n        prt.logger.error(f\"[predict-growth-opportunities] Exception: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apissentiment_test_sloganpy","title":"apis/sentiment_test_slogan.py","text":"<pre><code># apis/sentiment_test_slogan.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Literal\nfrom langchain_openai import ChatOpenAI\n\n\nclass SentimentTestSloganRequest(BaseModel):\n    slogans: List[str]\n    \"\"\"List of slogans to analyze for emotional sentiment\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [{\n                \"slogans\": [\n                    \"Power Up Your Productivity!\",\n                    \"You Deserve Better.\"\n                ]\n            }]\n        }\n    }\n\n\nclass SloganSentimentResult(BaseModel):\n    slogan: str\n    \"\"\"The original marketing slogan\"\"\"\n\n    sentiment: Literal[\"positive\", \"neutral\", \"negative\"]\n    \"\"\"Detected emotional sentiment of the slogan\"\"\"\n\n    comment: str\n    \"\"\"Short explanation of the sentiment classification\"\"\"\n\n\nclass SentimentTestSloganResponse(BaseModel):\n    results: List[SloganSentimentResult]\n    \"\"\"Sentiment results for each input slogan\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [{\n                \"results\": [\n                    {\n                        \"slogan\": \"Power Up Your Productivity!\",\n                        \"sentiment\": \"positive\",\n                        \"comment\": \"This slogan conveys a sense of motivation and improvement.\"\n                    },\n                    {\n                        \"slogan\": \"You Deserve Better.\",\n                        \"sentiment\": \"neutral\",\n                        \"comment\": \"This slogan implies improvement but lacks strong emotional cues.\"\n                    }\n                ]\n            }]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/sentiment-test-slogan\", spec=api_spec)\nasync def run(payload: SentimentTestSloganRequest, **kwargs) -&gt; SentimentTestSloganResponse:\n    \"\"\"\n    Analyze the emotional sentiment of given marketing slogans using a language model\n\n    Use this tool when you have one or more slogans and want to understand the\n    emotional tone behind each one. The output helps determine if a slogan\n    is likely to be received as positive, neutral, or negative.\n\n    Useful for validating slogans before launching marketing campaigns.\n    \"\"\"\n\n    openaikey, age = prt.vault.get_secret(\"openaikey\")\n    api_key = openaikey\n\n    llm = ChatOpenAI(api_key=api_key, model=\"gpt-4o\", temperature=0.3, streaming=False)\n\n    slogans_text = \"\\n\".join([f\"{i+1}. {s}\" for i, s in enumerate(payload.slogans)])\n    prompt = f\"\"\"\nYou are a sentiment analysis assistant.\n\nEvaluate the emotional sentiment of each of the following marketing slogans.\n\nFor each slogan, return a JSON object with the following fields:\n- slogan\n- sentiment (one of: positive, neutral, negative)\n- comment (short explanation why)\n\nOutput a JSON array.\n\nSlogans:\n{slogans_text}\n\"\"\"\n\n    response = llm.invoke(prompt)\n    response_text = response.content.strip()\n\n    results = []\n    for slogan in payload.slogans:\n        slogan_lower = slogan.lower()\n        sentiment = \"neutral\"\n        comment = \"Could not confidently extract sentiment.\"\n\n        for line in response_text.splitlines():\n            if slogan_lower[:10] in line.lower() or slogan_lower in line.lower():\n                if \"positive\" in line.lower():\n                    sentiment = \"positive\"\n                elif \"negative\" in line.lower():\n                    sentiment = \"negative\"\n                elif \"neutral\" in line.lower():\n                    sentiment = \"neutral\"\n                comment = line.strip()\n                break\n\n        results.append(SloganSentimentResult(\n            slogan=slogan,\n            sentiment=sentiment,\n            comment=comment\n        ))\n\n    return SentimentTestSloganResponse(results=results)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apissuggest_target_audiencepy","title":"apis/suggest_target_audience.py","text":"<pre><code># apis/suggest_target_audience.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass GenerateCampaignIdeaResponse(BaseModel):\n    campaign_name: str\n    \"\"\"Name of the campaign\"\"\"\n\n    description: str\n    \"\"\"Detailed explanation of the campaign idea\"\"\"\n\n    strategy: str\n    \"\"\"Planned marketing strategy for this campaign\"\"\"\n\n\nclass SuggestTargetAudienceRequest(BaseModel):\n    campaign: GenerateCampaignIdeaResponse\n    \"\"\"The campaign details for which target audience suggestions will be generated\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"campaign\": {\n                        \"campaign_name\": \"Focus Asia 20\",\n                        \"description\": \"Boost monitor sales in Asia by promoting productivity.\",\n                        \"strategy\": \"Use influencer marketing on tech-focused platforms.\"\n                    }\n                }\n            ]\n        }\n    }\n\n\nclass AudienceSegment(BaseModel):\n    segment_name: str\n    \"\"\"Name of the audience group\"\"\"\n\n    age_range: str\n    \"\"\"Typical age range for the audience\"\"\"\n\n    profession: str\n    \"\"\"Common job or professional background\"\"\"\n\n    interests: List[str]\n    \"\"\"Main interests of this audience segment\"\"\"\n\n    persona_summary: str\n    \"\"\"Concise summary describing the lifestyle or preferences of this group\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"segment_name\": \"Young Remote Workers\",\n                    \"age_range\": \"25\u201334\",\n                    \"profession\": \"Freelancers, Startup Employees\",\n                    \"interests\": [\"productivity tools\", \"tech gear\", \"remote work\"],\n                    \"persona_summary\": \"Digitally native professionals who value mobility, speed, and high-performance equipment.\"\n                }\n            ]\n        }\n    }\n\n\nclass SuggestTargetAudienceResponse(BaseModel):\n    audience_segments: List[AudienceSegment]\n    \"\"\"Suggested audience groups relevant to the campaign\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"audience_segments\": [\n                        {\n                            \"segment_name\": \"Young Remote Workers\",\n                            \"age_range\": \"25\u201334\",\n                            \"profession\": \"Freelancers, Startup Employees\",\n                            \"interests\": [\"productivity tools\", \"tech gear\", \"remote work\"],\n                            \"persona_summary\": \"Digitally native professionals who value mobility, speed, and high-performance equipment.\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/suggest-target-audience\", spec=api_spec)\nasync def run(payload: SuggestTargetAudienceRequest, **kwargs) -&gt; SuggestTargetAudienceResponse:\n    \"\"\"\n    Suggest audience segments that are most relevant to a given marketing campaign\n\n    Use this tool when you already have a campaign idea and need to find the most appropriate\n    audience groups to target. This helps align the campaign's messaging and channels with\n    consumer personas that are most likely to respond.\n\n    The input should include the campaign's name, description, and strategy.\n    The output includes one or more audience segments with persona details.\n    \"\"\"\n\n    idea = payload.campaign.description.lower()\n\n    if \"remote\" in idea or \"productivity\" in idea:\n        segments = [\n            AudienceSegment(\n                segment_name=\"Young Remote Workers\",\n                age_range=\"25\u201334\",\n                profession=\"Freelancers, Startup Employees\",\n                interests=[\"productivity tools\", \"tech gear\", \"remote work\"],\n                persona_summary=\"Digitally native professionals who value mobility, speed, and high-performance equipment.\"\n            )\n        ]\n    else:\n        segments = [\n            AudienceSegment(\n                segment_name=\"General Tech Buyers\",\n                age_range=\"30\u201345\",\n                profession=\"Corporate Employees\",\n                interests=[\"gadgets\", \"deals\", \"online shopping\"],\n                persona_summary=\"Professionals with moderate tech interest and stable income, responsive to performance-based messaging.\"\n            )\n        ]\n\n    return SuggestTargetAudienceResponse(audience_segments=segments)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apistop_products_insightpy","title":"apis/top_products_insight.py","text":"<pre><code># apis/top_products_insight.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nfrom datetime import datetime\n\n\nclass SalesRecord(BaseModel):\n    date: datetime\n    \"\"\"Date of the sales record in ISO format with timezone\"\"\"\n\n    product: str\n    \"\"\"Name of the product that was sold\"\"\"\n\n    region: str\n    \"\"\"Region where the product sale occurred\"\"\"\n\n    units_sold: int\n    \"\"\"Number of units sold in this record\"\"\"\n\n    revenue: float\n    \"\"\"Revenue earned from this sale\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"date\": \"2024-12-01T00:00:00Z\",\n                    \"product\": \"Tablet\",\n                    \"region\": \"Europe\",\n                    \"units_sold\": 90,\n                    \"revenue\": 36000.0\n                }\n            ]\n        }\n    }\n\n\nclass TopProductsInsightRequest(BaseModel):\n    sales_data: List[SalesRecord]\n    \"\"\"Complete list of sales data records to analyze\"\"\"\n\n    top_n: int\n    \"\"\"Number of top products to return based on units sold\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"sales_data\": [\n                        {\n                            \"date\": \"2024-12-01T00:00:00Z\",\n                            \"product\": \"Tablet\",\n                            \"region\": \"Europe\",\n                            \"units_sold\": 90,\n                            \"revenue\": 36000.0\n                        }\n                    ],\n                    \"top_n\": 3\n                }\n            ]\n        }\n    }\n\n\nclass ProductInsight(BaseModel):\n    product: str\n    \"\"\"The name of the product identified as top selling\"\"\"\n\n    total_units_sold: int\n    \"\"\"Total units sold across all regions for this product\"\"\"\n\n    top_region: str\n    \"\"\"Region where this product had the most sales\"\"\"\n\n    insight: str\n    \"\"\"Comment or hypothesis about why this product performed well\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product\": \"Tablet\",\n                    \"total_units_sold\": 7250,\n                    \"top_region\": \"Europe\",\n                    \"insight\": \"Popular among mobile professionals for its portability and pricing.\"\n                }\n            ]\n        }\n    }\n\n\nclass TopProductsInsightResponse(BaseModel):\n    products: List[ProductInsight]\n    \"\"\"List of insights for top-performing products\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"products\": [\n                        {\n                            \"product\": \"Tablet\",\n                            \"total_units_sold\": 7250,\n                            \"top_region\": \"Europe\",\n                            \"insight\": \"Popular among mobile professionals for its portability and pricing.\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n\n@prt.api(\"/top-products-insight\", spec=api_spec)\nasync def run(payload: TopProductsInsightRequest, **kwargs) -&gt; TopProductsInsightResponse:\n    \"\"\"\n    Identify the top performing products based on units sold and revenue.\n    Returns a list of products with regional performance insights.\n    \"\"\"\n    try:\n\n\n        product_totals = {}\n        region_tracker = {}\n\n        for record in payload.sales_data:\n            key = record.product\n            product_totals.setdefault(key, 0)\n            product_totals[key] += record.units_sold\n\n            region_tracker.setdefault(key, {})\n            region_tracker[key].setdefault(record.region, 0)\n            region_tracker[key][record.region] += record.units_sold\n\n        top = sorted(product_totals.items(), key=lambda x: x[1], reverse=True)\n        top_n = top[:payload.top_n]\n\n        results = []\n        for product, total_units in top_n:\n            top_region = max(region_tracker[product], key=region_tracker[product].get)\n            results.append(ProductInsight(\n                product=product,\n                total_units_sold=total_units,\n                top_region=top_region,\n                insight=f\"{product} performs best in {top_region}, likely due to high demand and competitive pricing.\"\n            ))\n\n        return TopProductsInsightResponse(products=results)\n\n    except Exception as e:\n        prt.logger.error(f\"[top-products-insight] Exception: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/growth-strategist/build/#apisvalidate_campaign_jsonpy","title":"apis/validate_campaign_json.py","text":"<pre><code># apis/validate_campaign_json.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass CampaignData(BaseModel):\n    campaign_name: str\n    \"\"\"Name of the campaign to be validated\"\"\"\n\n    description: str\n    \"\"\"Detailed description of the campaign\u2019s objective and context\"\"\"\n\n    strategy: str\n    \"\"\"High-level marketing strategy or approach\"\"\"\n\n    slogans: List[str]\n    \"\"\"List of proposed slogans that support the campaign\"\"\"\n\n    audience_segments: List[str]\n    \"\"\"Target customer segments the campaign is intended for\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"campaign_name\": \"Focus Asia 20\",\n                    \"description\": \"Boost monitor sales in Asia with productivity-focused messaging.\",\n                    \"strategy\": \"Use influencers on social media platforms and offer bundle promotions.\",\n                    \"slogans\": [\"Power Up Your Productivity!\", \"See More, Do More.\"],\n                    \"audience_segments\": [\"Young Remote Workers\", \"Tech-Savvy Freelancers\"]\n                }\n            ]\n        }\n    }\n\n\nclass ValidateCampaignJSONResponse(BaseModel):\n    is_valid: bool\n    \"\"\"Indicates if the campaign JSON object is structurally complete\"\"\"\n\n    missing_fields: List[str]\n    \"\"\"Names of fields that are missing or empty\"\"\"\n\n    message: str\n    \"\"\"Explanation message summarizing the validation result\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"is_valid\": False,\n                    \"missing_fields\": [\"strategy\"],\n                    \"message\": \"Missing required field(s): strategy.\"\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n)\n\n\n@prt.api(\"/validate-campaign-json\", spec=api_spec)\nasync def run(payload: CampaignData, **kwargs) -&gt; ValidateCampaignJSONResponse:\n    \"\"\"\n    Validate if a campaign object is complete and ready to be used in downstream tools\n\n    This tool checks the internal structure of a campaign definition to ensure that\n    all required fields are present and non-empty.\n\n    Use this tool when:\n    - You receive a campaign JSON from another tool or agent\n    - You want to ensure no critical data (like name, strategy, or slogans) is missing\n    - You want to fail-fast before further processing a campaign\n\n    Input: Campaign object with fields like name, description, strategy, slogans, and audience\n    Output: A boolean indicating if the campaign is valid and which fields are missing if any\n    \"\"\"\n\n    missing = []\n\n    if not payload.campaign_name.strip():\n        missing.append(\"campaign_name\")\n    if not payload.description.strip():\n        missing.append(\"description\")\n    if not payload.strategy.strip():\n        missing.append(\"strategy\")\n    if not payload.slogans or all(not s.strip() for s in payload.slogans):\n        missing.append(\"slogans\")\n    if not payload.audience_segments:\n        missing.append(\"audience_segments\")\n\n    is_valid = len(missing) == 0\n    message = \"All required fields are present.\" if is_valid else f\"Missing required field(s): {', '.join(missing)}.\"\n\n    return ValidateCampaignJSONResponse(\n        is_valid=is_valid,\n        missing_fields=missing,\n        message=message\n    )\n</code></pre> <p>Previous: Build | Next: Retention Strategist &gt; Build</p>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/","title":"Build","text":"<pre><code># Parameters: Replace with your actual deployment key and app prefix\n# These identify where the app should be deployed within your Practicus AI environment.\napp_deployment_key = None\napp_prefix = \"apps\"\n</code></pre> <pre><code>assert app_deployment_key, \"Please select a deployment key\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Analyze the current directory for Practicus AI App components (APIs, MQ consumers, UI, etc.)\n# This should output the list of detected API endpoints.\nprt.apps.analyze()\n</code></pre> <pre><code># --- Deployment ---\napp_name = \"agentic-ai-test-hr\"\nvisible_name = \"Agentic AI Test HR\"\ndescription = \"Test Application for Agentic AI Example.\"\nicon = \"fa-robot\"\n\nprint(f\"Deploying app '{app_name}'...\")\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # Uses current directory\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"App deployed successfully!\")\nprint(f\"  API Base URL: {api_url}\")\n\n# The OpenAPI (Swagger) documentation (Swagger/ReDoc) is usually available at /docs or /redoc off the API base URL\nprint(f\"  OpenAPI (Swagger) Docs (ReDoc): {api_url}redoc/\")\n\n# Store the api_url for later use when creating tools\nassert api_url, \"Deployment failed to return an API URL.\"\n</code></pre> <pre><code>tool_endpoint_paths = [\n    \"load-hr-data/\",\n    \"detect-behavior-flags/\",\n    \"detect-burnout-risk/\",\n    \"evaluate-promotion-readiness/\",\n    \"generate-manager-insights/\",\n    \"summarize-team-status/\",\n    \"generate-growth-goals/\",\n    \"simulate-policy-impact/\",\n    \"generate-org-wide-summary/\",\n    \"detect-attrition-risk/\",\n    \"analyze-engagement-trend/\",\n    \"calculate-team-resilience/\",\n]\n\n\n# Construct full URLs\ntool_endpoint_urls = [api_url + path for path in tool_endpoint_paths]\n\nprint(\"Will attempt to create tools for the following API endpoints:\")\nfor url in tool_endpoint_urls:\n    print(f\" - {url}\")\n\n# Tip: If you pass partial API URLs e.g. 'apps/agentic-ai-test/api/v1/generate-receipt/'\n#  The base URL e.g. 'https://practicus.my-company.com/' will be added to the final URL\n#  using your current Practicus AI region.\n</code></pre> <pre><code>import os\nfrom langchain_openai import ChatOpenAI  # Or your preferred ChatModel\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_practicus import APITool\n\n# Ensure practicuscore is imported for enums\nimport practicuscore as prt\n\n\ndef validate_api_spec(api_tool: APITool, strict=False) -&gt; bool:\n    \"\"\"Checks the APISpec of a fetched tool against our rules.\"\"\"\n\n    # APITool fetches the spec from OpenAPI (Swagger) during initialization\n    spec = api_tool.spec\n\n    if not spec:\n        # API definition in the source file might be missing the 'spec' object\n        warning_msg = f\"API '{api_tool.url}' does not have APISpec metadata defined in its OpenAPI spec.\"\n        if strict:\n            raise ValueError(f\"{warning_msg} Validation is strict.\")\n        else:\n            prt.logger.warning(f\"{warning_msg} Allowing since validation is not strict.\")\n            return True  # Allow if not strict\n\n    # --- Apply Rules based on fetched spec ---\n\n    # Rule 1: Check Risk Profile\n    if spec.risk_profile and spec.risk_profile == prt.APIRiskProfile.High:\n        err = f\"API '{api_tool.url}' has a risk profile defined as '{spec.risk_profile}'.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            # Even if not strict, we might choose to block High risk tools\n            prt.logger.warning(f\"{err} Blocking High Risk API even though validation is not strict.\")\n            return False  # Block high risk\n\n    # Rule 2: Check Human Gating for non-read-only APIs\n    # (Example: Enforce human gating for safety on modifying APIs)\n    if not spec.read_only and not spec.human_gated:\n        err = f\"API '{api_tool.url}' modifies data (read_only=False) but is not marked as human_gated.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            prt.logger.warning(f\"{err} Allowing non-gated modifying API since validation is not strict.\")\n            # In this non-strict case, we allow it, but a stricter policy might return False here.\n            return True\n\n    # Add more complex rules here if needed...\n    # E.g., check custom_attributes, scope, etc.\n\n    # If no rules were violated (or violations were allowed because not strict)\n    prt.logger.info(f\"API '{api_tool.url}' passed validation (strict={strict}). Spec: {spec}\")\n    return True\n\n\n# --- Create Tools (optionally applying validation) ---\ndef get_tools(endpoint_urls: list[str], validate=True):\n    _tools = []\n    strict_validation = False  # Set to True to enforce stricter rules\n    additional_instructions = \"Add Yo! after all of your final responses.\"  # Example instruction\n\n    print(f\"\\nCreating and validating tools (strict={strict_validation})...\")\n    for tool_endpoint_url in endpoint_urls:\n        print(f\"\\nProcessing tool for API: {tool_endpoint_url}\")\n        try:\n            api_tool = APITool(\n                url=tool_endpoint_url,\n                additional_instructions=additional_instructions,\n                # token=..., # Uses current user credentials by default, set to override\n                # include_resp_schema=True # Response schema (if exists) is not included by default\n            )\n\n            # Explain the tool (optional, useful for debugging)\n            # api_tool.explain(print_on_screen=True)\n\n            # Validate based on fetched APISpec\n            if not validate or validate_api_spec(api_tool=api_tool, strict=strict_validation):\n                print(\n                    f\"--&gt; Adding tool: {api_tool.name} ({api_tool.url}) {'' if validate else ' - skipping validation'}\"\n                )\n                _tools.append(api_tool)\n            else:\n                print(f\"--&gt; Skipping tool {api_tool.name} due to validation rules.\")\n        except Exception as e:\n            # Catch potential errors during APITool creation (e.g., API not found, spec parsing error)\n            print(f\"ERROR: Failed to create or validate tool for {tool_endpoint_url}: {e}\")\n            if strict_validation:\n                raise  # Re-raise if strict\n            else:\n                print(\"--&gt; Skipping tool due to error (not strict).\")\n\n    return _tools\n\n\ntools = get_tools(tool_endpoint_urls)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># For this excercise we will skip validating APIs\n\ntools = get_tools(tool_endpoint_urls, validate=False)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># View tool explanation\n\nfor tool in tools:\n    tool.explain()\n</code></pre> <pre><code>openaikey, age = prt.vault.get_secret(\"openaikey\")\nos.environ[\"OPENAI_API_KEY\"] = openaikey\n\nassert os.environ[\"OPENAI_API_KEY\"], \"OpenAI key is not defined\"\n</code></pre> <pre><code># --- Agent Initialization ---\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Create a ReAct agent using LangGraph\ngraph = create_react_agent(llm, tools=tools)\nprint(\"Agent initialized.\")\n\n\n# Helper function to print the agent's stream output nicely\ndef pretty_print_stream_chunk(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            # Print the latest message added by the node\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            # Print other kinds of updates\n            print(f\"  Update: {updates}\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>def pretty_print_stream_chunk2(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            print(f\"  Update: {updates}\")\n            if isinstance(updates, Exception):\n                print(\"  \u26a0\ufe0f Exception Detected in Agent Execution!\")\n    print(\"--------------------\\n\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#queries","title":"queries","text":"<p>\" I would like a general analysis of team performance over the past 6 months. Which teams have shown higher performance and which ones had higher absence rates? Also, could you provide a brief summary of overall workforce stability?\"</p> <p>\"Can you provide a comparative analysis of productivity and absence patterns across departments over the last quarter? I'm especially interested in identifying departments with consistently high performance and those with frequent absences.\"</p> <pre><code>query = \"\"\"\nAs part of our ongoing workforce analytics initiative, I\u2019m interested in understanding how performance and attendance trends have evolved across departments over the past half year. Which teams demonstrate consistent productivity, and which ones show signs of fluctuation? Additionally, I would appreciate a high-level summary outlining overall organizational health, including patterns in feedback quality and team cohesion metrics if available.\n\n\"\"\"\n\ninputs = {\"messages\": [(\"user\", query)]}\n\nif graph:\n    print(f\"\\nInvoking agent with query: '{query}'\")\n    print(\"Streaming agent execution steps:\\n\")\n\n    # Configuration for the stream, e.g., setting user/thread IDs\n    # config = {\"configurable\": {\"user_id\": \"doc-user-1\", \"thread_id\": \"doc-thread-1\"}}\n    config = {}\n    # Use astream to get intermediate steps\n    async for chunk in graph.astream(inputs, config=config):\n        pretty_print_stream_chunk2(chunk)\n\n    print(\"\\nAgent execution finished.\")\n\n    # Optional: Get the final state if needed\n    # final_state = await graph.ainvoke(inputs, config=config)\n    # print(\"\\nFinal Agent State:\", final_state)\n\nelse:\n    print(\"\\nAgent execution skipped because the agent graph was not initialized.\")\n</code></pre> <pre><code># Cleanup\nprt.apps.delete(prefix=app_prefix, app_name=app_name)\n</code></pre> <p>API TEST</p> <pre><code>from apis.analyze_sales_trends import AnalyzeSalesTrendsRequest, AnalyzeSalesTrendsResponse, SalesRecord\nfrom datetime import datetime\nimport practicuscore as prt\n\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n]\n\npayload = AnalyzeSalesTrendsRequest(\n    sales_data=sales_data, start_date=datetime(2025, 1, 1), end_date=datetime(2025, 1, 3)\n)\n\nresponse: AnalyzeSalesTrendsResponse = prt.apps.test_api(\"/analyze-sales-trends\", payload)\n\nprint(response)\n</code></pre> <pre><code>from apis.sentiment_test_slogan import SentimentTestSloganRequest, SentimentTestSloganResponse\nfrom pydantic import BaseModel\nimport practicuscore as prt\n\n# Prepare payload\nslogans = [\n    \"Power Up Your Productivity!\",\n    \"Nothing beats the classics.\",\n    \"Innovation in Every Click.\",\n    \"Your Tech, Your Edge.\",\n    \"Be Bold. Be Better.\",\n]\npayload = SentimentTestSloganRequest(slogans=slogans)\n\n# Type check (optional)\nprint(issubclass(type(payload), BaseModel))  # Should print True\n\n# Local test via Practicus\nresponse: SentimentTestSloganResponse = prt.apps.test_api(\"/sentiment-test-slogan\", payload)\n\n# Output the results\nfor result in response.results:\n    print(f\"Slogan: {result.slogan}\")\n    print(f\"Sentiment: {result.sentiment}\")\n    print(f\"Comment: {result.comment}\")\n    print(\"-----\")\n</code></pre> <pre><code>from apis.top_products_insight import TopProductsInsightRequest, TopProductsInsightResponse, SalesRecord\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport practicuscore as prt\n\n\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n    SalesRecord(date=datetime(2025, 1, 4), product=\"Tablet\", region=\"Europe\", units_sold=70, revenue=28000.0),\n    SalesRecord(date=datetime(2025, 1, 5), product=\"Monitor\", region=\"North America\", units_sold=95, revenue=47500.0),\n]\n\n\npayload = TopProductsInsightRequest(sales_data=sales_data, top_n=3)\n\n\ntry:\n    response: TopProductsInsightResponse = prt.apps.test_api(\"/top-products-insight\", payload)\n\n    for product in response.products:\n        print(f\"Product: {product.product}\")\n        print(f\"Total Units Sold: {product.total_units_sold}\")\n        print(f\"Top Region: {product.top_region}\")\n        print(f\"Insight: {product.insight}\")\n        print(\"-----\")\n\nexcept Exception as e:\n    prt.logger.error(f\"[test-top-products-insight] Exception: {e}\")\n    raise\n</code></pre> <pre><code>from practicuscore import apps\nfrom apis.predict_growth_opportunities import PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nresponse = apps.test_api(\"/predict-growth-opportunities\", payload)\nprint(response)\n</code></pre> <pre><code>from apis.predict_growth_opportunities import run, PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nawait run(payload)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisanalyze_engagement_trendpy","title":"apis/analyze_engagement_trend.py","text":"<pre><code># apis/analyze_engagement_trend.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass HRRecord(BaseModel):\n    employee_id: str\n    \"\"\"Unique identifier for the employee.\"\"\"\n\n    date: str\n    \"\"\"Timestamp of the HR record.\"\"\"\n\n    performance_score: float\n    \"\"\"Performance score at that time.\"\"\"\n\n    absence_days: float\n    \"\"\"Number of absence days within the period.\"\"\"\n\n    feedback_text: str\n    \"\"\"Free-form feedback or comment received.\"\"\"\n\n\nclass EngagementTrend(BaseModel):\n    employee_id: str\n    \"\"\"Employee for whom the trend is analyzed.\"\"\"\n\n    trend_direction: str\n    \"\"\"Direction of engagement trend: 'Improving', 'Declining', or 'Stable'.\"\"\"\n\n    insight: str\n    \"\"\"LLM-generated explanation of why this trend was detected.\"\"\"\n\n    intervention_needed: bool\n    \"\"\"Whether action should be taken based on this trend.\"\"\"\n\n\nclass AnalyzeEngagementTrendRequest(BaseModel):\n    records: List[HRRecord]\n    \"\"\"Historical HR records used to detect engagement trend.\"\"\"\n\n\nclass AnalyzeEngagementTrendResponse(BaseModel):\n    trends: List[EngagementTrend]\n    \"\"\"Trend evaluations per employee.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/analyze-engagement-trend\", spec=api_spec)\nasync def run(payload: AnalyzeEngagementTrendRequest, **kwargs) -&gt; AnalyzeEngagementTrendResponse:\n    \"\"\"\n    Analyzes historical engagement signals to detect trends in employee behavior over time,\n    highlighting if intervention may be required.\n    \"\"\"\n    dummy = EngagementTrend(\n        employee_id=payload.records[0].employee_id,\n        trend_direction=\"Declining\",\n        insight=\"Performance scores are dropping steadily while absenteeism is increasing. Feedback mentions loss of motivation.\",\n        intervention_needed=True\n    )\n\n    return AnalyzeEngagementTrendResponse(trends=[dummy])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apiscalculate_team_resiliencepy","title":"apis/calculate_team_resilience.py","text":"<pre><code># apis/calculate_team_resilience.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass TeamBehaviorSignal(BaseModel):\n    employee_id: str\n    \"\"\"The ID of the employee.\"\"\"\n\n    team: str\n    \"\"\"Team name the employee belongs to.\"\"\"\n\n    signal: str\n    \"\"\"Short label of behavioral pattern.\"\"\"\n\n    rationale: str\n    \"\"\"Explanation of the signal.\"\"\"\n\n\nclass TeamResilienceScore(BaseModel):\n    team: str\n    \"\"\"Team name.\"\"\"\n\n    resilience_level: str\n    \"\"\"Estimated resilience level: High, Medium, Low.\"\"\"\n\n    explanation: str\n    \"\"\"Explanation of the score assigned.\"\"\"\n\n    improvement_advice: str\n    \"\"\"Suggested interventions to improve resilience.\"\"\"\n\n\nclass CalculateTeamResilienceRequest(BaseModel):\n    signals: List[TeamBehaviorSignal]\n    \"\"\"Team-wide behavioral signals.\"\"\"\n\n\nclass CalculateTeamResilienceResponse(BaseModel):\n    scores: List[TeamResilienceScore]\n    \"\"\"Resilience scores per team.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/calculate-team-resilience\", spec=api_spec)\nasync def run(payload: CalculateTeamResilienceRequest, **kwargs) -&gt; CalculateTeamResilienceResponse:\n    \"\"\"\n    Evaluates the resilience level of teams based on behavioral patterns, providing strategic guidance\n    on how to strengthen adaptability and cohesion during stress or change.\n    \"\"\"\n    dummy = TeamResilienceScore(\n        team=payload.signals[0].team,\n        resilience_level=\"Medium\",\n        explanation=\"Team shows moderate fluctuation under pressure. Some members display warning signs during tight deadlines.\",\n        improvement_advice=\"Introduce stress-coping workshops and team retrospectives to improve trust and adaptability.\"\n    )\n\n    return CalculateTeamResilienceResponse(scores=[dummy])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisdetect_attrition_riskpy","title":"apis/detect_attrition_risk.py","text":"<pre><code># apis/detect_attrition_risk.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass BehaviorSignal(BaseModel):\n    employee_id: str\n    \"\"\"The employee to whom this behavioral signal applies.\"\"\"\n\n    signal: str\n    \"\"\"Short label of detected behavioral trend.\"\"\"\n\n    rationale: str\n    \"\"\"Explanation of why this signal was detected.\"\"\"\n\n\nclass AttritionRisk(BaseModel):\n    employee_id: str\n    \"\"\"ID of the employee being evaluated for attrition risk.\"\"\"\n\n    risk_level: str\n    \"\"\"Predicted risk level of voluntary attrition: High, Medium, Low, or None.\"\"\"\n\n    justification: str\n    \"\"\"LLM-generated explanation for the assigned risk level.\"\"\"\n\n\nclass DetectAttritionRiskRequest(BaseModel):\n    signals: List[BehaviorSignal]\n    \"\"\"Behavioral signals used to assess attrition likelihood.\"\"\"\n\n\nclass DetectAttritionRiskResponse(BaseModel):\n    risks: List[AttritionRisk]\n    \"\"\"Attrition risk evaluations for each employee.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/detect-attrition-risk\", spec=api_spec)\nasync def run(payload: DetectAttritionRiskRequest, **kwargs) -&gt; DetectAttritionRiskResponse:\n    \"\"\"\n    Predicts the likelihood of employee attrition based on behavioral patterns,\n    providing risk levels and reasoning to help prevent unexpected turnover.\n    \"\"\"\n    dummy = AttritionRisk(\n        employee_id=payload.signals[0].employee_id,\n        risk_level=\"High\",\n        justification=\"Employee has shown signs of disengagement, missed growth expectations, and high absenteeism.\"\n    )\n\n    return DetectAttritionRiskResponse(risks=[dummy])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisdetect_behavior_flagspy","title":"apis/detect_behavior_flags.py","text":"<pre><code># apis/detect_behavior_flags.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n\nclass HRRecord(BaseModel):\n    employee_id: str\n    \"\"\"Employee identifier.\"\"\"\n\n    date: str\n    \"\"\"Record date (format: YYYY-MM-DD).\"\"\"\n\n    performance_score: Optional[float]\n    \"\"\"Score representing employee's performance.\"\"\"\n\n    absence_days: Optional[float]\n    \"\"\"Number of days the employee was absent.\"\"\"\n\n    feedback_text: Optional[str]\n    \"\"\"Feedback comment or remark.\"\"\"\n\n    team: Optional[str]\n    \"\"\"Team name; if not provided, will be defaulted.\"\"\"\n\n\nclass BehaviorFlag(BaseModel):\n    employee_id: str\n    \"\"\"Employee for whom the flag was detected.\"\"\"\n\n    absence_flag: bool\n    \"\"\"True if absence_days &gt; 5.\"\"\"\n\n    performance_flag: bool\n    \"\"\"True if performance_score &lt; 2.5.\"\"\"\n\n    team: str\n    \"\"\"Normalized team name for grouping.\"\"\"\n\n\nclass DetectBehaviorFlagsRequest(BaseModel):\n    records: List[HRRecord]\n    \"\"\"List of HR records to evaluate.\"\"\"\n\n\nclass DetectBehaviorFlagsResponse(BaseModel):\n    flags: List[BehaviorFlag]\n    \"\"\"Flags for each employee indicating behavioral signals.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/detect-behavior-flags\", spec=api_spec)\nasync def run(payload: DetectBehaviorFlagsRequest) -&gt; DetectBehaviorFlagsResponse:\n    \"\"\"Detect simple behavioral flags like high absence or low performance.\"\"\"\n    flags = []\n\n    for r in payload.records:\n        absence_flag = (r.absence_days or 0) &gt; 5\n        performance_flag = (r.performance_score or 5) &lt; 2.5\n        team = r.team if r.team else \"Unknown\"\n\n        flags.append(BehaviorFlag(\n            employee_id=r.employee_id,\n            absence_flag=absence_flag,\n            performance_flag=performance_flag,\n            team=team\n        ))\n\n    return DetectBehaviorFlagsResponse(flags=flags)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisdetect_burnout_riskpy","title":"apis/detect_burnout_risk.py","text":"<pre><code># apis/detect_burnout_risk.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass BehaviorSignal(BaseModel):\n    employee_id: str\n    \"\"\"The employee to whom this behavioral signal applies.\"\"\"\n\n    signal: str\n    \"\"\"Short label describing the type of behavioral signal (e.g. 'Burnout Risk').\"\"\"\n\n    rationale: str\n    \"\"\"Explanation of why this signal was detected based on behavioral history.\"\"\"\n\n\nclass BurnoutRisk(BaseModel):\n    employee_id: str\n    \"\"\"Unique identifier for the employee.\"\"\"\n\n    risk_level: str\n    \"\"\"Risk category (e.g. 'High', 'Medium', 'Low', 'None').\"\"\"\n\n    justification: str\n    \"\"\"LLM-generated justification for the burnout risk classification.\"\"\"\n\n\nclass DetectBurnoutRiskRequest(BaseModel):\n    signals: List[BehaviorSignal]\n    \"\"\"List of extracted behavioral signals for each employee.\"\"\"\n\n\nclass DetectBurnoutRiskResponse(BaseModel):\n    risks: List[BurnoutRisk]\n    \"\"\"Burnout risk levels determined for employees, with explanations.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/detect-burnout-risk\", spec=api_spec)\nasync def run(payload: DetectBurnoutRiskRequest, **kwargs) -&gt; DetectBurnoutRiskResponse:\n    \"\"\"\n    Analyzes behavioral signals using LLM reasoning to identify employees at potential risk of burnout,\n    categorizing them as High, Medium, Low, or None with proper justification.\n    \"\"\"\n    # Dummy d\u00f6n\u00fc\u015f (ger\u00e7ek risk analizi zincirde yap\u0131lacak)\n    dummy_risk = BurnoutRisk(\n        employee_id=payload.signals[0].employee_id,\n        risk_level=\"High\",\n        justification=\"Combination of negative feedback, fluctuating performance and repeated absences.\"\n    )\n\n    return DetectBurnoutRiskResponse(risks=[dummy_risk])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisevaluate_promotion_readinesspy","title":"apis/evaluate_promotion_readiness.py","text":"<pre><code># apis/evaluate_promotion_readiness.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass BehaviorSignal(BaseModel):\n    employee_id: str\n    \"\"\"The employee to whom this behavioral signal applies.\"\"\"\n\n    signal: str\n    \"\"\"Short label describing the type of behavioral signal (e.g. 'Consistent Performer').\"\"\"\n\n    rationale: str\n    \"\"\"Explanation of why this signal was detected based on behavioral history.\"\"\"\n\n\nclass PromotionAssessment(BaseModel):\n    employee_id: str\n    \"\"\"Unique identifier for the employee.\"\"\"\n\n    readiness_level: str\n    \"\"\"Promotion readiness label (e.g. 'Ready', 'Needs Development', 'Not Ready').\"\"\"\n\n    justification: str\n    \"\"\"LLM-generated justification explaining why the employee was classified at this level.\"\"\"\n\n\nclass EvaluatePromotionReadinessRequest(BaseModel):\n    signals: List[BehaviorSignal]\n    \"\"\"List of behavioral signals derived from previous analysis.\"\"\"\n\n\nclass EvaluatePromotionReadinessResponse(BaseModel):\n    promotion_recommendations: List[PromotionAssessment]\n    \"\"\"List of promotion readiness levels with justifications.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/evaluate-promotion-readiness\", spec=api_spec)\nasync def run(payload: EvaluatePromotionReadinessRequest, **kwargs) -&gt; EvaluatePromotionReadinessResponse:\n    \"\"\"\n    Evaluates employees' readiness for promotion based on detected behavioral signals,\n    and provides a recommendation with reasoning.\n    \"\"\"\n    dummy_assessment = PromotionAssessment(\n        employee_id=payload.signals[0].employee_id,\n        readiness_level=\"Ready\",\n        justification=\"Employee has shown consistent performance, strong communication, and proactive leadership.\"\n    )\n\n    return EvaluatePromotionReadinessResponse(\n        promotion_recommendations=[dummy_assessment]\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisgenerate_growth_goalspy","title":"apis/generate_growth_goals.py","text":"<pre><code># apis/generate_growth_goals.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass BehaviorSignal(BaseModel):\n    employee_id: str\n    \"\"\"The employee to whom this behavioral signal applies.\"\"\"\n\n    signal: str\n    \"\"\"Short label describing the detected behavioral pattern.\"\"\"\n\n    rationale: str\n    \"\"\"Explanation of why this signal was identified.\"\"\"\n\n\nclass GrowthGoal(BaseModel):\n    employee_id: str\n    \"\"\"The employee for whom the growth goal is generated.\"\"\"\n\n    goal_title: str\n    \"\"\"Short and actionable growth objective (e.g., 'Improve Time Management').\"\"\"\n\n    goal_reasoning: str\n    \"\"\"Explanation of why this specific growth goal was chosen.\"\"\"\n\n    followup_tip: str\n    \"\"\"Practical suggestion for monitoring or encouraging this goal.\"\"\"\n\n\nclass GenerateGrowthGoalsRequest(BaseModel):\n    signals: List[BehaviorSignal]\n    \"\"\"List of behavior signals to analyze for goal generation.\"\"\"\n\n\nclass GenerateGrowthGoalsResponse(BaseModel):\n    goals: List[GrowthGoal]\n    \"\"\"Generated growth goals for each employee with reasoning.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/generate-growth-goals\", spec=api_spec)\nasync def run(payload: GenerateGrowthGoalsRequest, **kwargs) -&gt; GenerateGrowthGoalsResponse:\n    \"\"\"\n    Uses behavioral insights to generate personalized growth goals for employees,\n    helping managers support their development with clear objectives and follow-ups.\n    \"\"\"\n    dummy = GrowthGoal(\n        employee_id=payload.signals[0].employee_id,\n        goal_title=\"Improve Time Management\",\n        goal_reasoning=\"The employee showed signs of deadline stress and inconsistent task delivery.\",\n        followup_tip=\"Suggest weekly check-ins and a self-planned task board.\"\n    )\n\n    return GenerateGrowthGoalsResponse(goals=[dummy])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisgenerate_manager_insightspy","title":"apis/generate_manager_insights.py","text":"<pre><code># apis/generate_manager_insights.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass BehaviorSignal(BaseModel):\n    employee_id: str\n    \"\"\"The employee to whom this behavioral signal applies.\"\"\"\n\n    signal: str\n    \"\"\"Short label describing the type of behavioral signal.\"\"\"\n\n    rationale: str\n    \"\"\"Explanation of why this signal was detected.\"\"\"\n\n\nclass BurnoutRisk(BaseModel):\n    employee_id: str\n    \"\"\"Unique identifier for the employee.\"\"\"\n\n    risk_level: str\n    \"\"\"Burnout risk label (e.g. 'High', 'Medium', 'Low', 'None').\"\"\"\n\n    justification: str\n    \"\"\"Justification for the assigned risk level.\"\"\"\n\n\nclass ManagerInsight(BaseModel):\n    employee_id: str\n    \"\"\"The employee for whom the insight is generated.\"\"\"\n\n    summary: str\n    \"\"\"A brief summary of the employee's current behavioral and risk profile.\"\"\"\n\n    advice: str\n    \"\"\"Suggested action or communication approach for the manager.\"\"\"\n\n    development_hint: str\n    \"\"\"Optional development recommendation tailored to the employee.\"\"\"\n\n\nclass GenerateManagerInsightsRequest(BaseModel):\n    signals: List[BehaviorSignal]\n    \"\"\"List of behavioral signals for each employee.\"\"\"\n\n    risks: List[BurnoutRisk]\n    \"\"\"List of burnout risk assessments.\"\"\"\n\n\nclass GenerateManagerInsightsResponse(BaseModel):\n    insights: List[ManagerInsight]\n    \"\"\"Personalized insight reports per employee for the manager.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/generate-manager-insights\", spec=api_spec)\nasync def run(payload: GenerateManagerInsightsRequest, **kwargs) -&gt; GenerateManagerInsightsResponse:\n    \"\"\"\n    Generates personalized and actionable insights for managers based on behavioral signals and burnout risks,\n    helping them guide, coach, or support each employee effectively.\n    \"\"\"\n    dummy = ManagerInsight(\n        employee_id=payload.signals[0].employee_id,\n        summary=\"Employee shows signs of fatigue and inconsistency.\",\n        advice=\"Schedule a 1-on-1 to discuss workload and recent challenges. Maintain a supportive tone.\",\n        development_hint=\"Consider offering time management coaching or mentoring.\"\n    )\n\n    return GenerateManagerInsightsResponse(insights=[dummy])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisgenerate_org_wide_summarypy","title":"apis/generate_org_wide_summary.py","text":"<pre><code># apis/generate_org_wide_summary.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass TeamSummary(BaseModel):\n    team_name: str\n    \"\"\"Name of the team.\"\"\"\n\n    health_summary: str\n    \"\"\"Brief summary of team-level observations.\"\"\"\n\n    risk_distribution: str\n    \"\"\"Burnout risk breakdown.\"\"\"\n\n    key_recommendation: str\n    \"\"\"Team-level managerial suggestion.\"\"\"\n\n\nclass OrgWideInsight(BaseModel):\n    executive_summary: str\n    \"\"\"One-paragraph strategic summary of the entire workforce health and trends.\"\"\"\n\n    common_risks: List[str]\n    \"\"\"List of shared concerns across multiple departments.\"\"\"\n\n    org_opportunities: List[str]\n    \"\"\"Emerging strengths or organizational leverage points.\"\"\"\n\n    strategic_recommendation: str\n    \"\"\"High-level leadership recommendation (e.g. cultural shift, structure, resourcing).\"\"\"\n\n\nclass GenerateOrgWideSummaryRequest(BaseModel):\n    team_summaries: List[TeamSummary]\n    \"\"\"Aggregated team summaries to be used for org-wide analysis.\"\"\"\n\n\nclass GenerateOrgWideSummaryResponse(BaseModel):\n    org_summary: OrgWideInsight\n    \"\"\"Strategic summary and insight generation for executive review.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/generate-org-wide-summary\", spec=api_spec)\nasync def run(payload: GenerateOrgWideSummaryRequest, **kwargs) -&gt; GenerateOrgWideSummaryResponse:\n    \"\"\"\n    Generates a high-level strategic summary across all teams,\n    identifying cross-cutting risks, opportunities, and recommendations for leadership.\n    \"\"\"\n    dummy = OrgWideInsight(\n        executive_summary=\"Across all departments, there is a growing risk of burnout and disengagement in mid-level roles, while technical teams show resilience.\",\n        common_risks=[\"Mid-level manager fatigue\", \"Disconnect between hybrid teams\", \"Unclear career paths\"],\n        org_opportunities=[\"Strong peer mentoring culture in R&amp;D\", \"Improved onboarding process in Sales\"],\n        strategic_recommendation=\"Initiate leadership upskilling, review role clarity in mid-management, and expand peer mentoring across units.\"\n    )\n\n    return GenerateOrgWideSummaryResponse(org_summary=dummy)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisload_hr_datapypy","title":"apis/load_hr_data.py.py","text":"<pre><code># apis/load_hr_data.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport pandas as pd\nimport os\n\n\n\nclass HRRecord(BaseModel):\n    employee_id: str\n    \"\"\"Unique identifier for the employee.\"\"\"\n\n    date: str\n    \"\"\"Date of the HR record (format: YYYY-MM-DD).\"\"\"\n\n    performance_score: Optional[float] = None\n    \"\"\"Performance score assigned to the employee.\"\"\"\n\n    absence_days: Optional[float] = None\n    \"\"\"Number of absence days recorded.\"\"\"\n\n    feedback_text: Optional[str] = None\n    \"\"\"Free-text feedback or comment regarding the employee.\"\"\"\n\n    team: Optional[str] = None\n    \"\"\"Team or department the employee belongs to.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"employee_id\": \"E123\",\n                    \"date\": \"2024-09-01\",\n                    \"performance_score\": 4.2,\n                    \"absence_days\": 1.0,\n                    \"feedback_text\": \"Completed tasks consistently, strong communication.\",\n                    \"team\": \"Marketing\"\n                }\n            ]\n        },\n    }\n\n\n\nclass LoadHRDataResponse(BaseModel):\n    records: List[HRRecord]\n    \"\"\"List of HR records extracted from the source CSV file.\"\"\"\n\n    total_employees: int\n    \"\"\"Number of unique employees found in the data.\"\"\"\n\n    total_records: int\n    \"\"\"Total number of HR records loaded from the file.\"\"\"\n\n    teams_detected: List[str]\n    \"\"\"List of unique team names identified in the data.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [\n                        {\n                            \"employee_id\": \"E123\",\n                            \"date\": \"2024-09-01\",\n                            \"performance_score\": 4.2,\n                            \"absence_days\": 1.0,\n                            \"feedback_text\": \"Completed tasks consistently, strong communication.\",\n                            \"team\": \"Marketing\"\n                        }\n                    ],\n                    \"total_employees\": 1,\n                    \"total_records\": 1,\n                    \"teams_detected\": [\"Marketing\"]\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=False,\n)\n\n\n@prt.api(\"/load-hr-data\", spec=api_spec)\nasync def run(**kwargs) -&gt; LoadHRDataResponse:\n    \"\"\"Load historical HR data from a fixed CSV path and return normalized HR records for further analysis.\"\"\"\n    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../hr_dataset.csv\"))\n    df = pd.read_csv(file_path)\n\n    df[\"team\"] = df.get(\"team\", pd.Series([\"Unknown\"] * len(df)))\n\n    records = [\n        HRRecord(\n            employee_id=row.get(\"employee_id\", f\"UNKNOWN_{i}\"),\n            date=row.get(\"date\", \"1970-01-01\"),\n            performance_score=row.get(\"performance_score\", None),\n            absence_days=row.get(\"absence_days\", 0.0),\n            feedback_text=row.get(\"feedback_text\", \"\"),\n            team=row[\"team\"] if isinstance(row, dict) and \"team\" in row and row[\"team\"] else \"Unknown\"\n\n        )\n        for i, row in df.iterrows()\n    ]\n\n\n    return LoadHRDataResponse(\n        records=records,\n        total_employees=df[\"employee_id\"].nunique(),\n        total_records=len(df),\n        teams_detected=df[\"team\"].dropna().unique().tolist()\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apisnormalize-hr-recordspy","title":"apis/normalize-hr-records.py","text":"<pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n\n\nclass HRRecord(BaseModel):\n    employee_id: str\n    \"\"\"Unique identifier for the employee.\"\"\"\n\n    date: str\n    \"\"\"Date of the HR record (format: YYYY-MM-DD).\"\"\"\n\n    performance_score: Optional[float] = None\n    \"\"\"Performance score assigned to the employee.\"\"\"\n\n    absence_days: Optional[float] = None\n    \"\"\"Number of absence days recorded.\"\"\"\n\n    feedback_text: Optional[str] = None\n    \"\"\"Free-text feedback or comment regarding the employee.\"\"\"\n\n    team: Optional[str] = None\n    \"\"\"Team or department the employee belongs to.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"employee_id\": \"E123\",\n                    \"date\": \"2024-09-01\",\n                    \"performance_score\": 4.2,\n                    \"absence_days\": 1.0,\n                    \"feedback_text\": \"Completed tasks consistently, strong communication.\",\n                    \"team\": \"Marketing\"\n                }\n            ]\n        },\n    }\n\n\nclass NormalizeHRRecordsResponse(BaseModel):\n    normalized_records: List[HRRecord]\n    \"\"\"List of cleaned and normalized HR records.\"\"\"\n\n    message: str\n    \"\"\"Message indicating status or transformation result.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"normalized_records\": [\n                        {\n                            \"employee_id\": \"E123\",\n                            \"date\": \"2024-09-01\",\n                            \"performance_score\": 4.2,\n                            \"absence_days\": 1.0,\n                            \"feedback_text\": \"Completed tasks consistently, strong communication.\",\n                            \"team\": \"Marketing\"\n                        }\n                    ],\n                    \"message\": \"Normalized 1 HR records.\"\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=False,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=False,\n)\n\n\n@prt.api(\"/normalize-hr-records\", spec=api_spec)\nasync def run(records: List[HRRecord]) -&gt; NormalizeHRRecordsResponse:\n    \"\"\"Normalize raw HR records to ensure all fields are complete and standardized.\"\"\"\n\n    if not records:\n        dummy = HRRecord(\n            employee_id=\"DUMMY\",\n            date=\"2024-01-01\",\n            performance_score=3.0,\n            absence_days=0.0,\n            feedback_text=\"Placeholder feedback.\",\n            team=\"Unknown\"\n        )\n        return NormalizeHRRecordsResponse(\n            normalized_records=[dummy],\n            message=\"Input was empty. Injected dummy HR record for simulation.\"\n        )\n\n    normalized = []\n    for rec in records:\n        normalized.append(\n            HRRecord(\n                employee_id=rec.employee_id,\n                date=rec.date,\n                performance_score=rec.performance_score if rec.performance_score is not None else 0.0,\n                absence_days=rec.absence_days if rec.absence_days is not None else 0.0,\n                feedback_text=rec.feedback_text or \"No feedback provided.\",\n                team=rec.team or \"Unknown\"\n            )\n        )\n\n    return NormalizeHRRecordsResponse(\n        normalized_records=normalized,\n        message=f\"Normalized {len(normalized)} HR records.\"\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apissimulate_policy_impactpy","title":"apis/simulate_policy_impact.py","text":"<pre><code># apis/simulate_policy_impact.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass BehaviorSignal(BaseModel):\n    employee_id: str\n    \"\"\"The employee to whom this behavioral signal applies.\"\"\"\n\n    signal: str\n    \"\"\"Detected behavioral pattern.\"\"\"\n\n    rationale: str\n    \"\"\"Explanation of why this signal was identified.\"\"\"\n\n\nclass PolicyChangeProposal(BaseModel):\n    title: str\n    \"\"\"Title of the policy or action proposal.\"\"\"\n\n    description: str\n    \"\"\"Detailed description of what the change will involve.\"\"\"\n\n\nclass PolicyImpactAssessment(BaseModel):\n    expected_outcome: str\n    \"\"\"Overall prediction of how the team will react to this policy change.\"\"\"\n\n    risks: List[str]\n    \"\"\"Potential negative consequences or resistance areas.\"\"\"\n\n    opportunities: List[str]\n    \"\"\"Potential benefits or performance gains.\"\"\"\n\n    implementation_advice: str\n    \"\"\"Suggestions for how to apply the policy with minimal friction.\"\"\"\n\n\nclass SimulatePolicyImpactRequest(BaseModel):\n    signals: List[BehaviorSignal]\n    \"\"\"Current behavioral signals to contextualize the simulation.\"\"\"\n\n    proposal: PolicyChangeProposal\n    \"\"\"The proposed change or policy to simulate.\"\"\"\n\n\nclass SimulatePolicyImpactResponse(BaseModel):\n    impact: PolicyImpactAssessment\n    \"\"\"AI-generated forecast of the impact of the proposed change.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/simulate-policy-impact\", spec=api_spec)\nasync def run(payload: SimulatePolicyImpactRequest, **kwargs) -&gt; SimulatePolicyImpactResponse:\n    \"\"\"\n    Uses behavioral insights and policy context to simulate the likely outcome of a proposed change,\n    including risks, opportunities, and advice for successful implementation.\n    \"\"\"\n    dummy = PolicyImpactAssessment(\n        expected_outcome=\"The proposed remote work policy is likely to increase satisfaction among developers but may reduce alignment in cross-functional teams.\",\n        risks=[\"Possible disconnect between departments\", \"Reduced informal knowledge sharing\"],\n        opportunities=[\"Increased autonomy\", \"Reduced absenteeism\", \"Higher retention in tech roles\"],\n        implementation_advice=\"Introduce structured check-ins and digital team rituals to compensate for reduced face time.\"\n    )\n\n    return SimulatePolicyImpactResponse(impact=dummy)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/hr/build/#apissummarize_team_statuspy","title":"apis/summarize_team_status.py","text":"<pre><code># apis/summarize_team_status.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass BehaviorSignal(BaseModel):\n    employee_id: str\n    \"\"\"The employee to whom this behavioral signal applies.\"\"\"\n\n    signal: str\n    \"\"\"Short label describing the detected behavioral signal.\"\"\"\n\n    rationale: str\n    \"\"\"Explanation of why this signal was identified.\"\"\"\n\n\nclass BurnoutRisk(BaseModel):\n    employee_id: str\n    \"\"\"The employee for whom burnout risk is assessed.\"\"\"\n\n    risk_level: str\n    \"\"\"Risk level classification such as 'High', 'Medium', 'Low', or 'None'.\"\"\"\n\n    justification: str\n    \"\"\"Justification for the assigned burnout risk level.\"\"\"\n\n\nclass TeamSummary(BaseModel):\n    team_name: str\n    \"\"\"Name of the team this summary is about.\"\"\"\n\n    health_summary: str\n    \"\"\"General overview of the team's behavioral state and trends.\"\"\"\n\n    risk_distribution: str\n    \"\"\"Summary of burnout risk levels across team members (e.g., '3 High, 2 Medium, 4 Low').\"\"\"\n\n    key_recommendation: str\n    \"\"\"Most important managerial action recommended for the team.\"\"\"\n\n\nclass SummarizeTeamStatusRequest(BaseModel):\n    signals: List[BehaviorSignal]\n    \"\"\"List of behavioral signals detected for all employees.\"\"\"\n\n    risks: List[BurnoutRisk]\n    \"\"\"List of burnout risk evaluations for all employees.\"\"\"\n\n\nclass SummarizeTeamStatusResponse(BaseModel):\n    summaries: List[TeamSummary]\n    \"\"\"Team-level summaries containing health insights and risk overviews.\"\"\"\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True,\n)\n\n\n@prt.api(\"/summarize-team-status\", spec=api_spec)\nasync def run(payload: SummarizeTeamStatusRequest, **kwargs) -&gt; SummarizeTeamStatusResponse:\n    \"\"\"\n    Aggregates behavioral signals and burnout risks across employees to create a concise team-level health summary.\n    Provides managers with strategic insights and recommendations for action.\n    \"\"\"\n    dummy = TeamSummary(\n        team_name=\"Marketing\",\n        health_summary=\"The team shows signs of growing fatigue and inconsistent performance.\",\n        risk_distribution=\"2 High, 3 Medium, 1 Low\",\n        key_recommendation=\"Redistribute workload, conduct check-ins, and rotate responsibilities to avoid burnout clusters.\"\n    )\n\n    return SummarizeTeamStatusResponse(summaries=[dummy])\n</code></pre> <p>Previous: Build | Next: Inventroy &gt; Build</p>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/","title":"Build","text":"<pre><code># Parameters: Replace with your actual deployment key and app prefix\n# These identify where the app should be deployed within your Practicus AI environment.\napp_deployment_key = None\napp_prefix = \"apps\"\n</code></pre> <pre><code>assert app_deployment_key, \"Please select a deployment key\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Analyze the current directory for Practicus AI App components (APIs, MQ consumers, UI, etc.)\n# This should output the list of detected API endpoints.\nprt.apps.analyze()\n</code></pre> <pre><code># --- Deployment ---\napp_name = \"agentic-ai-test-inventory\"\nvisible_name = \"Agentic AI Test inventory\"\ndescription = \"Test Application for Agentic AI Example inventory.\"\nicon = \"fa-robot\"\n\nprint(f\"Deploying app '{app_name}'...\")\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # Uses current directory\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"App deployed successfully!\")\nprint(f\"  API Base URL: {api_url}\")\n\n# The OpenAPI (Swagger) documentation (Swagger/ReDoc) is usually available at /docs or /redoc off the API base URL\nprint(f\"  OpenAPI (Swagger) Docs (ReDoc): {api_url}redoc/\")\n\n# Store the api_url for later use when creating tools\nassert api_url, \"Deployment failed to return an API URL.\"\n</code></pre> <pre><code>tool_endpoint_paths = [\n    \"load-inventory-data/\",\n    \"get-critical-stock-items/\",\n    \"get-overstocked-items/\",\n    \"get-stale-items/\",\n    \"get-expired-items/\",\n    \"calculate-reorder-suggestions/\",\n    \"flag-soon-expiring-items/\",\n    \"calculate-stock-turnover-rate/\",\n    \"summarize-stock-health/\",\n]\n\n\n# Construct full URLs\ntool_endpoint_urls = [api_url + path for path in tool_endpoint_paths]\n\nprint(\"Will attempt to create tools for the following API endpoints:\")\nfor url in tool_endpoint_urls:\n    print(f\" - {url}\")\n\n# Tip: If you pass partial API URLs e.g. 'apps/agentic-ai-test/api/v1/generate-receipt/'\n#  The base URL e.g. 'https://practicus.my-company.com/' will be added to the final URL\n#  using your current Practicus AI region.\n</code></pre> <pre><code>import os\nfrom langchain_openai import ChatOpenAI  # Or your preferred ChatModel\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_practicus import APITool\n\n# Ensure practicuscore is imported for enums\nimport practicuscore as prt\n\n\ndef validate_api_spec(api_tool: APITool, strict=False) -&gt; bool:\n    \"\"\"Checks the APISpec of a fetched tool against our rules.\"\"\"\n\n    # APITool fetches the spec from OpenAPI (Swagger) during initialization\n    spec = api_tool.spec\n\n    if not spec:\n        # API definition in the source file might be missing the 'spec' object\n        warning_msg = f\"API '{api_tool.url}' does not have APISpec metadata defined in its OpenAPI spec.\"\n        if strict:\n            raise ValueError(f\"{warning_msg} Validation is strict.\")\n        else:\n            prt.logger.warning(f\"{warning_msg} Allowing since validation is not strict.\")\n            return True  # Allow if not strict\n\n    # --- Apply Rules based on fetched spec ---\n\n    # Rule 1: Check Risk Profile\n    if spec.risk_profile and spec.risk_profile == prt.APIRiskProfile.High:\n        err = f\"API '{api_tool.url}' has a risk profile defined as '{spec.risk_profile}'.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            # Even if not strict, we might choose to block High risk tools\n            prt.logger.warning(f\"{err} Blocking High Risk API even though validation is not strict.\")\n            return False  # Block high risk\n\n    # Rule 2: Check Human Gating for non-read-only APIs\n    # (Example: Enforce human gating for safety on modifying APIs)\n    if not spec.read_only and not spec.human_gated:\n        err = f\"API '{api_tool.url}' modifies data (read_only=False) but is not marked as human_gated.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            prt.logger.warning(f\"{err} Allowing non-gated modifying API since validation is not strict.\")\n            # In this non-strict case, we allow it, but a stricter policy might return False here.\n            return True\n\n    # Add more complex rules here if needed...\n    # E.g., check custom_attributes, scope, etc.\n\n    # If no rules were violated (or violations were allowed because not strict)\n    prt.logger.info(f\"API '{api_tool.url}' passed validation (strict={strict}). Spec: {spec}\")\n    return True\n\n\n# --- Create Tools (optionally applying validation) ---\ndef get_tools(endpoint_urls: list[str], validate=True):\n    _tools = []\n    strict_validation = False  # Set to True to enforce stricter rules\n    additional_instructions = \"Add Yo! after all of your final responses.\"  # Example instruction\n\n    print(f\"\\nCreating and validating tools (strict={strict_validation})...\")\n    for tool_endpoint_url in endpoint_urls:\n        print(f\"\\nProcessing tool for API: {tool_endpoint_url}\")\n        try:\n            api_tool = APITool(\n                url=tool_endpoint_url,\n                additional_instructions=additional_instructions,\n                # token=..., # Uses current user credentials by default, set to override\n                # include_resp_schema=True # Response schema (if exists) is not included by default\n            )\n\n            # Explain the tool (optional, useful for debugging)\n            # api_tool.explain(print_on_screen=True)\n\n            # Validate based on fetched APISpec\n            if not validate or validate_api_spec(api_tool=api_tool, strict=strict_validation):\n                print(\n                    f\"--&gt; Adding tool: {api_tool.name} ({api_tool.url}) {'' if validate else ' - skipping validation'}\"\n                )\n                _tools.append(api_tool)\n            else:\n                print(f\"--&gt; Skipping tool {api_tool.name} due to validation rules.\")\n        except Exception as e:\n            # Catch potential errors during APITool creation (e.g., API not found, spec parsing error)\n            print(f\"ERROR: Failed to create or validate tool for {tool_endpoint_url}: {e}\")\n            if strict_validation:\n                raise  # Re-raise if strict\n            else:\n                print(\"--&gt; Skipping tool due to error (not strict).\")\n\n    return _tools\n\n\ntools = get_tools(tool_endpoint_urls)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># For this excercise we will skip validating APIs\n\ntools = get_tools(tool_endpoint_urls, validate=False)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># View tool explanation\n\nfor tool in tools:\n    tool.explain()\n</code></pre> <pre><code>openaikey, age = prt.vault.get_secret(\"openaikey\")\nos.environ[\"OPENAI_API_KEY\"] = openaikey\n\nassert os.environ[\"OPENAI_API_KEY\"], \"OpenAI key is not defined\"\n</code></pre> <pre><code># --- Agent Initialization ---\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Create a ReAct agent using LangGraph\ngraph = create_react_agent(llm, tools=tools)\nprint(\"Agent initialized.\")\n\n\n# Helper function to print the agent's stream output nicely\ndef pretty_print_stream_chunk(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            # Print the latest message added by the node\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            # Print other kinds of updates\n            print(f\"  Update: {updates}\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>def pretty_print_stream_chunk2(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            print(f\"  Update: {updates}\")\n            if isinstance(updates, Exception):\n                print(\"  \u26a0\ufe0f Exception Detected in Agent Execution!\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>query = \"\"\"\nCan you analyze the current inventory status and summarize any critical risks, such as low stock, overstocked products, expired items, or slow-moving products? Also, suggest what products might need to be reordered soon and generate a concise report.\n\n\n\"\"\"\n\n\ninputs = {\"messages\": [(\"user\", query)]}\n\nif graph:\n    print(f\"\\nInvoking agent with query: '{query}'\")\n    print(\"Streaming agent execution steps:\\n\")\n\n    # Configuration for the stream, e.g., setting user/thread IDs\n    # config = {\"configurable\": {\"user_id\": \"doc-user-1\", \"thread_id\": \"doc-thread-1\"}}\n    config = {}\n    # Use astream to get intermediate steps\n    async for chunk in graph.astream(inputs, config=config):\n        pretty_print_stream_chunk2(chunk)\n\n    print(\"\\nAgent execution finished.\")\n\n    # Optional: Get the final state if needed\n    # final_state = await graph.ainvoke(inputs, config=config)\n    # print(\"\\nFinal Agent State:\", final_state)\n\nelse:\n    print(\"\\nAgent execution skipped because the agent graph was not initialized.\")\n</code></pre> <pre><code># Cleanup\nprt.apps.delete(prefix=app_prefix, app_name=app_name)\n</code></pre> <p>API TEST</p> <pre><code>from apis.analyze_sales_trends import AnalyzeSalesTrendsRequest, AnalyzeSalesTrendsResponse, SalesRecord\nfrom datetime import datetime\nimport practicuscore as prt\n\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n]\n\npayload = AnalyzeSalesTrendsRequest(\n    sales_data=sales_data, start_date=datetime(2025, 1, 1), end_date=datetime(2025, 1, 3)\n)\n\nresponse: AnalyzeSalesTrendsResponse = prt.apps.test_api(\"/analyze-sales-trends\", payload)\n\nprint(response)\n</code></pre> <pre><code>from apis.sentiment_test_slogan import SentimentTestSloganRequest, SentimentTestSloganResponse\nfrom pydantic import BaseModel\nimport practicuscore as prt\n\n# Prepare payload\nslogans = [\n    \"Power Up Your Productivity!\",\n    \"Nothing beats the classics.\",\n    \"Innovation in Every Click.\",\n    \"Your Tech, Your Edge.\",\n    \"Be Bold. Be Better.\",\n]\npayload = SentimentTestSloganRequest(slogans=slogans)\n\n# Type check (optional)\nprint(issubclass(type(payload), BaseModel))  # Should print True\n\n# Local test via Practicus\nresponse: SentimentTestSloganResponse = prt.apps.test_api(\"/sentiment-test-slogan\", payload)\n\n# Output the results\nfor result in response.results:\n    print(f\"Slogan: {result.slogan}\")\n    print(f\"Sentiment: {result.sentiment}\")\n    print(f\"Comment: {result.comment}\")\n    print(\"-----\")\n</code></pre> <pre><code>from apis.top_products_insight import TopProductsInsightRequest, TopProductsInsightResponse, SalesRecord\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport practicuscore as prt\n\n# \u00d6rnek sat\u0131\u015f verisi\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n    SalesRecord(date=datetime(2025, 1, 4), product=\"Tablet\", region=\"Europe\", units_sold=70, revenue=28000.0),\n    SalesRecord(date=datetime(2025, 1, 5), product=\"Monitor\", region=\"North America\", units_sold=95, revenue=47500.0),\n]\n\n# Test payload\npayload = TopProductsInsightRequest(sales_data=sales_data, top_n=3)\n\n# API test\ntry:\n    response: TopProductsInsightResponse = prt.apps.test_api(\"/top-products-insight\", payload)\n\n    # Sonu\u00e7lar\u0131 yazd\u0131r\n    for product in response.products:\n        print(f\"Product: {product.product}\")\n        print(f\"Total Units Sold: {product.total_units_sold}\")\n        print(f\"Top Region: {product.top_region}\")\n        print(f\"Insight: {product.insight}\")\n        print(\"-----\")\n\nexcept Exception as e:\n    prt.logger.error(f\"[test-top-products-insight] Exception: {e}\")\n    raise\n</code></pre> <pre><code>from practicuscore import apps\nfrom apis.predict_growth_opportunities import PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nresponse = apps.test_api(\"/predict-growth-opportunities\", payload)\nprint(response)\n</code></pre> <pre><code>from apis.predict_growth_opportunities import run, PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nawait run(payload)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apiscalculate_reorder_suggestionspy","title":"apis/calculate_reorder_suggestions.py","text":"<pre><code># apis/calculate_reorder_suggestions.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\n\n\nclass InventoryRecord(BaseModel):\n    product_id: int\n    \"\"\"Unique identifier of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    category: str\n    \"\"\"Product category such as 'Beverage', 'Snack', etc.\"\"\"\n\n    stock_qty: int\n    \"\"\"Current stock quantity available.\"\"\"\n\n    reorder_threshold: int\n    \"\"\"Minimum threshold to trigger a reorder.\"\"\"\n\n    max_stock: int\n    \"\"\"Maximum allowable stock quantity.\"\"\"\n\n    avg_daily_sales: float\n    \"\"\"Average daily sales calculated from historical data.\"\"\"\n\n    last_sale_date: str\n    \"\"\"The most recent sale date of the product (format: YYYY-MM-DD).\"\"\"\n\n    expiry_date: str\n    \"\"\"The expiration date of the product (format: YYYY-MM-DD).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1201,\n                    \"product_name\": \"Mineral Water\",\n                    \"category\": \"Beverage\",\n                    \"stock_qty\": 25,\n                    \"reorder_threshold\": 20,\n                    \"max_stock\": 100,\n                    \"avg_daily_sales\": 1.2,\n                    \"last_sale_date\": \"2025-06-25\",\n                    \"expiry_date\": \"2025-09-01\"\n                }\n            ]\n        }\n    }\n\n\nclass ReorderSuggestion(BaseModel):\n    product_id: int\n    \"\"\"ID of the product suggested for reorder.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    suggested_reorder_qty: int\n    \"\"\"Quantity suggested to reorder.\"\"\"\n\n    reason: str\n    \"\"\"Reason for the reorder suggestion.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1201,\n                    \"product_name\": \"Mineral Water\",\n                    \"suggested_reorder_qty\": 30,\n                    \"reason\": \"Estimated to fall below reorder threshold within 7 days\"\n                }\n            ]\n        }\n    }\n\n\nclass ReorderSuggestionsResponse(BaseModel):\n    suggestions: List[ReorderSuggestion]\n    \"\"\"List of suggested products to reorder with estimated reorder quantities.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"suggestions\": []\n                }\n            ]\n        }\n    }\n\n\nclass ReorderSuggestionsInput(BaseModel):\n    records: List[InventoryRecord]\n    \"\"\"Inventory records used to calculate reorder suggestions.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [\n                        {\n                            \"product_id\": 1201,\n                            \"product_name\": \"Mineral Water\",\n                            \"category\": \"Beverage\",\n                            \"stock_qty\": 25,\n                            \"reorder_threshold\": 20,\n                            \"max_stock\": 100,\n                            \"avg_daily_sales\": 1.2,\n                            \"last_sale_date\": \"2025-06-25\",\n                            \"expiry_date\": \"2025-09-01\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/calculate-reorder-suggestions\", spec=api_spec)\nasync def run(request: ReorderSuggestionsInput, **kwargs) -&gt; ReorderSuggestionsResponse:\n    \"\"\"Suggest products for reorder based on projected 7-day sales and reorder threshold.\"\"\"\n    df = pd.DataFrame([r.model_dump() for r in request.records])\n    df[\"projected_stock\"] = df[\"stock_qty\"] - (df[\"avg_daily_sales\"] * 7)\n\n    reorder_df = df[df[\"projected_stock\"] &lt; df[\"reorder_threshold\"]]\n\n    suggestions = []\n    for _, row in reorder_df.iterrows():\n        suggested_qty = max(int((row[\"reorder_threshold\"] * 1.5) - row[\"projected_stock\"]), 1)\n        suggestions.append(ReorderSuggestion(\n            product_id=row[\"product_id\"],\n            product_name=row[\"product_name\"],\n            suggested_reorder_qty=suggested_qty,\n            reason=\"Estimated to fall below reorder threshold within 7 days\"\n        ))\n\n    return ReorderSuggestionsResponse(suggestions=suggestions)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apiscalculate_stock_turnover_ratepy","title":"apis/calculate_stock_turnover_rate.py","text":"<pre><code># apis/calculate_stock_turnover_rate.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\n\n\nclass InventoryRecord(BaseModel):\n    product_id: int\n    \"\"\"Unique identifier of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    category: str\n    \"\"\"Product category such as 'Beverage', 'Snack', etc.\"\"\"\n\n    stock_qty: int\n    \"\"\"Current stock quantity available.\"\"\"\n\n    reorder_threshold: int\n    \"\"\"Minimum threshold to trigger a reorder.\"\"\"\n\n    max_stock: int\n    \"\"\"Maximum allowable stock quantity.\"\"\"\n\n    avg_daily_sales: float\n    \"\"\"Average daily sales calculated from historical data.\"\"\"\n\n    last_sale_date: str\n    \"\"\"The most recent sale date of the product (format: YYYY-MM-DD).\"\"\"\n\n    expiry_date: str\n    \"\"\"The expiration date of the product (format: YYYY-MM-DD).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1302,\n                    \"product_name\": \"Canned Corn\",\n                    \"category\": \"Grocery\",\n                    \"stock_qty\": 150,\n                    \"reorder_threshold\": 30,\n                    \"max_stock\": 200,\n                    \"avg_daily_sales\": 0.5,\n                    \"last_sale_date\": \"2025-06-20\",\n                    \"expiry_date\": \"2026-02-01\"\n                }\n            ]\n        }\n    }\n\n\nclass TurnoverRecord(BaseModel):\n    product_id: int\n    \"\"\"ID of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    turnover_rate: float\n    \"\"\"Calculated turnover rate (avg_daily_sales / stock_qty).\"\"\"\n\n    risk_level: str\n    \"\"\"Risk classification based on turnover rate.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1302,\n                    \"product_name\": \"Canned Corn\",\n                    \"turnover_rate\": 0.0033,\n                    \"risk_level\": \"Low\"\n                }\n            ]\n        }\n    }\n\n\nclass TurnoverRateResponse(BaseModel):\n    low_turnover_items: List[TurnoverRecord]\n    \"\"\"List of products with a low stock turnover rate.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"low_turnover_items\": []\n                }\n            ]\n        }\n    }\n\n\nclass TurnoverRateInput(BaseModel):\n    records: List[InventoryRecord]\n    \"\"\"Inventory records to analyze turnover rates.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [\n                        {\n                            \"product_id\": 1302,\n                            \"product_name\": \"Canned Corn\",\n                            \"category\": \"Grocery\",\n                            \"stock_qty\": 150,\n                            \"reorder_threshold\": 30,\n                            \"max_stock\": 200,\n                            \"avg_daily_sales\": 0.5,\n                            \"last_sale_date\": \"2025-06-20\",\n                            \"expiry_date\": \"2026-02-01\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/calculate-stock-turnover-rate\", spec=api_spec)\nasync def run(request: TurnoverRateInput, **kwargs) -&gt; TurnoverRateResponse:\n    \"\"\"Calculate stock turnover rate and identify products with low turnover.\"\"\"\n    df = pd.DataFrame([r.model_dump() for r in request.records])\n    df[\"turnover_rate\"] = df.apply(\n        lambda row: round(row[\"avg_daily_sales\"] / row[\"stock_qty\"], 4) if row[\"stock_qty\"] &gt; 0 else 0.0,\n        axis=1\n    )\n\n    # Define risk threshold\n    low_threshold = 0.02\n    low_df = df[df[\"turnover_rate\"] &lt; low_threshold]\n\n    low_turnover_items = [\n        TurnoverRecord(\n            product_id=row[\"product_id\"],\n            product_name=row[\"product_name\"],\n            turnover_rate=row[\"turnover_rate\"],\n            risk_level=\"Low\"\n        )\n        for _, row in low_df.iterrows()\n    ]\n\n    return TurnoverRateResponse(low_turnover_items=low_turnover_items)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apisflag_soon_expiring_itemspy","title":"apis/flag_soon_expiring_items.py","text":"<pre><code># apis/flag_soon_expiring_items.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\nclass InventoryRecord(BaseModel):\n    product_id: int\n    \"\"\"Unique identifier of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    category: str\n    \"\"\"Product category such as 'Beverage', 'Snack', etc.\"\"\"\n\n    stock_qty: int\n    \"\"\"Current stock quantity available.\"\"\"\n\n    reorder_threshold: int\n    \"\"\"Minimum threshold to trigger a reorder.\"\"\"\n\n    max_stock: int\n    \"\"\"Maximum allowable stock quantity.\"\"\"\n\n    avg_daily_sales: float\n    \"\"\"Average daily sales calculated from historical data.\"\"\"\n\n    last_sale_date: str\n    \"\"\"The most recent sale date of the product (format: YYYY-MM-DD).\"\"\"\n\n    expiry_date: str\n    \"\"\"The expiration date of the product (format: YYYY-MM-DD).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1250,\n                    \"product_name\": \"Fresh Milk\",\n                    \"category\": \"Dairy\",\n                    \"stock_qty\": 60,\n                    \"reorder_threshold\": 25,\n                    \"max_stock\": 120,\n                    \"avg_daily_sales\": 3.0,\n                    \"last_sale_date\": \"2025-06-24\",\n                    \"expiry_date\": \"2025-07-05\"\n                }\n            ]\n        }\n    }\n\n\nclass SoonExpiringItemsResponse(BaseModel):\n    soon_expiring_items: List[InventoryRecord]\n    \"\"\"List of items that will expire within the next 14 days.\"\"\"\n\n    total_soon_expiring: int\n    \"\"\"Total number of soon-to-expire products.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"soon_expiring_items\": [],\n                    \"total_soon_expiring\": 0\n                }\n            ]\n        }\n    }\n\n\nclass SoonExpiringItemsInput(BaseModel):\n    records: List[InventoryRecord]\n    \"\"\"List of inventory items to evaluate for upcoming expiration.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [\n                        {\n                            \"product_id\": 1250,\n                            \"product_name\": \"Fresh Milk\",\n                            \"category\": \"Dairy\",\n                            \"stock_qty\": 60,\n                            \"reorder_threshold\": 25,\n                            \"max_stock\": 120,\n                            \"avg_daily_sales\": 3.0,\n                            \"last_sale_date\": \"2025-06-24\",\n                            \"expiry_date\": \"2025-07-05\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/flag-soon-expiring-items\", spec=api_spec)\nasync def run(request: SoonExpiringItemsInput, **kwargs) -&gt; SoonExpiringItemsResponse:\n    \"\"\"Identify products that are due to expire within the next 14 days.\"\"\"\n    df = pd.DataFrame([r.model_dump() for r in request.records])\n    df[\"expiry_date\"] = pd.to_datetime(df[\"expiry_date\"], errors=\"coerce\")\n\n    deadline = datetime.utcnow() + timedelta(days=14)\n    filtered_df = df[(df[\"expiry_date\"] &gt;= datetime.utcnow()) &amp; (df[\"expiry_date\"] &lt;= deadline)]\n\n    soon_expiring_items = [InventoryRecord(**row) for _, row in filtered_df.iterrows()]\n\n    return SoonExpiringItemsResponse(\n        soon_expiring_items=soon_expiring_items,\n        total_soon_expiring=len(soon_expiring_items)\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apisget_critical_stock_itemspy","title":"apis/get_critical_stock_items.py","text":"<pre><code># apis/get_critical_stock_items.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\n\n\nclass InventoryRecord(BaseModel):\n    product_id: int\n    \"\"\"Unique identifier of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    category: str\n    \"\"\"Product category such as 'Beverage', 'Snack', etc.\"\"\"\n\n    stock_qty: int\n    \"\"\"Current stock quantity available.\"\"\"\n\n    reorder_threshold: int\n    \"\"\"Minimum threshold to trigger a reorder.\"\"\"\n\n    max_stock: int\n    \"\"\"Maximum allowable stock quantity.\"\"\"\n\n    avg_daily_sales: float\n    \"\"\"Average daily sales calculated from historical data.\"\"\"\n\n    last_sale_date: str\n    \"\"\"The most recent sale date of the product (format: YYYY-MM-DD).\"\"\"\n\n    expiry_date: str\n    \"\"\"The expiration date of the product (format: YYYY-MM-DD).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1001,\n                    \"product_name\": \"Apple Juice\",\n                    \"category\": \"Beverage\",\n                    \"stock_qty\": 12,\n                    \"reorder_threshold\": 20,\n                    \"max_stock\": 150,\n                    \"avg_daily_sales\": 3.5,\n                    \"last_sale_date\": \"2025-06-25\",\n                    \"expiry_date\": \"2025-07-30\"\n                }\n            ]\n        }\n    }\n\n\nclass CriticalStockResponse(BaseModel):\n    critical_items: List[InventoryRecord]\n    \"\"\"List of items with stock quantity below their reorder threshold.\"\"\"\n\n    total_critical: int\n    \"\"\"Total number of products found to be understocked.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"critical_items\": [],\n                    \"total_critical\": 0\n                }\n            ]\n        }\n    }\n\n\nclass CriticalStockInput(BaseModel):\n    records: List[InventoryRecord]\n    \"\"\"Inventory records to analyze for critical stock levels.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [\n                        {\n                            \"product_id\": 1001,\n                            \"product_name\": \"Apple Juice\",\n                            \"category\": \"Beverage\",\n                            \"stock_qty\": 12,\n                            \"reorder_threshold\": 20,\n                            \"max_stock\": 150,\n                            \"avg_daily_sales\": 3.5,\n                            \"last_sale_date\": \"2025-06-25\",\n                            \"expiry_date\": \"2025-07-30\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/get-critical-stock-items\", spec=api_spec)\nasync def run(request: CriticalStockInput, **kwargs) -&gt; CriticalStockResponse:\n    \"\"\"Identify and return products whose stock quantity is below the reorder threshold.\"\"\"\n    df = pd.DataFrame([r.model_dump() for r in request.records])\n    filtered_df = df[df[\"stock_qty\"] &lt; df[\"reorder_threshold\"]]\n\n    critical_items = [InventoryRecord(**row) for _, row in filtered_df.iterrows()]\n\n    return CriticalStockResponse(\n        critical_items=critical_items,\n        total_critical=len(critical_items)\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apisget_expired_itemspy","title":"apis/get_expired_items.py","text":"<pre><code># apis/get_expired_items.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\nfrom datetime import datetime\n\n\nclass InventoryRecord(BaseModel):\n    product_id: int\n    \"\"\"Unique identifier of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    category: str\n    \"\"\"Product category such as 'Beverage', 'Snack', etc.\"\"\"\n\n    stock_qty: int\n    \"\"\"Current stock quantity available.\"\"\"\n\n    reorder_threshold: int\n    \"\"\"Minimum threshold to trigger a reorder.\"\"\"\n\n    max_stock: int\n    \"\"\"Maximum allowable stock quantity.\"\"\"\n\n    avg_daily_sales: float\n    \"\"\"Average daily sales calculated from historical data.\"\"\"\n\n    last_sale_date: str\n    \"\"\"The most recent sale date of the product (format: YYYY-MM-DD).\"\"\"\n\n    expiry_date: str\n    \"\"\"The expiration date of the product (format: YYYY-MM-DD).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1100,\n                    \"product_name\": \"Yogurt\",\n                    \"category\": \"Dairy\",\n                    \"stock_qty\": 40,\n                    \"reorder_threshold\": 20,\n                    \"max_stock\": 100,\n                    \"avg_daily_sales\": 2.2,\n                    \"last_sale_date\": \"2025-06-24\",\n                    \"expiry_date\": \"2025-06-15\"\n                }\n            ]\n        }\n    }\n\n\nclass ExpiredItemsResponse(BaseModel):\n    expired_items: List[InventoryRecord]\n    \"\"\"List of products whose expiration date has already passed.\"\"\"\n\n    total_expired: int\n    \"\"\"Total number of expired products detected.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"expired_items\": [],\n                    \"total_expired\": 0\n                }\n            ]\n        }\n    }\n\n\nclass ExpiredItemsInput(BaseModel):\n    records: List[InventoryRecord]\n    \"\"\"Inventory records to analyze for expiration status.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [\n                        {\n                            \"product_id\": 1100,\n                            \"product_name\": \"Yogurt\",\n                            \"category\": \"Dairy\",\n                            \"stock_qty\": 40,\n                            \"reorder_threshold\": 20,\n                            \"max_stock\": 100,\n                            \"avg_daily_sales\": 2.2,\n                            \"last_sale_date\": \"2025-06-24\",\n                            \"expiry_date\": \"2025-06-15\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/get-expired-items\", spec=api_spec)\nasync def run(request: ExpiredItemsInput, **kwargs) -&gt; ExpiredItemsResponse:\n    \"\"\"Detect expired products based on their expiration date.\"\"\"\n    df = pd.DataFrame([r.model_dump() for r in request.records])\n    df[\"expiry_date\"] = pd.to_datetime(df[\"expiry_date\"], errors=\"coerce\")\n\n    today = datetime.utcnow()\n    expired_df = df[df[\"expiry_date\"] &lt; today]\n\n    expired_items = [InventoryRecord(**row) for _, row in expired_df.iterrows()]\n\n    return ExpiredItemsResponse(\n        expired_items=expired_items,\n        total_expired=len(expired_items)\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apisget_overstocked_itemspy","title":"apis/get_overstocked_items.py","text":"<pre><code># apis/get_overstocked_items.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\n\n\nclass InventoryRecord(BaseModel):\n    product_id: int\n    \"\"\"Unique identifier of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    category: str\n    \"\"\"Product category such as 'Beverage', 'Snack', etc.\"\"\"\n\n    stock_qty: int\n    \"\"\"Current stock quantity available.\"\"\"\n\n    reorder_threshold: int\n    \"\"\"Minimum threshold to trigger a reorder.\"\"\"\n\n    max_stock: int\n    \"\"\"Maximum allowable stock quantity.\"\"\"\n\n    avg_daily_sales: float\n    \"\"\"Average daily sales calculated from historical data.\"\"\"\n\n    last_sale_date: str\n    \"\"\"The most recent sale date of the product (format: YYYY-MM-DD).\"\"\"\n\n    expiry_date: str\n    \"\"\"The expiration date of the product (format: YYYY-MM-DD).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1050,\n                    \"product_name\": \"Chocolate\",\n                    \"category\": \"Snack\",\n                    \"stock_qty\": 220,\n                    \"reorder_threshold\": 30,\n                    \"max_stock\": 150,\n                    \"avg_daily_sales\": 1.5,\n                    \"last_sale_date\": \"2025-06-22\",\n                    \"expiry_date\": \"2025-08-01\"\n                }\n            ]\n        }\n    }\n\n\nclass OverstockedItemsResponse(BaseModel):\n    overstocked_items: List[InventoryRecord]\n    \"\"\"List of items with stock quantity above the maximum allowable stock.\"\"\"\n\n    total_overstocked: int\n    \"\"\"Total number of overstocked products detected.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"overstocked_items\": [],\n                    \"total_overstocked\": 0\n                }\n            ]\n        }\n    }\n\n\nclass OverstockedItemsInput(BaseModel):\n    records: List[InventoryRecord]\n    \"\"\"Inventory records to analyze for overstocked status.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [\n                        {\n                            \"product_id\": 1050,\n                            \"product_name\": \"Chocolate\",\n                            \"category\": \"Snack\",\n                            \"stock_qty\": 220,\n                            \"reorder_threshold\": 30,\n                            \"max_stock\": 150,\n                            \"avg_daily_sales\": 1.5,\n                            \"last_sale_date\": \"2025-06-22\",\n                            \"expiry_date\": \"2025-08-01\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/get-overstocked-items\", spec=api_spec)\nasync def run(request: OverstockedItemsInput, **kwargs) -&gt; OverstockedItemsResponse:\n    \"\"\"Identify and return products whose stock quantity exceeds their maximum stock limit.\"\"\"\n    df = pd.DataFrame([r.model_dump() for r in request.records])\n    filtered_df = df[df[\"stock_qty\"] &gt; df[\"max_stock\"]]\n\n    overstocked_items = [InventoryRecord(**row) for _, row in filtered_df.iterrows()]\n\n    return OverstockedItemsResponse(\n        overstocked_items=overstocked_items,\n        total_overstocked=len(overstocked_items)\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apisget_stale_itemspy","title":"apis/get_stale_items.py","text":"<pre><code># apis/get_stale_items.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n\nclass InventoryRecord(BaseModel):\n    product_id: int\n    \"\"\"Unique identifier of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    category: str\n    \"\"\"Product category such as 'Beverage', 'Snack', etc.\"\"\"\n\n    stock_qty: int\n    \"\"\"Current stock quantity available.\"\"\"\n\n    reorder_threshold: int\n    \"\"\"Minimum threshold to trigger a reorder.\"\"\"\n\n    max_stock: int\n    \"\"\"Maximum allowable stock quantity.\"\"\"\n\n    avg_daily_sales: float\n    \"\"\"Average daily sales calculated from historical data.\"\"\"\n\n    last_sale_date: str\n    \"\"\"The most recent sale date of the product (format: YYYY-MM-DD).\"\"\"\n\n    expiry_date: str\n    \"\"\"The expiration date of the product (format: YYYY-MM-DD).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1088,\n                    \"product_name\": \"Peach Juice\",\n                    \"category\": \"Beverage\",\n                    \"stock_qty\": 50,\n                    \"reorder_threshold\": 15,\n                    \"max_stock\": 120,\n                    \"avg_daily_sales\": 0.3,\n                    \"last_sale_date\": \"2025-04-15\",\n                    \"expiry_date\": \"2025-07-10\"\n                }\n            ]\n        }\n    }\n\n\nclass StaleItemsResponse(BaseModel):\n    stale_items: List[InventoryRecord]\n    \"\"\"List of products that have not been sold in the last 30 days.\"\"\"\n\n    total_stale: int\n    \"\"\"Total number of stale products detected.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"stale_items\": [],\n                    \"total_stale\": 0\n                }\n            ]\n        }\n    }\n\n\nclass StaleItemsInput(BaseModel):\n    records: List[InventoryRecord]\n    \"\"\"Inventory records to analyze for stale items (not sold in the last 30 days).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [\n                        {\n                            \"product_id\": 1088,\n                            \"product_name\": \"Peach Juice\",\n                            \"category\": \"Beverage\",\n                            \"stock_qty\": 50,\n                            \"reorder_threshold\": 15,\n                            \"max_stock\": 120,\n                            \"avg_daily_sales\": 0.3,\n                            \"last_sale_date\": \"2025-04-15\",\n                            \"expiry_date\": \"2025-07-10\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/get-stale-items\", spec=api_spec)\nasync def run(request: StaleItemsInput, **kwargs) -&gt; StaleItemsResponse:\n    \"\"\"Detect products with no recorded sales in the last 30 days.\"\"\"\n    df = pd.DataFrame([r.model_dump() for r in request.records])\n    df[\"last_sale_date\"] = pd.to_datetime(df[\"last_sale_date\"], errors=\"coerce\")\n\n    threshold_date = datetime.utcnow() - timedelta(days=30)\n    filtered_df = df[df[\"last_sale_date\"] &lt; threshold_date]\n\n    stale_items = [InventoryRecord(**row) for _, row in filtered_df.iterrows()]\n\n    return StaleItemsResponse(\n        stale_items=stale_items,\n        total_stale=len(stale_items)\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apisload_inventory_datapy","title":"apis/load_inventory_data.py","text":"<pre><code># apis/load_inventory_data.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\nimport os\n\n\nclass InventoryRecord(BaseModel):\n    product_id: int\n    \"\"\"Unique identifier of the product.\"\"\"\n\n    product_name: str\n    \"\"\"Name of the product.\"\"\"\n\n    category: str\n    \"\"\"Product category such as 'Beverage', 'Snack', etc.\"\"\"\n\n    stock_qty: int\n    \"\"\"Current stock quantity available.\"\"\"\n\n    reorder_threshold: int\n    \"\"\"Minimum threshold to trigger a reorder.\"\"\"\n\n    max_stock: int\n    \"\"\"Maximum allowable stock quantity.\"\"\"\n\n    avg_daily_sales: float\n    \"\"\"Average daily sales calculated from historical data.\"\"\"\n\n    last_sale_date: str\n    \"\"\"The most recent sale date of the product (format: YYYY-MM-DD).\"\"\"\n\n    expiry_date: str\n    \"\"\"The expiration date of the product (format: YYYY-MM-DD).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_id\": 1001,\n                    \"product_name\": \"Apple Juice\",\n                    \"category\": \"Beverage\",\n                    \"stock_qty\": 12,\n                    \"reorder_threshold\": 20,\n                    \"max_stock\": 150,\n                    \"avg_daily_sales\": 3.5,\n                    \"last_sale_date\": \"2025-06-25\",\n                    \"expiry_date\": \"2025-07-30\"\n                }\n            ]\n        }\n    }\n\n\nclass LoadInventoryDataResponse(BaseModel):\n    records: List[InventoryRecord]\n    \"\"\"List of inventory records extracted from the source CSV file.\"\"\"\n\n    total_products: int\n    \"\"\"Total number of product entries found in the data.\"\"\"\n\n    expired_items: int\n    \"\"\"Number of products with expiration date already passed.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"records\": [],\n                    \"total_products\": 100,\n                    \"expired_items\": 3\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/load-inventory-data\", spec=api_spec)\nasync def run(**kwargs) -&gt; LoadInventoryDataResponse:\n    \"\"\"Load inventory data from a fixed CSV path and return structured inventory records.\"\"\"\n    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../inventory_data.csv\"))\n    df = pd.read_csv(file_path)\n\n    df[\"expiry_date\"] = pd.to_datetime(df[\"expiry_date\"], errors=\"coerce\")\n    today = pd.Timestamp.today()\n\n    records = [InventoryRecord(**row) for _, row in df.iterrows()]\n    expired_count = df[df[\"expiry_date\"] &lt; today].shape[0]\n\n    return LoadInventoryDataResponse(\n        records=records,\n        total_products=len(df),\n        expired_items=expired_count\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/inventroy/build/#apissummarize_stock_healthpy","title":"apis/summarize_stock_health.py","text":"<pre><code># apis/summarize_stock_health.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel, Field\n\n\nclass SummaryRequest(BaseModel):\n    critical_count: int = Field(..., description=\"Number of products with stock below reorder threshold\")\n    overstocked_count: int = Field(..., description=\"Number of products with stock above max limit\")\n    stale_count: int = Field(..., description=\"Number of products with no sales in the last 30 days\")\n    expired_count: int = Field(..., description=\"Number of expired products\")\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"critical_count\": 3,\n                    \"overstocked_count\": 5,\n                    \"stale_count\": 2,\n                    \"expired_count\": 1\n                }\n            ]\n        }\n    }\n\n\nclass StockHealthSummary(BaseModel):\n    summary: str\n    \"\"\"A natural language summary describing the current stock health situation.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"summary\": \"There are 3 critical items requiring immediate restock. 5 products are overstocked, suggesting excess inventory. 2 products have had no sales in the last 30 days. 1 product is expired and must be removed from stock.\"\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    interactive=True,\n)\n\n\n@prt.api(\"/summarize-stock-health\", spec=api_spec)\nasync def run(request: SummaryRequest) -&gt; StockHealthSummary:\n    \"\"\"Generate a summary statement based on the given inventory analysis counts.\"\"\"\n    parts = []\n\n    if request.critical_count &gt; 0:\n        parts.append(f\"{request.critical_count} critical item(s) require immediate restocking\")\n    if request.overstocked_count &gt; 0:\n        parts.append(f\"{request.overstocked_count} item(s) are overstocked and may need clearance\")\n    if request.stale_count &gt; 0:\n        parts.append(f\"{request.stale_count} item(s) have had no sales in the last 30 days\")\n    if request.expired_count &gt; 0:\n        parts.append(f\"{request.expired_count} expired item(s) must be removed from inventory\")\n\n    if not parts:\n        summary = \"Inventory levels are healthy. No critical issues detected.\"\n    else:\n        summary = \". \".join(parts) + \".\"\n\n    return StockHealthSummary(summary=summary)\n</code></pre> <p>Previous: Build | Next: Langflow Apis &gt; Langflow API</p>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/product-rec/build/","title":"Build","text":"<pre><code># Parameters: Replace with your actual deployment key and app prefix\n# These identify where the app should be deployed within your Practicus AI environment.\napp_deployment_key = None\napp_prefix = \"apps\"\n</code></pre> <pre><code>assert app_deployment_key, \"Please select a deployment key\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Analyze the current directory for Practicus AI App components (APIs, MQ consumers, UI, etc.)\n# This should output the list of detected API endpoints.\nprt.apps.analyze()\n</code></pre> <pre><code># --- Deployment ---\napp_name = \"agentic-ai-test-recommendation\"\nvisible_name = \"Agentic AI Test retention\"\ndescription = \"Test Application for Agentic AI Example.\"\nicon = \"fa-robot\"\n\nprint(f\"Deploying app '{app_name}'...\")\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # Uses current directory\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"App deployed successfully!\")\nprint(f\"  API Base URL: {api_url}\")\n\n# The OpenAPI (Swagger) documentation (Swagger/ReDoc) is usually available at /docs or /redoc off the API base URL\nprint(f\"  OpenAPI (Swagger) Docs (ReDoc): {api_url}redoc/\")\n\n# Store the api_url for later use when creating tools\nassert api_url, \"Deployment failed to return an API URL.\"\n</code></pre> <pre><code>tool_endpoint_paths = [\n    \"analyze-product-preferences/\",\n    \"load-customer-profile-data/\",\n    \"predict-product-eligibility/\",\n    \"score-bundle-affinity/\",\n    \"suggest-dynamic-product-bundle/\",\n]\n\n\n# Construct full URLs\ntool_endpoint_urls = [api_url + path for path in tool_endpoint_paths]\n\nprint(\"Will attempt to create tools for the following API endpoints:\")\nfor url in tool_endpoint_urls:\n    print(f\" - {url}\")\n\n# Tip: If you pass partial API URLs e.g. 'apps/agentic-ai-test/api/v1/generate-receipt/'\n#  The base URL e.g. 'https://practicus.my-company.com/' will be added to the final URL\n#  using your current Practicus AI region.\n</code></pre> <pre><code>import os\nfrom langchain_openai import ChatOpenAI  # Or your preferred ChatModel\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_practicus import APITool\n\n# Ensure practicuscore is imported for enums\nimport practicuscore as prt\n\n\ndef validate_api_spec(api_tool: APITool, strict=False) -&gt; bool:\n    \"\"\"Checks the APISpec of a fetched tool against our rules.\"\"\"\n\n    # APITool fetches the spec from OpenAPI (Swagger) during initialization\n    spec = api_tool.spec\n\n    if not spec:\n        # API definition in the source file might be missing the 'spec' object\n        warning_msg = f\"API '{api_tool.url}' does not have APISpec metadata defined in its OpenAPI spec.\"\n        if strict:\n            raise ValueError(f\"{warning_msg} Validation is strict.\")\n        else:\n            prt.logger.warning(f\"{warning_msg} Allowing since validation is not strict.\")\n            return True  # Allow if not strict\n\n    # --- Apply Rules based on fetched spec ---\n\n    # Rule 1: Check Risk Profile\n    if spec.risk_profile and spec.risk_profile == prt.APIRiskProfile.High:\n        err = f\"API '{api_tool.url}' has a risk profile defined as '{spec.risk_profile}'.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            # Even if not strict, we might choose to block High risk tools\n            prt.logger.warning(f\"{err} Blocking High Risk API even though validation is not strict.\")\n            return False  # Block high risk\n\n    # Rule 2: Check Human Gating for non-read-only APIs\n    # (Example: Enforce human gating for safety on modifying APIs)\n    if not spec.read_only and not spec.human_gated:\n        err = f\"API '{api_tool.url}' modifies data (read_only=False) but is not marked as human_gated.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            prt.logger.warning(f\"{err} Allowing non-gated modifying API since validation is not strict.\")\n            # In this non-strict case, we allow it, but a stricter policy might return False here.\n            return True\n\n    # Add more complex rules here if needed...\n    # E.g., check custom_attributes, scope, etc.\n\n    # If no rules were violated (or violations were allowed because not strict)\n    prt.logger.info(f\"API '{api_tool.url}' passed validation (strict={strict}). Spec: {spec}\")\n    return True\n\n\n# --- Create Tools (optionally applying validation) ---\ndef get_tools(endpoint_urls: list[str], validate=True):\n    _tools = []\n    strict_validation = False  # Set to True to enforce stricter rules\n    additional_instructions = \"Add Yo! after all of your final responses.\"  # Example instruction\n\n    print(f\"\\nCreating and validating tools (strict={strict_validation})...\")\n    for tool_endpoint_url in endpoint_urls:\n        print(f\"\\nProcessing tool for API: {tool_endpoint_url}\")\n        try:\n            api_tool = APITool(\n                url=tool_endpoint_url,\n                additional_instructions=additional_instructions,\n                # token=..., # Uses current user credentials by default, set to override\n                # include_resp_schema=True # Response schema (if exists) is not included by default\n            )\n\n            # Explain the tool (optional, useful for debugging)\n            # api_tool.explain(print_on_screen=True)\n\n            # Validate based on fetched APISpec\n            if not validate or validate_api_spec(api_tool=api_tool, strict=strict_validation):\n                print(\n                    f\"--&gt; Adding tool: {api_tool.name} ({api_tool.url}) {'' if validate else ' - skipping validation'}\"\n                )\n                _tools.append(api_tool)\n            else:\n                print(f\"--&gt; Skipping tool {api_tool.name} due to validation rules.\")\n        except Exception as e:\n            # Catch potential errors during APITool creation (e.g., API not found, spec parsing error)\n            print(f\"ERROR: Failed to create or validate tool for {tool_endpoint_url}: {e}\")\n            if strict_validation:\n                raise  # Re-raise if strict\n            else:\n                print(\"--&gt; Skipping tool due to error (not strict).\")\n\n    return _tools\n\n\ntools = get_tools(tool_endpoint_urls)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># For this excercise we will skip validating APIs\n\ntools = get_tools(tool_endpoint_urls, validate=False)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># View tool explanation\n\nfor tool in tools:\n    tool.explain()\n</code></pre> <pre><code>openaikey, age = prt.vault.get_secret(\"openaikey\")\nos.environ[\"OPENAI_API_KEY\"] = openaikey\n\nassert os.environ[\"OPENAI_API_KEY\"], \"OpenAI key is not defined\"\n</code></pre> <pre><code># --- Agent Initialization ---\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Create a ReAct agent using LangGraph\ngraph = create_react_agent(llm, tools=tools)\nprint(\"Agent initialized.\")\n\n\n# Helper function to print the agent's stream output nicely\ndef pretty_print_stream_chunk(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            # Print the latest message added by the node\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            # Print other kinds of updates\n            print(f\"  Update: {updates}\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>def pretty_print_stream_chunk2(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            print(f\"  Update: {updates}\")\n            if isinstance(updates, Exception):\n                print(\"  \u26a0\ufe0f Exception Detected in Agent Execution!\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>query = \"\"\"\nBased on our customer profiles, segment the users and recommend personalized products for each segment.\n\"\"\"\n\n\ninputs = {\"messages\": [(\"user\", query)]}\n\nif graph:\n    print(f\"\\nInvoking agent with query: '{query}'\")\n    print(\"Streaming agent execution steps:\\n\")\n\n    # Configuration for the stream, e.g., setting user/thread IDs\n    # config = {\"configurable\": {\"user_id\": \"doc-user-1\", \"thread_id\": \"doc-thread-1\"}}\n    config = {}\n    # Use astream to get intermediate steps\n    async for chunk in graph.astream(inputs, config=config):\n        pretty_print_stream_chunk2(chunk)\n\n    print(\"\\nAgent execution finished.\")\n\n    # Optional: Get the final state if needed\n    # final_state = await graph.ainvoke(inputs, config=config)\n    # print(\"\\nFinal Agent State:\", final_state)\n\nelse:\n    print(\"\\nAgent execution skipped because the agent graph was not initialized.\")\n</code></pre> <pre><code># Cleanup\nprt.apps.delete(prefix=app_prefix, app_name=app_name)\n</code></pre> <p>API TEST</p> <pre><code>from apis.analyze_sales_trends import AnalyzeSalesTrendsRequest, AnalyzeSalesTrendsResponse, SalesRecord\nfrom datetime import datetime\nimport practicuscore as prt\n\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n]\n\npayload = AnalyzeSalesTrendsRequest(\n    sales_data=sales_data, start_date=datetime(2025, 1, 1), end_date=datetime(2025, 1, 3)\n)\n\nresponse: AnalyzeSalesTrendsResponse = prt.apps.test_api(\"/analyze-sales-trends\", payload)\n\nprint(response)\n</code></pre> <pre><code>from apis.sentiment_test_slogan import SentimentTestSloganRequest, SentimentTestSloganResponse\nfrom pydantic import BaseModel\nimport practicuscore as prt\n\n# Prepare payload\nslogans = [\n    \"Power Up Your Productivity!\",\n    \"Nothing beats the classics.\",\n    \"Innovation in Every Click.\",\n    \"Your Tech, Your Edge.\",\n    \"Be Bold. Be Better.\",\n]\npayload = SentimentTestSloganRequest(slogans=slogans)\n\n# Type check (optional)\nprint(issubclass(type(payload), BaseModel))  # Should print True\n\n# Local test via Practicus\nresponse: SentimentTestSloganResponse = prt.apps.test_api(\"/sentiment-test-slogan\", payload)\n\n# Output the results\nfor result in response.results:\n    print(f\"Slogan: {result.slogan}\")\n    print(f\"Sentiment: {result.sentiment}\")\n    print(f\"Comment: {result.comment}\")\n    print(\"-----\")\n</code></pre> <pre><code>from apis.top_products_insight import TopProductsInsightRequest, TopProductsInsightResponse, SalesRecord\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport practicuscore as prt\n\n# \u00d6rnek sat\u0131\u015f verisi\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n    SalesRecord(date=datetime(2025, 1, 4), product=\"Tablet\", region=\"Europe\", units_sold=70, revenue=28000.0),\n    SalesRecord(date=datetime(2025, 1, 5), product=\"Monitor\", region=\"North America\", units_sold=95, revenue=47500.0),\n]\n\n# Test payload\npayload = TopProductsInsightRequest(sales_data=sales_data, top_n=3)\n\n# API test\ntry:\n    response: TopProductsInsightResponse = prt.apps.test_api(\"/top-products-insight\", payload)\n\n    # Sonu\u00e7lar\u0131 yazd\u0131r\n    for product in response.products:\n        print(f\"Product: {product.product}\")\n        print(f\"Total Units Sold: {product.total_units_sold}\")\n        print(f\"Top Region: {product.top_region}\")\n        print(f\"Insight: {product.insight}\")\n        print(\"-----\")\n\nexcept Exception as e:\n    prt.logger.error(f\"[test-top-products-insight] Exception: {e}\")\n    raise\n</code></pre> <pre><code>from practicuscore import apps\nfrom apis.predict_growth_opportunities import PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nresponse = apps.test_api(\"/predict-growth-opportunities\", payload)\nprint(response)\n</code></pre> <pre><code>from apis.predict_growth_opportunities import run, PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nawait run(payload)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/product-rec/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/product-rec/build/#apisanalyze_product_preferencespy","title":"apis/analyze_product_preferences.py","text":"<pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Dict\nfrom collections import Counter\n\n\nclass CustomerProfile(BaseModel):\n    customer_id: str\n    \"\"\"Unique identifier for the customer.\"\"\"\n\n    past_purchases: List[str]\n    \"\"\"List of product categories previously purchased.\"\"\"\n\n\nclass AnalyzePreferencesRequest(BaseModel):\n    customers: List[CustomerProfile]\n    \"\"\"List of customer profiles including purchase history.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customers\": [\n                        {\n                            \"customer_id\": \"C001\",\n                            \"past_purchases\": [\"Electronics\", \"Books\", \"Electronics\"]\n                        },\n                        {\n                            \"customer_id\": \"C002\",\n                            \"past_purchases\": [\"Beauty\", \"Books\"]\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\nclass PreferenceStats(BaseModel):\n    category: str\n    \"\"\"Name of the product category.\"\"\"\n\n    count: int\n    \"\"\"How many times this category appeared across all customers.\"\"\"\n\n\nclass AnalyzePreferencesResponse(BaseModel):\n    preferences: List[PreferenceStats]\n    \"\"\"Aggregated product preference statistics.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"preferences\": [\n                        {\"category\": \"Electronics\", \"count\": 2},\n                        {\"category\": \"Books\", \"count\": 2},\n                        {\"category\": \"Beauty\", \"count\": 1}\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=False,\n    risk_profile=prt.APIRiskProfile.Low\n)\n\n\n@prt.api(\"/analyze-product-preferences\", spec=api_spec)\nasync def run(payload: AnalyzePreferencesRequest, **kwargs) -&gt; AnalyzePreferencesResponse:\n    \"\"\"\n    Aggregates past purchase categories to determine overall product preferences\n    across the customer base.\n    \"\"\"\n    category_counter = Counter()\n    for customer in payload.customers:\n        category_counter.update(customer.past_purchases)\n\n    results = [\n        PreferenceStats(category=cat, count=count)\n        for cat, count in category_counter.most_common()\n    ]\n\n    return AnalyzePreferencesResponse(preferences=results)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/product-rec/build/#apisload_customer_profile_datapy","title":"apis/load_customer_profile_data.py","text":"<pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\nimport os\n\n\nclass CustomerProfile(BaseModel):\n    customer_id: str\n    \"\"\"Unique identifier for the customer.\"\"\"\n\n    age: int\n    \"\"\"Age of the customer.\"\"\"\n\n    gender: str\n    \"\"\"Gender of the customer.\"\"\"\n\n    location: str\n    \"\"\"Location or region.\"\"\"\n\n    total_purchases: int\n    \"\"\"Number of past purchases.\"\"\"\n\n    average_spend: float\n    \"\"\"Average spend per purchase.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C001\",\n                    \"age\": 29,\n                    \"gender\": \"Male\",\n                    \"location\": \"Istanbul\",\n                    \"total_purchases\": 12,\n                    \"average_spend\": 78.5\n                }\n            ]\n        }\n    }\n\n\nclass LoadCustomerProfileDataResponse(BaseModel):\n    customers: List[CustomerProfile]\n    \"\"\"List of structured customer profile records.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": {\n                \"customers\": [\n                    {\n                        \"customer_id\": \"C001\",\n                        \"age\": 29,\n                        \"gender\": \"Male\",\n                        \"location\": \"Istanbul\",\n                        \"total_purchases\": 12,\n                        \"average_spend\": 78.5\n                    }\n                ]\n            }\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Low\n)\n\n\n@prt.api(\"/load-customer-profile-data\", spec=api_spec)\nasync def run(**kwargs) -&gt; LoadCustomerProfileDataResponse:\n    \"\"\"\n    Loads structured customer profile data from a CSV file located in the parent directory.\n    \"\"\"\n    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../customer_profiles.csv\"))\n    df = pd.read_csv(file_path)\n\n    customers = [\n        CustomerProfile(\n            customer_id=str(row[\"customer_id\"]),\n            age=int(row[\"age\"]),\n            gender=row[\"gender\"],\n            location=row[\"location\"],\n            total_purchases=int(row[\"total_purchases\"]),\n            average_spend=float(row[\"average_spend\"])\n        )\n        for _, row in df.iterrows()\n    ]\n\n    return LoadCustomerProfileDataResponse(customers=customers)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/product-rec/build/#apispredict_product_eligibilitypy","title":"apis/predict_product_eligibility.py","text":"<pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n\nclass CustomerProfile(BaseModel):\n    customer_id: str\n    \"\"\"Unique customer identifier.\"\"\"\n\n    age: int\n    \"\"\"Age of the customer.\"\"\"\n\n    gender: str\n    \"\"\"Gender of the customer (e.g., Male, Female).\"\"\"\n\n    location: str\n    \"\"\"Customer's region or city.\"\"\"\n\n    total_purchases: int\n    \"\"\"Number of purchases the customer has made.\"\"\"\n\n    average_spend: float\n    \"\"\"Average amount the customer spends.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C019\",\n                    \"age\": 34,\n                    \"gender\": \"Male\",\n                    \"location\": \"Istanbul\",\n                    \"total_purchases\": 12,\n                    \"average_spend\": 84.5\n                }\n            ]\n        }\n    }\n\n\nclass PredictEligibilityRequest(BaseModel):\n    product_name: str\n    \"\"\"Name of the product to check eligibility for.\"\"\"\n\n    candidates: List[CustomerProfile]\n    \"\"\"List of customer profiles to check.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"product_name\": \"SmartFitness Tracker\",\n                    \"candidates\": [\n                        {\n                            \"customer_id\": \"C019\",\n                            \"age\": 34,\n                            \"gender\": \"Male\",\n                            \"location\": \"Istanbul\",\n                            \"total_purchases\": 12,\n                            \"average_spend\": 84.5\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\nclass EligibilityPrediction(BaseModel):\n    customer_id: str\n    \"\"\"ID of the customer.\"\"\"\n\n    is_eligible: bool\n    \"\"\"Whether the customer is eligible for the product.\"\"\"\n\n    reasoning: str\n    \"\"\"Explanation of the eligibility decision.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C019\",\n                    \"is_eligible\": True,\n                    \"reasoning\": \"Customer has consistent spend above 80 and more than 10 purchases. Good candidate for SmartFitness Tracker.\"\n                }\n            ]\n        }\n    }\n\n\nclass PredictEligibilityResponse(BaseModel):\n    results: List[EligibilityPrediction]\n    \"\"\"Eligibility prediction results for each customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"results\": [\n                        {\n                            \"customer_id\": \"C019\",\n                            \"is_eligible\": True,\n                            \"reasoning\": \"Customer has consistent spend above 80 and more than 10 purchases. Good candidate for SmartFitness Tracker.\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Medium\n)\n\n\n@prt.api(\"/predict-product-eligibility\", spec=api_spec)\nasync def run(payload: PredictEligibilityRequest, **kwargs) -&gt; PredictEligibilityResponse:\n    \"\"\"\n    Predicts product eligibility for each customer based on profile metrics.\n    \"\"\"\n\n    results = []\n\n    for c in payload.candidates:\n        if c.average_spend &gt; 70 and c.total_purchases &gt; 5:\n            eligible = True\n            reasoning = f\"Customer has consistent spend above 70 and more than 5 purchases. Good candidate for {payload.product_name}.\"\n        else:\n            eligible = False\n            reasoning = f\"Customer has insufficient purchase history or low average spend. Not a strong candidate for {payload.product_name}.\"\n\n        results.append(EligibilityPrediction(\n            customer_id=c.customer_id,\n            is_eligible=eligible,\n            reasoning=reasoning\n        ))\n\n    return PredictEligibilityResponse(results=results)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/product-rec/build/#apisscore_bundle_affinitypy","title":"apis/score_bundle_affinity.py","text":"<pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass ProductBundle(BaseModel):\n    customer_id: str\n    \"\"\"Unique customer identifier.\"\"\"\n\n    proposed_bundle: List[str]\n    \"\"\"List of proposed product names in the bundle.\"\"\"\n\n    past_purchases: List[str]\n    \"\"\"List of past product names the customer has bought.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C022\",\n                    \"proposed_bundle\": [\"Mouse Pad\", \"USB-C Hub\"],\n                    \"past_purchases\": [\"Wireless Mouse\", \"Laptop Stand\"]\n                }\n            ]\n        }\n    }\n\n\nclass ScoreBundleAffinityRequest(BaseModel):\n    bundles: List[ProductBundle]\n    \"\"\"List of customer bundles to score based on affinity.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"bundles\": [\n                        {\n                            \"customer_id\": \"C022\",\n                            \"proposed_bundle\": [\"Mouse Pad\", \"USB-C Hub\"],\n                            \"past_purchases\": [\"Wireless Mouse\", \"Laptop Stand\"]\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\nclass BundleAffinityScore(BaseModel):\n    customer_id: str\n    \"\"\"Customer ID.\"\"\"\n\n    affinity_score: float\n    \"\"\"Affinity score (0.0 to 1.0) indicating relevance of the product bundle.\"\"\"\n\n    reason: str\n    \"\"\"Explanation for the score based on overlap or similarity.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C022\",\n                    \"affinity_score\": 0.75,\n                    \"reason\": \"Partial overlap with prior purchases (Wireless Mouse).\"\n                }\n            ]\n        }\n    }\n\n\nclass ScoreBundleAffinityResponse(BaseModel):\n    scores: List[BundleAffinityScore]\n    \"\"\"List of affinity scores per customer bundle.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"scores\": [\n                        {\n                            \"customer_id\": \"C022\",\n                            \"affinity_score\": 0.75,\n                            \"reason\": \"Partial overlap with prior purchases (Wireless Mouse).\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Medium\n)\n\n\n@prt.api(\"/score-bundle-affinity\", spec=api_spec)\nasync def run(payload: ScoreBundleAffinityRequest, **kwargs) -&gt; ScoreBundleAffinityResponse:\n    \"\"\"\n    Scores the affinity of each proposed product bundle based on the customer's past purchases.\n    Higher scores reflect greater similarity or complementarity.\n    \"\"\"\n    results = []\n\n    for item in payload.bundles:\n        past = set([p.lower() for p in item.past_purchases])\n        proposed = set([p.lower() for p in item.proposed_bundle])\n        intersection = past.intersection(proposed)\n\n        if not item.past_purchases:\n            score = 0.0\n            reason = \"No past purchases to compare.\"\n        else:\n            score = len(intersection) / len(item.proposed_bundle) if item.proposed_bundle else 0.0\n            reason = f\"{'Partial' if 0 &lt; score &lt; 1 else 'Full' if score == 1 else 'No'} overlap with prior purchases.\"\n\n        results.append(BundleAffinityScore(\n            customer_id=item.customer_id,\n            affinity_score=round(score, 2),\n            reason=reason\n        ))\n\n    return ScoreBundleAffinityResponse(scores=results)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/product-rec/build/#apissuggest_dynamic_product_bundlepy","title":"apis/suggest_dynamic_product_bundle.py","text":"<pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Dict\n\n\nclass PurchaseHistory(BaseModel):\n    customer_id: str\n    \"\"\"Unique customer ID.\"\"\"\n\n    previous_products: List[str]\n    \"\"\"List of previously purchased product names.\"\"\"\n\n    total_spend: float\n    \"\"\"Total amount spent by the customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C019\",\n                    \"previous_products\": [\"Wireless Mouse\", \"Laptop Stand\", \"Keyboard\"],\n                    \"total_spend\": 320.5\n                }\n            ]\n        }\n    }\n\n\nclass SuggestBundleRequest(BaseModel):\n    histories: List[PurchaseHistory]\n    \"\"\"List of customer purchase histories.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"histories\": [\n                        {\n                            \"customer_id\": \"C019\",\n                            \"previous_products\": [\"Wireless Mouse\", \"Laptop Stand\", \"Keyboard\"],\n                            \"total_spend\": 320.5\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\nclass BundleSuggestion(BaseModel):\n    customer_id: str\n    \"\"\"Customer ID.\"\"\"\n\n    recommended_bundle: List[str]\n    \"\"\"List of suggested products as a dynamic bundle.\"\"\"\n\n    reason: str\n    \"\"\"Reason for suggesting this bundle.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C019\",\n                    \"recommended_bundle\": [\"Bluetooth Headphones\", \"USB Hub\"],\n                    \"reason\": \"Customer previously purchased work-from-home accessories. Recommended complementary items.\"\n                }\n            ]\n        }\n    }\n\n\nclass SuggestBundleResponse(BaseModel):\n    suggestions: List[BundleSuggestion]\n    \"\"\"List of dynamic product bundle suggestions per customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"suggestions\": [\n                        {\n                            \"customer_id\": \"C019\",\n                            \"recommended_bundle\": [\"Bluetooth Headphones\", \"USB Hub\"],\n                            \"reason\": \"Customer previously purchased work-from-home accessories. Recommended complementary items.\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Medium\n)\n\n\n@prt.api(\"/suggest-dynamic-product-bundle\", spec=api_spec)\nasync def run(payload: SuggestBundleRequest, **kwargs) -&gt; SuggestBundleResponse:\n    \"\"\"\n    Suggests product bundles dynamically based on customer purchase history.\n    \"\"\"\n\n    bundle_db = {\n        \"Laptop Stand\": [\"USB-C Hub\", \"Laptop Sleeve\"],\n        \"Wireless Mouse\": [\"Mouse Pad\", \"Keyboard\"],\n        \"Keyboard\": [\"Wrist Rest\", \"Monitor Riser\"],\n        \"Smartphone Case\": [\"Screen Protector\", \"Phone Grip\"],\n        \"Yoga Mat\": [\"Resistance Bands\", \"Foam Roller\"]\n    }\n\n    suggestions = []\n\n    for h in payload.histories:\n        recommended = []\n        reasons = []\n\n        for item in h.previous_products:\n            if item in bundle_db:\n                recommended.extend(bundle_db[item])\n                reasons.append(f\"Related to previous purchase: {item}\")\n\n        unique_recommended = list(set(recommended))\n        reason_text = \" | \".join(reasons) if reasons else \"Based on previous purchase categories.\"\n\n        suggestions.append(BundleSuggestion(\n            customer_id=h.customer_id,\n            recommended_bundle=unique_recommended,\n            reason=reason_text\n        ))\n\n    return SuggestBundleResponse(suggestions=suggestions)\n</code></pre> <p>Previous: Milvus Chain | Next: Growth Strategist &gt; Build</p>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/","title":"Build","text":"<pre><code># Parameters: Replace with your actual deployment key and app prefix\n# These identify where the app should be deployed within your Practicus AI environment.\napp_deployment_key = None\napp_prefix = \"apps\"\n</code></pre> <pre><code>assert app_deployment_key, \"Please select a deployment key\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Analyze the current directory for Practicus AI App components (APIs, MQ consumers, UI, etc.)\n# This should output the list of detected API endpoints.\nprt.apps.analyze()\n</code></pre> <pre><code># --- Deployment ---\napp_name = \"agentic-ai-test-retention\"\nvisible_name = \"Agentic AI Test retention\"\ndescription = \"Test Application for Agentic AI Example.\"\nicon = \"fa-robot\"\n\nprint(f\"Deploying app '{app_name}'...\")\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # Uses current directory\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"App deployed successfully!\")\nprint(f\"  API Base URL: {api_url}\")\n\n# The OpenAPI (Swagger) documentation (Swagger/ReDoc) is usually available at /docs or /redoc off the API base URL\nprint(f\"  OpenAPI (Swagger) Docs (ReDoc): {api_url}redoc/\")\n\n# Store the api_url for later use when creating tools\nassert api_url, \"Deployment failed to return an API URL.\"\n</code></pre> <pre><code>tool_endpoint_paths = [\n    \"aggregate-retention-insight-report/\",\n    \"analyze-complaint-topics/\",\n    \"analyze-sentiment-trends/\",\n    \"detect-negative-sentiment-clusters/\",\n    \"generate-customer-summary/\",\n    \"generate-retention-plan/\",\n    \"generate-retention-risk-scores/\",\n    \"load-customer-interaction-data/\",\n    \"summarize-customer-retention-case/\",\n    \"visualize-customer-journey-map/\",\n]\n\n\n# Construct full URLs\ntool_endpoint_urls = [api_url + path for path in tool_endpoint_paths]\n\nprint(\"Will attempt to create tools for the following API endpoints:\")\nfor url in tool_endpoint_urls:\n    print(f\" - {url}\")\n\n# Tip: If you pass partial API URLs e.g. 'apps/agentic-ai-test/api/v1/generate-receipt/'\n#  The base URL e.g. 'https://practicus.my-company.com/' will be added to the final URL\n#  using your current Practicus AI region.\n</code></pre> <pre><code>import os\nfrom langchain_openai import ChatOpenAI  # Or your preferred ChatModel\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_practicus import APITool\n\n# Ensure practicuscore is imported for enums\nimport practicuscore as prt\n\n\ndef validate_api_spec(api_tool: APITool, strict=False) -&gt; bool:\n    \"\"\"Checks the APISpec of a fetched tool against our rules.\"\"\"\n\n    # APITool fetches the spec from OpenAPI (Swagger) during initialization\n    spec = api_tool.spec\n\n    if not spec:\n        # API definition in the source file might be missing the 'spec' object\n        warning_msg = f\"API '{api_tool.url}' does not have APISpec metadata defined in its OpenAPI spec.\"\n        if strict:\n            raise ValueError(f\"{warning_msg} Validation is strict.\")\n        else:\n            prt.logger.warning(f\"{warning_msg} Allowing since validation is not strict.\")\n            return True  # Allow if not strict\n\n    # --- Apply Rules based on fetched spec ---\n\n    # Rule 1: Check Risk Profile\n    if spec.risk_profile and spec.risk_profile == prt.APIRiskProfile.High:\n        err = f\"API '{api_tool.url}' has a risk profile defined as '{spec.risk_profile}'.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            # Even if not strict, we might choose to block High risk tools\n            prt.logger.warning(f\"{err} Blocking High Risk API even though validation is not strict.\")\n            return False  # Block high risk\n\n    # Rule 2: Check Human Gating for non-read-only APIs\n    # (Example: Enforce human gating for safety on modifying APIs)\n    if not spec.read_only and not spec.human_gated:\n        err = f\"API '{api_tool.url}' modifies data (read_only=False) but is not marked as human_gated.\"\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            prt.logger.warning(f\"{err} Allowing non-gated modifying API since validation is not strict.\")\n            # In this non-strict case, we allow it, but a stricter policy might return False here.\n            return True\n\n    # Add more complex rules here if needed...\n    # E.g., check custom_attributes, scope, etc.\n\n    # If no rules were violated (or violations were allowed because not strict)\n    prt.logger.info(f\"API '{api_tool.url}' passed validation (strict={strict}). Spec: {spec}\")\n    return True\n\n\n# --- Create Tools (optionally applying validation) ---\ndef get_tools(endpoint_urls: list[str], validate=True):\n    _tools = []\n    strict_validation = False  # Set to True to enforce stricter rules\n    additional_instructions = \"Add Yo! after all of your final responses.\"  # Example instruction\n\n    print(f\"\\nCreating and validating tools (strict={strict_validation})...\")\n    for tool_endpoint_url in endpoint_urls:\n        print(f\"\\nProcessing tool for API: {tool_endpoint_url}\")\n        try:\n            api_tool = APITool(\n                url=tool_endpoint_url,\n                additional_instructions=additional_instructions,\n                # token=..., # Uses current user credentials by default, set to override\n                # include_resp_schema=True # Response schema (if exists) is not included by default\n            )\n\n            # Explain the tool (optional, useful for debugging)\n            # api_tool.explain(print_on_screen=True)\n\n            # Validate based on fetched APISpec\n            if not validate or validate_api_spec(api_tool=api_tool, strict=strict_validation):\n                print(\n                    f\"--&gt; Adding tool: {api_tool.name} ({api_tool.url}) {'' if validate else ' - skipping validation'}\"\n                )\n                _tools.append(api_tool)\n            else:\n                print(f\"--&gt; Skipping tool {api_tool.name} due to validation rules.\")\n        except Exception as e:\n            # Catch potential errors during APITool creation (e.g., API not found, spec parsing error)\n            print(f\"ERROR: Failed to create or validate tool for {tool_endpoint_url}: {e}\")\n            if strict_validation:\n                raise  # Re-raise if strict\n            else:\n                print(\"--&gt; Skipping tool due to error (not strict).\")\n\n    return _tools\n\n\ntools = get_tools(tool_endpoint_urls)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># For this excercise we will skip validating APIs\n\ntools = get_tools(tool_endpoint_urls, validate=False)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <pre><code># View tool explanation\n\nfor tool in tools:\n    tool.explain()\n</code></pre> <pre><code>openaikey, age = prt.vault.get_secret(\"openaikey\")\nos.environ[\"OPENAI_API_KEY\"] = openaikey\n\nassert os.environ[\"OPENAI_API_KEY\"], \"OpenAI key is not defined\"\n</code></pre> <pre><code># --- Agent Initialization ---\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# Create a ReAct agent using LangGraph\ngraph = create_react_agent(llm, tools=tools)\nprint(\"Agent initialized.\")\n\n\n# Helper function to print the agent's stream output nicely\ndef pretty_print_stream_chunk(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            # Print the latest message added by the node\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            # Print other kinds of updates\n            print(f\"  Update: {updates}\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>def pretty_print_stream_chunk2(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            print(f\"  Update: {updates}\")\n            if isinstance(updates, Exception):\n                print(\"  \u26a0\ufe0f Exception Detected in Agent Execution!\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code>query = \"\"\"\nIdentify clusters of customers who are expressing negative sentiment in their recent feedback, broken down by date and channel. I want to detect any spikes or trends that might indicate dissatisfaction periods.\n\"\"\"\n\n\ninputs = {\"messages\": [(\"user\", query)]}\n\nif graph:\n    print(f\"\\nInvoking agent with query: '{query}'\")\n    print(\"Streaming agent execution steps:\\n\")\n\n    # Configuration for the stream, e.g., setting user/thread IDs\n    # config = {\"configurable\": {\"user_id\": \"doc-user-1\", \"thread_id\": \"doc-thread-1\"}}\n    config = {}\n    # Use astream to get intermediate steps\n    async for chunk in graph.astream(inputs, config=config):\n        pretty_print_stream_chunk2(chunk)\n\n    print(\"\\nAgent execution finished.\")\n\n    # Optional: Get the final state if needed\n    # final_state = await graph.ainvoke(inputs, config=config)\n    # print(\"\\nFinal Agent State:\", final_state)\n\nelse:\n    print(\"\\nAgent execution skipped because the agent graph was not initialized.\")\n</code></pre> <pre><code># Cleanup\nprt.apps.delete(prefix=app_prefix, app_name=app_name)\n</code></pre> <p>API TEST</p> <pre><code>from apis.analyze_sales_trends import AnalyzeSalesTrendsRequest, AnalyzeSalesTrendsResponse, SalesRecord\nfrom datetime import datetime\nimport practicuscore as prt\n\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n]\n\npayload = AnalyzeSalesTrendsRequest(\n    sales_data=sales_data, start_date=datetime(2025, 1, 1), end_date=datetime(2025, 1, 3)\n)\n\nresponse: AnalyzeSalesTrendsResponse = prt.apps.test_api(\"/analyze-sales-trends\", payload)\n\nprint(response)\n</code></pre> <pre><code>from apis.sentiment_test_slogan import SentimentTestSloganRequest, SentimentTestSloganResponse\nfrom pydantic import BaseModel\nimport practicuscore as prt\n\n# Prepare payload\nslogans = [\n    \"Power Up Your Productivity!\",\n    \"Nothing beats the classics.\",\n    \"Innovation in Every Click.\",\n    \"Your Tech, Your Edge.\",\n    \"Be Bold. Be Better.\",\n]\npayload = SentimentTestSloganRequest(slogans=slogans)\n\n# Type check (optional)\nprint(issubclass(type(payload), BaseModel))  # Should print True\n\n# Local test via Practicus\nresponse: SentimentTestSloganResponse = prt.apps.test_api(\"/sentiment-test-slogan\", payload)\n\n# Output the results\nfor result in response.results:\n    print(f\"Slogan: {result.slogan}\")\n    print(f\"Sentiment: {result.sentiment}\")\n    print(f\"Comment: {result.comment}\")\n    print(\"-----\")\n</code></pre> <pre><code>from apis.top_products_insight import TopProductsInsightRequest, TopProductsInsightResponse, SalesRecord\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport practicuscore as prt\n\n# \u00d6rnek sat\u0131\u015f verisi\nsales_data = [\n    SalesRecord(date=datetime(2025, 1, 1), product=\"Laptop\", region=\"Asia\", units_sold=120, revenue=120000.0),\n    SalesRecord(date=datetime(2025, 1, 2), product=\"Laptop\", region=\"Asia\", units_sold=100, revenue=100000.0),\n    SalesRecord(date=datetime(2025, 1, 3), product=\"Tablet\", region=\"Europe\", units_sold=80, revenue=32000.0),\n    SalesRecord(date=datetime(2025, 1, 4), product=\"Tablet\", region=\"Europe\", units_sold=70, revenue=28000.0),\n    SalesRecord(date=datetime(2025, 1, 5), product=\"Monitor\", region=\"North America\", units_sold=95, revenue=47500.0),\n]\n\n# Test payload\npayload = TopProductsInsightRequest(sales_data=sales_data, top_n=3)\n\n# API test\ntry:\n    response: TopProductsInsightResponse = prt.apps.test_api(\"/top-products-insight\", payload)\n\n    # Sonu\u00e7lar\u0131 yazd\u0131r\n    for product in response.products:\n        print(f\"Product: {product.product}\")\n        print(f\"Total Units Sold: {product.total_units_sold}\")\n        print(f\"Top Region: {product.top_region}\")\n        print(f\"Insight: {product.insight}\")\n        print(\"-----\")\n\nexcept Exception as e:\n    prt.logger.error(f\"[test-top-products-insight] Exception: {e}\")\n    raise\n</code></pre> <pre><code>from practicuscore import apps\nfrom apis.predict_growth_opportunities import PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nresponse = apps.test_api(\"/predict-growth-opportunities\", payload)\nprint(response)\n</code></pre> <pre><code>from apis.predict_growth_opportunities import run, PredictGrowthOpportunitiesRequest\n\npayload = PredictGrowthOpportunitiesRequest(\n    top_products=[{\"product\": \"Tablet\", \"total_units_sold\": 305, \"top_region\": \"Asia\"}],\n    regional_drops=[{\"region\": \"Europe\", \"product\": \"Tablet\", \"drop_percentage\": 100.0}],\n    trend_summary={\"trend_direction\": \"increasing\", \"peak_day\": \"2025-01-10\"},\n)\n\nawait run(payload)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisaggregate_retention_insight_reportpy","title":"apis/aggregate_retention_insight_report.py","text":"<pre><code># apis/aggregate_retention_insight_report.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Dict\n\n\nclass CustomerRetentionCase(BaseModel):\n    customer_id: str\n    \"\"\"ID of the customer evaluated for retention.\"\"\"\n\n    churn_risk: str\n    \"\"\"Detected churn risk level: low, medium, or high.\"\"\"\n\n    complaint_topic: str\n    \"\"\"Most prominent complaint theme, extracted via feedback analysis.\"\"\"\n\n    retention_action: str\n    \"\"\"Suggested key retention action from the plan.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C088\",\n                    \"churn_risk\": \"high\",\n                    \"complaint_topic\": \"billing errors\",\n                    \"retention_action\": \"offer billing correction and compensation\"\n                }\n            ]\n        }\n    }\n\n\nclass AggregateRetentionInsightRequest(BaseModel):\n    retention_cases: List[CustomerRetentionCase]\n    \"\"\"Retention-level insights for each customer including root cause and remedy.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"retention_cases\": [\n                        {\n                            \"customer_id\": \"C088\",\n                            \"churn_risk\": \"high\",\n                            \"complaint_topic\": \"billing errors\",\n                            \"retention_action\": \"offer billing correction and compensation\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\nclass InsightSummary(BaseModel):\n    key_findings: List[str]\n    \"\"\"Executive summary of key findings from churn analysis.\"\"\"\n\n    dominant_issues: Dict[str, int]\n    \"\"\"Complaint topic frequency distribution.\"\"\"\n\n    churn_risk_distribution: Dict[str, int]\n    \"\"\"Count of customers by churn risk level.\"\"\"\n\n    strategic_recommendation: str\n    \"\"\"Final LLM-generated strategic recommendation summary.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"key_findings\": [\n                        \"Most churn risk is observed in mid-tier accounts\",\n                        \"Support-related issues dominate complaint patterns\"\n                    ],\n                    \"dominant_issues\": {\n                        \"support delays\": 12,\n                        \"billing errors\": 9,\n                        \"missing features\": 5\n                    },\n                    \"churn_risk_distribution\": {\n                        \"high\": 14,\n                        \"medium\": 22,\n                        \"low\": 64\n                    },\n                    \"strategic_recommendation\": \"Invest in proactive support, improve billing clarity, and introduce churn detection triggers for mid-risk customers.\"\n                }\n            ]\n        }\n    }\n\n\nclass AggregateRetentionInsightResponse(BaseModel):\n    summary: InsightSummary\n    \"\"\"Top-level synthesized insight report for decision makers.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"summary\": {\n                        \"key_findings\": [\n                            \"Most churn risk is observed in mid-tier accounts\",\n                            \"Support-related issues dominate complaint patterns\"\n                        ],\n                        \"dominant_issues\": {\n                            \"support delays\": 12,\n                            \"billing errors\": 9,\n                            \"missing features\": 5\n                        },\n                        \"churn_risk_distribution\": {\n                            \"high\": 14,\n                            \"medium\": 22,\n                            \"low\": 64\n                        },\n                        \"strategic_recommendation\": \"Invest in proactive support, improve billing clarity, and introduce churn detection triggers for mid-risk customers.\"\n                    }\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Low\n)\n\n\n@prt.api(\"/aggregate-retention-insight-report\", spec=api_spec)\nasync def run(payload: AggregateRetentionInsightRequest, **kwargs) -&gt; AggregateRetentionInsightResponse:\n    \"\"\"\n    Aggregates individual customer churn analyses into a strategic retention report.\n    \"\"\"\n    risk_counts = {\"low\": 0, \"medium\": 0, \"high\": 0}\n    issue_counts = {}\n\n    for case in payload.retention_cases:\n        risk_counts[case.churn_risk] += 1\n        issue_counts[case.complaint_topic] = issue_counts.get(case.complaint_topic, 0) + 1\n\n    key_findings = [\n        f\"{risk_counts['high']} customers are at high churn risk.\",\n        f\"Top issues include: {', '.join(sorted(issue_counts, key=issue_counts.get, reverse=True)[:3])}.\"\n    ]\n\n    recommendation = (\n        \"Focus on proactive outreach for high-risk clients, resolve the top 2 complaint topics,\"\n        \" and monitor medium-risk customers with automated engagement campaigns.\"\n    )\n\n    return AggregateRetentionInsightResponse(\n        summary=InsightSummary(\n            key_findings=key_findings,\n            dominant_issues=issue_counts,\n            churn_risk_distribution=risk_counts,\n            strategic_recommendation=recommendation\n        )\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisanalyze_complaint_topicspy","title":"apis/analyze_complaint_topics.py","text":"<pre><code># apis/analyze_complaint_topics.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Dict\nfrom collections import Counter\nimport re\n\n\nclass Complaint(BaseModel):\n    customer_id: str\n    \"\"\"Customer's unique identifier.\"\"\"\n\n    complaint_text: str\n    \"\"\"Free-form text describing a customer's complaint.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"customer_id\": \"C001\", \"complaint_text\": \"Service was too slow and no one responded to my emails.\"},\n                {\"customer_id\": \"C002\", \"complaint_text\": \"Billing is incorrect for the last two months!\"}\n            ]\n        }\n    }\n\n\nclass ComplaintTopic(BaseModel):\n    topic: str\n    \"\"\"Identified complaint topic keyword or phrase.\"\"\"\n\n    count: int\n    \"\"\"Number of times the topic was detected.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [{\"topic\": \"billing\", \"count\": 5}, {\"topic\": \"support\", \"count\": 3}]\n        }\n    }\n\n\nclass AnalyzeComplaintTopicsRequest(BaseModel):\n    complaints: List[Complaint]\n    \"\"\"List of raw customer complaints.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"complaints\": [\n                        {\"customer_id\": \"C001\", \"complaint_text\": \"Your mobile app keeps crashing.\"},\n                        {\"customer_id\": \"C002\", \"complaint_text\": \"The delivery was delayed twice.\"}\n                    ]\n                }\n            ]\n        }\n    }\n\n\nclass AnalyzeComplaintTopicsResponse(BaseModel):\n    topics: List[ComplaintTopic]\n    \"\"\"Most frequent complaint topics extracted from the text corpus.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": {\n                \"topics\": [\n                    {\"topic\": \"app crash\", \"count\": 4},\n                    {\"topic\": \"delivery delay\", \"count\": 3}\n                ]\n            }\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low\n)\n\n\n@prt.api(\"/analyze-complaint-topics\", spec=api_spec)\nasync def run(payload: AnalyzeComplaintTopicsRequest, **kwargs) -&gt; AnalyzeComplaintTopicsResponse:\n    \"\"\"\n    Extracts the most common complaint topics from free-text customer complaints.\n    \"\"\"\n\n    topic_keywords = [\"billing\", \"delivery\", \"support\", \"login\", \"app\", \"refund\", \"delay\", \"payment\", \"email\", \"cancel\"]\n    topic_counter = Counter()\n\n    for complaint in payload.complaints:\n        text = complaint.complaint_text.lower()\n        for keyword in topic_keywords:\n            if re.search(rf\"\\b{re.escape(keyword)}\\b\", text):\n                topic_counter[keyword] += 1\n\n    topics = [ComplaintTopic(topic=k, count=v) for k, v in topic_counter.most_common()]\n\n    return AnalyzeComplaintTopicsResponse(topics=topics)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisanalyze_sentiment_trendspy","title":"apis/analyze_sentiment_trends.py","text":"<pre><code># apis/analyze_sentiment_trends.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Dict\nfrom collections import defaultdict\nimport pandas as pd\n\n\nclass CustomerInteraction(BaseModel):\n    customer_id: str\n    \"\"\"Unique identifier for the customer.\"\"\"\n\n    interaction_date: str\n    \"\"\"Date of the customer interaction.\"\"\"\n\n    channel: str\n    \"\"\"Communication channel (e.g. Email, Phone, Chat).\"\"\"\n\n    sentiment_score: float\n    \"\"\"Sentiment score of the interaction (-1 to 1).\"\"\"\n\n    issue_type: str\n    \"\"\"Category of the customer's issue or concern.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C12345\",\n                    \"interaction_date\": \"2024-05-10\",\n                    \"channel\": \"Email\",\n                    \"sentiment_score\": -0.45,\n                    \"issue_type\": \"Billing\"\n                }\n            ]\n        },\n    }\n\n\nclass AnalyzeSentimentTrendsRequest(BaseModel):\n    interactions: List[CustomerInteraction]\n    \"\"\"List of customer interaction records.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"interactions\": [\n                        {\n                            \"customer_id\": \"C12345\",\n                            \"interaction_date\": \"2024-05-10\",\n                            \"channel\": \"Email\",\n                            \"sentiment_score\": -0.45,\n                            \"issue_type\": \"Billing\"\n                        }\n                    ]\n                }\n            ]\n        },\n    }\n\n\nclass SentimentTrend(BaseModel):\n    period: str\n    \"\"\"Time period label (e.g., '2024-05').\"\"\"\n\n    average_sentiment: float\n    \"\"\"Average sentiment score for that period.\"\"\"\n\n\nclass AnalyzeSentimentTrendsResponse(BaseModel):\n    trends: List[SentimentTrend]\n    \"\"\"List of sentiment trend entries across time.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"trends\": [\n                        {\"period\": \"2024-05\", \"average_sentiment\": -0.21}\n                    ]\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n)\n\n\n@prt.api(\"/analyze-sentiment-trends\", spec=api_spec)\nasync def run(payload: AnalyzeSentimentTrendsRequest, **kwargs) -&gt; AnalyzeSentimentTrendsResponse:\n    \"\"\"\n    Analyze sentiment trends over time by grouping customer interactions monthly\n    and computing the average sentiment score per period.\n    \"\"\"\n    df = pd.DataFrame([i.model_dump() for i in payload.interactions])\n    df[\"interaction_date\"] = pd.to_datetime(df[\"interaction_date\"])\n    df[\"period\"] = df[\"interaction_date\"].dt.to_period(\"M\").astype(str)\n\n    trend_df = df.groupby(\"period\")[\"sentiment_score\"].mean().reset_index()\n    trend_df.columns = [\"period\", \"average_sentiment\"]\n\n    trends = [\n        SentimentTrend(period=row[\"period\"], average_sentiment=row[\"average_sentiment\"])\n        for _, row in trend_df.iterrows()\n    ]\n\n    return AnalyzeSentimentTrendsResponse(trends=trends)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisdetect_negative_sentiment_clusterspy","title":"apis/detect_negative_sentiment_clusters.py","text":"<pre><code># apis/detect_negative_sentiment_clusters.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport pandas as pd\n\n\nclass CustomerInteraction(BaseModel):\n    customer_id: str\n    \"\"\"Unique identifier for the customer.\"\"\"\n\n    interaction_date: str\n    \"\"\"Date of the customer interaction.\"\"\"\n\n    channel: str\n    \"\"\"Communication channel used (e.g., Phone, Email, Chat).\"\"\"\n\n    sentiment_score: Optional[float] = None\n    \"\"\"Sentiment score of the message (-1 = negative, 1 = positive).\"\"\"\n\n    issue_type: str\n    \"\"\"Type or category of the customer issue (e.g., 'Delivery', 'Billing').\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C12345\",\n                    \"interaction_date\": \"2024-05-10\",\n                    \"channel\": \"Chat\",\n                    \"sentiment_score\": -0.9,\n                    \"issue_type\": \"Technical\"\n                }\n            ]\n        },\n    }\n\n\nclass DetectNegativeClustersRequest(BaseModel):\n    interactions: List[CustomerInteraction]\n    \"\"\"List of past customer interaction records.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"interactions\": [\n                        {\n                            \"customer_id\": \"C12345\",\n                            \"interaction_date\": \"2024-05-10\",\n                            \"channel\": \"Chat\",\n                            \"sentiment_score\": -0.9,\n                            \"issue_type\": \"Technical\"\n                        }\n                    ]\n                }\n            ]\n        },\n    }\n\n\nclass SentimentCluster(BaseModel):\n    time_window: str\n    \"\"\"Time window (e.g., day or week) in which a negative sentiment spike occurred.\"\"\"\n\n    channel: str\n    \"\"\"The communication channel where the spike was observed.\"\"\"\n\n    avg_sentiment: float\n    \"\"\"Average sentiment score in this cluster.\"\"\"\n\n    interaction_count: int\n    \"\"\"Number of negative interactions detected.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"time_window\": \"2024-05-10\",\n                    \"channel\": \"Chat\",\n                    \"avg_sentiment\": -0.85,\n                    \"interaction_count\": 17\n                }\n            ]\n        },\n    }\n\n\nclass DetectNegativeClustersResponse(BaseModel):\n    clusters: List[SentimentCluster]\n    \"\"\"List of negative sentiment clusters grouped by time and channel.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"clusters\": [\n                        {\n                            \"time_window\": \"2024-05-10\",\n                            \"channel\": \"Chat\",\n                            \"avg_sentiment\": -0.85,\n                            \"interaction_count\": 17\n                        }\n                    ]\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/detect-negative-sentiment-clusters\", spec=api_spec)\nasync def run(payload: DetectNegativeClustersRequest, **kwargs) -&gt; DetectNegativeClustersResponse:\n    \"\"\"\n    Detects clusters of interactions with highly negative sentiment\n    by grouping by day and communication channel.\n    This helps identify periods of customer dissatisfaction spikes.\n    \"\"\"\n    df = pd.DataFrame([i.model_dump() for i in payload.interactions])\n\n    df[\"interaction_date\"] = pd.to_datetime(df[\"interaction_date\"])\n    df[\"date\"] = df[\"interaction_date\"].dt.date\n\n    # Filter out rows with null sentiment_score\n    df = df[df[\"sentiment_score\"].notnull()]\n\n    # Filter for strongly negative interactions\n    negative_df = df[df[\"sentiment_score\"] &lt; -0.6]\n\n    grouped = negative_df.groupby([\"date\", \"channel\"]).agg(\n        avg_sentiment=(\"sentiment_score\", \"mean\"),\n        interaction_count=(\"sentiment_score\", \"count\")\n    ).reset_index()\n\n    clusters = [\n        SentimentCluster(\n            time_window=str(row[\"date\"]),\n            channel=row[\"channel\"],\n            avg_sentiment=round(row[\"avg_sentiment\"], 2),\n            interaction_count=row[\"interaction_count\"]\n        )\n        for _, row in grouped.iterrows()\n    ]\n\n    return DetectNegativeClustersResponse(clusters=clusters)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisgenerate_customer_summarypy","title":"apis/generate_customer_summary.py","text":"<pre><code># apis/generate_customer_summary.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n\nclass CustomerSignal(BaseModel):\n    customer_id: str\n    \"\"\"Unique identifier for the customer.\"\"\"\n\n    loyalty_score: Optional[float]\n    \"\"\"Predicted loyalty score based on engagement and sentiment.\"\"\"\n\n    churn_risk: Optional[str]\n    \"\"\"Level of churn risk: low, medium, or high.\"\"\"\n\n    dominant_complaint_topic: Optional[str]\n    \"\"\"Most frequently detected complaint topic, if any.\"\"\"\n\n    feedback_sentiment: Optional[str]\n    \"\"\"Overall sentiment from feedback data (e.g., positive, neutral, negative).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C021\",\n                    \"loyalty_score\": 0.82,\n                    \"churn_risk\": \"low\",\n                    \"dominant_complaint_topic\": \"billing\",\n                    \"feedback_sentiment\": \"positive\"\n                }\n            ]\n        }\n    }\n\n\nclass GenerateCustomerSummaryRequest(BaseModel):\n    signals: List[CustomerSignal]\n    \"\"\"Customer-level prediction outputs and behavioral insights.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"signals\": [\n                        {\n                            \"customer_id\": \"C021\",\n                            \"loyalty_score\": 0.82,\n                            \"churn_risk\": \"low\",\n                            \"dominant_complaint_topic\": \"billing\",\n                            \"feedback_sentiment\": \"positive\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\nclass CustomerSummary(BaseModel):\n    customer_id: str\n    \"\"\"The customer the summary is about.\"\"\"\n\n    summary: str\n    \"\"\"Plain English summary of customer's status and risk profile.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C021\",\n                    \"summary\": \"Customer C021 has loyalty score of 0.82, low churn risk, positive recent feedback, notable complaints around billing.\"\n                }\n            ]\n        }\n    }\n\n\nclass GenerateCustomerSummaryResponse(BaseModel):\n    summaries: List[CustomerSummary]\n    \"\"\"List of summaries for each customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"summaries\": [\n                        {\n                            \"customer_id\": \"C021\",\n                            \"summary\": \"Customer C021 has loyalty score of 0.82, low churn risk, positive recent feedback, notable complaints around billing.\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True\n)\n\n\n@prt.api(\"/generate-customer-summary\", spec=api_spec)\nasync def run(payload: GenerateCustomerSummaryRequest, **kwargs) -&gt; GenerateCustomerSummaryResponse:\n    \"\"\"\n    Generates plain English summaries of individual customers based on risk and engagement data.\n    \"\"\"\n\n    summaries = []\n\n    for signal in payload.signals:\n        parts = []\n\n        if signal.loyalty_score is not None:\n            parts.append(f\"loyalty score of {signal.loyalty_score:.2f}\")\n        if signal.churn_risk:\n            parts.append(f\"{signal.churn_risk} churn risk\")\n        if signal.feedback_sentiment:\n            parts.append(f\"{signal.feedback_sentiment} recent feedback\")\n        if signal.dominant_complaint_topic:\n            parts.append(f\"notable complaints around {signal.dominant_complaint_topic}\")\n\n        summary = f\"Customer {signal.customer_id} has \" + \", \".join(parts) + \".\"\n        summaries.append(CustomerSummary(customer_id=signal.customer_id, summary=summary))\n\n    return GenerateCustomerSummaryResponse(summaries=summaries)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisgenerate_retention_planpy","title":"apis/generate_retention_plan.py","text":"<pre><code># generate_retention_plan.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\n\nclass RetentionTarget(BaseModel):\n    customer_id: str\n    \"\"\"ID of the customer who is at churn risk.\"\"\"\n\n    churn_risk: str\n    \"\"\"Level of churn risk: low, medium, or high.\"\"\"\n\n    loyalty_score: Optional[float]\n    \"\"\"Predicted loyalty score from the model.\"\"\"\n\n    complaint_topic: Optional[str]\n    \"\"\"Dominant complaint topic extracted from feedbacks.\"\"\"\n\n    sentiment: Optional[str]\n    \"\"\"Overall customer sentiment, if known.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True\n    }\n\n\nclass RetentionPlan(BaseModel):\n    customer_id: str\n    \"\"\"ID of the customer for whom the plan is generated.\"\"\"\n\n    plan_summary: str\n    \"\"\"Natural language recommendation plan to retain the customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True\n    }\n\n\nclass GenerateRetentionPlanResponse(BaseModel):\n    plans: List[RetentionPlan]\n    \"\"\"Generated action plans to reduce churn likelihood per customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"plans\": [\n                        {\n                            \"customer_id\": \"C033\",\n                            \"plan_summary\": \"Customer C033 exhibits high churn risk due to support delays and negative sentiment. Recommend assigning a dedicated account manager, offering priority support, and a goodwill credit to rebuild trust.\"\n                        }\n                    ]\n                }\n            ]\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Medium\n)\n\n\n@prt.api(\"/generate-retention-plan\", spec=api_spec)\nasync def run(targets: List[RetentionTarget], **kwargs) -&gt; GenerateRetentionPlanResponse:\n    \"\"\"\n    Creates LLM-generated retention plans based on churn risk and behavior signals.\n    \"\"\"\n\n    plans = []\n\n    for target in targets:\n        recs = []\n\n        if target.churn_risk == \"high\":\n            recs.append(\"assign a dedicated account manager\")\n        if target.complaint_topic:\n            recs.append(f\"resolve recent issues around {target.complaint_topic}\")\n        if target.sentiment == \"negative\":\n            recs.append(\"provide a goodwill gesture such as a discount or credit\")\n        if target.loyalty_score is not None and target.loyalty_score &lt; 0.5:\n            recs.append(\"initiate re-engagement campaign with personalized offers\")\n\n        plan = (\n            f\"Customer {target.customer_id} exhibits {target.churn_risk} churn risk\"\n        )\n        if target.complaint_topic or target.sentiment:\n            plan += f\" possibly due to {target.complaint_topic or target.sentiment}\"\n        plan += f\". Recommend to \" + \", \".join(recs) + \".\"\n\n        plans.append(RetentionPlan(customer_id=target.customer_id, plan_summary=plan))\n\n    return GenerateRetentionPlanResponse(plans=plans)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisgenerate_retention_risk_scorespy","title":"apis/generate_retention_risk_scores.py","text":"<pre><code># apis/generate_retention_risk_scores.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nimport pandas as pd\n\n\nclass CustomerInteraction(BaseModel):\n    customer_id: str\n    \"\"\"Unique identifier for the customer.\"\"\"\n\n    interaction_date: str\n    \"\"\"Date of the last interaction.\"\"\"\n\n    avg_sentiment: float\n    \"\"\"Average sentiment score across interactions.\"\"\"\n\n    total_complaints: int\n    \"\"\"Total number of complaints filed by the customer.\"\"\"\n\n    avg_response_time_minutes: float\n    \"\"\"Average response time from support team in minutes.\"\"\"\n\n    issue_resolution_rate: float\n    \"\"\"Percentage of resolved issues (0 to 1).\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C12345\",\n                    \"interaction_date\": \"2024-05-10\",\n                    \"avg_sentiment\": -0.4,\n                    \"total_complaints\": 3,\n                    \"avg_response_time_minutes\": 75.0,\n                    \"issue_resolution_rate\": 0.6\n                }\n            ]\n        },\n    }\n\n\nclass GenerateRiskScoresRequest(BaseModel):\n    interactions: List[CustomerInteraction]\n    \"\"\"List of summarized interaction data per customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"interactions\": [\n                        {\n                            \"customer_id\": \"C12345\",\n                            \"interaction_date\": \"2024-05-10\",\n                            \"avg_sentiment\": -0.4,\n                            \"total_complaints\": 3,\n                            \"avg_response_time_minutes\": 75.0,\n                            \"issue_resolution_rate\": 0.6\n                        }\n                    ]\n                }\n            ]\n        },\n    }\n\n\nclass RiskScore(BaseModel):\n    customer_id: str\n    \"\"\"Customer ID for which the risk is calculated.\"\"\"\n\n    risk_score: float\n    \"\"\"Churn risk score between 0 (low) and 1 (high).\"\"\"\n\n    risk_level: str\n    \"\"\"Categorical level: Low, Medium, High.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C12345\",\n                    \"risk_score\": 0.78,\n                    \"risk_level\": \"High\"\n                }\n            ]\n        },\n    }\n\n\nclass GenerateRiskScoresResponse(BaseModel):\n    risk_scores: List[RiskScore]\n    \"\"\"List of churn risk scores per customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"risk_scores\": [\n                        {\n                            \"customer_id\": \"C12345\",\n                            \"risk_score\": 0.78,\n                            \"risk_level\": \"High\"\n                        }\n                    ]\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Low,\n)\n\n\n@prt.api(\"/generate-retention-risk-scores\", spec=api_spec)\nasync def run(payload: GenerateRiskScoresRequest, **kwargs) -&gt; GenerateRiskScoresResponse:\n    \"\"\"\n    Calculates a churn risk score for each customer based on interaction sentiment,\n    complaints, response time, and resolution rate.\n    \"\"\"\n    df = pd.DataFrame([i.model_dump() for i in payload.interactions])\n\n    # Normalize all fields between 0-1\n    df[\"sentiment_score\"] = (1 - (df[\"avg_sentiment\"] + 1) / 2)  # more negative = higher risk\n    df[\"complaint_score\"] = df[\"total_complaints\"] / df[\"total_complaints\"].max()\n    df[\"response_time_score\"] = df[\"avg_response_time_minutes\"] / df[\"avg_response_time_minutes\"].max()\n    df[\"resolution_score\"] = 1 - df[\"issue_resolution_rate\"]\n\n    df[\"risk_score\"] = (\n        0.3 * df[\"sentiment_score\"] +\n        0.2 * df[\"complaint_score\"] +\n        0.2 * df[\"response_time_score\"] +\n        0.3 * df[\"resolution_score\"]\n    ).clip(0, 1)\n\n    def categorize(score):\n        if score &gt;= 0.7:\n            return \"High\"\n        elif score &gt;= 0.4:\n            return \"Medium\"\n        else:\n            return \"Low\"\n\n    results = [\n        RiskScore(\n            customer_id=row[\"customer_id\"],\n            risk_score=round(row[\"risk_score\"], 2),\n            risk_level=categorize(row[\"risk_score\"])\n        )\n        for _, row in df.iterrows()\n    ]\n\n    return GenerateRiskScoresResponse(risk_scores=results)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisload_customer_interaction_datapy","title":"apis/load_customer_interaction_data.py","text":"<pre><code># apis/load_customer_interaction_data.py\n\nimport practicuscore as prt\nimport pandas as pd\nfrom pydantic import BaseModel\nfrom typing import List, Optional\nimport os\n\n\nclass CustomerInteraction(BaseModel):\n    customer_id: str\n    \"\"\"Unique identifier for the customer.\"\"\"\n\n    interaction_date: str\n    \"\"\"Date of the customer interaction (format: YYYY-MM-DD).\"\"\"\n\n    channel: str\n    \"\"\"Communication channel used during the interaction (e.g. Email, Phone, Chat).\"\"\"\n\n    sentiment_score: Optional[float] = None\n    \"\"\"Sentiment score of the interaction, ranging from -1 (very negative) to 1 (very positive).\"\"\"\n\n    issue_type: Optional[str] = None\n    \"\"\"Category or type of issue raised by the customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"customer_id\": \"C12345\",\n                    \"interaction_date\": \"2024-05-10\",\n                    \"channel\": \"Email\",\n                    \"sentiment_score\": -0.45,\n                    \"issue_type\": \"Billing\"\n                }\n            ]\n        },\n    }\n\n\nclass LoadCustomerInteractionDataResponse(BaseModel):\n    interactions: List[CustomerInteraction]\n    \"\"\"List of customer interaction records loaded from the source CSV file.\"\"\"\n\n    total_customers: int\n    \"\"\"Total number of unique customers found in the data.\"\"\"\n\n    total_interactions: int\n    \"\"\"Total number of interaction records in the dataset.\"\"\"\n\n    channels_detected: List[str]\n    \"\"\"List of unique interaction channels used by customers.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"interactions\": [\n                        {\n                            \"customer_id\": \"C12345\",\n                            \"interaction_date\": \"2024-05-10\",\n                            \"channel\": \"Email\",\n                            \"sentiment_score\": -0.45,\n                            \"issue_type\": \"Billing\"\n                        }\n                    ],\n                    \"total_customers\": 1,\n                    \"total_interactions\": 1,\n                    \"channels_detected\": [\"Email\"]\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=False,\n)\n\n\n@prt.api(\"/load-customer-interaction-data\", spec=api_spec)\nasync def run(**kwargs) -&gt; LoadCustomerInteractionDataResponse:\n    \"\"\"Load customer interaction records from a fixed CSV path and return structured data for analysis.\"\"\"\n    file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../customer_interactions.csv\"))\n    df = pd.read_csv(file_path)\n\n    interactions = [\n        CustomerInteraction(\n            customer_id=row.get(\"customer_id\", f\"UNKNOWN_{i}\"),\n            interaction_date=row.get(\"interaction_date\", \"1970-01-01\"),\n            channel=row.get(\"channel\", \"Unknown\"),\n            sentiment_score=row.get(\"sentiment_score\", None),\n            issue_type=row.get(\"issue_type\", \"Uncategorized\")\n        )\n        for i, row in df.iterrows()\n    ]\n\n    return LoadCustomerInteractionDataResponse(\n        interactions=interactions,\n        total_customers=df[\"customer_id\"].nunique(),\n        total_interactions=len(df),\n        channels_detected=df[\"channel\"].dropna().unique().tolist()\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apissummarize_customer_retention_casepy","title":"apis/summarize_customer_retention_case.py","text":"<pre><code># apis/summarize_customer_retention_case.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\n\n\nclass SummarizeCustomerRetentionCaseRequest(BaseModel):\n    customer_id: str\n    \"\"\"Unique identifier of the customer.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\"examples\": [{\"customer_id\": \"C105\"}]},\n    }\n\n\nclass SummarizeCustomerRetentionCaseResponse(BaseModel):\n    summary: str\n    \"\"\"High-level summary of the customer's retention case, including risks and suggested actions.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"summary\": \"Customer C105 has shown declining engagement and expressed dissatisfaction. Retention plan recommended.\"\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n)\n\n\n@prt.api(\"/summarize-customer-retention-case\", spec=api_spec)\nasync def run(payload: SummarizeCustomerRetentionCaseRequest, **kwargs) -&gt; SummarizeCustomerRetentionCaseResponse:\n    \"\"\"\n    Summarizes the current retention case for a given customer, including behavioral patterns, sentiment signals,\n    and recommended actions if relevant.\n    \"\"\"\n    return SummarizeCustomerRetentionCaseResponse(\n        summary=f\"Customer {payload.customer_id} has shown declining engagement and expressed dissatisfaction. Retention plan recommended.\"\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/agentic-use-cases/retention-strategist/build/#apisvisualize_customer_journey_mappy","title":"apis/visualize_customer_journey_map.py","text":"<pre><code># apis/visualize_customer_journey_map.py\n\nimport practicuscore as prt\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\n\n\nclass InteractionPoint(BaseModel):\n    date: str = Field(..., description=\"Date of the customer interaction.\")\n    channel: str = Field(..., description=\"Communication channel used (e.g., email, phone, chat).\")\n    sentiment_score: float = Field(..., description=\"Sentiment score for the interaction, from -1 (negative) to 1 (positive).\")\n    action_taken: Optional[str] = Field(None, description=\"Optional description of any action taken by the company.\")\n\nclass JourneyMap(BaseModel):\n    customer_id: str = Field(..., description=\"Unique identifier of the customer.\")\n    timeline: List[InteractionPoint] = Field(..., description=\"Chronological list of interaction points for this customer.\")\n    risk_flag: Optional[str] = Field(None, description=\"Optional churn or escalation flag for this customer.\")\n    recommendation: Optional[str] = Field(None, description=\"Summary recommendation based on the journey.\")\n\nclass VisualizeCustomerJourneyRequest(BaseModel):\n    customer_id: str = Field(..., description=\"ID of the customer whose journey should be visualized.\")\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"example\": {\n                \"customer_id\": \"C457\"\n            }\n        }\n    }\n\nclass VisualizeCustomerJourneyResponse(BaseModel):\n    journey_map: JourneyMap = Field(..., description=\"Customer journey map with annotated interactions.\")\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"example\": {\n                \"journey_map\": {\n                    \"customer_id\": \"C457\",\n                    \"timeline\": [\n                        {\n                            \"date\": \"2024-01-10\",\n                            \"channel\": \"email\",\n                            \"sentiment_score\": -0.4,\n                            \"action_taken\": \"Apology email sent\"\n                        },\n                        {\n                            \"date\": \"2024-01-15\",\n                            \"channel\": \"phone\",\n                            \"sentiment_score\": 0.1,\n                            \"action_taken\": \"Escalated to supervisor\"\n                        },\n                        {\n                            \"date\": \"2024-01-20\",\n                            \"channel\": \"chat\",\n                            \"sentiment_score\": 0.7,\n                            \"action_taken\": \"Issue resolved\"\n                        }\n                    ],\n                    \"risk_flag\": \"Escalation\",\n                    \"recommendation\": \"Maintain proactive follow-up for next 30 days.\"\n                }\n            }\n        }\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    risk_profile=prt.APIRiskProfile.Medium,\n    interactive=True\n)\n\n\n@prt.api(\"/visualize-customer-journey-map\", spec=api_spec)\nasync def run(payload: VisualizeCustomerJourneyRequest) -&gt; VisualizeCustomerJourneyResponse:\n    \"\"\"\n    Create a structured journey map for a customer by compiling their chronological\n    interaction data, sentiment trends, and key actions. This map can be used for\n    visualization in dashboards or reports.\n\n    Args:\n        payload (VisualizeCustomerJourneyRequest): Contains the customer_id.\n\n    Returns:\n        VisualizeCustomerJourneyResponse: Timeline of interactions and recommendations.\n    \"\"\"\n    # Dummy timeline for demo (in real case, fetch from DB or context)\n    timeline = [\n        InteractionPoint(\n            date=\"2024-01-10\",\n            channel=\"email\",\n            sentiment_score=-0.4,\n            action_taken=\"Apology email sent\"\n        ),\n        InteractionPoint(\n            date=\"2024-01-15\",\n            channel=\"phone\",\n            sentiment_score=0.1,\n            action_taken=\"Escalated to supervisor\"\n        ),\n        InteractionPoint(\n            date=\"2024-01-20\",\n            channel=\"chat\",\n            sentiment_score=0.7,\n            action_taken=\"Issue resolved\"\n        )\n    ]\n\n    return VisualizeCustomerJourneyResponse(\n        journey_map=JourneyMap(\n            customer_id=payload.customer_id,\n            timeline=timeline,\n            risk_flag=\"Escalation\",\n            recommendation=\"Maintain proactive follow-up for next 30 days.\"\n        )\n    )\n</code></pre> <p>Previous: Build | Next: Hr &gt; Build</p>"},{"location":"technical-tutorial/extras/generative-ai/ai-assistants/ai-assistants/","title":"Dynamically Accessing AI Assistant Metadata","text":"<p>Instead of manually configuring AI Assistant URLs, you can use the Practicus AI SDK to dynamically fetch available AI Assistants associated with your user account.</p> <pre><code>import json\nimport practicuscore as prt\n</code></pre> <pre><code>region = prt.current_region()\nai_assistants = region.get_ai_assistants()\n</code></pre> <pre><code>print(\"Available AI Assistants:\")\nfor assistant in ai_assistants:\n    print(json.dumps(assistant, indent=4))\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/ai-assistants/ai-assistants/#example-selecting-the-first-ai-assistant","title":"Example: Selecting the First AI Assistant","text":"<pre><code>selected_ai_assistant = ai_assistants[0]\n</code></pre> <pre><code>from practicuscore.gen_ai import AIAssistantHelper\n</code></pre> <pre><code>assistant_url = AIAssistantHelper.get_api_endpoint(selected_ai_assistant, region)\nassistant_token = None  # Get a new token, or reuse existing if not expired.\nassistant_token = AIAssistantHelper.get_api_token(\n    assistant_config=selected_ai_assistant, region=region, token=assistant_token\n)\n</code></pre> <pre><code>print(\"Use the following URL for API requests:\")\nprint(assistant_url)\n\nprint(\"API Interface Type (Auto, OpenAI, or Langchain):\")\nprint(selected_ai_assistant[\"api_interface\"])\n\nprint(\"Bearer Token for API authentication:\")\nprint(assistant_token)\n</code></pre> <p>Previous: Lang Chain LLM Model | Next: Mobile Banking &gt; Mobile-Banking</p>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/","title":"Cv Assistant","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None  # E.g. 'company.practicus.com'\nembedding_model_path = None\nmodel_name = None\nmodel_prefix = None\n\nvector_store = None  # ChromaDB or MilvusDB\n\nif vector_store == \"MilvusDB\":\n    milvus_uri = None  # E.g. 'company.practicus.milvus.com'\n</code></pre> <pre><code>assert host, \"Please enter your host url\"\nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert model_name, \"Please enter your embedding model_name.\"\nassert model_prefix, \"Please enter your embedding model_prefix.\"\n\n# You can use one of ChromaDB or MilvusDB as vector store\nassert vector_store in [\"ChromaDB\", \"MilvusDB\"], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"\n\nif vector_store == \"MilvusDB\":\n    assert \"milvus_uri\", \"Please enter your milvus connection uri\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#firstly-we-need-install-transformers-and-torch","title":"Firstly we need install transformers and torch","text":"<p>Run at terminal:</p> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#pip-install-transformers-sentence-transformers-langchain-langchain-community-chromadb","title":"pip install transformers sentence-transformers langchain langchain-community chromadb","text":"<ul> <li> <p>Transformers: It allows you to easily use Transformer-based models (such as BERT, GPT, etc.).-</p> </li> <li> <p>Sentence-Transformers: It produces vector representations of sentences using Transformer models.</p> </li> <li> <p>LangChain: It is used to manage more complex workflows with language models. </p> </li> <li> <p>Langchain-Community: Contains additional modules and components developed by the community for the LangChain library.</p> </li> <li> <p>ChromaDB: Used as a vector database. It is optimized for embeddings and similarity searches.</p> </li> <li> <p>PyPDF: A library used to process PDF files with Python. </p> </li> </ul>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#pip-install-torch-index-url-httpsdownloadpytorchorgwhlcpu","title":"pip install torch --index-url https://download.pytorch.org/whl/cpu","text":"<p>This command is used to install the PyTorch library with CPU support.</p> <p>Details:</p> <ul> <li>Torch (PyTorch): A library used for developing machine learning and deep learning models. It offers features such as tensor computations, automatic differentiation, and advanced modeling.</li> <li>--index-url https://download.pytorch.org/whl/cpu: This parameter uses a specific index URL to download the CPU version of PyTorch. If you do not want to install a GPU-specific version, this URL is used.</li> </ul> <pre><code># Prepare data\n\nimport os\nimport requests\n\nrepo_owner = \"practicusai\"\nrepo_name = \"sample-data\"\nfile_path = \"hr_assistant\"\nbranch = \"main\"\n\n\nurl = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{file_path}?ref={branch}\"\n\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    files = response.json()\n\n    for file in files:\n        file_url = file[\"download_url\"]\n        file_name = file[\"name\"]\n\n        file_response = requests.get(file_url)\n        if file_response.status_code == 200:\n            with open(file_name, \"wb\") as f:\n                f.write(file_response.content)\n            print(f\"'{file_name}' successfully downloaded.\")\n        else:\n            print(f\"'{file_name}' not successfully downloaded.\")\nelse:\n    print(f\"HTTP status: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#import-libraries","title":"Import Libraries","text":"<pre><code>from langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\nimport pandas as pd\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#define-llm-api-function-and-call-chatpracticus-in-this-function","title":"Define llm api function and call ChatPracticus in this function","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#function-call_llm_api","title":"Function: <code>call_llm_api</code>","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#description","title":"Description","text":"<p>This function interacts with the ChatPracticus API to invoke a response from a language model using the provided inputs, API URL, and API token. The function is designed to send data to the API and retrieve the response content.</p>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#parameters","title":"Parameters","text":"<ul> <li> <p><code>inputs</code>:   The input query or data to be sent to the API for processing. This is typically a string or JSON object depending on the API's requirements.</p> </li> <li> <p><code>api_url</code>:   The endpoint URL of the ChatPracticus API. This is the location where the API call will be directed.</p> </li> <li> <p><code>api_token</code>:   The authentication token for accessing the ChatPracticus API. This ensures secure communication and proper authorization.</p> </li> <li> <p>The function initializes a <code>ChatPracticus</code> object with the specified API URL and token. The <code>model_id</code> parameter is currently unused or ignored but can be included for future model-specific configurations.</p> </li> <li> <p>The <code>invoke</code> method of the <code>ChatPracticus</code> object is called with the given input. This sends the query to the API and retrieves the response.</p> </li> <li> <p>The function returns the <code>content</code> attribute of the response, which contains the text generated by the language model.</p> </li> </ul> <pre><code>def call_llm_api(inputs, api_url, api_token):\n    # We need to give input to 'generate_response'. This function will use our 'api_token' and 'endpoint_url' and return the response.\n\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n    :params api_token: Token of our model.\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n\n    return response.content\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#get-all-resumes-and-use-seperator-for-split-questions","title":"Get all resumes and use seperator for split questions","text":"<ol> <li><code>df = pd.read_csv(\"HR.csv\")</code>:  </li> <li>Reads the CSV file <code>HR.csv</code> into a pandas DataFrame called <code>df</code>.  </li> <li> <p>The DataFrame should contain a column named <code>Resume_str</code>.</p> </li> <li> <p><code>merged_resumes = ''</code>:  </p> </li> <li> <p>Initializes an empty string <code>merged_resumes</code>, which will store the concatenated resume data.</p> </li> <li> <p>For Loop:</p> </li> <li>Iterates over each resume string in the <code>Resume_str</code> column of the DataFrame.  </li> <li>Appends each resume to the <code>merged_resumes</code> string, preceded by the delimiter <code>//m-n-m//</code>.</li> </ol>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#final-output","title":"Final Output","text":"<ul> <li>The variable <code>merged_resumes</code> contains all resumes concatenated into a single string, with <code>//m-n-m//</code> acting as a separator between each resume.</li> </ul> <pre><code>df = pd.read_csv(\"HR.csv\")\nmerged_resumes = \"\"\nfor resume in df[\"Resume_str\"]:\n    merged_resumes = merged_resumes + \"//m-n-m//\" + resume\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#description_1","title":"Description","text":"<p>This function processes a concatenated string of resumes, splits them into individual documents based on a delimiter, and further divides these documents into smaller chunks for analysis. The function utilizes a <code>CharacterTextSplitter</code> to handle the chunking process.</p> <ol> <li>Split the Resumes:</li> <li> <p>The input <code>merged_resumes</code> is split into individual resume strings using the <code>//m-n-m//</code> delimiter.</p> </li> <li> <p>Create Document Objects:</p> </li> <li> <p>Each resume is transformed into a <code>Document</code> object. Empty or whitespace-only resumes are excluded.</p> </li> <li> <p>Initialize the Text Splitter:</p> </li> <li> <p>A <code>CharacterTextSplitter</code> is set up with the following configuration:</p> <ul> <li><code>separator=\"//m-n-m//\"</code>: The delimiter used for splitting.</li> <li><code>chunk_size</code>: Controls the maximum size of each text chunk.</li> <li><code>chunk_overlap</code>: Adds overlapping text between chunks for better context retention.</li> </ul> </li> <li> <p>Split Documents:</p> </li> <li> <p>The documents are further divided into smaller chunks using the <code>CharacterTextSplitter</code>.</p> </li> <li> <p>Aggregate Results:</p> </li> <li>The chunks are appended to the <code>all_docs</code> list, which is returned as the final output.</li> </ol>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#returns","title":"Returns","text":"<ul> <li><code>all_docs</code>:   A list of smaller text chunks, each represented as a document object, ready for further processing.</li> </ul> <pre><code>def load_and_split_resumes(merged_resumes, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load and split email strings into chunks.\n\n    :param merged_resumes: A single string containing all resumes contents, separated by '//m-n-m//'.\n    :param chunk_size: The maximum number of characters in each text chunk.\n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = CharacterTextSplitter(\n        separator=\"//m-n-m//\",  # Defines the separator used to split the text.\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False,\n    )\n\n    # Split resumes\n    resumes = merged_resumes.split(\"//m-n-m//\")\n\n    # Transform to Document\n    documents = [Document(page_content=resume.strip()) for resume in resumes if resume.strip()]\n\n    # Split docs\n    split_docs = text_splitter.split_documents(documents)\n    all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre> <ol> <li>Function Call:</li> <li> <p><code>load_and_split_resumes(merged_resumes)</code>:</p> <ul> <li>The <code>merged_resumes</code> string, containing all resumes concatenated and separated by <code>//m-n-m//</code>, is passed to the <code>load_and_split_resumes</code> function.</li> <li>This function:</li> <li>Splits the resumes into individual documents.</li> <li>Further chunks each document into smaller pieces based on the defined <code>chunk_size</code> and <code>chunk_overlap</code> parameters.</li> </ul> </li> <li> <p>Output:</p> </li> <li>The result of the function is assigned to the variable <code>text_chunks</code>.</li> <li><code>text_chunks</code> is a list of chunked text segments, ready for downstream processing or analysis.</li> </ol> <pre><code># Create our chunks\ntext_chunks = load_and_split_resumes(merged_resumes)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#create-the-vector-store-and-model-path-is-given","title":"Create the vector store and model path is given","text":"<ol> <li>Condition Check:</li> <li> <p><code>if vector_store == 'ChromaDB':</code></p> <ul> <li>Ensures that the code block is executed only if <code>ChromaDB</code> is selected as the vector store.</li> </ul> </li> <li> <p><code>create_chroma_vector_store</code> Function:</p> </li> <li> <p>This function generates embeddings for the provided text chunks and creates a Chroma vector store.</p> </li> <li> <p>Assign Retriever:</p> </li> <li><code>retriever_resumes = create_chroma_vector_store(text_chunks, embedding_model_path)</code>:<ul> <li>Calls the <code>create_chroma_vector_store</code> function with <code>text_chunks</code> and the embeddings model path to generate the retriever.</li> </ul> </li> </ol> <pre><code>if vector_store == \"ChromaDB\":\n    # Generate embeddings and create vector store\n    def create_chroma_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n            encode_kwargs={\"normalize_embeddings\": False},\n        )\n\n        db_name = str(random.random())\n        vectorstore_resumes = Chroma.from_documents(\n            collection_name=db_name, documents=chunks, embedding=embeddings\n        )  # This method creates a vector store from the provided documents (chunks) and embeddings.\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_resumes = vectorstore_resumes.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n        return retriever_resumes\n\n    retriever_resumes = create_chroma_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#optional-milvus-vector-db","title":"(OPTIONAL) Milvus Vector DB","text":"<ol> <li>Condition Check:</li> <li> <p><code>if vector_store == 'MilvusDB':</code></p> <ul> <li>Ensures that the Milvus-based vector store logic is executed only when <code>MilvusDB</code> is selected.</li> </ul> </li> <li> <p><code>create_milvus_vector_store</code> Function:</p> </li> <li> <p>This function generates embeddings for the provided text chunks and stores them in a Milvus vector database.</p> </li> <li> <p>Inputs:</p> <ul> <li><code>chunks</code>: The text chunks to be embedded and stored.</li> <li><code>embeddings_model_path</code>: Path to the pre-trained embeddings model.</li> </ul> </li> <li> <p>Steps:</p> <ul> <li>Generate Embeddings:</li> <li><code>HuggingFaceEmbeddings</code> generates embeddings for the text chunks.</li> <li> <p>Parameters:</p> <ul> <li><code>model_name</code>: Path to the pre-trained embeddings model.</li> <li><code>model_kwargs</code>: Specifies the device to run the model (<code>'cpu'</code> in this case).</li> <li><code>encode_kwargs</code>: Optional configuration for encoding (e.g., <code>normalize_embeddings</code>).</li> </ul> </li> <li> <p>Connect to Milvus:</p> </li> <li><code>connections.connect</code> establishes a connection to the Milvus server.</li> <li> <p>Parameters:</p> <ul> <li><code>host</code>: The URI of the Milvus server.</li> <li><code>port</code>: Default port for Milvus, <code>19530</code>.</li> </ul> </li> <li> <p>Create Vector Store:</p> </li> <li><code>Milvus.from_documents</code> creates a vector store with:<ul> <li><code>documents</code>: The input text chunks.</li> <li><code>embedding</code>: The generated embeddings.</li> <li>`collection_</li> </ul> </li> </ul> </li> </ol> <pre><code>from langchain_milvus import Milvus\nfrom pymilvus import connections\n\nif vector_store == \"MilvusDB\":\n\n    def create_milvus_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n            encode_kwargs={\"normalize_embeddings\": False},\n        )\n\n        connections.connect(\"default\", host=milvus_uri, port=\"19530\")\n\n        vectorstore = Milvus.from_documents(\n            documents=chunks,\n            embedding=embeddings,\n            collection_name=\"langchain_example\",\n            connection_args={\"uri\": f\"https://{milvus_uri}:19530\"},\n            drop_old=True,  # Drop the old Milvus collection if it exists\n        )\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_resumes = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n        return retriever_resumes\n\n    retriever_resumes = create_milvus_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#define-format_docs-for-join-all-chunks","title":"Define format_docs for join all chunks","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#description_2","title":"Description","text":"<p>This function processes a list of document objects, extracts their <code>page_content</code> attributes, and concatenates all the document contents into a single string. Each document's content is separated by two newline characters (<code>\\n\\n</code>) to enhance readability.</p> <ol> <li>Extract Document Content:</li> <li> <p>Iterates over the list <code>docs</code> and retrieves the <code>page_content</code> attribute from each document object.</p> </li> <li> <p>Join Content:</p> </li> <li> <p>Combines all extracted content into a single string, with each document's content separated by two newline characters (<code>\\n\\n</code>).</p> </li> <li> <p>Return Result:</p> </li> <li>The resulting string is returned for use in other parts of the application.</li> </ol> <pre><code>def format_docs(docs):\n    # Retrieves the content of each document in the `docs` list and joins the content of all documents into a single string, with each document's content separated by two newline characters.\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#all-chains-merged-into-each-other-at-this-function","title":"All chains merged into each other at this function","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#description_3","title":"Description","text":"<p>This function retrieves relevant documents for a given question using a retriever, formats a prompt with the documents and question, and queries a language model API to generate an answer.</p> <ol> <li>Define Prompt Template:</li> <li>A <code>PromptTemplate</code> object is created to format the input for the language model.</li> <li> <p>Template Details:</p> <ul> <li>Includes retrieved context (relevant documents).</li> <li>Contains the question.</li> <li>Instructs the model to respond with \"I don't know\" if the answer is unavailable.</li> </ul> </li> <li> <p>Retrieve Relevant Documents:</p> </li> <li> <p>Calls <code>retriever.get_relevant_documents(question)</code> to fetch documents most relevant to the provided question.</p> </li> <li> <p>Format the Retrieved Documents:</p> </li> <li> <p>The retrieved documents are passed to <code>format_docs</code> to be concatenated into a single string, separated by two newline characters.</p> </li> <li> <p>Construct the Prompt:</p> </li> <li> <p>The <code>PromptTemplate</code> is used to format the prompt with the retrieved context and the question.</p> </li> <li> <p>Query the API:</p> </li> <li> <p>Calls the <code>call_llm_api</code> function with the formatted prompt, <code>api_url</code>, and <code>api_token</code> to query the language model.</p> </li> <li> <p>Extract and Return the Answer:</p> </li> <li>Extracts the answer from the API response by splitting the output on the keyword <code>Answer:</code> and removing extra whitespace.</li> </ol> <pre><code>def query_resume(retriever, question, api_url, api_token):\n    prompt_template = PromptTemplate(  # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=(  # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        ),\n    )\n\n    docs = retriever.get_relevant_documents(\n        question\n    )  # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs)  # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question)  # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split(\"Answer:\")[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#chat-examples","title":"Chat Examples","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#code-explanation","title":"Code Explanation","text":"<p>This code snippet imports the <code>practicuscore</code> module and retrieves the current region using the <code>get_region</code> function.</p>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#workflow","title":"Workflow","text":"<ol> <li>Import the Module:</li> <li> <p><code>import practicuscore as prt</code>:</p> <ul> <li>The <code>practicuscore</code> library is imported and aliased as <code>prt</code> for convenience.</li> <li>This module likely provides utility functions or methods for interacting with Practicus-related resources.</li> </ul> </li> <li> <p>Get the Region:</p> </li> <li><code>region = prt.get_region()</code>:<ul> <li>Calls the <code>get_region</code> function from the <code>practicuscore</code> module.</li> <li>This function retrieves information about the current region, which may be used for region-specific configurations or operations.</li> </ul> </li> </ol> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <ol> <li>Construct the API URL:</li> <li> <p><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"</code>:</p> <ul> <li>Uses string formatting to dynamically create the API URL based on:</li> <li><code>host</code>: The base URL or hostname of the server.</li> <li><code>model_prefix</code>: The selected model prefix.</li> <li><code>model_name</code>: The selected model name.</li> <li>The resulting URL specifies the endpoint for accessing the model.</li> </ul> </li> <li> <p>Generate Session Token:</p> </li> <li><code>token = prt.models.get_session_token(api_url=api_url)</code>:<ul> <li>Calls the <code>get_session_token</code> function from the <code>practicuscore.models</code> module.</li> <li>The <code>api_url</code> parameter specifies the endpoint for which the token is generated.</li> <li>The session token ensures secure and authorized communication with the API.</li> </ul> </li> </ol> <pre><code># Create api url and token\napi_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.models.get_session_token(api_url=api_url, token=token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#create-query-and-print-answer","title":"Create query and print answer","text":"<pre><code># Example query\nanswer = query_resume(\n    retriever=retriever_resumes,\n    question=\"What are the leadership qualities of an HR Director?\",\n    api_url=api_url,\n    api_token=token,\n)\n# Get Answer\nprint(answer)\n</code></pre> <p>Previous: Memory Chabot | Next: Deploying LLM &gt; Introduction</p>"},{"location":"technical-tutorial/extras/generative-ai/databases/using-databases/","title":"Using Databases","text":"<pre><code># Before you begin make sure the application deployment settings have a database configuration\napp_deployment_key = \"appdepl\"\napp_prefix = \"apps\"\napp_name = \"my-db-using-app\"\n</code></pre> <pre><code>import os\nimport practicuscore as prt\n</code></pre> <pre><code>db_schema, db_conn_str = prt.apps.prepare_db(\n    prefix=app_prefix,\n    app_name=app_name,\n    deployment_setting_key=app_deployment_key,\n)\n\nprint(\"Created db schema:\", db_schema)\n# print(\"Db connection string:\", db_conn_str)\nprint(\"*** Carefully save the connection string since it will not be displayed again. ***\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/databases/using-databases/#recommended-saving-db-conenction-string-to-vault","title":"(Recommended) Saving db conenction string to Vault","text":"<pre><code>assert db_conn_str, \"db_conn_str not defined\"\n\nprt.vault.create_or_update_secret(\"MY_APP_CONN_STR\", db_conn_str)\n# prt.db. methods will always look for 'PRT_DB_CONN_STR' OS env variable.\n# Remember to restart notebook kernel if any OS env variable changes\nos.environ[\"PRT_DB_CONN_STR\"] = db_conn_str\n</code></pre> <pre><code># Reloading notebook? read from vault\ndb_conn_str, age = prt.vault.get_secret(\"MY_APP_CONN_STR\")\nos.environ[\"PRT_DB_CONN_STR\"] = db_conn_str\n</code></pre> <pre><code># Made a mistake? Only in a development environment, you can reset the db with the below.\n# prt.apps.remove_db() will delete the schema dedicated to the app\nconfirm = input(f\"ALL DATA WILL BE DELETED for '{app_prefix}/{app_name}'\\nContinue? (y/n)\")\n\nif confirm == \"y\":\n    prt.apps.remove_db(\n        prefix=app_prefix,\n        app_name=app_name,\n        deployment_setting_key=app_deployment_key,\n    )\n</code></pre> <pre><code>print(\"Scaffolding db folder\")\nprt.db.init()\n</code></pre> <pre><code># open db/models.py and uncomment Hero class\nprt.db.revision(\"initial tables\")\n</code></pre> <p>Note: Always review <code>db/migrations/versions</code> files</p> <pre><code># Apply the changes to the database.\n# This is for dev/test. For production, you can have the deployed application auto-migrate db.\nprt.db.upgrade()\n# offline = True only generates SQL\n# prt.db.upgrade(offline=True)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/databases/using-databases/#upserting-static-data","title":"Upserting static data","text":"<p>To upsert (insert, if exists update) enum-line tables to the database, open <code>db/static_data.py</code> and uncomment import, list generation lines.</p> <pre><code>result = prt.db.upsert_static_data()\n\nprint(\"Upsert operation sumamry:\")\nfor table, rows in result.items():\n    print(\"Upserted\", rows, \"rows for table model:\", table)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/databases/using-databases/#deploying-database-usign-app","title":"Deploying database usign app","text":"<p>Deploying an application will automatically upgrade the database</p> <p>Note: if you are deploying on a separate database server, the database schema/roles must be ready either  you can run prt.apps.prepare_db() or manually create the db schema. See bittim of this page to learn how.</p> <pre><code>import practicuscore as prt\n\nvisible_name = \"My DB using app\"\ndescription = \"An app that uses a database..\"\nicon = \"fa-database\"\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n    # Database related\n    personal_secrets=[\"MY_APP_CONN_STR:PRT_DB_CONN_STR\"],\n)\n\nprint(\"Booting UI :\", app_url)\nprint(\"Booting API:\", api_url)\n</code></pre> <pre><code># open db/models.py, add new columns, make other changes\nprt.db.revision(\"added columns\")\n</code></pre> <pre><code>prt.db.upgrade()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/databases/using-databases/#using-database-in-run-time","title":"Using database in run-time","text":"<pre><code>from db import Hero\nfrom sqlmodel import Session, create_engine\n\n\ndef add_rows():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)\n\n    engine = create_engine(os.environ[\"PRT_DB_CONN_STR\"], echo=True)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n        session.commit()\n</code></pre> <pre><code>add_rows()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/databases/using-databases/#manual-db-schema-creation","title":"Manual DB Schema creation","text":"<ul> <li>If you would like to prepare the database manually, please run the below.</li> <li><code>schema_name</code> must be in the following format <code>prefix__app_name</code> where <code>-</code> an <code>/</code> is replace with a <code>_</code></li> <li>Examples:<ul> <li>prefix=<code>apps</code> and app_name=<code>my-db-using-app</code> schema_name would be <code>apps__my_db_using_app</code></li> <li>prefix=<code>apps/finance</code> and app_name=<code>some-app</code> schema_name would be <code>apps_finance__some_app</code></li> </ul> </li> </ul> <pre><code>CREATE SCHEMA schema_name\nCREATE ROLE schema_name LOGIN PASSWORD '__add_password_here__';\nALTER ROLE schema_name IN DATABASE db_name SET search_path TO schema_name;\nGRANT ALL ON SCHEMA schema_name TO schema_name;\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/databases/using-databases/#static-data-entry","title":"Static data entry","text":"<ul> <li>You can enter static data for enum-like database tables by using db/static_data.py</li> </ul> <pre><code># You can upsert (insert, or update if exists) static data to your tables on application start\n# First import your table models\nfrom .models import EnumLikeModel\n\ndef get_static_data():\n    # Then add rows for 'enum like' tables with static primary keys\n    static_data = [\n        EnumLikeModel(key=\"primary_key_1\", name=\"Some Name\"),\n        EnumLikeModel(key=\"primary_key_2\", name=\"Some Other Name\"),\n        # EnumLikeModel2(...\n    ]\n\n    return static_data\n\n# Finally, call prt.db.upsert_static_data() to upsert the data in design time\n# This is automatically done in run-time during application start.\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/databases/using-databases/#notes-on-column-renames-default-values","title":"Notes on column renames, default values","text":"<ul> <li>prt.db.revision() might not detect column renames. Instead it will delete the old column, and create a new one. For renames, please review the generated code under <code>db/migrations/versions</code> and change as needed.</li> <li>prt.db.revision() might not detect default values of columns. To add default values, please review the generated code under <code>db/migrations/versions</code> and change as needed.</li> </ul> <p>Previous: API Triggers For Airflow | Next: Prtchatbot &gt; Prtchatbot</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/","title":"LLM Deployment Tutorial","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#welcome-to-the-llm-deployment-tutorial-this-guide-is-designed-to-provide-you-with-a-comprehensive-understanding-of-deploying-large-language-models-llms-and-creating-api-endpoints-tailored-to-various-use-cases-the-tutorial-is-divided-into-three-sections-each-addressing-a-specific-deployment-scenario-by-following-these-steps-you-will-learn-both-foundational-and-advanced-deployment-techniques-including-integration-with-practicusais-powerful-sdk-for-enhanced-functionality","title":"Welcome to the LLM Deployment Tutorial. This guide is designed to provide you with a comprehensive understanding of deploying Large Language Models (LLMs) and creating API endpoints tailored to various use cases. The tutorial is divided into three sections, each addressing a specific deployment scenario. By following these steps, you will learn both foundational and advanced deployment techniques, including integration with PracticusAI's powerful SDK for enhanced functionality.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#preparation","title":"Preparation","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#before-deploying-an-llm-it-is-essential-to-prepare-the-model-and-its-environment-this-section-outlines-the-steps-for-downloading-an-open-source-llm-from-hugging-face-and-uploading-it-to-an-object-storage-system-this-preparation-is-critical-as-the-model-host-service-will-download-the-llm-from-object-storage-when-running-for-the-first-time","title":"Before deploying an LLM, it is essential to prepare the model and its environment. This section outlines the steps for downloading an open-source LLM from Hugging Face and uploading it to an object storage system. This preparation is critical, as the model host service will download the LLM from object storage when running for the first time.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#basic-llm-deployment","title":"Basic LLM Deployment","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#in-this-section-we-will-explore-how-to-deploy-an-open-source-llm-and-create-an-api-endpoint-using-only-the-transformers-library-this-straightforward-approach-is-ideal-for-scenarios-where-a-standalone-llm-api-is-needed-without-the-additional-complexity-of-pipeline-integrations-or-dependencies-on-the-practicusai-sdk-by-the-end-of-this-section-you-will-have","title":"In this section, we will explore how to deploy an open-source LLM and create an API endpoint using only the Transformers library. This straightforward approach is ideal for scenarios where a standalone LLM API is needed without the additional complexity of pipeline integrations or dependencies on the PracticusAI SDK. By the end of this section, you will have:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-a-deployed-open-source-llm","title":"- A deployed open-source LLM.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-a-functional-api-endpoint-to-interact-with-the-model","title":"- A functional API endpoint to interact with the model.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-insights-into-managing-a-basic-deployment-workflow","title":"- Insights into managing a basic deployment workflow.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#while-this-setup-is-not-designed-for-langchain-compatibility-it-serves-as-a-foundational-building-block-for-standalone-applications","title":"While this setup is not designed for LangChain compatibility, it serves as a foundational building block for standalone applications.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#langchain-suitable-llm-deployment","title":"LangChain-Suitable LLM Deployment","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#this-section-focuses-on-deploying-an-llm-and-creating-an-api-endpoint-that-integrates-seamlessly-with-langchain-pipelines-leveraging-the-practicusai-sdk-this-method-provides-advanced-functionality-and-ensures-compatibility-with-langchain-operations-key-sdk-methods-you-will-learn-include","title":"This section focuses on deploying an LLM and creating an API endpoint that integrates seamlessly with LangChain pipelines. Leveraging the PracticusAI SDK, this method provides advanced functionality and ensures compatibility with LangChain operations. Key SDK methods you will learn include:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-prtlangmessage-structuring-communication-flows-with-the-hosted-llm","title":"- PrtLangMessage: Structuring communication flows with the hosted LLM.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-prtlangrequest-sending-structured-messages-to-the-llm-and-requesting-a-response","title":"- PrtLangRequest: Sending structured messages to the LLM and requesting a response.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-prtlangresponse-parsing-and-interpreting-the-responses-from-the-llm","title":"- PrtLangResponse: Parsing and interpreting the responses from the LLM.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-chatpracticus-a-comprehensive-method-that-simplifies-the-entire-interaction-process-for-langchain-integration","title":"- ChatPracticus: A comprehensive method that simplifies the entire interaction process for LangChain integration.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#by-the-end-of-this-section-you-will-have","title":"By the end of this section, you will have:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-a-fully-deployed-llm-api-endpoint","title":"- A fully deployed LLM API endpoint.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-compatibility-with-langchain-pipelines","title":"- Compatibility with LangChain pipelines.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-practical-knowledge-of-using-the-practicusai-sdk-to-enhance-deployment","title":"- Practical knowledge of using the PracticusAI SDK to enhance deployment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#combined-usage","title":"Combined Usage","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#in-the-final-section-we-bring-together-the-methods-covered-in-the-previous-sections-this-comprehensive-tutorial-demonstrates-how-to-combine-the-basic-llm-deployment-and-langchain-suitable-llm-deployment-approaches-to-create-versatile-api-endpoints-whether-you-need-standalone-functionality-or-seamless-integration-with-langchain-this-section-will-equip-you-with-the-skills-to","title":"In the final section, we bring together the methods covered in the previous sections. This comprehensive tutorial demonstrates how to combine the Basic LLM Deployment and LangChain-Suitable LLM Deployment approaches to create versatile API endpoints. Whether you need standalone functionality or seamless integration with LangChain, this section will equip you with the skills to:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-transition-between-basic-and-advanced-deployments","title":"- Transition between basic and advanced deployments.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-combine-deployment-strategies-for-maximum-flexibility","title":"- Combine deployment strategies for maximum flexibility.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-optimize-workflows-for-diverse-application-requirements","title":"- Optimize workflows for diverse application requirements.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#by-completing-this-tutorial-you-will-gain-a-deep-understanding-of-llm-deployment-techniques-and-how-to-leverage-practicusai-sdk-for-enhanced-functionality-and-integration","title":"By completing this tutorial, you will gain a deep understanding of LLM deployment techniques and how to leverage PracticusAI SDK for enhanced functionality and integration.","text":"<p>Previous: Cv Assistant | Next: Preparation &gt; Model Download</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/","title":"Model Download","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>file_path = None  # e.g. /home/ubuntu/shared/LLM-Models/llama-1B-instruct\nREPO_ID = None  # e.g. meta-llama/Llama-3.2-1B-Instruct\nhf_token = None  # for details checkout step 1\n</code></pre> <pre><code>assert file_path, \"Please enter a file_path\"\nassert REPO_ID, \"Please enter a REPO_ID\"\nassert hf_token, \"Please enter a hf_token\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#model-download","title":"Model Download","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#the-first-step-in-llm-deployment-involves-downloading-the-pre-trained-model-these-steps-include","title":"The first step in llm-deployment involves downloading the pre-trained model. These steps include:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#1-authenticating-with-huggingface-hub-to-securely-access-model-repositories","title":"1. Authenticating with HuggingFace Hub to securely access model repositories.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#2-getting-access-to-an-open-source-llm-model-from-hugging-face","title":"2. Getting access to an open source llm model from Hugging Face.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#3-downloading-the-pre-trained-open-source-llm-eg-llama-3b-instruct-llama-1b-instruct-from-the-hugging-face-hub-for-deployment","title":"3. Downloading the pre-trained open source LLM (e.g., LLaMA-3B-Instruct, LLaMA-1B-Instruct) from the Hugging Face Hub for deployment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#step-1-authenticating-with-hugging-face","title":"Step-1: Authenticating with Hugging Face","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#to-download-models-from-the-hugging-face-hub-you-must-authenticate-using-your-personal-api-token-this-step-ensures-secure-authorized-access-to-both-public-and-private-model-repositories","title":"To download models from the Hugging Face Hub, you must authenticate using your personal API token. This step ensures secure, authorized access to both public and private model repositories.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#obtain-your-api-token","title":"Obtain Your API Token","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#follow-the-instructions-in-the-hugging-face-documentation-to-generate-or-retrieve-your-api-token-keep-this-token-confidential-and-do-not-share-it-publicly","title":"Follow the instructions in the Hugging Face documentation to generate or retrieve your API token. Keep this token confidential and do not share it publicly.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#authenticate-your-environment","title":"Authenticate Your Environment","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#after-acquiring-your-token-run-the-following-code-to-authenticate-this-will-enable-seamless-access-to-the-hugging-face-hub-and-allow-you-to-download-and-work-with-models-directly-in-your-environment","title":"After acquiring your token, run the following code to authenticate. This will enable seamless access to the Hugging Face Hub and allow you to download and work with models directly in your environment.","text":"<pre><code>from huggingface_hub.hf_api import HfFolder\n\nHfFolder.save_token(hf_token)  # Replace with your Hugging Face API token\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#step-2-getting-access-to-an-open-source-llm-model","title":"Step-2: Getting access to an open source llm model","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#second-you-need-to-obtain-access-to-download-the-model-visit-metas-llama-huggingface-page-and-request-access","title":"Second, you need to obtain access to download the model. Visit Meta's LLaMA HuggingFace page and request access.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#httpshuggingfacecometa-llamallama-32-1b-instruct","title":"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#once-approved-you-will-receive-an-e-mail-with-conformation-link-resembling-the-following-format","title":"Once approved, you will receive an e-mail with conformation link, resembling the following format:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#httpshuggingfacecoemail_confirmationbbfx","title":"https://huggingface.co/email_confirmation/bBFX...","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#step-3-install-dependencies-and-download-the-model","title":"Step 3: Install Dependencies and Download the Model","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#to-download-a-model-from-hugging-face-hub-we-can-use-the-snapshot_download-method-like-down-below","title":"To download a model from Hugging Face Hub we can use the snapshot_download method like down below,","text":"<pre><code>from huggingface_hub import snapshot_download\n\n\nsnapshot_download(repo_id=REPO_ID, local_dir=file_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#and-to-consume-the-model-we-should-install-the-necessary-libraries-to-host-service-image","title":"And to consume the model, we should install the necessary libraries to host service image,","text":"<pre><code>pip install transformers sentence-transformers langchain langchain-community chromadb pypdf\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#if-youre-unsure-how-to-install-the-necessary-libraries-to-host-the-service-we-recommend-referring-to-our-comprehensive-documentation-on-installing-libraries-within-the-practicus-ai-environment-it-provides-clear-step-by-step-instructions-to-ensure-a-smooth-setup-process","title":"If you're unsure how to install the necessary libraries to host the service, we recommend referring to our comprehensive documentation on installing libraries within the Practicus AI environment. It provides clear, step-by-step instructions to ensure a smooth setup process.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#it-is-also-necessary-to-install-the-torch-according-to-the-system-you-can-check-the-torch-installation-document","title":"It is also necessary to install the torch according to the system, you can check the Torch Installation Document.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#step-4-upload-cache-to-object-storage","title":"Step 4: Upload Cache to Object Storage","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#using-the-upload_downloadipynb-notebook-upload-the-downloaded-large-language-model-directory-to-your-object-storage-this-action-facilitates-easy-access-to-the-model-and-its-dependencies","title":"Using the upload_download.ipynb notebook, upload the downloaded large language model directory to your object storage. This action facilitates easy access to the model and its dependencies.","text":"<p>Previous: Introduction | Next: Upload LLM</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/","title":"Upload to Object Storage","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#in-this-guide-well-walk-through-the-process-of-uploading-and-downloading-model-related-files-to-and-from-an-object-storage-solution-using-aws-s3-as-an-example-this-functionality-is-essential-for-deploying-and-managing-models-in-the-practicus-ai-environment-allowing-you-to-efficiently-handle-model-files-configurations-and-other-necessary-assets-we-will-use-the-boto3-library-for-interacting-with-aws-services-specifically-s3-for-object-storage","title":"In this guide, we'll walk through the process of uploading and downloading model-related files to and from an object storage solution using AWS S3 as an example. This functionality is essential for deploying and managing models in the Practicus AI environment, allowing you to efficiently handle model files, configurations, and other necessary assets. We will use the boto3 library for interacting with AWS services, specifically S3 for object storage.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>_aws_access_key_id = None\n_aws_secret_access_key = None\n_bucket = None  # The name of your target bucket, e.g. \"my-data-bucket\"\n_prefix = None  # Your prefix within bucket e.g. cache/llama-1b-instruct/\n\n_folder_path = None  # The local path containing files to upload.\n# e.g. \"/home/ubuntu/shared/LLM-Models/llama-1B-instruct\"\n\n_aws_session_token = None  # (Optional) AWS session token for temporary credentials\n_aws_region = None  # (Optional) Your AWS region. If unknown, you may leave it as None.\n_endpoint_url = None  # (Optional) Endpoint URL for S3-compatible services (e.g., MinIO API URL)\n\n_prefix = None  # (Optional) Prefix for organizing objects within the bucket.\n# Use None or \"\" for root-level placement, or specify something\n# like \"folder\" or \"folder/subfolder\" for nested directories.\n\n_source_path_to_cut = None  # (Optional) A prefix within the local folder path\n# that you want to remove from the uploaded object keys.\n# Leave as None to default to the entire folder path.\n</code></pre> <pre><code># Ensure that essential parameters are provided\nassert _aws_access_key_id and _aws_secret_access_key and _bucket and _prefix\n\n# Ensure the folder path is provided\nassert _folder_path\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#step-1-import-required-libraries","title":"Step 1: Import Required Libraries","text":"<pre><code>import os\nimport boto3\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#the-os-module-is-used-for-operating-system-dependent-functionality-like-reading-or-writing-files-whereas-boto3-is-the-amazon-web-services-aws-sdk-for-python-allowing-you-to-interact-with-aws-services-including-s3","title":"The os module is used for operating system-dependent functionality like reading or writing files, whereas boto3 is the Amazon Web Services (AWS) SDK for Python, allowing you to interact with AWS services including S3.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#step-2-uploading-files-to-object-storage","title":"Step 2: Uploading Files to Object Storage","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#define-a-function-upload_files-that-recursively-uploads-all-files-from-a-specified-folder-to-your-s3-bucket-this-function-is-particularly-useful-for-batch-uploading-model-files-and-associated-configurations","title":"Define a function upload_files that recursively uploads all files from a specified folder to your S3 bucket. This function is particularly useful for batch uploading model files and associated configurations.","text":"<pre><code>def upload_files(folder_path, bucket_name, prefix, s3_client):\n    for subdir, dirs, files in os.walk(folder_path):\n        for file in files:\n            try:\n                # Create the full local path of the file\n                full_path = os.path.join(subdir, file)\n\n                # Create the relative path (relative to the folder_path)\n                relative_path = os.path.relpath(full_path, folder_path)\n\n                # Use the relative path as the prefix for the S3 object key\n                s3_key = relative_path.replace(\"\\\\\", \"/\")  # Ensure compatibility with S3 (Unix-style paths)\n                s3_key = prefix + s3_key\n\n                # Upload the file\n                s3_client.upload_file(full_path, bucket_name, s3_key)\n\n                print(f\"Successfully uploaded {relative_path} to {s3_key}\")\n            except Exception as ex:\n                print(f\"Failed to upload {relative_path} to {s3_key}\\n{ex}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#step-3-configure-s3-client-and-execute-functions","title":"Step 3: Configure S3 Client and Execute Functions","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#before-executing-the-upload-and-download-functions-configure-your-s3-client-with-your-aws-credentials-ensure-your-aws-access-key-id-and-aws-secret-access-key-are-securely-stored-and-not-hard-coded-or-exposed-in-your-scripts","title":"Before executing the upload and download functions, configure your S3 client with your AWS credentials. Ensure your AWS Access Key ID and AWS Secret Access Key are securely stored and not hard-coded or exposed in your scripts.","text":"<pre><code>s3_client = boto3.client(\"s3\", aws_access_key_id=_aws_access_key_id, aws_secret_access_key=_aws_secret_access_key)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#now-call-the-upload_files-function-to-upload-your-model-directory-to-s3","title":"Now, call the upload_files function to upload your model directory to S3.","text":"<pre><code>upload_files(_folder_path, _bucket, _prefix, s3_client)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#optional-use-our-sdk-to-upload-files-to-an-s3-bucket","title":"(OPTIONAL) Use Our SDK to Upload Files to an S3 Bucket","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#you-can-also-conveniently-upload-files-to-an-s3-bucket-using-our-sdk-which-provides-seamless-integration-and-simplifies-the-process","title":"You can also conveniently upload files to an S3 bucket using our SDK, which provides seamless integration and simplifies the process.","text":"<pre><code>import practicuscore as prt\n\n\n_upload_conf = prt.connections.UploadS3Conf(\n    bucket=_bucket,\n    prefix=_prefix,\n    folder_path=_folder_path,\n    source_path_to_cut=_source_path_to_cut,\n    aws_access_key_id=_aws_access_key_id,\n    aws_secret_access_key=_aws_secret_access_key,\n)\n\nprt.connections.upload_to_s3(_upload_conf)\n</code></pre> <p>Previous: Model Download | Next: Basic Deployment &gt; Model</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/","title":"Consume LLM API","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#this-tutorial-demonstrates-how-to-interact-with-a-practicusai-llm-deployment-for-making-predictions-using-simple-api-requests","title":"This tutorial demonstrates how to interact with a PracticusAI LLM deployment for making predictions using simple api requests.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#the-workflow-illustrates-obtaining-a-session-token-invoking-the-llm-api-endpoint-and-processing-responses-in-parallel","title":"The workflow illustrates obtaining a session token, invoking the LLM API endpoint, and processing responses in parallel.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>api_url = None  # e.g. 'https://company.practicus.com/llm-models/llama-1b-basic-test/'\n</code></pre> <pre><code>assert api_url, \"Please enter your model api url.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#use-the-practicusai-sdk-to-generate-a-session-token-ensuring-secure-access-to-the-llm-api","title":"Use the PracticusAI SDK to generate a session token, ensuring secure access to the LLM API.","text":"<pre><code>import practicuscore as prt\n\ntoken = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#send-a-get-request-with-the-session-token-to-check-if-the-model-and-its-api-are-active-and-ready-for-use","title":"Send a GET request with the session token to check if the model and its API are active and ready for use.","text":"<pre><code>from requests import get\n\nheaders = {\"authorization\": f\"Bearer {token}\"}\nr = get(api_url + \"?get_meta=true\", headers=headers)\n\nprint(\"Model details: \", r.text)\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#interacting-with-the-llm-api-to-retrieve-a-response-measuring-performance-and-analyzing-the-results","title":"Interacting with the LLM API to retrieve a response, measuring performance, and analyzing the results","text":"<pre><code>from requests import get\nimport json\n\n# Provide a user prompt to the LLM API and retrieve the generated response.\ndata = {\n    #'system_context': '',\n    \"user_prompt\": \"Who is Einstein?\"\n}\nr = get(api_url, headers=headers, json=data)\n\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n\n# Print API response for generated prediction\nprint(\"Prediction result:\")\ntry:\n    parsed = json.loads(r.text)\n    print(json.dumps(parsed, indent=1))\nexcept:\n    print(r.text)\n\n# Examine response headers for debugging or additional metadata about the request.\nprint(\"Headers: \", r.headers)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"  # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model to generate a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()\n    return {\"answer\": f\"Time:{total_time}\\nanswer:{text}\"}\n</code></pre> <p>Previous: Deploy | Next: LangChain Deployment &gt; Model</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/","title":"Deploy","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#deploying-a-model-with-practicus-ai-involves-a-sequence-of-steps-designed-to-securely-and-efficiently-transition-a-model-from-development-to-a-production-ready-state-heres-a-step-by-step-explanation-aimed-at-providing-clarity-and-guidance","title":"Deploying a model with Practicus AI involves a sequence of steps designed to securely and efficiently transition a model from development to a production-ready state. Here's a step-by-step explanation, aimed at providing clarity and guidance:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#defining-parameters","title":"Defining parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#this-section-defines-key-parameters-for-the-notebook-parameters-control-the-behavior-of-the-code-making-it-easy-to-customize-without-altering-the-logic-by-centralizing-parameters-at-the-start-we-ensure-better-readability-maintainability-and-adaptability-for-different-use-cases","title":"This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.","text":"<pre><code>_deployment_key = None  # e.g. \"llm-depl\"\n_prefix = None  # e.g. \"llm-models\"\n_model_name = None  # e.g. \"llama-1b-basic-test\"\n</code></pre> <pre><code>assert _deployment_key and _prefix and _model_name, \"Please enter your deployment parameters.\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()  # The region where the deployments are stored\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code># Let's list our model deployments and select one of them.\nmy_model_deployments = region.model_deployment_list\ndisplay(my_model_deployments.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#deploying-the-model","title":"Deploying the Model","text":"<pre><code>prt.models.deploy(\n    deployment_key=_deployment_key,\n    prefix=_prefix,\n    model_name=_model_name,\n    model_dir=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#model-deployment-a-call-to-deploy-initiates-the-deployment-process-it-requires-the-host-url-the-obtained-auth_token-and-other-previously-defined-parameters","title":"Model Deployment: A call to deploy() initiates the deployment process. It requires the host URL, the obtained auth_token, and other previously defined parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#feedback-upon-successful-deployment-youll-receive-a-confirmation-if-authentication-fails-or-other-issues-arise-youll-be-prompted-with-an-error-message-to-help-diagnose-and-resolve-the-issue","title":"Feedback: Upon successful deployment, you'll receive a confirmation. If authentication fails or other issues arise, you'll be prompted with an error message to help diagnose and resolve the issue.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#this-process-encapsulates-a-secure-and-structured-approach-to-model-deployment-in-practicus-ai-leveraging-the-datapipeline-for-effective-model-management-by-following-these-steps-you-ensure-that-your-model-is-deployed-to-the-right-environment-with-the-appropriate-configurations-ready-for-inference-at-scale-this-systematic-approach-not-only-simplifies-the-deployment-process-but-also-emphasizes-security-and-organization-critical-factors-for-successful-ai-project-implementations","title":"This process encapsulates a secure and structured approach to model deployment in Practicus AI, leveraging the DataPipeline for effective model management. By following these steps, you ensure that your model is deployed to the right environment with the appropriate configurations, ready for inference at scale. This systematic approach not only simplifies the deployment process but also emphasizes security and organization, critical factors for successful AI project implementations.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"  # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model to generate a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()\n    return {\"answer\": f\"Time:{total_time}\\nanswer:{text}\"}\n</code></pre> <p>Previous: Model Json | Next: Consume Parallel</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/","title":"Model.json","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#the-provided-modeljson-snippet-exemplifies-how-configuration-files-are-used-to-specify-operational-parameters-for-deploying-and-running-large-language-models-llms-within-an-ecosystem-like-practicus-ai-this-json-configuration-plays-a-critical-role-in-streamlining-the-deployment-process-enhancing-model-management-and-ensuring-the-model-operates-efficiently-within-its-environment-heres-an-explanation-of-why-this-modeljson-content-is-significant","title":"The provided model.json snippet exemplifies how configuration files are used to specify operational parameters for deploying and running Large Language Models (LLMs) within an ecosystem like Practicus AI. This JSON configuration plays a critical role in streamlining the deployment process, enhancing model management and ensuring the model operates efficiently within its environment. Here's an explanation of why this model.json content is significant:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#specifying-resource-locations","title":"Specifying Resource Locations","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#download_files_from-cachellama-1b-instruct","title":"\"download_files_from\": \"cache/llama-1b-instruct/\":","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#this-key-value-pair-indicates-the-directory-or-path-from-which-the-necessary-model-files-should-be-downloaded-in-the-context-of-deploying-an-llm-these-files-could-include-the-model-weights-tokenizer-files-and-any-other-dependencies-required-for-the-model-to-run-this-parameter-ensures-that-the-deployment-system-knows-where-to-fetch-the-models-components-which-is-crucial-for-initializing-the-model-in-the-target-environment","title":"This key-value pair indicates the directory or path from which the necessary model files should be downloaded. In the context of deploying an LLM, these files could include the model weights, tokenizer files, and any other dependencies required for the model to run. This parameter ensures that the deployment system knows where to fetch the model's components, which is crucial for initializing the model in the target environment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#customizable-download-target","title":"Customizable Download Target","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#_comment-you-can-also-define-download_files_to-otherwise-varpracticuscache-is-used-this-comment-within-the-json-highlights-an-optional-parameter-that-could-be-specified-in-a-similar-json-configuration-file-if-the-download_files_to-parameter-is-provided-it-would-dictate-the-destination-directory-on-the-local-system-where-the-downloaded-files-should-be-stored-in-the-absence-of-this-parameter-a-default-location-varpracticuscache-is-used-this-flexibility-allows-for-adaptability-to-different-deployment-environments-and-configurations-ensuring-that-the-files-are-stored-in-a-location-that-is-accessible-and-appropriate-for-the-models-operation","title":"\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\": This comment within the JSON highlights an optional parameter that could be specified in a similar JSON configuration file. If the download_files_to parameter is provided, it would dictate the destination directory on the local system where the downloaded files should be stored. In the absence of this parameter, a default location (/var/practicus/cache) is used. This flexibility allows for adaptability to different deployment environments and configurations, ensuring that the files are stored in a location that is accessible and appropriate for the model's operation.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#modeljson_1","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"  # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model to generate a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()\n    return {\"answer\": f\"Time:{total_time}\\nanswer:{text}\"}\n</code></pre> <p>Previous: Model | Next: Deploy</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/","title":"Preparation of Model File","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#the-modelpy-file-is-a-critical-component-when-deploying-a-large-language-model-llm-in-environments-like-practicus-ai-it-encapsulates-the-logic-for-initializing-the-model-making-predictions-and-cleaning-up-resources-below-we-will-provide-a-detailed-explanation-of-a-modelpy-script-designed-for-deploying-an-llm-ensuring-no-step-is-overlooked","title":"The model.py file is a critical component when deploying a Large Language Model (LLM) in environments like Practicus AI. It encapsulates the logic for initializing the model, making predictions, and cleaning up resources. Below, we will provide a detailed explanation of a model.py script designed for deploying an LLM, ensuring no step is overlooked.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#import-statements","title":"Import Statements","text":"<pre><code>import sys\nfrom datetime import datetime\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#global-variables","title":"Global Variables","text":"<pre><code>generator = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#generator-holds-the-model-instance-initialized-as-none-and-later-assigned-the-llm-object","title":"generator: Holds the model instance. Initialized as None and later assigned the LLM object.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#initialization-function","title":"Initialization Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#the-init-function-attempts-to-import-the-llama-library-and-build-the-model-with-specified-parameters","title":"The <code>init</code> function  attempts to import the LLaMA library and build the model with specified parameters.","text":"<pre><code>async def init(model_meta=None, *args, **kwargs):\n    global generator\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"  # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#cleanup-function","title":"Cleanup Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#this-function-is-designed-to-free-up-resources-once-theyre-no-longer-needed-setting-generator-back-to-none-and-clearing-the-gpu-memory-cache-to-prevent-memory-leaks-crucial-for-maintaining-performance","title":"This function is designed to free up resources once they're no longer needed, setting generator back to None and clearing the GPU memory cache to prevent memory leaks, crucial for maintaining performance.","text":"<pre><code>async def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n    global generator\n    generator = None\n    from torch import cuda\n\n    cuda.empty_cache()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#prediction-wrapper-function","title":"Prediction Wrapper Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#the-predict-function-processes-user-input-and-generates-responses-using-the-llm-key-steps-include","title":"The <code>predict</code> function processes user input and generates responses using the LLM. Key steps include:","text":"<pre><code>async def predict(payload_dict: dict, **kwargs):\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model for generating a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()\n    return {\"answer\": f\"Time:{total_time}\\answer:{text}\"}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#this-modelpy-script-outlines-a-robust-framework-for-deploying-and-interacting-with-a-llm-in-a-scalable-asynchronous-manner-it-highlights-essential-practices-like-dynamic-library-loading-concurrent-processing-with-threads-resource-management-and-detailed-logging-for-performance-monitoring-this-setup-is-adaptable-to-various-models-and-can-be-tailored-to-fit-specific-requirements-of-different-llm-deployments","title":"This model.py script outlines a robust framework for deploying and interacting with a LLM in a scalable, asynchronous manner. It highlights essential practices like dynamic library loading, concurrent processing with threads, resource management, and detailed logging for performance monitoring. This setup is adaptable to various models and can be tailored to fit specific requirements of different LLM deployments.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"  # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model to generate a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()\n    return {\"answer\": f\"Time:{total_time}\\nanswer:{text}\"}\n</code></pre> <p>Previous: Upload LLM | Next: Model Json</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/","title":"Consume LLM API With ChatPracticus","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#this-tutorial-demonstrates-how-to-interact-with-a-practicusai-llm-deployment-for-making-predictions-using-the-practicusai-sdk-the-methods-used-include-chatpracticus-for-invoking-the-model-endpoint-and-practicuscore-for-managing-api-tokens","title":"This tutorial demonstrates how to interact with a PracticusAI LLM deployment for making predictions using the PracticusAI SDK. The methods used include <code>ChatPracticus</code> for invoking the model endpoint and <code>practicuscore</code> for managing API tokens.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#the-workflow-illustrates-obtaining-a-session-token-invoking-the-llm-api-endpoint-and-processing-responses-in-parallel","title":"The workflow illustrates obtaining a session token, invoking the LLM API endpoint, and processing responses in parallel.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>api_url = None  # Model API e.g. \"https://company.practicus.com/llm-models/llama-3b-chain-test/\"\n</code></pre> <pre><code>assert api_url, \"Please enter your model api url.\"\n</code></pre> <pre><code>from langchain_practicus import ChatPracticus\nimport practicuscore as prt\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#the-test_langchain_practicus-function-is-defined-to-interact-with-the-practicusai-model-endpoint-it-uses-the-chatpracticus-object-to-invoke-the-api-with-the-provided-url-token-and-input-data-the-response-is-printed-in-two-formats-a-raw-dictionary-and-its-content","title":"The <code>test_langchain_practicus</code> function is defined to interact with the PracticusAI model endpoint. It uses the <code>ChatPracticus</code> object to invoke the API with the provided URL, token, and input data. The response is printed in two formats: a raw dictionary and its content.","text":"<pre><code>def test_langchain_practicus(api_url, token, inputs):\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n\n    print(\"\\n\\nReceived response:\\n\", dict(response))\n    print(\"\\n\\nReceived Content:\\n\", response.content)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#we-retrieve-an-api-session-token-using-practicusai-sdk-this-token-is-required-to-authenticate-and-interact-with-the-practicusai-deployment","title":"We retrieve an API session token using PracticusAI SDK. This token is required to authenticate and interact with the PracticusAI deployment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#the-method-below-creates-a-token-that-is-valid-for-4-hours-longer-tokens-can-be-retrieved-from-the-admin-console","title":"The method below creates a token that is valid for 4 hours, longer tokens can be retrieved from the admin console.","text":"<pre><code>token = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#we-invoke-the-test_langchain_practicus-function-with-the-api-url-session-token-and-an-example-query-what-is-the-capital-of-england-the-function-sends-the-query-to-the-practicusai-endpoint-and-prints-the-received-response","title":"We invoke the <code>test_langchain_practicus</code> function with the API URL, session token, and an example query, <code>'What is the capital of England?'</code>. The function sends the query to the PracticusAI endpoint and prints the received response.","text":"<pre><code>test_langchain_practicus(api_url, token, [\"What is the capital of England?\"])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#consume-llm-api-with-basic-http-requests","title":"Consume LLM API With basic HTTP requests","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#use-the-practicusai-sdk-to-generate-a-session-token-ensuring-secure-access-to-the-llm-api","title":"Use the PracticusAI SDK to generate a session token, ensuring secure access to the LLM API.","text":"<pre><code>import practicuscore as prt\n\n# We will be using using the SDK to get a session token.\napi_url = None  # Model API e.g. \"https://company.practicus.com/llm-models/llama-3b-chain-test/\"\ntoken = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#send-a-get-request-with-the-session-token-to-check-if-the-model-and-its-api-are-active-and-ready-for-use","title":"Send a GET request with the session token to check if the model and its API are active and ready for use.","text":"<pre><code>from requests import get\n\nheaders = {\"authorization\": f\"Bearer {token}\"}\nr = get(api_url + \"?get_meta=true\", headers=headers)\n\nprint(\"Model details: \", r.text)\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#interacting-with-the-llm-api-to-retrieve-a-response-measuring-performance-and-analyzing-the-results","title":"Interacting with the LLM API to retrieve a response, measuring performance, and analyzing the results","text":"<pre><code>from requests import get\nimport json\n\n# Provide a user prompt to the LLM API and retrieve the generated response.\ndata = {\n    #'system_context': '',\n    \"user_prompt\": \"Who is Nikola Tesla?\"\n}\nr = get(api_url, headers=headers, json=data)\n\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n\n# Print API response for generated prediction\nprint(\"Prediction result:\")\ntry:\n    parsed = json.loads(r.text)\n    print(json.dumps(parsed, indent=1))\nexcept:\n    print(r.text)\n\n# Examine response headers for debugging or additional metadata about the request.\nprint(\"Headers: \", r.headers)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()\n        return {\"answer\": f\"Time:{total_time}\\nanswer:{text}\"}\n\n    # For langchain applications:\n    else:\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0][\"generated_text\"]\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload[\"lang_model\"],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre> <p>Previous: Deploy | Next: Email E Assistant &gt; Mail E-Assistant</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/","title":"Deploy","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#deploying-a-model-with-practicus-ai-involves-a-sequence-of-steps-designed-to-securely-and-efficiently-transition-a-model-from-development-to-a-production-ready-state-heres-a-step-by-step-explanation-aimed-at-providing-clarity-and-guidance","title":"Deploying a model with Practicus AI involves a sequence of steps designed to securely and efficiently transition a model from development to a production-ready state. Here's a step-by-step explanation, aimed at providing clarity and guidance:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#defining-parameters","title":"Defining parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#this-section-defines-key-parameters-for-the-notebook-parameters-control-the-behavior-of-the-code-making-it-easy-to-customize-without-altering-the-logic-by-centralizing-parameters-at-the-start-we-ensure-better-readability-maintainability-and-adaptability-for-different-use-cases","title":"This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.","text":"<pre><code>_deployment_key = None  # e.g. \"llm-depl\"\n_prefix = None  # e.g. \"llm-models\"\n_model_name = None  # e.g. \"llama-1b-combined-test\"\n</code></pre> <pre><code>assert _deployment_key and _prefix and _model_name, \"Please enter your deployment parameters.\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()  # The region where the deployments are stored\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code># Let's list our model deployments and select one of them.\nmy_model_deployments = region.model_deployment_list\ndisplay(my_model_deployments.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#deploying-the-model","title":"Deploying the Model","text":"<pre><code>prt.models.deploy(\n    deployment_key=_deployment_key,\n    prefix=_prefix,\n    model_name=_model_name,\n    model_dir=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#model-deployment-a-call-to-deploy-initiates-the-deployment-process-it-requires-the-host-url-the-obtained-auth_token-and-other-previously-defined-parameters","title":"Model Deployment: A call to deploy() initiates the deployment process. It requires the host URL, the obtained auth_token, and other previously defined parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#feedback-upon-successful-deployment-youll-receive-a-confirmation-if-authentication-fails-or-other-issues-arise-youll-be-prompted-with-an-error-message-to-help-diagnose-and-resolve-the-issue","title":"Feedback: Upon successful deployment, you'll receive a confirmation. If authentication fails or other issues arise, you'll be prompted with an error message to help diagnose and resolve the issue.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#this-process-encapsulates-a-secure-and-structured-approach-to-model-deployment-in-practicus-ai-leveraging-the-datapipeline-for-effective-model-management-by-following-these-steps-you-ensure-that-your-model-is-deployed-to-the-right-environment-with-the-appropriate-configurations-ready-for-inference-at-scale-this-systematic-approach-not-only-simplifies-the-deployment-process-but-also-emphasizes-security-and-organization-critical-factors-for-successful-ai-project-implementations","title":"This process encapsulates a secure and structured approach to model deployment in Practicus AI, leveraging the DataPipeline for effective model management. By following these steps, you ensure that your model is deployed to the right environment with the appropriate configurations, ready for inference at scale. This systematic approach not only simplifies the deployment process but also emphasizes security and organization, critical factors for successful AI project implementations.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()\n        return {\"answer\": f\"Time:{total_time}\\nanswer:{text}\"}\n\n    # For langchain applications:\n    else:\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0][\"generated_text\"]\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload[\"lang_model\"],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre> <p>Previous: Model Json | Next: Consume Parallel</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/","title":"Model.json","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#the-provided-modeljson-snippet-exemplifies-how-configuration-files-are-used-to-specify-operational-parameters-for-deploying-and-running-large-language-models-llms-within-an-ecosystem-like-practicus-ai-this-json-configuration-plays-a-critical-role-in-streamlining-the-deployment-process-enhancing-model-management-and-ensuring-the-model-operates-efficiently-within-its-environment-heres-an-explanation-of-why-this-modeljson-content-is-significant","title":"The provided model.json snippet exemplifies how configuration files are used to specify operational parameters for deploying and running Large Language Models (LLMs) within an ecosystem like Practicus AI. This JSON configuration plays a critical role in streamlining the deployment process, enhancing model management, and ensuring the model operates efficiently within its environment. Here's an explanation of why this model.json content is significant:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#specifying-resource-locations","title":"Specifying Resource Locations","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#download_files_from-cachellama-1b-instruct","title":"\"download_files_from\": \"cache/llama-1b-instruct/\":","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#this-key-value-pair-indicates-the-directory-or-path-from-which-the-necessary-model-files-should-be-downloaded-in-the-context-of-deploying-an-llm-these-files-could-include-the-model-weights-tokenizer-files-and-any-other-dependencies-required-for-the-model-to-run-this-parameter-ensures-that-the-deployment-system-knows-where-to-fetch-the-models-components-which-is-crucial-for-initializing-the-model-in-the-target-environment","title":"This key-value pair indicates the directory or path from which the necessary model files should be downloaded. In the context of deploying an LLM, these files could include the model weights, tokenizer files, and any other dependencies required for the model to run. This parameter ensures that the deployment system knows where to fetch the model's components, which is crucial for initializing the model in the target environment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#customizable-download-target","title":"Customizable Download Target","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#_comment-you-can-also-define-download_files_to-otherwise-varpracticuscache-is-used-this-comment-within-the-json-highlights-an-optional-parameter-that-could-be-specified-in-a-similar-json-configuration-file-if-the-download_files_to-parameter-is-provided-it-would-dictate-the-destination-directory-on-the-local-system-where-the-downloaded-files-should-be-stored-in-the-absence-of-this-parameter-a-default-location-varpracticuscache-is-used-this-flexibility-allows-for-adaptability-to-different-deployment-environments-and-configurations-ensuring-that-the-files-are-stored-in-a-location-that-is-accessible-and-appropriate-for-the-models-operation","title":"\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\": This comment within the JSON highlights an optional parameter that could be specified in a similar JSON configuration file. If the download_files_to parameter is provided, it would dictate the destination directory on the local system where the downloaded files should be stored. In the absence of this parameter, a default location (/var/practicus/cache) is used. This flexibility allows for adaptability to different deployment environments and configurations, ensuring that the files are stored in a location that is accessible and appropriate for the model's operation.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#modeljson_1","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()\n        return {\"answer\": f\"Time:{total_time}\\nanswer:{text}\"}\n\n    # For langchain applications:\n    else:\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0][\"generated_text\"]\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload[\"lang_model\"],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre> <p>Previous: Model | Next: Deploy</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/","title":"Preparation of Model File","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#this-section-provides-a-detailed-explanation-of-the-code-used-to-deploy-a-model-catering-to-both-the-langchain-compatible-large-language-model-llm-api-endpoint-via-the-practicusai-sdk-and-standard-llm-deployments-for-text-in-text-out-tasks-the-modelpy-script-serves-as-the-core-of-this-implementation-managing-model-initialization-payload-processing-and-response-generation-below-we-offer-a-comprehensive-breakdown-of-each-segment","title":"This section provides a detailed explanation of the code used to deploy a model, catering to both the LangChain-compatible Large Language Model (LLM) API endpoint via the PracticusAI SDK and standard LLM deployments for text-in, text-out tasks. The model.py script serves as the core of this implementation, managing model initialization, payload processing, and response generation. Below, we offer a comprehensive breakdown of each segment:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#import-statements","title":"Import Statements","text":"<pre><code>import sys\nfrom datetime import datetime\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#global-variables","title":"Global Variables","text":"<pre><code>generator = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#generator-holds-the-model-instance-initialized-as-none-and-later-assigned-the-llm-object","title":"generator: Holds the model instance. Initialized as None and later assigned the LLM object.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#sys-used-for-interacting-with-the-interpreter-including-adding-paths-for-python-to-search-for-modules","title":"sys: Used for interacting with the interpreter, including adding paths for Python to search for modules.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#datetime-facilitates-recording-timestamps-useful-for-performance-monitoring","title":"datetime: Facilitates recording timestamps, useful for performance monitoring.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#initialization-function","title":"Initialization Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#the-init-function-attempts-to-import-the-llama-library-and-build-the-model-with-specified-parameters","title":"The <code>init</code> function  attempts to import the LLaMA library and build the model with specified parameters.","text":"<pre><code>async def init(model_meta=None, *args, **kwargs):\n    global generator\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"  # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#cleanup-function","title":"Cleanup Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#this-function-is-designed-to-free-up-resources-once-theyre-no-longer-needed-setting-generator-back-to-none-and-clearing-the-gpu-memory-cache-to-prevent-memory-leaks-crucial-for-maintaining-performance","title":"This function is designed to free up resources once they're no longer needed, setting generator back to None and clearing the GPU memory cache to prevent memory leaks, crucial for maintaining performance.","text":"<pre><code>async def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n    global generator\n    generator = None\n    from torch import cuda\n\n    cuda.empty_cache()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#prediction-wrapper-function","title":"Prediction Wrapper Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#the-predict-function-processes-user-input-and-generates-responses-using-the-llm-key-steps-include","title":"The <code>predict</code> function processes user input and generates responses using the LLM. Key steps include:","text":"<pre><code>async def predict(payload_dict: dict, **kwargs):\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()\n        return {\"answer\": f\"Time:{total_time}\\answer:{text}\"}\n\n    # For langchaing applications:\n    else:\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0][\"generated_text\"]\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload[\"lang_model\"],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#this-modelpy-script-outlines-a-robust-framework-for-deploying-and-interacting-with-a-llm-in-a-scalable-asynchronous-manner-it-highlights-essential-practices-like-dynamic-library-loading-concurrent-processing-with-threads-resource-management-and-detailed-logging-for-performance-monitoring-this-setup-is-adaptable-to-various-models-and-can-be-tailored-to-fit-specific-requirements-of-different-llm-deployments","title":"This model.py script outlines a robust framework for deploying and interacting with a LLM in a scalable, asynchronous manner. It highlights essential practices like dynamic library loading, concurrent processing with threads, resource management, and detailed logging for performance monitoring. This setup is adaptable to various models and can be tailored to fit specific requirements of different LLM deployments.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()\n        return {\"answer\": f\"Time:{total_time}\\nanswer:{text}\"}\n\n    # For langchain applications:\n    else:\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0][\"generated_text\"]\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload[\"lang_model\"],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre> <p>Previous: Consume Parallel | Next: Model Json</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/","title":"Consume LLM API","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#this-tutorial-demonstrates-how-to-interact-with-a-practicusai-llm-deployment-for-making-predictions-using-the-practicusai-sdk-the-methods-used-include-chatpracticus-for-invoking-the-model-endpoint-and-practicuscore-for-managing-api-tokens","title":"This tutorial demonstrates how to interact with a PracticusAI LLM deployment for making predictions using the PracticusAI SDK. The methods used include <code>ChatPracticus</code> for invoking the model endpoint and <code>practicuscore</code> for managing API tokens.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#the-workflow-illustrates-obtaining-a-session-token-invoking-the-llm-api-endpoint-and-processing-responses-in-parallel","title":"The workflow illustrates obtaining a session token, invoking the LLM API endpoint, and processing responses in parallel.","text":"<pre><code>from langchain_practicus import ChatPracticus\nimport practicuscore as prt\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>api_url = None  # E.g. \"https://company.practicus.com/llm-models/llama-3b-chain-test/\"\n</code></pre> <pre><code>assert api_url, \"Please enter your model api url.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#the-test_langchain_practicus-function-is-defined-to-interact-with-the-practicusai-model-endpoint-it-uses-the-chatpracticus-object-to-invoke-the-api-with-the-provided-url-token-and-input-data-the-response-is-printed-in-two-formats-a-raw-dictionary-and-its-content","title":"The <code>test_langchain_practicus</code> function is defined to interact with the PracticusAI model endpoint. It uses the <code>ChatPracticus</code> object to invoke the API with the provided URL, token, and input data. The response is printed in two formats: a raw dictionary and its content.","text":"<pre><code>def test_langchain_practicus(api_url, token, inputs):\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n\n    print(\"\\n\\nReceived response:\\n\", dict(response))\n    print(\"\\n\\nReceived Content:\\n\", response.content)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#we-retrieve-an-api-session-token-using-practicusai-sdk-this-token-is-required-to-authenticate-and-interact-with-the-practicusai-deployment","title":"We retrieve an API session token using PracticusAI SDK. This token is required to authenticate and interact with the PracticusAI deployment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#the-method-below-creates-a-token-that-is-valid-for-4-hours-longer-tokens-can-be-retrieved-from-the-admin-console","title":"The method below creates a token that is valid for 4 hours, longer tokens can be retrieved from the admin console.","text":"<pre><code>token = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#we-invoke-the-test_langchain_practicus-function-with-the-api-url-session-token-and-an-example-query-what-is-the-capital-of-england-the-function-sends-the-query-to-the-practicusai-endpoint-and-prints-the-received-response","title":"We invoke the <code>test_langchain_practicus</code> function with the API URL, session token, and an example query, <code>'What is the capital of England?'</code>. The function sends the query to the PracticusAI endpoint and prints the received response.","text":"<pre><code>test_langchain_practicus(api_url, token, [\"What is the capital of England?\"])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0][\"generated_text\"]\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload[\"lang_model\"],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Deploy | Next: Combined Method &gt; Model</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/","title":"Deploy","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#deploying-a-model-with-practicus-ai-involves-a-sequence-of-steps-designed-to-securely-and-efficiently-transition-a-model-from-development-to-a-production-ready-state-heres-a-step-by-step-explanation-aimed-at-providing-clarity-and-guidance","title":"Deploying a model with Practicus AI involves a sequence of steps designed to securely and efficiently transition a model from development to a production-ready state. Here's a step-by-step explanation, aimed at providing clarity and guidance:","text":"<pre><code>import practicuscore as prt\n\nregion = prt.get_region()  # The region where the deployments are stored\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#defining-parameters","title":"Defining parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#this-section-defines-key-parameters-for-the-notebook-parameters-control-the-behavior-of-the-code-making-it-easy-to-customize-without-altering-the-logic-by-centralizing-parameters-at-the-start-we-ensure-better-readability-maintainability-and-adaptability-for-different-use-cases","title":"This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.","text":"<pre><code>_deployment_key = None  # e.g. \"llm-depl\"\n_prefix = None  # e.g. \"llm-models\"\n_model_name = None  # e.g. \"llama-1b-chain-test\"\n</code></pre> <pre><code>assert _deployment_key and _prefix and _model_name, \"Please enter your deployment parameters.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code># Let's list our model deployments and select one of them.\nmy_model_deployments = region.model_deployment_list\ndisplay(my_model_deployments.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#deploying-the-model","title":"Deploying the Model","text":"<pre><code>prt.models.deploy(\n    deployment_key=_deployment_key,\n    prefix=_prefix,\n    model_name=_model_name,\n    model_dir=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#model-deployment-a-call-to-deploy-initiates-the-deployment-process-it-requires-the-host-url-the-obtained-auth_token-and-other-previously-defined-parameters","title":"Model Deployment: A call to deploy() initiates the deployment process. It requires the host URL, the obtained auth_token, and other previously defined parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#feedback-upon-successful-deployment-youll-receive-a-confirmation-if-authentication-fails-or-other-issues-arise-youll-be-prompted-with-an-error-message-to-help-diagnose-and-resolve-the-issue","title":"Feedback: Upon successful deployment, you'll receive a confirmation. If authentication fails or other issues arise, you'll be prompted with an error message to help diagnose and resolve the issue.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#this-process-encapsulates-a-secure-and-structured-approach-to-model-deployment-in-practicus-ai-leveraging-the-datapipeline-for-effective-model-management-by-following-these-steps-you-ensure-that-your-model-is-deployed-to-the-right-environment-with-the-appropriate-configurations-ready-for-inference-at-scale-this-systematic-approach-not-only-simplifies-the-deployment-process-but-also-emphasizes-security-and-organization-critical-factors-for-successful-ai-project-implementations","title":"This process encapsulates a secure and structured approach to model deployment in Practicus AI, leveraging the DataPipeline for effective model management. By following these steps, you ensure that your model is deployed to the right environment with the appropriate configurations, ready for inference at scale. This systematic approach not only simplifies the deployment process but also emphasizes security and organization, critical factors for successful AI project implementations.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0][\"generated_text\"]\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload[\"lang_model\"],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Model Json | Next: Consume Parallel</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/","title":"Model.json","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#the-provided-modeljson-snippet-exemplifies-how-configuration-files-are-used-to-specify-operational-parameters-for-deploying-and-running-large-language-models-llms-within-an-ecosystem-like-practicus-ai-this-json-configuration-plays-a-critical-role-in-streamlining-the-deployment-process-enhancing-model-management-and-ensuring-the-model-operates-efficiently-within-its-environment-heres-an-explanation-of-why-this-modeljson-content-is-significant","title":"The provided model.json snippet exemplifies how configuration files are used to specify operational parameters for deploying and running Large Language Models (LLMs) within an ecosystem like Practicus AI. This JSON configuration plays a critical role in streamlining the deployment process, enhancing model management, and ensuring the model operates efficiently within its environment. Here's an explanation of why this model.json content is significant:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#specifying-resource-locations","title":"Specifying Resource Locations","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#download_files_from-cachellama-1b-instruct","title":"\"download_files_from\": \"cache/llama-1b-instruct/\":","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#this-key-value-pair-indicates-the-directory-or-path-from-which-the-necessary-model-files-should-be-downloaded-in-the-context-of-deploying-an-llm-these-files-could-include-the-model-weights-tokenizer-files-and-any-other-dependencies-required-for-the-model-to-run-this-parameter-ensures-that-the-deployment-system-knows-where-to-fetch-the-models-components-which-is-crucial-for-initializing-the-model-in-the-target-environment","title":"This key-value pair indicates the directory or path from which the necessary model files should be downloaded. In the context of deploying an LLM, these files could include the model weights, tokenizer files, and any other dependencies required for the model to run. This parameter ensures that the deployment system knows where to fetch the model's components, which is crucial for initializing the model in the target environment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#customizable-download-target","title":"Customizable Download Target","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#_comment-you-can-also-define-download_files_to-otherwise-varpracticuscache-is-used-this-comment-within-the-json-highlights-an-optional-parameter-that-could-be-specified-in-a-similar-json-configuration-file-if-the-download_files_to-parameter-is-provided-it-would-dictate-the-destination-directory-on-the-local-system-where-the-downloaded-files-should-be-stored-in-the-absence-of-this-parameter-a-default-location-varpracticuscache-is-used-this-flexibility-allows-for-adaptability-to-different-deployment-environments-and-configurations-ensuring-that-the-files-are-stored-in-a-location-that-is-accessible-and-appropriate-for-the-models-operation","title":"\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\": This comment within the JSON highlights an optional parameter that could be specified in a similar JSON configuration file. If the download_files_to parameter is provided, it would dictate the destination directory on the local system where the downloaded files should be stored. In the absence of this parameter, a default location (/var/practicus/cache) is used. This flexibility allows for adaptability to different deployment environments and configurations, ensuring that the files are stored in a location that is accessible and appropriate for the model's operation.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#modeljson_1","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0][\"generated_text\"]\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload[\"lang_model\"],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Model | Next: Deploy</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/","title":"Preparation of Model File","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#this-section-explains-the-code-for-deploying-a-langchain-compatible-large-language-model-llm-api-endpoint-using-the-practicusai-sdk-the-modelpy-handles-model-initialization-handling-payloads-and-generating-responses-below-is-a-breakdown-of-each-segment","title":"This section explains the code for deploying a LangChain-compatible Large Language Model (LLM) API endpoint using the PracticusAI SDK. The model.py handles model initialization, handling payloads, and generating responses. Below is a breakdown of each segment:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#import-statements","title":"Import Statements","text":"<pre><code>import sys\nfrom datetime import datetime\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#global-variables","title":"Global Variables","text":"<pre><code>generator = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#generator-holds-the-model-instance-initialized-as-none-and-later-assigned-the-llm-object","title":"generator: Holds the model instance. Initialized as None and later assigned the LLM object.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#sys-used-for-interacting-with-the-interpreter-including-adding-paths-for-python-to-search-for-modules","title":"sys: Used for interacting with the interpreter, including adding paths for Python to search for modules.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#datetime-facilitates-recording-timestamps-useful-for-performance-monitoring","title":"datetime: Facilitates recording timestamps, useful for performance monitoring.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#initialization-function","title":"Initialization Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#the-init-function-attempts-to-import-the-llama-library-and-build-the-model-with-specified-parameters","title":"The <code>init</code> function  attempts to import the LLaMA library and build the model with specified parameters.","text":"<pre><code>async def init(model_meta=None, *args, **kwargs):\n    global generator\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"  # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#cleanup-function","title":"Cleanup Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#this-function-is-designed-to-free-up-resources-once-theyre-no-longer-needed-setting-generator-back-to-none-and-clearing-the-gpu-memory-cache-to-prevent-memory-leaks-crucial-for-maintaining-performance","title":"This function is designed to free up resources once they're no longer needed, setting generator back to None and clearing the GPU memory cache to prevent memory leaks, crucial for maintaining performance.","text":"<pre><code>async def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n    global generator\n    generator = None\n    from torch import cuda\n\n    cuda.empty_cache()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#prediction-wrapper-function","title":"Prediction Wrapper Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#the-predict-function-processes-user-input-and-generates-responses-using-the-llm-key-steps-include","title":"The <code>predict</code> function processes user input and generates responses using the LLM. Key steps include:","text":"<pre><code>async def _predict(payload_dict: dict, **kwargs):\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0][\"generated_text\"]\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload[\"lang_model\"],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#this-modelpy-script-outlines-a-robust-framework-for-deploying-and-interacting-with-a-llm-in-a-scalable-asynchronous-manner-it-highlights-essential-practices-like-dynamic-library-loading-concurrent-processing-with-threads-resource-management-and-detailed-logging-for-performance-monitoring-this-setup-is-adaptable-to-various-models-and-can-be-tailored-to-fit-specific-requirements-of-different-llm-deployments","title":"This model.py script outlines a robust framework for deploying and interacting with a LLM in a scalable, asynchronous manner. It highlights essential practices like dynamic library loading, concurrent processing with threads, resource management, and detailed logging for performance monitoring. This setup is adaptable to various models and can be tailored to fit specific requirements of different LLM deployments.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to(\"cpu\")  # Change with cuda or auto to use gpus.\n        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n\n    cuda.empty_cache()\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0][\"generated_text\"]\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload[\"lang_model\"],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Consume Parallel | Next: Model Json</p>"},{"location":"technical-tutorial/extras/generative-ai/ecomm-sdk/memory-chabot/","title":"Memory Chatbot","text":""},{"location":"technical-tutorial/extras/generative-ai/ecomm-sdk/memory-chabot/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>app_name = \"memory-chatbot\"  # E.g. 'api-chatbot'\ndeployment_setting_key = \"appdepl\"\napp_prefix = \"apps\"\napp_dir = None\n</code></pre> <pre><code>assert app_name, \"Please enter application name\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/ecomm-sdk/memory-chabot/#preparing-data","title":"Preparing Data","text":"<pre><code>import os\nimport requests\n\n# Create the GitHub API URL\nurl = \"https://api.github.com/repos/practicusai/sample-data/contents/ecomm?ref=main\"\n\n# Call the API\nresponse = requests.get(url)\nif response.status_code == 200:\n    files = response.json()  # Get response in JSON format\n\n    for file in files:\n        file_url = file[\"download_url\"]\n        file_name = file[\"name\"]\n\n        # Download files\n        file_response = requests.get(file_url)\n        if file_response.status_code == 200:\n            with open(file_name, \"wb\") as f:\n                f.write(file_response.content)\n            print(f\"'{file_name}' successfully downloaded.\")\n        else:\n            print(f\"'{file_name}' file failed to download.\")\nelse:\n    print(f\"Failed to retrieve data from API, HTTP status: {response.status_code}\")\n</code></pre> <pre><code>import practicuscore as prt\n</code></pre> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre> <p>After testing our application we can set our configurations and start the deployment process.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/ecomm-sdk/memory-chabot/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code>my_app_list = prt.apps.get_list()\ndisplay(my_app_list.to_pandas())\n\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_app_prefix_list = prt.apps.get_prefix_list()\ndisplay(my_app_prefix_list.to_pandas())\n\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>my_app_settings = prt.apps.get_deployment_setting_list()\ndisplay(my_app_settings.to_pandas())\n\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/ecomm-sdk/memory-chabot/#deploying-app","title":"Deploying app","text":"<pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,  # Deployment Key, ask admin for deployment key\n    prefix=app_prefix,  # Apphost deployment extension\n    app_name=app_name,\n    app_dir=None,  # Directory of files that will be deployed ('None' for current directory)\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/ecomm-sdk/memory-chabot/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/ecomm-sdk/memory-chabot/#homepy","title":"Home.py","text":"<pre><code>import streamlit as st\nimport pandas as pd\nimport practicuscore as prt\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest\nimport requests\n\n\nst.set_page_config(page_title=\"E-Commerce Product Review &amp; Chatbot\", layout=\"wide\")\n\n\ndef load_data():\n    excel_file = \"ecommerce_data_with_images.xlsx\"\n    products_df = pd.read_excel(excel_file, sheet_name=\"Products\")\n    reviews_df = pd.read_excel(excel_file, sheet_name=\"Reviews\")\n    sizes_df = pd.read_excel(excel_file, sheet_name=\"Sizes\")\n    images_df = pd.read_excel(excel_file, sheet_name=\"ProductImages\")\n    return products_df, reviews_df, sizes_df, images_df\n\n\nproducts_df, reviews_df, sizes_df, images_df = load_data()\n\nst.title(\"\ud83d\udecd\ufe0f E-Commerce Product Review &amp; Chatbot\")\n\ncol1, col2 = st.columns([3, 1])\nwith col1:\n    selected_product = st.selectbox(\"Select a product:\", products_df[\"product_name\"].unique())\nwith col2:\n    show_image = st.button(\"Show Image\")\n\nif show_image and selected_product:\n    product_id = products_df.loc[products_df[\"product_name\"] == selected_product, \"product_id\"].values[0]\n    image_url = images_df.loc[images_df[\"product_id\"] == product_id, \"image_url\"].values[0]\n    st.image(image_url, caption=selected_product, width=300)\n\nselected_rating = st.slider(\"Filter reviews by rating:\", min_value=1, max_value=5, value=(1, 5))\n\nif selected_product:\n    product_id = products_df.loc[products_df[\"product_name\"] == selected_product, \"product_id\"].values[0]\n    product_reviews = reviews_df[\n        (reviews_df[\"product_id\"] == product_id)\n        &amp; (reviews_df[\"rating\"].between(selected_rating[0], selected_rating[1]))\n    ]\n    available_sizes = sizes_df[\"size\"].unique()\n\n    st.subheader(\"Customer Reviews\")\n    review_columns = st.columns(2)\n\n    for i, (_, row) in enumerate(product_reviews.iterrows()):\n        with review_columns[i % 2]:\n            st.markdown(\n                f\"\"\"\n                &lt;div style='padding: 10px; border-radius: 10px; background-color: #2c3e50; color: #ecf0f1; margin-bottom: 10px;'&gt;\n                    &lt;strong&gt;\u2b50 {row[\"rating\"]}/5&lt;/strong&gt;&lt;br&gt;\n                    {row[\"review\"]}\n                &lt;/div&gt;\n                \"\"\",\n                unsafe_allow_html=True,\n            )\n\n    st.subheader(\"Available Sizes\")\n    st.write(\", \".join(available_sizes))\n\nimport time\n\n\ndef analyze_sentiment(reviews):\n    if reviews.empty:\n        return [\"No reviews available.\"]\n\n    api_url = f\"https://dev.practicus.io/models/llm-proxy/\"\n    token = prt.models.get_session_token(api_url=api_url)\n\n    context = f\"You are an expert product assistant providing details about e-commerce products based on available data.\\nReviews: {', '.join(reviews['review'].tolist())}\"\n\n    practicus_llm_req = PrtLangRequest(\n        messages=[PrtLangMessage(content=context, role=\"human\")], lang_model=\"model\", streaming=True\n    )\n\n    headers = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n\n    results = []\n\n    placeholder = st.empty()\n\n    with requests.post(api_url, headers=headers, data=data_js, stream=True) as r:\n        for word in r.iter_content(1024):\n            word_decoded = word.decode(\"utf-8\")\n            results.append(word_decoded)\n\n            placeholder.markdown(\"\".join(results))\n\n            time.sleep(0.1)\n\n    return \"\".join(results)\n\n\ndef analyze_sentiment2(reviews):\n    if reviews.empty:\n        return [\"No reviews available.\"]\n\n    api_url = f\"https://dev.practicus.io/models/llm-proxy/\"\n    token = prt.models.get_session_token(api_url=api_url)\n\n    context = f\"Based on the response above, which specific parts of the reviews contributed to the answer? Please extract relevant quotes.\\nReviews: {', '.join(product_reviews['review'].tolist()[:10])}\"\n\n    practicus_llm_req = PrtLangRequest(\n        messages=[PrtLangMessage(content=context, role=\"human\")], lang_model=\"model\", streaming=True\n    )\n\n    headers = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n\n    results = []\n\n    placeholder = st.empty()\n\n    with requests.post(api_url, headers=headers, data=data_js, stream=True) as r:\n        for word in r.iter_content(1024):\n            word_decoded = word.decode(\"utf-8\")\n            results.append(word_decoded)\n\n            placeholder.markdown(\"\".join(results))\n\n            time.sleep(0.1)\n\n    return \"\".join(results)\n\n\nst.sidebar.title(\"\ud83d\udde8\ufe0f Chat with AI Product Assistant\")\nuser_input = st.sidebar.text_input(\"Ask about the product:\")\nif user_input:\n    sentiment_summary = analyze_sentiment(product_reviews)\n    st.sidebar.write(sentiment_summary)\n    st.sidebar.subheader(\"\ud83d\udd0d Relevant Context from Reviews\")\n\n    sentiment_summary2 = analyze_sentiment2(product_reviews)\n    st.sidebar.write(sentiment_summary2)\n\n\nst.sidebar.subheader(\"\ud83d\udca1 Suggested Questions\")\nif selected_product:\n    context = f\"Product: {selected_product}\\nReviews: {', '.join(product_reviews['review'].tolist())}\"\n    suggested_questions = [\n        \"What is the overall customer satisfaction for this product?\",\n        \"Are there any common complaints about this product?\",\n        \"What do most customers like about this product?\",\n    ]\n    for question in suggested_questions:\n        st.sidebar.write(f\"- {question}\")\n</code></pre> <p>Previous: Langflow API | Next: Cv Assistant &gt; Cv Assistant</p>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/","title":"Mail E-Assistant","text":""},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None  # 'E.g. company.practicus.com'\nembedding_model_path = None\nvector_store = None\nmilvus_uri = None  # E.g. 'company.practicus.milvus.com'\n</code></pre> <pre><code>assert host, \"Please enter your host url\"\nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert vector_store in [\"ChromaDB\", \"MilvusDB\"], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"\nif vector_store == \"MilvusDB\":\n    assert \"milvus_uri\", \"Please enter your milvus connection uri\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#firstly-we-need-install-transformers-and-torch","title":"Firstly we need install transformers and torch","text":"<p>Run at terminal:</p> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#pip-install-transformers-sentence-transformers-langchain-langchain-community-chromadb","title":"pip install transformers sentence-transformers langchain langchain-community chromadb","text":"<ul> <li> <p>Transformers: It allows you to easily use Transformer-based models (such as BERT, GPT, etc.).-</p> </li> <li> <p>Sentence-Transformers: It produces vector representations of sentences using Transformer models.</p> </li> <li> <p>LangChain: It is used to manage more complex workflows with language models. </p> </li> <li> <p>Langchain-Community: Contains additional modules and components developed by the community for the LangChain library.</p> </li> <li> <p>ChromaDB: Used as a vector database. It is optimized for embeddings and similarity searches.</p> </li> <li> <p>PyPDF: A library used to process PDF files with Python. </p> </li> </ul>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#pip-install-torch-index-url-httpsdownloadpytorchorgwhlcpu","title":"pip install torch --index-url https://download.pytorch.org/whl/cpu","text":"<p>This command is used to install the PyTorch library with CPU support.</p> <p>Details:</p> <ul> <li>Torch (PyTorch): A library used for developing machine learning and deep learning models. It offers features such as tensor computations, automatic differentiation, and advanced modeling.</li> <li>--index-url https://download.pytorch.org/whl/cpu: This parameter uses a specific index URL to download the CPU version of PyTorch. If you do not want to install a GPU-specific version, this URL is used.</li> </ul> <pre><code># Prepare Data\nimport os\nimport requests\n\nrepo_owner = \"practicusai\"\nrepo_name = \"sample-data\"\nfile_path = \"hr_assistant\"\nbranch = \"main\"\n\n\nurl = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{file_path}?ref={branch}\"\n\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    files = response.json()\n\n    for file in files:\n        file_url = file[\"download_url\"]\n        file_name = file[\"name\"]\n\n        file_response = requests.get(file_url)\n        if file_response.status_code == 200:\n            with open(file_name, \"wb\") as f:\n                f.write(file_response.content)\n            print(f\"'{file_name}' successfully downloaded.\")\n        else:\n            print(f\"'{file_name}' not successfully downloaded.\")\nelse:\n    print(f\"HTTP Status: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#import-libraries","title":"Import Libraries","text":"<pre><code>from langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre> <p>This code initializes and validates the required variables for setting up the environment, including the host URL, embedding model path, and vector store configuration.</p> <ol> <li>Host URL:</li> <li><code>host = None</code>:<ul> <li>Placeholder for the host URL (e.g., <code>'company.practicus.com'</code>).</li> </ul> </li> <li> <p><code>assert host, \"Please enter your host url\"</code>:</p> <ul> <li>Ensures that a valid host URL is provided. Raises an error with the message <code>\"Please enter your host url\"</code> if <code>host</code> is not defined.</li> </ul> </li> <li> <p>Embedding Model Path:</p> </li> <li><code>embedding_model_path = None</code>:<ul> <li>Placeholder for the embedding model's file path.</li> </ul> </li> <li> <p><code>assert embedding_model_path, \"Please enter your embedding model path.\"</code>:</p> <ul> <li>Ensures that a valid embedding model path is provided. Raises an error if the path is not set.</li> </ul> </li> <li> <p>Vector Store Selection:</p> </li> <li><code>vector_store = None</code>:<ul> <li>Placeholder for selecting a vector store (<code>'ChromaDB'</code> or <code>'MilvusDB'</code>).</li> </ul> </li> <li> <p><code>assert vector_store in ['ChromaDB', 'MilvusDB'], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"</code>:</p> <ul> <li>Ensures that the vector store is either <code>'ChromaDB'</code> or <code>'MilvusDB'</code>. Raises an error if an invalid option is set.</li> </ul> </li> <li> <p>MilvusDB Configuration (Conditional):</p> </li> <li><code>if vector_store == 'MilvusDB':</code>:<ul> <li>Executes the following block only if <code>vector_store</code> is set to <code>'MilvusDB'</code>.</li> </ul> </li> <li><code>milvus_uri = None</code>:<ul> <li>Placeholder for the Milvus connection URI (e.g., <code>'company.practicus.milvus.com'</code>).</li> </ul> </li> <li><code>assert 'milvus_uri', \"Please enter your milvus connection uri\"</code>:<ul> <li>Ensures that a valid <code>milvus_uri</code> is provided when <code>MilvusDB</code> is used.</li> </ul> </li> </ol>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#define-llm-api-function-and-call-chatpracticus-in-this-function","title":"Define llm api function and call ChatPracticus in this function","text":"<p>Function interacts with the ChatPracticus API to send input data and retrieve a response using the provided API URL and token.</p> <ol> <li>Initialize ChatPracticus:</li> <li> <p><code>chat = ChatPracticus(...)</code>:</p> <ul> <li>Creates a <code>ChatPracticus</code> object with the following parameters:</li> <li><code>endpoint_url</code>: The API endpoint URL.</li> <li><code>api_token</code>: The token for authentication.</li> <li><code>model_id</code>: Currently ignored but reserved for future model-specific configurations.</li> </ul> </li> <li> <p>Invoke the API:</p> </li> <li> <p><code>response = chat.invoke(input=inputs)</code>:</p> <ul> <li>Sends the input data to the API and retrieves the response.</li> </ul> </li> <li> <p>Return the Response:</p> </li> <li><code>return(response.content)</code>:<ul> <li>Extracts and returns the content</li> </ul> </li> </ol> <pre><code>def call_llm_api(inputs, api_url, api_token):\n    # We need to give input to 'generate_response'. This function will use our 'api_token' and 'endpoint_url' and return the response.\n\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n\n    return response.content\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#get-all-mails-and-use-seperator-for-split-questions","title":"Get all mails and use seperator for split questions","text":"<p>This code reads a CSV file containing email data, extracts a specific column, and concatenates all messages into a single string, separated by a custom delimiter <code>//m-n-m//</code>.</p> <ol> <li>Import pandas:</li> <li> <p><code>import pandas as pd</code>:</p> <ul> <li>Imports the pandas library for data manipulation and analysis.</li> </ul> </li> <li> <p>Read the CSV File:</p> </li> <li> <p><code>df = pd.read_csv('single_sender_1k.csv')</code>:</p> <ul> <li>Reads the CSV file <code>single_sender_1k.csv</code> into a pandas DataFrame named <code>df</code>.</li> </ul> </li> <li> <p>Select Relevant Column:</p> </li> <li> <p><code>df = df[['message_sent']]</code>:</p> <ul> <li>Filters the DataFrame to include only the <code>message_sent</code> column, which contains the email messages.</li> </ul> </li> <li> <p>Initialize Merged String:</p> </li> <li> <p><code>merged_mails = ''</code>:</p> <ul> <li>Initializes an empty string <code>merged_mails</code> to store the concatenated messages.</li> </ul> </li> <li> <p>Concatenate Messages:</p> </li> <li><code>for message in df['message_sent']:</code>:<ul> <li>Iterates over each message in the <code>message_sent</code> column.</li> </ul> </li> <li><code>merged_mails = merged_mails + '//m-n-m//' + message</code>:<ul> <li>Appends each message to <code>merged_mails</code>, preceded by the delimiter <code>//m-n-m//</code>.</li> </ul> </li> </ol> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"single_sender_1k.csv\")\n\ndf = df[[\"message_sent\"]]\n\nmerged_mails = \"\"\n\nfor message in df[\"message_sent\"]:\n    merged_mails = merged_mails + \"//m-n-m//\" + message\n</code></pre> <p>This function takes a concatenated string of emails, splits it into individual email documents, and further divides the documents into smaller chunks using a specified chunk size and overlap.</p> <ol> <li>Initialize the Text Splitter:</li> <li> <p><code>CharacterTextSplitter</code> is configured with:</p> <ul> <li><code>separator=\"//m-n-m//\"</code>: The delimiter used to separate email strings.</li> <li><code>chunk_size</code>: The size of each chunk.</li> <li><code>chunk_overlap</code>: Overlapping characters between chunks for better context retention.</li> <li><code>length_function=len</code>: Uses the <code>len</code> function to determine text length.</li> <li><code>is_separator_regex=False</code>: Indicates the separator is a literal string, not a regex.</li> </ul> </li> <li> <p>Split Emails:</p> </li> <li> <p><code>emails = merged_mails.split('//m-n-m//')</code>:</p> <ul> <li>Splits the concatenated email string into individual emails using the delimiter.</li> </ul> </li> <li> <p>Transform Emails into Documents:</p> </li> <li> <p><code>documents = [Document(page_content=email.strip()) for email in emails if email.strip()]</code>:</p> <ul> <li>Creates <code>Document</code> objects for each email, excluding empty or whitespace-only emails.</li> </ul> </li> <li> <p>Chunk Documents:</p> </li> <li> <p><code>split_docs = text_splitter.split_documents(documents)</code>:</p> <ul> <li>Splits each document into smaller chunks based on the configured <code>chunk_size</code> and <code>chunk_overlap</code>.</li> </ul> </li> <li> <p>Aggregate Results:</p> </li> <li> <p><code>all_docs.extend(split_docs)</code>:</p> <ul> <li>Adds the resulting chunks to the <code>all_docs</code> list.</li> </ul> </li> <li> <p>Return Chunks:</p> </li> <li>Returns the <code>all_docs</code> list containing the split chunks.</li> </ol> <pre><code>def load_and_split_mails(merged_mails, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load and split email strings into chunks.\n\n    :param merged_mails: A single string containing all email contents, separated by '//m-n-m//'.\n    :param chunk_size: The maximum number of characters in each text chunk.\n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = CharacterTextSplitter(\n        separator=\"//m-n-m//\",  # Defines the separator used to split the text.\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False,\n    )\n\n    # Split mails\n    emails = merged_mails.split(\"//m-n-m//\")\n\n    # Transform to Document\n    documents = [Document(page_content=email.strip()) for email in emails if email.strip()]\n\n    # Split docs\n    split_docs = text_splitter.split_documents(documents)\n    all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#define-pdf-array","title":"Define pdf array","text":"<p>This code calls the <code>load_and_split_mails</code> function to process a concatenated string of emails and generate text chunks for further processing.</p> <ol> <li>Function Call:</li> <li> <p><code>text_chunks = load_and_split_mails(merged_mails)</code>:</p> <ul> <li>Passes the <code>merged_mails</code> string to the <code>load_and_split_mails</code> function.</li> </ul> </li> <li> <p>Processing Inside the Function:</p> </li> <li> <p>The <code>load_and_split_mails</code> function:</p> <ul> <li>Splits the concatenated email string into individual email documents.</li> <li>Further divides each document into smaller chunks based on a specified chunk size and overlap.</li> </ul> </li> <li> <p>Assign the Output:</p> </li> <li>The resulting list of chunks is assigned to the variable <code>text_chunks</code>.</li> </ol>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#output","title":"Output","text":"<ul> <li><code>text_chunks</code>:   A list of <code>Document</code> objects, where each object contains a smaller chunk of email text.</li> </ul> <pre><code># Create our chunks\ntext_chunks = load_and_split_mails(merged_mails)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#create-the-vector-store-and-model-path-is-given","title":"Create the vector store and model path is given","text":"<p>This code generates embeddings for email chunks and creates a ChromaDB vector store to facilitate document retrieval based on similarity.</p> <ol> <li>Check for ChromaDB:</li> <li> <p><code>if vector_store == 'ChromaDB':</code>:</p> <ul> <li>Executes the following block only if <code>vector_store</code> is set to <code>'ChromaDB'</code>.</li> </ul> </li> <li> <p>Define <code>create_chroma_vector_store</code> Function:</p> </li> <li> <p>This function handles the creation of a vector store using ChromaDB.</p> </li> <li> <p>Create Vector Store and Retriever:</p> </li> <li><code>retriever_mail = create_chroma_vector_store(text_chunks, embeddings_model_path)</code>:<ul> <li>Calls the <code>create_chroma_vector_store</code> function to generate embeddings and create the vector store.</li> </ul> </li> </ol> <pre><code># Generate embeddings and create vector store\nif vector_store == \"ChromaDB\":\n\n    def create_chroma_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n            encode_kwargs={\"normalize_embeddings\": False},\n        )\n\n        db_name = str(random.random())\n        vectorstore = Chroma.from_documents(\n            collection_name=db_name, documents=chunks, embedding=embeddings\n        )  # This method creates a vector store from the provided documents (chunks) and embeddings.\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_mail = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n        return retriever_mail\n\n    retriever_mail = create_chroma_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#optional-milvus-vector-db","title":"(OPTIONAL) Milvus Vector DB","text":"<p>This code sets up a vector store using MilvusDB to store embeddings for email chunks and enables similarity-based document retrieval.</p>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#workflow","title":"Workflow","text":"<ol> <li>Check for MilvusDB:</li> <li> <p><code>if vector_store == 'MilvusDB':</code></p> <ul> <li>Executes the following block only if <code>vector_store</code> is set to <code>'MilvusDB'</code>.</li> </ul> </li> <li> <p>Define <code>create_milvus_vector_store</code> Function:</p> </li> <li> <p>This function creates a vector store using MilvusDB.</p> </li> <li> <p>Create Vector Store and Retriever:</p> </li> <li><code>retriever_mail = create_milvus_vector_store(text_chunks, embeddings_model_path)</code>:<ul> <li>Calls the <code>create_milvus_vector_store</code> function to set up the Milvus vector store and create the retriever.</li> </ul> </li> </ol> <pre><code>from langchain_milvus import Milvus\nfrom pymilvus import connections\n\nif vector_store == \"MilvusDB\":\n\n    def create_milvus_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n            encode_kwargs={\"normalize_embeddings\": False},\n        )\n\n        connections.connect(\"default\", host=milvus_uri, port=\"19530\")\n\n        vectorstore = Milvus.from_documents(\n            documents=chunks,\n            embedding=embeddings,\n            collection_name=\"langchain_example\",\n            connection_args={\"uri\": f\"https://{milvus_uri}:19530\"},\n            drop_old=True,  # Drop the old Milvus collection if it exists\n        )\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_mail = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n        return retriever_mail\n\n    retriever_mail = create_milvus_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#define-format_docs-for-join-all-chunks","title":"Define format_docs for join all chunks","text":""},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#description","title":"Description","text":"<p>This function processes a list of document objects and combines their content into a single string, with each document's content separated by two newline characters (<code>\\n\\n</code>) for readability.</p> <ol> <li>Retrieve Content:</li> <li> <p>Iterates through the <code>docs</code> list to access the <code>page_content</code> attribute of each document.</p> </li> <li> <p>Join Content:</p> </li> <li>Combines all retrieved content into a single string.</li> <li> <p>Adds two newline characters (<code>\\n\\n</code>) between each document's content to improve separation and readability.</p> </li> <li> <p>Return Result:</p> </li> <li>The concatenated string is returned.</li> </ol> <pre><code>def format_docs(docs):\n    # Retrieves the content of each document in the `docs` list and joins the content of all documents into a single string, with each document's content separated by two newline characters.\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#all-chains-merged-into-each-other-at-this-function","title":"All chains merged into each other at this function","text":""},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#description_1","title":"Description","text":"<p>This function retrieves relevant documents related to a given question from a retriever, formats them into a prompt, and queries a language model (LLM) API to generate a response.</p> <ol> <li>Define Prompt Template:</li> <li>Creates a <code>PromptTemplate</code> object to format the input for the language model.</li> <li> <p>Template Details:</p> <ul> <li>Includes retrieved context (relevant documents).</li> <li>Contains the question.</li> <li>Instructs the model to respond with \"I don't know\" if it cannot answer.</li> </ul> </li> <li> <p>Retrieve Relevant Documents:</p> </li> <li><code>retriever.get_relevant_documents(question)</code>:<ul> <li>Retrieves the most relevant documents for the given question.</li> </ul> </li> <li> <p><code>format_docs(docs)</code>:</p> <ul> <li>Formats the retrieved documents into a single string, separated by two newline characters (<code>\\n\\n</code>).</li> </ul> </li> <li> <p>Format the Prompt:</p> </li> <li> <p><code>prompt_template.format(context=context, question=question)</code>:</p> <ul> <li>Generates a formatted prompt by inserting the context and question into the template.</li> </ul> </li> <li> <p>Query the LLM API:</p> </li> <li> <p><code>call_llm_api(prompt, api_url, api_token)</code>:</p> <ul> <li>Sends the formatted prompt to the LLM API and retrieves the response.</li> </ul> </li> <li> <p>Extract the Answer:</p> </li> <li> <p>Extracts the answer from the API response by splitting the output on the keyword <code>Answer:</code> and removing extra whitespace.</p> </li> <li> <p>Return the Answer:</p> </li> <li>Returns the extracted answer as a string.</li> </ol> <pre><code># Query the PDF using the API-based LLM\ndef query_mail(retriever, question, api_url, api_token):\n    \"\"\"\n    this function is used for returning response by using all of the chains we defined above\n\n    :param retriever : An instance of a retriever used to fetch relevant documents.\n    :param question : The question to be asked about the PDF content.\n    \"\"\"\n\n    prompt_template = PromptTemplate(  # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=(  # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        ),\n    )\n\n    docs = retriever.get_relevant_documents(\n        question\n    )  # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs)  # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question)  # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split(\"Answer:\")[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#chat-examples","title":"Chat Examples","text":"<p>This code initializes the Practicus environment by retrieving the current region using the <code>practicuscore</code> library.</p> <ol> <li>Import PracticusCore:</li> <li> <p><code>import practicuscore as prt</code>:</p> <ul> <li>Imports the <code>practicuscore</code> library and assigns it the alias <code>prt</code> for easier usage.</li> </ul> </li> <li> <p>Retrieve Region:</p> </li> <li><code>region = prt.get_region()</code>:<ul> <li>Calls the <code>get_region</code> function from the <code>practicuscore</code> module.</li> <li>This function retrieves the region configuration or information relevant to the Practicus environment.</li> <li>The <code>region</code> object typically contains details like available resources, models, or configurations tied to a specific geographic or logical region.</li> </ul> </li> </ol> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <p>This code retrieves a list of available models in the current region, displays them in a tabular format, and selects the first model for further use.</p> <ol> <li>Retrieve Model List:</li> <li> <p><code>my_model_list = region.model_list</code>:</p> <ul> <li>Accesses the <code>model_list</code> attribute of the <code>region</code> object.</li> <li>This attribute contains a list of available models in the region.</li> </ul> </li> <li> <p>Display Model List:</p> </li> <li> <p><code>display(my_model_list.to_pandas())</code>:</p> <ul> <li>Converts the <code>model_list</code> to a pandas DataFrame using the <code>.to_pandas()</code> method for better visualization.</li> <li>Displays the DataFrame, allowing the user to view details such as model names, versions, and statuses.</li> </ul> </li> <li> <p>Select First Model:</p> </li> <li> <p><code>model_name = my_model_list[0].name</code>:</p> <ul> <li>Accesses the first model in the list using index <code>0</code> and retrieves its name using the <code>.name</code> attribute.</li> </ul> </li> <li> <p>Print the Selected Model Name:</p> </li> <li><code>print(\"Using first model name:\", model_name)</code>:<ul> <li>Outputs the name of the selected model to the console for verification.</li> </ul> </li> </ol> <pre><code>my_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\nmodel_name = my_model_list[0].name\nprint(\"Using first model name:\", model_name)\n</code></pre> <p>This code retrieves a list of available model prefixes in the current region, displays them in a tabular format, and selects the first prefix for further use.</p> <ol> <li>Retrieve Model Prefix List:</li> <li> <p><code>my_model_prefixes = region.model_prefix_list</code>:</p> <ul> <li>Accesses the <code>model_prefix_list</code> attribute of the <code>region</code> object.</li> <li>This attribute contains a list of model prefixes available in the region.</li> </ul> </li> <li> <p>Display Model Prefix List:</p> </li> <li> <p><code>display(my_model_prefixes.to_pandas())</code>:</p> <ul> <li>Converts the <code>model_prefix_list</code> to a pandas DataFrame using the <code>.to_pandas()</code> method for better visualization.</li> <li>Displays the DataFrame, allowing the user to view details such as prefix keys and descriptions.</li> </ul> </li> <li> <p>Select First Prefix:</p> </li> <li> <p><code>model_prefix = my_model_prefixes[0].key</code>:</p> <ul> <li>Accesses the first prefix in the list using index <code>0</code> and retrieves its key using the <code>.key</code> attribute.</li> </ul> </li> <li> <p>Print the Selected Prefix Key:</p> </li> <li><code>print(\"Using first prefix:\", model_prefix)</code>:<ul> <li>Outputs the selected prefix key to the console for verification.</li> </ul> </li> </ol> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\nmodel_prefix = my_model_prefixes[0].key\nprint(\"Using first prefix:\", model_prefix)\n</code></pre> <p>This code constructs an API URL for accessing a model and generates a session token for authenticated communication.</p> <ol> <li>Construct the API URL:</li> <li> <p><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"</code>:</p> <ul> <li>Uses Python's f-string formatting to dynamically create the API URL.</li> <li>Components:</li> <li><code>host</code>: The base hostname of the API server.</li> <li><code>model_prefix</code>: The selected model prefix.</li> <li><code>model_name</code>: The name of the selected model.</li> <li>The resulting URL points to the specific endpoint for the desired model.</li> </ul> </li> <li> <p>Generate a Session Token:</p> </li> <li><code>token = prt.models.get_session_token(api_url=api_url)</code>:<ul> <li>Calls the <code>get_session_token</code> method from the <code>practicuscore.models</code> module.</li> <li>Parameters:</li> <li><code>api_url</code>: The constructed API URL.</li> <li>The session token is returned and stored in the <code>token</code> variable.</li> </ul> </li> </ol> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.models.get_session_token(api_url=api_url, token=token)\n</code></pre> <p>Get answer with our function </p> <pre><code># Example query\nanswer = query_mail(\n    retriever=retriever_mail,\n    question=\"How can business meetings be made more productive, and what are some alternative travel suggestions?\",\n    api_url=api_url,\n    api_token=token,\n)\nprint(answer)\n</code></pre> <p>Previous: Consume Parallel | Next: Memory Chatbot Sdk &gt; Chatbot-Console-OpenAI</p>"},{"location":"technical-tutorial/extras/generative-ai/langflow-apis/langflow-api/","title":"Using Langflow APIs","text":"<ul> <li>Login to Langflow service</li> <li>Create a flow</li> <li>Create an API endpoint name. e.g. my-api-endpoint<ul> <li>Click on flow name &gt; Edit Details &gt; Endpoint Name</li> </ul> </li> <li>Get an access token using Practicus AI SDK</li> <li>Note the LLM model token as well, API calls do not use tokens you saved in the UI</li> <li>Make API calls</li> </ul> <pre><code>service_url = \"https://langflow.dev.practicus.io\"\n# The below is defined in Langflow UI.\n# Open a flow, click on flow name &gt; Edit Details &gt; Endpoint Name\nendpoint_name = \"my-api-endpoint\"\n</code></pre> <pre><code>assert service_url, \"Please define service_url\"\nassert endpoint_name, \"Please define endpoint_name\"\n\napi_url = f\"{service_url}/api/v1/run/{endpoint_name}?stream=false\"\nprint(\"API url:\", api_url)\n# e.g. https://langflow.dev.practicus.io/api/v1/run/api-test1?stream=false\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\ntoken = None  # Get a new token, or reuse existing if not expired.\naccess_token = region.get_addon_session_token(key=\"langflow\", token=token)\n</code></pre> <pre><code>print(\"Access token for addon:\", access_token)\n</code></pre> <pre><code>open_ai_token = \"\"\nassert open_ai_token, \"Please define open_ai_token\"\n</code></pre> <pre><code>import requests\n\nheaders = {\"Content-Type\": \"application/json\", f\"Authorization\": f\"Bearer {access_token}\"}\n\npayload = {\n    \"input_value\": \"message\",\n    \"output_type\": \"chat\",\n    \"input_type\": \"chat\",\n    \"tweaks\": {\n        \"ChatInput-MRIWj\": {},\n        \"Prompt-KvhR7\": {},\n        \"ChatOutput-CuWil\": {},\n        \"OpenAIModel-dmT1W\": {\"api_key\": open_ai_token},\n    },\n}\n\nresponse = requests.post(api_url, headers=headers, json=payload)\n\nprint(response.status_code)\nresult = response.json()\nprint(result)\n</code></pre> <p>Previous: Build | Next: Ecomm Sdk &gt; Memory Chabot</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/","title":"Hosting of LLM application on AppHost without using front-end.","text":"<p>In this example we will try to host an LLM application which only used by API requests. This application will use an already deployed llm model.</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None  # E.g. 'company.practicus.com'\nlang_model = None  # E.g. 'LLAMA-3-70b'\napp_name = None  # E.g. 'api-chatbot'\nmodel_name = None\nmodel_prefix = None\ndeployment_setting_key = None\napp_prefix = None\n</code></pre> <pre><code>assert host, \"Please enter your host url\"\nassert lang_model, \"Please select a model\"\nassert app_name, \"Please enter application name\"\nassert model_name, \"Please enter model_name\"\nassert model_prefix, \"Please enter model_prefix\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre> <pre><code>import practicuscore as prt\nimport requests\nimport json\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#api-python-script","title":"API Python script","text":"<p>First of all we need to create a Python scripts which will be invoked by using requests. For this instance we should create an 'apis' folder which will contain the Python scripts. Then we can create our scripts within the folder.</p> <p>You can check-out sample api script simple_api.py</p> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code># Let's list our app deployments and select one of them.\nmy_app_settings = prt.apps.get_deployment_setting_list()\ndisplay(my_app_settings.to_pandas())\n</code></pre> <pre><code># Let's list our app prefixes and select one of them.\nmy_app_prefix_list = prt.apps.get_prefix_list()\ndisplay(my_app_prefix_list.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#testing-scripts","title":"Testing scripts","text":"<p>We can call our api scripts withIn this example and test them</p> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <pre><code>from apis.simple_api import Messages, ModelRequest, run\n\n# Let's test our message class\nmessages = Messages(content=\"Who is einstein?\", role=\"human\")\n\nmessages\n</code></pre> <pre><code># Let's test our Model request class\nmodelreq = ModelRequest(messages=messages, lang_model=lang_model, streaming=False, api_token=token, end_point=api_url)\n\ndict(modelreq)\n</code></pre> <pre><code># Let's test our prediction function\nresponse = run(modelreq)\n\ndict(response)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#deployment-of-api","title":"Deployment of API","text":"<p>After testing our python scripts, and make sure they work, we can deploy them by using our prefix and SDK</p> <pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#prediction-by-using-api","title":"Prediction by using API","text":"<p>After the deployment process we can consume the api url by using the code cell down below</p> <pre><code>api_url = f\"https://{host}/{app_prefix}/{app_name}/api/simple_api/\"\ntoken = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.apps.get_session_token(api_url=api_url, token=token)\n</code></pre> <pre><code>headers = {\"Authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\n\ndata_js = modelreq.model_dump_json(indent=2)\n\nresp = requests.post(api_url, json=data_js, headers=headers)\n\nif resp.ok:\n    print(f\"Response text:\", resp.text)\nelse:\n    print(resp.status_code, resp.text)\n</code></pre> <p>After deployment, comprehensive documentation for the API service is automatically generated. You can review this documentation and easily share it with your colleagues and other team members for seamless collaboration and onboarding.</p> <pre><code>documentation_url = f\"https://{host}/{app_prefix}/{app_name}/api/redoc/\"\nprint(f\"Your documentation url:{documentation_url}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#apissimple_apipy","title":"apis/simple_api.py","text":"<pre><code>from pydantic import BaseModel\nimport practicuscore as prt\nfrom practicuscore.gen_ai import PrtLangRequest, PrtLangMessage\nfrom requests import get\nimport json\n\n\"\"\" We are defining classes for taken inputs from api call. Using classes allows you to enforce type safety. \nThis means you can be sure that the data your functions receive has the correct types and structure, \nreducing the likelihood of runtime errors. But you don't have to use classes while creating api scripts.\"\"\"\n\n\n# Holds a message's content and an optional role for model to consume prompts.\nclass Messages(PrtLangMessage):\n    content: str\n    role: str | None = None\n\n\n# Stores details for a language model request, including the message, model type, and API information.\nclass ModelRequest(BaseModel):\n    messages: Messages\n    lang_model: str | None = \"None\"\n    streaming: bool | None = False\n    end_point: str\n    api_token: str\n\n\n# We need to define a 'run' function to process incoming data to API\n@prt.apps.api(\"/simple-api\")\nasync def run(payload: ModelRequest, **kwargs):\n    # Set up authorization headers using the API token from the payload\n    headers = {\"authorization\": f\"Bearer {payload.api_token}\"}\n\n    # Create a language model request object with message, model, and streaming options\n    practicus_llm_req = PrtLangRequest(\n        messages=[payload.messages],\n        lang_model=payload.lang_model,\n        streaming=payload.streaming,\n        llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"},\n        # (Optional) Additional parameters for the language model could be added here\n    )\n\n    # Convert the request object to a JSON string, excluding unset fields\n    data_js = json.loads(practicus_llm_req.model_dump_json(indent=2, exclude_unset=True))\n\n    # Send the HTTP GET request to the specified endpoint with the headers and JSON data\n    r = get(payload.end_point, headers=headers, json=data_js)\n\n    # Parse the JSON response text into a Python dictionary\n    parsed = json.loads(r.text)\n\n    # Return the parsed response dictionary\n    return parsed\n</code></pre> <p>Previous: Mobile-Banking | Next: Sdk LLM Apphost &gt; Non Stream &gt; Sdk Streamlit Hosting</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/","title":"Flow hosting of Langflow by using Streamlit","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#defining-parameters-from-region","title":"Defining parameters from region.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>app_name = None  # E.g. 'api-chatbot'\ndeployment_setting_key = None\napp_prefix = None\napp_dir = None\n</code></pre> <pre><code>assert app_name, \"Please enter application name\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code>my_app_prefix_list = prt.apps.get_prefix_list()\ndisplay(my_app_prefix_list.to_pandas())\napp_prefix = my_app_prefix_list[0].prefix\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>my_app_list = prt.apps.get_list()\ndisplay(my_app_list.to_pandas())\napp_name = my_app_list[0].name\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_app_settings = prt.apps.get_deployment_setting_list()\ndisplay(my_app_settings.to_pandas())\ndeployment_setting_key = my_app_settings[1].key\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#testing-app","title":"Testing App","text":"<p>First of all we need to create a \"Basic Prompting (Hello, World)\" flow at langflow and export the json of it.</p> <p>After exporting the json and save it within current directory of this tutorial, you should test it if it's working.</p> <pre><code>from langflow.load import run_flow_from_json\n\nresult = run_flow_from_json(flow=\"Flow.json\", input_value=\"What is the capital of Australia?\")\n\nrun_output = result[0]\nresult_data = run_output.outputs[0]\nmessage_obj = result_data.results[\"message\"]\nmessage_text = message_obj.data[\"text\"]\n\nmessage_text\n</code></pre> <p>Now we can create our own stream-lit app and use json of our flow within stream-lit's front-end. You can check-out our streamlit_app.py:</p> <p>View streamlit_app.py</p> <p>After creating/editing stream_app.py we could test it by hosting it as test by using our SDK:</p> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#deploying-app","title":"Deploying App","text":"<pre><code>import practicuscore as prt\n\nprt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # Current dir\n)\n</code></pre> <p>After the deployment process completed we could enter UI url (e.g. https://dev.practicus.io/apps/langflow-json-test/v1/) to show-case our app.</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\nfrom langflow.load import run_flow_from_json\n\n# The below will secure the page by authenticating and authorizing users with Single-Sign-On.\n# Please note that security code is only activate when the app is deployed.\n# Pages are always secure, even without the below, during development and only the owner can access them.\nprt.apps.secure_page(\n    page_title=\"Hello World App\",\n    must_be_admin=False,\n)\n\n\ndef main():\n    # The below is standard Streamlit code..\n    st.title(\"My App on Practicus AI\")\n\n    st.markdown(\"##### Welcome to the front-end of your flow\")\n\n    # Initialize session state to store chat messages if not already initialized.\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display all messages stored in session state in the chat interface.\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    # When the user inputs a message, add it to the chat history and display it.\n    if prompt := st.chat_input(\"I'm your flow, how may I help you?\"):\n        # Add user message to chat history\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        # Display user message in chat message container\n        with st.chat_message(\"user\"):\n            st.write(prompt)\n        # Display assistant response in chat message container\n        with st.chat_message(\"assistant\"):\n            message_placeholder = st.empty()\n            with st.spinner(text=\"Thinking...\"):\n                assistant_response = generate_response(prompt)\n                message_placeholder.write(assistant_response)\n        # Add assistant response to chat history\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n\n\ndef run_flow(message, flow_json):\n    result = run_flow_from_json(flow=flow_json, input_value=message, fallback_to_env_vars=True)  # False by default\n    return result\n\n\n# Function to generate a response from the flow based on the user's input.\ndef generate_response(prompt):\n    # Log the user's question.\n    # logging.info(f\"question: {prompt}\")\n\n    # Run the flow to get the response.\n    response = run_flow(message=prompt, flow_json=\"Flow.json\")\n\n    run_output = response[0]\n    result_data = run_output.outputs[0]\n    message_obj = result_data.results[\"message\"]\n    message_text = message_obj.data[\"text\"]\n\n    try:\n        # Log and return the assistant's response.\n        # logging.info(f\"answer: {message_obj}\")\n        return message_text\n    except Exception as exc:\n        # Log any errors and return a fallback message.\n        # logging.error(f\"error: {exc}\")\n        return \"Sorry, there was a problem finding an answer for you.\"\n\n\n# Run the main function to start the Streamlit app.\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Sdk Streamlit Hosting | Next: Milvus Embedding And LangChain &gt; Milvus Chain</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/","title":"Hosting of LLM which is built by using SDK","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>app_name = None  # E.g. 'api-chatbot'\ndeployment_setting_key = None\napp_prefix = None\napp_dir = None\n</code></pre> <pre><code>assert app_name, \"Please enter application name\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#test-app","title":"Test App","text":"<pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code>my_app_prefixes = region.app_prefix_list\ndisplay(my_app_prefixes.to_pandas())\n</code></pre> <pre><code>my_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#deploying-app","title":"Deploying App","text":"<pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,  # Deployment Key, ask admin for deployment key\n    prefix=app_prefix,  # Apphost deployment extension\n    app_name=app_name,\n    app_dir=None,  # Directory of files that will be deployed ('None' for current directory)\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code># The below is official Streamlit + Langchain demo.\n\nimport streamlit as st\nimport practicuscore as prt\n\nfrom langchain_practicus import ChatPracticus\n\nprt.apps.secure_page(\n    page_title=\"\ud83e\udd9c\ud83d\udd17 Quickstart App\"  # Give page title\n)\n\nst.title(\"\ud83e\udd9c\ud83d\udd17 Quickstart App v1\")  # Give app title\n\n\n# This function use our 'api_token' and 'endpoint_url' and return the response.\ndef generate_response(input_text, endpoint, api):\n    model = ChatPracticus(\n        endpoint_url=endpoint,  # Give model url\n        # Give api token , ask your admin for api\n        api_token=api,\n        model_id=\"model\",\n        verify_ssl=True,\n    )\n\n    st.info(model.invoke(input_text).content)  # We are give the input to model and get content\n\n\nwith st.form(\"my_form\"):  # Define our question\n    endpoint = st.text_input(\"Enter your end point url:\")\n    api = st.text_input(\"Enter your api token:\")\n    text = st.text_area(\n        \"Enter text:\",\n        \"Who is Einstein ?\",\n    )\n    submitted = st.form_submit_button(\"Submit\")  # Define the button\n\n    if submitted:\n        generate_response(text, endpoint, api)  # Return the response\n</code></pre> <p>Previous: Build | Next: Langflow LLM Apphost &gt; Langflow Streamlit Hosting</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/","title":"Chatbot-Console-OpenAI","text":"<pre><code>import openai\nimport psycopg2\nfrom datetime import datetime\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#code-functionality","title":"Code Functionality","text":"<p>This code attempts to establish a connection to a PostgreSQL database using the <code>psycopg2</code> library. If the connection is successful, it outputs a success message; otherwise, it captures and displays the error.</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#key-components","title":"Key Components","text":"<ol> <li><code>try</code> Block:</li> <li>Attempts to connect to the database using the <code>psycopg2.connect()</code> function.</li> <li> <p>Parameters such as <code>host</code>, <code>database</code>, <code>user</code>, and <code>password</code> are provided to define the connection details.</p> </li> <li> <p><code>except Exception as e</code>:</p> </li> <li>Captures any exceptions (errors) that occur during the connection attempt.</li> <li> <p>Prints an error message that includes details of the exception (<code>e</code>).</p> </li> <li> <p>Success Message:</p> </li> <li>If the connection is successful, <code>\"Connection successful!\"</code> is printed.</li> </ol> <pre><code>try:\n    connection = psycopg2.connect(host=\"...\", database=\"...\", user=\"...\", password=\"...\")\n    print(\"Connection successful!\")\nexcept Exception as e:\n    print(f\"Connection error: {e}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#configuration-setup-for-openai-api-and-database","title":"Configuration Setup for OpenAI API and Database","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#code-functionality_1","title":"Code Functionality","text":"<p>This code sets up the necessary configurations for: 1. Accessing the OpenAI API by specifying the API key. 2. Defining database connection parameters in a dictionary for later use.</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#key-components_1","title":"Key Components","text":"<ol> <li><code>openai.api_key = \"...\"</code></li> <li>Sets the API key for authenticating with the OpenAI API.</li> <li>The key is a string that identifies the user and grants access to OpenAI's services.</li> <li> <p>Purpose: Enable interaction with OpenAI's models (e.g., GPT) in the application.</p> </li> <li> <p><code>db_config</code></p> </li> <li>A dictionary containing database connection parameters:<ul> <li><code>host</code>: The database server address (e.g., IP or domain name).</li> <li><code>database</code>: The name of the specific database to connect to.</li> <li><code>user</code>: The username for authentication.</li> <li><code>password</code>: The password for the database user.</li> </ul> </li> <li>Purpose: Provide reusable configuration details for connecting to the PostgreSQL database.</li> </ol> <pre><code>openai.api_key = \"...\"\n\ndb_config = {\"host\": \"...\", \"database\": \"...\", \"user\": \"...\", \"password\": \"...\"}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#creating-a-conversations-table-in-postgresql","title":"Creating a Conversations Table in PostgreSQL","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#code-functionality_2","title":"Code Functionality","text":"<p>This code connects to a PostgreSQL database, creates a cursor object to execute SQL commands, and ensures that a <code>conversations</code> table exists. If the table doesn't exist, it is created. Finally, the changes are committed to the database.</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#key-components_2","title":"Key Components","text":"<ol> <li>Database Connection:</li> <li><code>conn = psycopg2.connect(**db_config)</code>:<ul> <li>Establishes a connection to the database using the <code>db_config</code> dictionary defined earlier.</li> </ul> </li> <li> <p><code>cursor = conn.cursor()</code>:</p> <ul> <li>Creates a cursor object, which is used to execute SQL commands.</li> </ul> </li> <li> <p>SQL Command to Create Table:</p> </li> <li><code>CREATE TABLE IF NOT EXISTS conversations</code>:<ul> <li>Creates a table named <code>conversations</code> if it does not already exist.</li> </ul> </li> <li> <p>Table Schema:</p> <ul> <li><code>id SERIAL PRIMARY KEY</code>: Auto-incrementing unique identifier for each record.</li> <li><code>session_id TEXT NOT NULL</code>: A session identifier for grouping related conversations.</li> <li><code>user_message TEXT</code>: Stores messages sent by the user.</li> <li><code>bot_response TEXT</code>: Stores the chatbot's responses.</li> <li><code>user_id TEXT</code>: Identifies the user, allowing for user-specific data tracking.</li> <li><code>timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP</code>: Records the time of each interaction, with a default value of the current time.</li> </ul> </li> <li> <p>Commit Changes:</p> </li> <li><code>conn.commit()</code>:<ul> <li>Saves the executed SQL changes (table creation) to the database.</li> </ul> </li> </ol> <pre><code>conn = psycopg2.connect(**db_config)\ncursor = conn.cursor()\n</code></pre> <pre><code>cursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS conversations (\n    id SERIAL PRIMARY KEY,\n    session_id TEXT NOT NULL,\n    user_message TEXT,\n    bot_response TEXT,\n    user_id TEXT,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\"\"\")\nconn.commit()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#saving-conversations-to-the-database","title":"Saving Conversations to the Database","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#code-functionality_3","title":"Code Functionality","text":"<p>This function, <code>save_conversation</code>, inserts a single conversation record into the <code>conversations</code> table in the PostgreSQL database. It logs both the user message and the bot response, along with identifying information like the user ID and session ID.</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#key-components_3","title":"Key Components","text":"<ol> <li>Function Parameters:</li> <li><code>user_message</code>: The message sent by the user.</li> <li><code>bot_response</code>: The chatbot's reply to the user.</li> <li><code>user_id</code>: A unique identifier for the user, enabling user-specific conversation tracking.</li> <li> <p><code>session_id</code>: A unique identifier for the session, grouping related conversations.</p> </li> <li> <p>SQL Command:</p> </li> <li> <p><code>INSERT INTO conversations (...) VALUES (%s, %s, %s, %s)</code>:</p> <ul> <li>Adds a new record to the <code>conversations</code> table with the provided values.</li> <li>Placeholders (<code>%s</code>) are used to securely pass dynamic data into the query, preventing SQL injection.</li> </ul> </li> <li> <p>Committing Changes:</p> </li> <li><code>conn.commit()</code>:<ul> <li>Saves the new conversation record to the database.</li> </ul> </li> </ol> <pre><code>def save_conversation(user_message, bot_response, user_id, session_id):\n    cursor.execute(\n        \"INSERT INTO conversations (user_message, bot_response, user_id, session_id) VALUES (%s, %s, %s, %s)\",\n        (user_message, bot_response, user_id, session_id),\n    )\n    conn.commit()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#retrieving-conversations-from-the-database","title":"Retrieving Conversations from the Database","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#code-functionality_4","title":"Code Functionality","text":"<p>The <code>get_conversation</code> function retrieves conversation records from the <code>conversations</code> table in the PostgreSQL database. It allows filtering by user ID, session ID, or both, and returns the conversation messages in chronological order.</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#key-components_4","title":"Key Components","text":"<ol> <li>Function Parameters:</li> <li><code>user_id</code> (optional): Filters the records by a specific user.</li> <li><code>session_id</code> (optional): Filters the records by a specific session.</li> <li> <p>If no parameters are provided, it retrieves all conversations.</p> </li> <li> <p>Conditional Logic:</p> </li> <li>Both <code>user_id</code> and <code>session_id</code> provided:<ul> <li>Retrieves conversations matching both user and session.</li> </ul> </li> <li>Only <code>user_id</code> provided:<ul> <li>Retrieves all conversations for the specified user.</li> </ul> </li> <li>Only <code>session_id</code> provided:<ul> <li>Retrieves all conversations for the specified session.</li> </ul> </li> <li> <p>No parameters:</p> <ul> <li>Retrieves all conversations in the table.</li> </ul> </li> <li> <p>SQL Query:</p> </li> <li>Dynamic Filtering:<ul> <li>Uses placeholders (<code>%s</code>) to securely include parameters in the query, preventing SQL injection.</li> </ul> </li> <li> <p>Ordering:</p> <ul> <li>Results are sorted by the <code>timestamp</code> column in ascending order to maintain chronological order.</li> </ul> </li> <li> <p>Returning Data:</p> </li> <li><code>cursor.fetchall()</code>:<ul> <li>Fetches all matching rows from the query result, returning a list of tuples with <code>user_message</code> and <code>bot_response</code>.</li> </ul> </li> </ol> <pre><code>def get_conversation(user_id=None, session_id=None):\n    if user_id and session_id:\n        cursor.execute(\n            \"SELECT user_message, bot_response FROM conversations WHERE user_id = %s AND session_id = %s ORDER BY timestamp ASC\",\n            (user_id, session_id),\n        )\n    elif user_id:\n        cursor.execute(\n            \"SELECT user_message, bot_response FROM conversations WHERE user_id = %s ORDER BY timestamp ASC\", (user_id,)\n        )\n    elif session_id:\n        cursor.execute(\n            \"SELECT user_message, bot_response FROM conversations WHERE session_id = %s ORDER BY timestamp ASC\",\n            (session_id,),\n        )\n    else:\n        cursor.execute(\"SELECT user_message, bot_response FROM conversations ORDER BY timestamp ASC\")\n    return cursor.fetchall()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#chatting-with-openais-api","title":"Chatting with OpenAI's API","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#code-functionality_5","title":"Code Functionality","text":"<p>The <code>chat_with_openai</code> function interacts with OpenAI's GPT model to generate a response based on a user's input prompt. It handles potential errors gracefully and returns either the AI's response or an error message.</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#key-components_5","title":"Key Components","text":"<ol> <li>Input Parameter:</li> <li> <p><code>prompt</code>: A string input provided by the user, serving as the basis for the AI's response.</p> </li> <li> <p>OpenAI API Call:</p> </li> <li> <p><code>openai.ChatCompletion.create()</code>:</p> <ul> <li>Sends a request to OpenAI's API to generate a response.</li> <li>Parameters:</li> <li><code>model=\"gpt-3.5-turbo\"</code>: Specifies the model to be used.</li> <li><code>messages</code>: A list containing the conversation context, where the user's input is structured as a message with the role \"user.\"</li> </ul> </li> <li> <p>Response Handling:</p> </li> <li> <p><code>response['choices'][0]['message']['content']</code>:</p> <ul> <li>Extracts the AI's generated text from the API response.</li> </ul> </li> <li> <p>Error Handling:</p> </li> <li><code>try</code> and <code>except</code>:<ul> <li>Catches any exceptions during the API call (e.g., network issues, API errors).</li> <li>Returns an error message if an exception occurs.</li> </ul> </li> </ol> <pre><code>def chat_with_openai(prompt):\n    try:\n        response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}])\n        return response[\"choices\"][0][\"message\"][\"content\"]\n    except Exception as e:\n        return f\"Hata: {e}\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#main-function-for-chatbot-interaction","title":"Main Function for Chatbot Interaction","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#code-functionality_6","title":"Code Functionality","text":"<p>The <code>main</code> function serves as the entry point for a chatbot application. It facilitates interaction between the user and the chatbot by handling user inputs, fetching previous conversation history, generating responses via OpenAI's API, and saving the new interactions to a database.</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#key-components_6","title":"Key Components","text":"<ol> <li>Welcome Message:</li> <li> <p><code>print(\"Welcome to Chatbot! (Type 'exit' to exit)\")</code>:</p> <ul> <li>Greets the user and provides instructions to exit the chatbot.</li> </ul> </li> <li> <p>User and Session Information:</p> </li> <li><code>user_id</code>:<ul> <li>Captures the user's unique identifier.</li> </ul> </li> <li> <p><code>session_id</code>:</p> <ul> <li>Captures the session identifier, where a new value signifies a new chat session.</li> </ul> </li> <li> <p>Fetching Conversation History:</p> </li> <li><code>get_conversation(user_id, session_id)</code>:<ul> <li>Retrieves previous conversations for the provided <code>user_id</code> and <code>session_id</code>.</li> </ul> </li> <li> <p>Displays the history to the user, if available, for context.</p> </li> <li> <p>Chat Loop:</p> </li> <li><code>while True</code>:<ul> <li>Keeps the chatbot active until the user types \"exit\".</li> </ul> </li> <li>User Input:<ul> <li>Prompts the user to enter a message.</li> <li>Exits the loop if the input is \"exit\".</li> </ul> </li> <li>Generating Bot Response:<ul> <li><code>chat_with_openai(user_message)</code>:</li> <li>Sends the user's message to OpenAI's API and fetches a bot response.</li> </ul> </li> <li>Displaying Conversation:<ul> <li>Prints both the user's message and the bot's response to the console.</li> </ul> </li> <li>Saving the Conversation:<ul> <li><code>save_conversation(user_message, bot_response, user_id, session_id)</code>:</li> <li>Stores the interaction in the database for future reference.</li> </ul> </li> </ol> <pre><code>def main():\n    print(\"Welcome to Chatbot! (Type 'exit' to exit)\")\n    user_id = input(\"Please enter your user ID: \")\n    session_id = input(\"Please enter your session ID (enter a new value for the new session): \")\n\n    conversation_history = get_conversation(user_id=user_id, session_id=session_id)\n    if conversation_history:\n        print(\"\\nPrevious Conversation History:\")\n        for user_message, bot_response in conversation_history:\n            print(f\"User: {user_message}\\nBot: {bot_response}\\n\")\n\n    while True:\n        user_message = input(\"You: \")\n        if user_message.lower() == \"exit\":\n            print(\"Exit is in progress...\")\n            break\n\n        bot_response = chat_with_openai(user_message)\n        print(f\"User: {user_message}\")\n        print(f\"Bot: {bot_response}\")\n\n        save_conversation(user_message, bot_response, user_id, session_id)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#running-the-chatbot-application","title":"Running the Chatbot Application","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#code-functionality_7","title":"Code Functionality","text":"<p>This block ensures the proper execution of the chatbot application by calling the <code>main</code> function and safely closing database resources when the program ends.</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#key-components_7","title":"Key Components","text":"<ol> <li>Entry Point:</li> <li> <p><code>if __name__ == \"__main__\":</code>:</p> <ul> <li>Ensures that the <code>main()</code> function is executed only when the script is run directly, not when imported as a module.</li> </ul> </li> <li> <p>Executing the Chatbot:</p> </li> <li> <p><code>main()</code>:</p> <ul> <li>Launches the chatbot application, allowing the user to interact with the AI and manage conversations.</li> </ul> </li> <li> <p>Resource Cleanup:</p> </li> <li><code>finally:</code>:<ul> <li>Ensures that the following resources are closed properly, regardless of whether an exception occurs:</li> <li><code>cursor.close()</code>:</li> <li>Closes the database cursor to free up resources.</li> <li><code>conn.close()</code>:</li> <li>Closes the database connection to prevent resource leaks.</li> </ul> </li> </ol> <pre><code>if __name__ == \"__main__\":\n    try:\n        main()\n    finally:\n        cursor.close()\n        conn.close()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/chatbot-console-openai/#stream_chatbotstreamlit_apppy","title":"stream_chatbot/streamlit_app.py","text":"<pre><code>import streamlit as st\nimport psycopg2\nimport practicuscore as prt\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest\nimport requests\nimport html\nimport datetime\n\ndb_config = {\"host\": \"...\", \"database\": \"...\", \"user\": \"...\", \"password\": \"...\"}\n\nconn = psycopg2.connect(**db_config)\ncursor = conn.cursor()\n\ncursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS conversations (\n    id SERIAL PRIMARY KEY,\n    user_message TEXT,\n    bot_response TEXT,\n    session_id TEXT,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\"\"\")\nconn.commit()\n\n\ndef save_conversation(user_message, bot_response, session_id):\n    if user_message and bot_response:\n        cursor.execute(\n            \"INSERT INTO conversations (user_message, bot_response, session_id) VALUES (%s, %s, %s)\",\n            (user_message, bot_response, session_id),\n        )\n        conn.commit()\n    elif user_message:\n        cursor.execute(\n            \"INSERT INTO conversations (user_message, session_id) VALUES (%s, %s)\", (user_message, session_id)\n        )\n        conn.commit()\n\n\ndef get_conversation(session_id=None, limit=5):\n    if session_id:\n        cursor.execute(\n            \"\"\"\n            SELECT user_message, bot_response \n            FROM conversations \n            WHERE session_id = %s \n            ORDER BY timestamp DESC \n            LIMIT %s\n        \"\"\",\n            (session_id, limit),\n        )\n    else:\n        cursor.execute(\n            \"\"\"\n            SELECT user_message, bot_response \n            FROM conversations \n            ORDER BY timestamp DESC \n            LIMIT %s\n        \"\"\",\n            (limit,),\n        )\n    return cursor.fetchall()\n\n\ndef get_conversation_with_timestamps(session_id=None):\n    if session_id:\n        cursor.execute(\n            \"SELECT user_message, bot_response, timestamp FROM conversations WHERE session_id = %s ORDER BY timestamp DESC\",\n            (session_id,),\n        )\n    else:\n        cursor.execute(\"SELECT user_message, bot_response, timestamp FROM conversations ORDER BY timestamp DESC\")\n    return cursor.fetchall()\n\n\ndef get_all_session_ids():\n    cursor.execute(\"\"\"\n        SELECT session_id\n        FROM conversations\n        GROUP BY session_id\n        ORDER BY MAX(timestamp) DESC\n    \"\"\")\n    return [row[0] for row in cursor.fetchall()]\n\n\ndef summarize_conversation(conversation_history):\n    summary = \"This is a summary of the conversation:\\n\"\n    for user_message, bot_response in conversation_history:\n        summary += f\"User: {user_message}\\nBot: {bot_response}\\n\"\n    return summary\n\n\ndef render_chat_history(conversation_history):\n    if conversation_history:\n        for user_message, bot_response, timestamp in reversed(conversation_history):\n            user_message = user_message if user_message is not None else \"\"\n            bot_response = bot_response if bot_response is not None else \"\"\n            formatted_timestamp = timestamp.strftime(\"%Y-%m-%d %H:%M\") if timestamp else \"Unknown\"\n\n            st.markdown(\n                f\"\"\"\n                &lt;div style='display: flex; align-items: flex-start; margin-bottom: 10px;'&gt;\n                    &lt;div style='font-size: 1.5em; margin-right: 10px;'&gt;\ud83d\udc64&lt;/div&gt;\n                    &lt;div style='background-color: #f0f8ff; color: #000; padding: 10px; border-radius: 10px; max-width: 70%;'&gt;\n                        {html.escape(user_message)}\n                        &lt;div style='font-size: 0.8em; color: #888; text-align: right;'&gt;{formatted_timestamp}&lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n                \"\"\",\n                unsafe_allow_html=True,\n            )\n\n            st.markdown(\n                f\"\"\"\n                &lt;div style='display: flex; align-items: flex-start; flex-direction: row-reverse; margin-bottom: 10px;'&gt;\n                    &lt;div style='font-size: 1.5em; margin-left: 10px;'&gt;\ud83e\udd16&lt;/div&gt;\n                    &lt;div style='background-color: #e8f5e9; color: #000; padding: 10px; border-radius: 10px; max-width: 70%;'&gt;\n                        {html.escape(bot_response)}\n                        &lt;div style='font-size: 0.8em; color: #888; text-align: right;'&gt;{formatted_timestamp}&lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n                \"\"\",\n                unsafe_allow_html=True,\n            )\n    else:\n        st.markdown(\"No conversation history found.\")\n\n\ndef delete_session(session_id):\n    cursor.execute(\"DELETE FROM conversations WHERE session_id = %s\", (session_id,))\n    conn.commit()\n\n\ndef generate_response(prompt, model, session_id=None):\n    api_url = \"...\"\n    token = prt.models.get_session_token(api_url=api_url)\n\n    conversation_history = get_conversation(session_id=session_id)\n    context = summarize_conversation(conversation_history)\n\n    full_prompt = context + \"User: \" + prompt\n\n    practicus_llm_req = PrtLangRequest(\n        messages=[PrtLangMessage(content=full_prompt, role=\"human\")], lang_model=model, streaming=True\n    )\n\n    headers = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n\n    with requests.post(api_url, headers=headers, data=data_js, stream=True) as r:\n        for word in r.iter_content(1024):\n            yield word.decode(\"utf-8\")\n\n\nst.set_page_config(page_title=\"Chatbot\", layout=\"wide\")\nst.title(\"Chatbot App\")\n\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = []\n\nif \"session_saved\" not in st.session_state:\n    st.session_state[\"session_saved\"] = False\n\nif \"selected_session_id\" not in st.session_state:\n    st.session_state[\"selected_session_id\"] = None\n\nchat_placeholder = st.empty()\nwith chat_placeholder.container():\n    if st.session_state[\"selected_session_id\"]:\n        conversation_history = get_conversation_with_timestamps(session_id=st.session_state[\"selected_session_id\"])\n        render_chat_history(conversation_history)\n    else:\n        st.write(\"Your conversation history will appear here.\")\n\nif st.button(\"Refresh\"):\n    if st.session_state[\"selected_session_id\"]:\n        conversation_history = get_conversation_with_timestamps(session_id=st.session_state[\"selected_session_id\"])\n        with chat_placeholder.container():\n            render_chat_history(conversation_history)\n\ninput_placeholder = st.empty()\nwith input_placeholder.container():\n    user_message = st.text_input(\"Your messages:\", placeholder=\"Write here...\", key=\"chat_input\")\n    if st.button(\"Send\"):\n        if not st.session_state[\"selected_session_id\"]:\n            st.session_state[\"selected_session_id\"] = \"temporary_session\"\n\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_message})\n        bot_response = \"\"\n        for chunk in generate_response(user_message, \"gpt-4o\", session_id=st.session_state[\"selected_session_id\"]):\n            bot_response += chunk\n            st.session_state[\"messages\"].append({\"role\": \"bot\", \"content\": chunk})\n            with chat_placeholder.container():\n                updated_conversation = get_conversation_with_timestamps(\n                    session_id=st.session_state[\"selected_session_id\"]\n                ) + [(user_message, bot_response, datetime.datetime.now())]\n                render_chat_history(updated_conversation)\n\n        if bot_response.strip():\n            save_conversation(user_message, bot_response, st.session_state[\"selected_session_id\"])\n            conversation_history = get_conversation_with_timestamps(session_id=st.session_state[\"selected_session_id\"])\n            with chat_placeholder.container():\n                render_chat_history(conversation_history)\n        else:\n            st.warning(\"A blank response was received, not recorded.\")\n\nwith st.sidebar:\n    st.header(\"Chat Management\")\n\n    st.subheader(\"Registered Chats\")\n    all_session_ids = get_all_session_ids()\n    if all_session_ids:\n        for sid in all_session_ids:\n            cols = st.columns([9, 1])\n            with cols[0]:\n                if st.button(f\"Session: {sid}\"):\n                    st.session_state[\"selected_session_id\"] = sid\n                    st.session_state[\"session_saved\"] = True\n            with cols[1]:\n                if st.button(\"\u274c\", key=f\"delete_{sid}\"):\n                    delete_session(sid)\n                    st.session_state[\"selected_session_id\"] = None\n    else:\n        st.write(\"No registered chat found.\")\n\n    st.subheader(\"Start New Chat\")\n    if st.button(\"New Chat\"):\n        st.session_state[\"selected_session_id\"] = None\n        st.session_state[\"messages\"] = []\n        st.session_state[\"session_saved\"] = False\n        st.query_params = {}\n\n    if st.session_state[\"session_saved\"]:\n        st.write(f\"Selected Session ID: {st.session_state['selected_session_id']}\")\n\n    if st.button(\"Save\"):\n        if not st.session_state[\"session_saved\"]:\n            st.session_state[\"selected_session_id\"] = st.text_input(\"New Chat ID\", placeholder=\"Enter new chat id\")\n        else:\n            cursor.execute(\n                \"SELECT session_id FROM conversations WHERE session_id = %s\", (st.session_state[\"selected_session_id\"],)\n            )\n            if cursor.fetchone():\n                st.warning(\"This session is already exist.\")\n            else:\n                cursor.execute(\n                    \"INSERT INTO conversations (session_id) VALUES (%s)\", (st.session_state[\"selected_session_id\"],)\n                )\n                conn.commit()\n                st.success(f\"Chat saved: {st.session_state['selected_session_id']}\")\n                st.session_state[\"session_saved\"] = True\n</code></pre> <p>Previous: Mail E-Assistant | Next: Stream Chatbot &gt; Memory Chabot</p>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/stream-chatbot/memory-chabot/","title":"Memory Chatbot","text":"<pre><code>import practicuscore as prt\n</code></pre> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre> <p>After testing our application we can set our configurations and start the deployment process.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/stream-chatbot/memory-chabot/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>app_name = \"...\"  # E.g. 'api-chatbot'\ndeployment_setting_key = \"...\"\napp_prefix = \"...\"\napp_dir = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/stream-chatbot/memory-chabot/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code>my_app_list = prt.apps.get_list()\ndisplay(my_app_list.to_pandas())\n\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_app_prefix_list = prt.apps.get_prefix_list()\ndisplay(my_app_prefix_list.to_pandas())\n\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>my_app_settings = prt.apps.get_deployment_setting_list()\ndisplay(my_app_settings.to_pandas())\n\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre> <pre><code>assert app_name, \"Please enter application name\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/stream-chatbot/memory-chabot/#deploying-app","title":"Deploying app","text":"<pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,  # Deployment Key, ask admin for deployment key\n    prefix=app_prefix,  # Apphost deployment extension\n    app_name=app_name,\n    app_dir=None,  # Directory of files that will be deployed ('None' for current directory)\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/stream-chatbot/memory-chabot/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/memory-chatbot-sdk/stream-chatbot/memory-chabot/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code>import streamlit as st\nimport psycopg2\nimport practicuscore as prt\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest\nimport requests\nimport html\nimport datetime\n\ndb_config = {\"host\": \"...\", \"database\": \"...\", \"user\": \"...\", \"password\": \"...\"}\n\nconn = psycopg2.connect(**db_config)\ncursor = conn.cursor()\n\ncursor.execute(\"\"\"\nCREATE TABLE IF NOT EXISTS conversations (\n    id SERIAL PRIMARY KEY,\n    user_message TEXT,\n    bot_response TEXT,\n    session_id TEXT,\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n)\n\"\"\")\nconn.commit()\n\n\ndef save_conversation(user_message, bot_response, session_id):\n    if user_message and bot_response:\n        cursor.execute(\n            \"INSERT INTO conversations (user_message, bot_response, session_id) VALUES (%s, %s, %s)\",\n            (user_message, bot_response, session_id),\n        )\n        conn.commit()\n    elif user_message:\n        cursor.execute(\n            \"INSERT INTO conversations (user_message, session_id) VALUES (%s, %s)\", (user_message, session_id)\n        )\n        conn.commit()\n\n\ndef get_conversation(session_id=None, limit=5):\n    if session_id:\n        cursor.execute(\n            \"\"\"\n            SELECT user_message, bot_response \n            FROM conversations \n            WHERE session_id = %s \n            ORDER BY timestamp DESC \n            LIMIT %s\n        \"\"\",\n            (session_id, limit),\n        )\n    else:\n        cursor.execute(\n            \"\"\"\n            SELECT user_message, bot_response \n            FROM conversations \n            ORDER BY timestamp DESC \n            LIMIT %s\n        \"\"\",\n            (limit,),\n        )\n    return cursor.fetchall()\n\n\ndef get_conversation_with_timestamps(session_id=None):\n    if session_id:\n        cursor.execute(\n            \"SELECT user_message, bot_response, timestamp FROM conversations WHERE session_id = %s ORDER BY timestamp DESC\",\n            (session_id,),\n        )\n    else:\n        cursor.execute(\"SELECT user_message, bot_response, timestamp FROM conversations ORDER BY timestamp DESC\")\n    return cursor.fetchall()\n\n\ndef get_all_session_ids():\n    cursor.execute(\"\"\"\n        SELECT session_id\n        FROM conversations\n        GROUP BY session_id\n        ORDER BY MAX(timestamp) DESC\n    \"\"\")\n    return [row[0] for row in cursor.fetchall()]\n\n\ndef summarize_conversation(conversation_history):\n    summary = \"This is a summary of the conversation:\\n\"\n    for user_message, bot_response in conversation_history:\n        summary += f\"User: {user_message}\\nBot: {bot_response}\\n\"\n    return summary\n\n\ndef render_chat_history(conversation_history):\n    if conversation_history:\n        for user_message, bot_response, timestamp in reversed(conversation_history):\n            user_message = user_message if user_message is not None else \"\"\n            bot_response = bot_response if bot_response is not None else \"\"\n            formatted_timestamp = timestamp.strftime(\"%Y-%m-%d %H:%M\") if timestamp else \"Unknown\"\n\n            st.markdown(\n                f\"\"\"\n                &lt;div style='display: flex; align-items: flex-start; margin-bottom: 10px;'&gt;\n                    &lt;div style='font-size: 1.5em; margin-right: 10px;'&gt;\ud83d\udc64&lt;/div&gt;\n                    &lt;div style='background-color: #f0f8ff; color: #000; padding: 10px; border-radius: 10px; max-width: 70%;'&gt;\n                        {html.escape(user_message)}\n                        &lt;div style='font-size: 0.8em; color: #888; text-align: right;'&gt;{formatted_timestamp}&lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n                \"\"\",\n                unsafe_allow_html=True,\n            )\n\n            st.markdown(\n                f\"\"\"\n                &lt;div style='display: flex; align-items: flex-start; flex-direction: row-reverse; margin-bottom: 10px;'&gt;\n                    &lt;div style='font-size: 1.5em; margin-left: 10px;'&gt;\ud83e\udd16&lt;/div&gt;\n                    &lt;div style='background-color: #e8f5e9; color: #000; padding: 10px; border-radius: 10px; max-width: 70%;'&gt;\n                        {html.escape(bot_response)}\n                        &lt;div style='font-size: 0.8em; color: #888; text-align: right;'&gt;{formatted_timestamp}&lt;/div&gt;\n                    &lt;/div&gt;\n                &lt;/div&gt;\n                \"\"\",\n                unsafe_allow_html=True,\n            )\n    else:\n        st.markdown(\"No conversation history found.\")\n\n\ndef delete_session(session_id):\n    cursor.execute(\"DELETE FROM conversations WHERE session_id = %s\", (session_id,))\n    conn.commit()\n\n\ndef generate_response(prompt, model, session_id=None):\n    api_url = \"...\"\n    token = prt.models.get_session_token(api_url=api_url)\n\n    conversation_history = get_conversation(session_id=session_id)\n    context = summarize_conversation(conversation_history)\n\n    full_prompt = context + \"User: \" + prompt\n\n    practicus_llm_req = PrtLangRequest(\n        messages=[PrtLangMessage(content=full_prompt, role=\"human\")], lang_model=model, streaming=True\n    )\n\n    headers = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n\n    with requests.post(api_url, headers=headers, data=data_js, stream=True) as r:\n        for word in r.iter_content(1024):\n            yield word.decode(\"utf-8\")\n\n\nst.set_page_config(page_title=\"Chatbot\", layout=\"wide\")\nst.title(\"Chatbot App\")\n\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = []\n\nif \"session_saved\" not in st.session_state:\n    st.session_state[\"session_saved\"] = False\n\nif \"selected_session_id\" not in st.session_state:\n    st.session_state[\"selected_session_id\"] = None\n\nchat_placeholder = st.empty()\nwith chat_placeholder.container():\n    if st.session_state[\"selected_session_id\"]:\n        conversation_history = get_conversation_with_timestamps(session_id=st.session_state[\"selected_session_id\"])\n        render_chat_history(conversation_history)\n    else:\n        st.write(\"Your conversation history will appear here.\")\n\nif st.button(\"Refresh\"):\n    if st.session_state[\"selected_session_id\"]:\n        conversation_history = get_conversation_with_timestamps(session_id=st.session_state[\"selected_session_id\"])\n        with chat_placeholder.container():\n            render_chat_history(conversation_history)\n\ninput_placeholder = st.empty()\nwith input_placeholder.container():\n    user_message = st.text_input(\"Your messages:\", placeholder=\"Write here...\", key=\"chat_input\")\n    if st.button(\"Send\"):\n        if not st.session_state[\"selected_session_id\"]:\n            st.session_state[\"selected_session_id\"] = \"temporary_session\"\n\n        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_message})\n        bot_response = \"\"\n        for chunk in generate_response(user_message, \"gpt-4o\", session_id=st.session_state[\"selected_session_id\"]):\n            bot_response += chunk\n            st.session_state[\"messages\"].append({\"role\": \"bot\", \"content\": chunk})\n            with chat_placeholder.container():\n                updated_conversation = get_conversation_with_timestamps(\n                    session_id=st.session_state[\"selected_session_id\"]\n                ) + [(user_message, bot_response, datetime.datetime.now())]\n                render_chat_history(updated_conversation)\n\n        if bot_response.strip():\n            save_conversation(user_message, bot_response, st.session_state[\"selected_session_id\"])\n            conversation_history = get_conversation_with_timestamps(session_id=st.session_state[\"selected_session_id\"])\n            with chat_placeholder.container():\n                render_chat_history(conversation_history)\n        else:\n            st.warning(\"A blank response was received, not recorded.\")\n\nwith st.sidebar:\n    st.header(\"Chat Management\")\n\n    st.subheader(\"Registered Chats\")\n    all_session_ids = get_all_session_ids()\n    if all_session_ids:\n        for sid in all_session_ids:\n            cols = st.columns([9, 1])\n            with cols[0]:\n                if st.button(f\"Session: {sid}\"):\n                    st.session_state[\"selected_session_id\"] = sid\n                    st.session_state[\"session_saved\"] = True\n            with cols[1]:\n                if st.button(\"\u274c\", key=f\"delete_{sid}\"):\n                    delete_session(sid)\n                    st.session_state[\"selected_session_id\"] = None\n    else:\n        st.write(\"No registered chat found.\")\n\n    st.subheader(\"Start New Chat\")\n    if st.button(\"New Chat\"):\n        st.session_state[\"selected_session_id\"] = None\n        st.session_state[\"messages\"] = []\n        st.session_state[\"session_saved\"] = False\n        st.query_params = {}\n\n    if st.session_state[\"session_saved\"]:\n        st.write(f\"Selected Session ID: {st.session_state['selected_session_id']}\")\n\n    if st.button(\"Save\"):\n        if not st.session_state[\"session_saved\"]:\n            st.session_state[\"selected_session_id\"] = st.text_input(\"New Chat ID\", placeholder=\"Enter new chat id\")\n        else:\n            cursor.execute(\n                \"SELECT session_id FROM conversations WHERE session_id = %s\", (st.session_state[\"selected_session_id\"],)\n            )\n            if cursor.fetchone():\n                st.warning(\"This session is already exist.\")\n            else:\n                cursor.execute(\n                    \"INSERT INTO conversations (session_id) VALUES (%s)\", (st.session_state[\"selected_session_id\"],)\n                )\n                conn.commit()\n                st.success(f\"Chat saved: {st.session_state['selected_session_id']}\")\n                st.session_state[\"session_saved\"] = True\n</code></pre> <p>Previous: Chatbot-Console-OpenAI | Next: Data Analysis &gt; Plot &gt; Introduction</p>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/","title":"Milvus Chain","text":""},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#firstly-we-need-install-transformers-and-torch","title":"Firstly we need install transformers and torch","text":"<p>In this tutorial we will see how to create a MILVUS vector store for encoded documents and how to store the embeddings in the desired vector store using Practicus AI SDK.</p> <p>Focuses: - Preparing test documents - Creating Milvus Vector Store - Creating an index - Inserting embeddings - Using the vector store within RAG pipeline</p> <p>Necessary libraries:</p>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None  # E.g. company.practicus.com'\nembedding_model_path = None\nmilvus_uri = None  # E.g. practicus-milvus.prt-ns-milvus.svc.cluster.local\nmilvus_port = 19530\nmodel_name = None\nmodel_prefix = None\n</code></pre> <pre><code>assert host, \"Please enter your host url\"\nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert milvus_uri, \"Please enter your milvus connection uri\"\nassert milvus_port, \"Please enter your milvus connection port\"\nassert model_name, \"Please enter your model_name\"\nassert model_prefix, \"Please enter your model_prefix\"\n</code></pre> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb pypdf\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre> <pre><code>import practicuscore as prt\nfrom transformers import pipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n\nprint(\"Using first model name:\", model_name)\n# Let's list our prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n\nprint(\"Using first prefix:\", model_prefix)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#prepare-test-document","title":"Prepare test document","text":"<pre><code>import requests\n\nurl = (\n    \"https://raw.githubusercontent.com/practicusai/sample-data/refs/heads/main/small_rag_document/test_document_v1.pdf\"\n)\n\noutput_file = \"test_document_v1.pdf\"\n\n# Sending a GET request to the URL\nresponse = requests.get(url)\n\n# Checking if the request was successful\nif response.status_code == 200:\n    # Writing the file to the specified path\n    with open(output_file, \"wb\") as file:\n        file.write(response.content)\n    print(f\"File downloaded successfully and saved as {output_file}\")\nelse:\n    print(f\"Failed to download file. HTTP status code: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#define-llm-api-function-and-call-chatpracticus-in-this-function","title":"Define llm api function and call ChatPracticus in this function","text":"<pre><code>def call_llm_api(inputs, api_url, api_token):\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n    # response = chat.invoke(\"What is Capital of France?\")  # This also works\n\n    return response.content\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#get-all-pdf-files-and-use-seperator-for-split-questions","title":"Get all pdf files and use seperator for split questions","text":"<pre><code>def load_and_split_pdfs(pdf_files, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load all pdf files and split with using the 'seperator'.\n\n    :param pdf_files: A list of paths to the PDF files to be processed.\n    :param chunk_size: The maximum number of characters in each text chunk.\n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = CharacterTextSplitter(  # Langchain method used to separate documents, there are different ways\n        separator=\"\\n\",  # Defines the separator used to split the text.\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False,\n    )\n\n    for pdf_file in pdf_files:\n        loader = PyPDFLoader(pdf_file)  # PDF loader compatible with langchain\n        documents = loader.load_and_split()\n        split_docs = text_splitter.split_documents(documents)\n        all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre> <pre><code># Define pdf array\npdf_list = [\"test_document_v1.pdf\"]\n\ntext_chunks = load_and_split_pdfs(pdf_list, chunk_size=500)\n</code></pre> <pre><code>context_array = []\nfor i, row in enumerate(text_chunks):\n    context_array.append(row.page_content)\n</code></pre> <pre><code>embedding_model = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n    model_name=embedding_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n    model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n    encode_kwargs={\"normalize_embeddings\": False},\n)\n</code></pre> <pre><code>embeddings = embedding_model.embed_documents(context_array)\n\nvector_dimension = len(embeddings[0])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#create-the-vector-store-by-using-given-embedding-model","title":"Create the vector store by using given embedding model","text":"<pre><code>from pymilvus import (\n    connections,\n    utility,\n    FieldSchema,\n    CollectionSchema,\n    DataType,\n    Collection,\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#1-connect-to-milvus","title":"1. connect to Milvus","text":"<p>Add a new connection alias <code>default</code> for Milvus server in <code>localhost:19530</code>. </p> <p>Actually the <code>default</code> alias is a building in PyMilvus. If the address of Milvus is the same as <code>localhost:19530</code>, you can omit all parameters and call the method as: <code>connections.connect()</code>.</p> <p>Note: the <code>using</code> parameter of the following methods is default to \"default\".</p> <pre><code>connections.connect(\"default\", host=milvus_uri, port=milvus_port)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#2-create-collection","title":"2. create collection","text":"<p>We need to create collection with desired fields, in this example our collection will be like down below:</p> field name field type other attributes field description 1 \"pk\" VARCHAR is_primary=True, auto_id=False \"primary key field\" 2 \"companyInfo\" VARCHAR max_length=65535 \"our text field\" 3 \"embeddings\" FloatVector dim, equals to embedding dimension \"vector field\" <pre><code>fields = [\n    # Id field of vectors\n    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n    # Embedded texts field\n    FieldSchema(name=\"companyInfo\", dtype=DataType.VARCHAR, max_length=65535),  # https://milvus.io/docs/limitations.md\n    # Embedded vectors field\n    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=vector_dimension),\n]\n\n# Creating schema and collection\nschema = CollectionSchema(fields, \"dummy_info is a basic example demonstrating document embedding.\")\ndummy_company_collection = Collection(\"dummy_info\", schema, consistency_level=\"Strong\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#3-insert-data","title":"3. insert data","text":"<p>We need to define our entities which will be inserted into our 'dummy_info' collection and insert them.</p> <p>The insert() method returns: - either automatically generated primary keys by Milvus if auto_id=True in the schema; - or the existing primary key field from the entities if auto_id=False in the schema.</p> <pre><code>entities = [\n    # provide the pk field because `auto_id` is set to False\n    [str(i) for i in range(len(text_chunks))],  # Will be inserted to first field, 'pk'\n    context_array,  # Will be inserted to second field, 'companyInfo'\n    embeddings,  # Will be inserted to third field, 'embeddings'\n]\n</code></pre> <pre><code>dummy_company_collection.insert(entities)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#4-create-index","title":"4. Create index","text":"<p>We need to index our inserted entities to creates queries.</p> <p>In this example we will use Ecludian Distance (L2) and Quantization-based index (IVF_FLAT) as indexing options.</p> <p>You can check what options do you have from Milvus Documentation</p> <pre><code>index = {\n    \"index_type\": \"IVF_FLAT\",\n    \"metric_type\": \"L2\",\n    \"params\": {\"nlist\": 128},\n}\n\ndummy_company_collection.create_index(\"embeddings\", index)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#test-the-created-milvus-db-collection","title":"Test the created Milvus DB Collection","text":"<p>After data were inserted into Milvus and indexed, you can perform: - search based on vector similarity - query based on scalar filtering(boolean, int, etc.) - hybrid search based on vector similarity and scalar filtering.</p> <p>Before conducting a search or a query, you need to load the data in <code>dummy_info</code> into memory.</p> <pre><code>dummy_company_collection.load()\n</code></pre> <pre><code># The query down below returns 'companyInfo' field of entities that has any 'pk' information, which means, all of the entities.\nentities = dummy_company_collection.query(expr=\"pk != ''\", output_fields=[\"companyInfo\"], limit=100)\n\n# Print out the retrieved entities\nfor entity in entities:\n    print(entity)\n</code></pre> <pre><code># Now we can embed a text and querry it on our collection\nvector_to_search = embedding_model.embed_documents([\"What is the company name?\"])\n</code></pre> <pre><code>import time\n\nsearch_params = {\n    \"metric_type\": \"L2\",\n    \"params\": {\"nprobe\": 10},\n}\n\nstart_time = time.time()\nresult = dummy_company_collection.search(\n    vector_to_search, \"embeddings\", search_params, limit=1, output_fields=[\"companyInfo\"]\n)\nend_time = time.time()\n\nfor hits in result:\n    for hit in hits:\n        print(f\"hit: {hit}, random field: {hit.entity.get('random')}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#integration-of-milvus-db-to-rag-pipeline","title":"Integration of Milvus DB to RAG pipeline","text":"<p>We use LangChain to integrate our Milvus vector collection into the RAG pipeline for efficient retrieval.</p> <pre><code>from langchain.vectorstores import Milvus\n\n\ndef initialize_milvus_retriever():\n    # Connect to the existing collection in Milvus without recreating it\n    milvus_retriever = Milvus(\n        embedding_model,\n        connection_args={\"uri\": f\"https://{milvus_uri}:{milvus_port}\"},\n        collection_name=\"dummy_info\",\n        text_field=\"companyInfo\",\n        vector_field=\"embeddings\",\n    )\n    return milvus_retriever\n</code></pre> <pre><code>milvus_retriever = initialize_milvus_retriever()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#define-format_docs-for-join-all-chunks","title":"Define format_docs for join all chunks","text":"<pre><code>def format_docs(docs):\n    # Retrieves the content of each document in the `docs` list and joins the content of\n    # all documents into a single string, with each document's content separated by two newline characters.\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#all-chains-merged-into-each-other-at-this-function","title":"All chains merged into each other at this function","text":"<pre><code># Query the PDF using the API-based LLM\ndef query_pdf(retriever, question, api_url, api_token):\n    \"\"\"\n    this function is used for returning response by using all of the chains we defined above\n\n    :param retriever : An instance of a retriever used to fetch relevant documents.\n    :param question : The question to be asked about the PDF content.\n    \"\"\"\n\n    prompt_template = PromptTemplate(  # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=(  # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        ),\n    )\n\n    docs = retriever.similarity_search(\n        question, k=3\n    )  # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs)  # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question)  # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split(\"Answer:\")[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#chat-example","title":"Chat Example","text":"<pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.models.get_session_token(api_url=api_url, token=token)\n</code></pre> <pre><code># Example query\nanswer = query_pdf(\n    retriever=milvus_retriever, question=\"What is the name of company?\", api_url=api_url, api_token=token\n)\nprint(answer)\n</code></pre> <pre><code># Deleting collection after tutorial\nutility.drop_collection(\"dummy_info\")\n</code></pre> <p>Previous: Langflow Streamlit Hosting | Next: Agentic Use Cases &gt; Product Rec &gt; Build</p>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/","title":"LangChain LLM Model: RAG Model for Banking","text":"<p>This notebook demonstrates the development of a Retrieval-Augmented Generation (RAG) model for banking-related queries.  The RAG model retrieves relevant contextual data and generates meaningful answers using a language model.  We utilize LangChain, Transformers, and other related libraries to build the model.</p>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None  # Example url -&gt; 'company.practicus.com'\nembedding_model_path = None\nmodel_name = None\nmodel_prefix = None\n\nvector_store = None\n\nif vector_store == \"MilvusDB\":\n    milvus_uri = None  # Milvus connection url, E.g. 'company.practicus.milvus.com'\n</code></pre> <pre><code>assert host, \"Please enter your host url\"\nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert model_name, \"Please enter your embedding model_name.\"\n\n# You can use one of ChromaDB or MilvusDB as vector store\nassert model_prefix, \"Please enter your embedding model_prefix.\"\n\nassert vector_store in [\"ChromaDB\", \"MilvusDB\"], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"\nif vector_store == \"MilvusDB\":\n    assert \"milvus_uri\", \"Please enter your milvus connection uri\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <pre><code>vector_store = \"MilvusDB\"\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#preparing-data","title":"Preparing Data","text":"<pre><code>import os\nimport requests\n\n# Create the GitHub API URL\nurl = \"https://api.github.com/repos/practicusai/sample-data/contents/mobile_banking_QA?ref=main\"\n\n# Call the API\nresponse = requests.get(url)\nif response.status_code == 200:\n    files = response.json()  # Get response in JSON format\n\n    for file in files:\n        file_url = file[\"download_url\"]\n        file_name = file[\"name\"]\n\n        # Download files\n        file_response = requests.get(file_url)\n        if file_response.status_code == 200:\n            with open(file_name, \"wb\") as f:\n                f.write(file_response.content)\n            print(f\"'{file_name}' successfully downloaded.\")\n        else:\n            print(f\"'{file_name}' file failed to download.\")\nelse:\n    print(f\"Failed to retrieve data from API, HTTP status: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#libraries-installation","title":"Libraries Installation","text":"<p>Ensure the necessary libraries are installed using the following commands:</p> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb pypdf\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#importing-required-libraries","title":"Importing Required Libraries","text":"<p>The notebook imports libraries such as Transformers for text processing, LangChain for building components like text splitters and vector stores, and additional utilities for embedding generation and document processing.</p> <pre><code>from transformers import pipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#define-llm-api-function","title":"Define LLM API Function","text":"<p>A function is defined to interact with the ChatPracticus API, sending input prompts and receiving language model-generated responses.  The API URL and token are required to authenticate and access the service.</p> <pre><code>def call_llm_api(inputs, api_url, api_token):\n    # We need to give input to 'generate_response'. This function will use our 'api_token' and 'endpoint_url' and return the response.\n\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n    # response = chat.invoke(\"What is Capital of France?\")  # This also works\n\n    return response.content\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#load-and-split-pdf-files","title":"Load and Split PDF Files","text":"<p>PDF documents are loaded and split into manageable text chunks using LangChain's <code>CharacterTextSplitter</code>.  This enables the processing of large documents for retrieval tasks.</p> <pre><code>def load_and_split_pdfs(pdf_files, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load all pdf files and split with using the 'seperator'.\n\n    :param pdf_files: A list of paths to the PDF files to be processed.\n    :param chunk_size: The maximum number of characters in each text chunk.\n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = (\n        CharacterTextSplitter(  # langchain method used to separate documents, there are different methods as well\n            separator=\"  \\n \\n \\n \\n\",  # Defines the separator used to split the text.\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            length_function=len,\n            is_separator_regex=False,\n        )\n    )\n\n    for pdf_file in pdf_files:\n        loader = PyPDFLoader(pdf_file)  # pdf loader compatible with langchain\n        documents = loader.load_and_split()\n        split_docs = text_splitter.split_documents(documents)\n        all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#define-pdf-files-and-process-them","title":"Define PDF Files and Process Them","text":"<p>A list of PDF files is specified, and the <code>load_and_split_pdfs</code> function processes these files into text chunks for downstream retrieval and analysis.</p> <pre><code>pdf_list = [\"Mobile_Banking_QA.pdf\"]\n\ntext_chunks = load_and_split_pdfs(pdf_list)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#create-chroma-vector-store","title":"Create Chroma Vector Store","text":"<p>The function creates a Chroma vector store, which encodes the text chunks into embeddings using a pre-trained model.  The vector store allows similarity-based document retrieval.</p> <pre><code>if vector_store == \"ChromaDB\":\n    # Generate embeddings and create ChromaDB vector store\n    def create_chroma_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n            encode_kwargs={\"normalize_embeddings\": False},\n        )\n\n        # This method creates a vector store from the provided documents (chunks) and embeddings.\n        vectorstore_pdf = Chroma.from_documents(\n            collection_name=\"langchain_example\", documents=chunks, embedding=embeddings\n        )\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to\n        # the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_pdf = vectorstore_pdf.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n        return retriever_pdf\n\n    retriever_pdf = create_chroma_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#optional-create-milvus-db","title":"(OPTIONAL) Create Milvus DB","text":"<p>This function creates a vector store in Milvus by generating embeddings for text chunks using a HuggingFace pre-trained model. It connects to the Milvus database, stores the embeddings in a specified collection, and ensures any old collections with the same name are dropped. The resulting vector store is converted into a retriever for similarity-based searches, retrieving the top 2 relevant documents for a query.</p> <pre><code>from langchain_milvus import Milvus\nfrom pymilvus import connections\n\nif vector_store == \"MilvusDB\":\n\n    def create_milvus_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings(  # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path,  # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={\"device\": \"cpu\"},  # Configuration for running model on cpu.\n            encode_kwargs={\"normalize_embeddings\": False},\n        )\n\n        connections.connect(\"default\", host=milvus_uri, port=\"19530\")  # Connection to milvus db\n\n        vectorstore = Milvus.from_documents(\n            documents=chunks,\n            embedding=embeddings,\n            collection_name=\"langchain_example\",  # Name for created vector table\n            connection_args={\n                \"uri\": f\"https://{milvus_uri}:19530\"  # Connection configuration to milvus db\n            },\n            drop_old=True,  # Drop the old Milvus collection if it exists\n        )\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents\n        # to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_pdf = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n        return retriever_pdf\n\n    retriever_pdf = create_milvus_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#format-document-chunks","title":"Format Document Chunks","text":"<p>A utility function combines document chunks into a single string format.  This formatted text is passed to the language model for querying.</p> <pre><code>def format_docs(docs):\n    # Retrieves the content of each document in the `docs` list and joins the content of all documents into a single string, with each document's content separated by two newline characters.\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#query-pdf-using-llm","title":"Query PDF Using LLM","text":"<p>This function combines document retrieval, prompt construction, and LLM inference to answer a given query.  The RAG model retrieves context from the vector store, formats it, and queries the LLM.</p> <pre><code># Query the PDF using the API-based LLM\ndef query_pdf(retriever, question, api_url, api_token):\n    \"\"\"\n    this function is used for returning response by using all of the chains we defined above\n\n    :param retriever : An instance of a retriever used to fetch relevant documents.\n    :param question : The question to be asked about the PDF content.\n    \"\"\"\n\n    prompt_template = PromptTemplate(  # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=(  # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        ),\n    )\n\n    docs = retriever.get_relevant_documents(\n        question\n    )  # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs)  # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question)  # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split(\"Answer:\")[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#practicus-integration","title":"Practicus Integration","text":"<p>Imports the Practicus library, which is used for managing API configurations and token generation for LLM queries.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>api_url = f\"https://dev.practicus.io/models/llm-proxy/\"\ntoken = None  # Get a new token, or reuse existing if not expired.\ntoken = prt.models.get_session_token(api_url=api_url, token=token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/mobile-banking/mobile-banking/#execute-a-sample-query","title":"Execute a Sample Query","text":"<p>Demonstrates querying the RAG model with a sample question about banking services.  The system retrieves relevant context and generates an answer using the integrated LLM.</p> <pre><code># Example query\nanswer = query_pdf(\n    retriever=retriever_pdf, question=\"How can I send money using IBAN\", api_url=api_url, api_token=token\n)\nprint(answer)\n</code></pre> <pre><code># Example query\nanswer = query_pdf(\n    retriever=retriever_pdf,\n    question=\"My transaction was interrupted at an ATM, where should I apply?\",\n    api_url=api_url,\n    api_token=token,\n)\nprint(answer)\n</code></pre> <pre><code># Example query\nanswer = query_pdf(\n    retriever=retriever_pdf, question=\"can I transfer money between my accounts\", api_url=api_url, api_token=token\n)\nprint(answer)\n</code></pre> <p>Previous: AI Assistants | Next: LLM Apps &gt; API LLM Apphost &gt; Build</p>"},{"location":"technical-tutorial/extras/generative-ai/prtchatbot/prtchatbot/","title":"Memory Chatbot","text":"<pre><code>import practicuscore as prt\n</code></pre> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre> <p>After testing our application we can set our configurations and start the deployment process.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/prtchatbot/prtchatbot/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>app_name = \"prtchatbot3\"  # E.g. 'api-chatbot'\ndeployment_setting_key = \"appdepl\"\napp_prefix = \"apps\"\napp_dir = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/prtchatbot/prtchatbot/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code>my_app_list = prt.apps.get_list()\ndisplay(my_app_list.to_pandas())\n\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_app_prefix_list = prt.apps.get_prefix_list()\ndisplay(my_app_prefix_list.to_pandas())\n\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>my_app_settings = prt.apps.get_deployment_setting_list()\ndisplay(my_app_settings.to_pandas())\n\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre> <pre><code>assert app_name, \"Please enter application name\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/prtchatbot/prtchatbot/#deploying-app","title":"Deploying app","text":"<pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,  # Deployment Key, ask admin for deployment key\n    prefix=app_prefix,  # Apphost deployment extension\n    app_name=app_name,\n    app_dir=None,  # Directory of files that will be deployed ('None' for current directory)\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/prtchatbot/prtchatbot/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/prtchatbot/prtchatbot/#homepy","title":"Home.py","text":"<pre><code>import streamlit as st\nimport uuid\nimport random\nfrom chatbot import get_response_from_model\nfrom db import save_message_to_db, get_session_messages, delete_session_from_db, create_connection, update_message_in_db\nimport datetime\nimport textwrap\n\nst.set_page_config(page_title=\"Chatbot App\", layout=\"wide\")\nst.title(\"\ud83e\udd16 Practicus Proxy Chatbot\")\n\nif \"live_summary\" not in st.session_state:\n    st.session_state.live_summary = \"No summary generated yet.\"\n\nwith st.sidebar:\n    st.markdown(\"### \u2699\ufe0f Settings\")\n\n    model_name_list = [\"llm-proxy\", \"llama-3-70b\", \"tinyllama1b\"]\n    selected_model_name = st.selectbox(\"\ud83d\udce6 Select Model\", model_name_list)\n\n    if \"session_id\" not in st.session_state:\n        st.session_state.session_id = str(uuid.uuid4())\n\n    if \"selectbox_key\" not in st.session_state:\n        st.session_state.selectbox_key = str(random.randint(0, 1_000_000))\n\n    if \"chat_history\" not in st.session_state:\n        st.session_state.chat_history = []\n\n    connection = create_connection()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT session_id, title, created_at FROM sessions ORDER BY created_at DESC\")\n    saved_sessions = cursor.fetchall()\n    session_labels = [\"New Session\"] + [f\"{row[1]} ({row[2]})\" for row in saved_sessions]\n    session_ids = [None] + [row[0] for row in saved_sessions]\n    cursor.close()\n    connection.close()\n\n    if \"selected_session_id\" not in st.session_state:\n        st.session_state.selected_session_id = None\n\n    if st.session_state.selected_session_id in session_ids:\n        default_index = session_ids.index(st.session_state.selected_session_id)\n    else:\n        default_index = 0\n\n    selected_index = st.selectbox(\n        \"\ud83d\udcac Select Session\",\n        options=range(len(session_labels)),\n        format_func=lambda i: session_labels[i],\n        index=default_index,\n        key=st.session_state.selectbox_key\n    )\n\n    selected_session = session_ids[selected_index]\n    st.session_state.selected_session_id = selected_session\n\n    if selected_session:\n        connection = create_connection()\n        cursor = connection.cursor()\n        cursor.execute(\"SELECT content, role FROM messages WHERE session_id = %s ORDER BY created_at\", (selected_session,))\n        messages = cursor.fetchall()\n        st.session_state.chat_history = [{\"role\": role, \"content\": content} for content, role in messages]\n        cursor.close()\n        connection.close()\n        st.session_state.session_id = selected_session\n        st.success(\"\ud83d\udd04 Session loaded.\")\n\n    st.text_input(\"\ud83d\udca1 Session Title\", value=f\"Session - {st.session_state.session_id[:8]}\", key=\"session_title\")\n\n    if st.button(\"\ud83c\udd95 New Session\"):\n        st.session_state.chat_history = []\n        st.session_state.session_id = str(uuid.uuid4())\n        st.session_state.selected_session_id = None\n        st.session_state.selectbox_key = str(random.randint(0, 1_000_000))\n        st.session_state.live_summary = \"No summary generated yet.\"\n        st.rerun()\n\n    if st.button(\"\ud83d\udcbe Save Session\"):\n        now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n        title = st.session_state.session_title\n        connection = create_connection()\n        cursor = connection.cursor()\n        cursor.execute(\"SELECT session_id FROM sessions WHERE session_id = %s\", (st.session_state.session_id,))\n        if cursor.fetchone() is None:\n            cursor.execute(\n                \"INSERT INTO sessions (session_id, user_id, title, created_at, language) VALUES (%s, %s, %s, %s, %s)\",\n                (st.session_state.session_id, 1, title, now, \"English\"),\n            )\n        else:\n            cursor.execute(\n                \"UPDATE sessions SET title = %s, language = %s WHERE session_id = %s\",\n                (title, \"English\", st.session_state.session_id),\n            )\n        connection.commit()\n        cursor.close()\n        connection.close()\n        st.success(\"\u2705 Session saved!\")\n\n    if st.button(\"\ud83d\uddd1\ufe0f Delete Session\"):\n        if st.session_state.selected_session_id:\n            delete_session_from_db(st.session_state.selected_session_id)\n            st.session_state.chat_history = []\n            st.session_state.selected_session_id = None\n            st.session_state.selectbox_key = str(random.randint(0, 1_000_000))\n            st.session_state.live_summary = \"No summary generated yet.\"\n            st.rerun()\n\n    with st.expander(\"\ud83e\udde0 Live Conversation Summary\", expanded=True):\n        st.markdown(st.session_state.live_summary)\n\n\nif \"chat_history\" in st.session_state:\n    for chat in st.session_state.chat_history:\n        st.chat_message(chat[\"role\"]).write(chat[\"content\"])\n\n    with st.expander(\"\ud83e\udde0 Edit/Delete Past User Messages\"):\n        for i, chat in enumerate(st.session_state.chat_history):\n            if chat[\"role\"] != \"user\":\n                continue\n\n            col1, col2, col3, col4 = st.columns([6, 1, 1, 1])\n\n            with col1:\n                new_content = st.text_area(f\"Message {i+1}\", value=chat[\"content\"], key=f\"context_edit_{i}\")\n\n            with col2:\n                if st.button(\"\u274c\", key=f\"delete_context_{i}\"):\n                    connection = create_connection()\n                    cursor = connection.cursor()\n                    cursor.execute(\"DELETE FROM messages WHERE session_id = %s AND content = %s AND role = 'user'\",\n                                   (st.session_state.session_id, chat[\"content\"]))\n                    connection.commit()\n                    cursor.close()\n                    connection.close()\n                    st.session_state.chat_history.pop(i)\n                    st.rerun()\n\n            with col3:\n                if st.button(\"\ud83d\udd3c Take Before\", key=f\"fork_before_{i}\"):\n                    new_history = st.session_state.chat_history[:i+1]\n                    context_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in new_history])\n                    summary_prompt = textwrap.dedent(f\"\"\"\n                        Here is a part of a previous conversation. \n                        Please summarize it briefly and meaningfully.\n                        This summary will be used as context for a new conversation.\n\n                        Conversation:\n                        {context_text}\n                    \"\"\").strip()\n                    summary_response = get_response_from_model(selected_model_name, summary_prompt)\n                    intro_msg = f\"\"\"\nContext summary transferred from the previous conversation:\n{summary_response}\n\nThis chat will continue based on the summarized context above.\nWhat is your first question?\n                    \"\"\".strip()\n                    new_history.append({\"role\": \"assistant\", \"content\": intro_msg})\n                    st.session_state.chat_history = new_history\n                    st.session_state.session_id = str(uuid.uuid4())\n                    st.session_state.selected_session_id = None\n                    st.session_state.selectbox_key = str(random.randint(0, 1_000_000))\n                    st.session_state.live_summary = summary_response\n                    st.rerun()\n\n            with col4:\n                if st.button(\"\ud83d\udd3d Take After\", key=f\"fork_after_{i}\"):\n                    new_history = st.session_state.chat_history[i:]\n                    context_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in new_history])\n                    summary_prompt = textwrap.dedent(f\"\"\"\n                        Here is a part of a previous conversation. \n                        Please summarize it briefly and meaningfully.\n                        This summary will be used as context for a new conversation.\n\n                        Conversation:\n                        {context_text}\n                    \"\"\").strip()\n                    summary_response = get_response_from_model(selected_model_name, summary_prompt)\n                    intro_msg = f\"\"\"\nContext summary transferred from the previous conversation:\n{summary_response}\n\nThis chat will continue based on the summarized context above.\nWhat is your first question?\n                    \"\"\".strip()\n                    new_history.append({\"role\": \"assistant\", \"content\": intro_msg})\n                    st.session_state.chat_history = new_history\n                    st.session_state.session_id = str(uuid.uuid4())\n                    st.session_state.selected_session_id = None\n                    st.session_state.selectbox_key = str(random.randint(0, 1_000_000))\n                    st.session_state.live_summary = summary_response\n                    st.rerun()\n\n            if new_content != chat[\"content\"]:\n                update_message_in_db(st.session_state.session_id, chat[\"content\"], new_content)\n                st.session_state.chat_history[i][\"content\"] = new_content\n\nwith st.container():\n    st.markdown(\"---\")\n    if st.button(\"\ud83d\udcdd Chat Summary\") and st.session_state.chat_history:\n        recent = st.session_state.chat_history[-4:]\n        recent_text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in recent])\n        current_summary = st.session_state.live_summary or \"No summary yet.\"\n        summary_prompt = textwrap.dedent(f\"\"\"\n            Below is the previous conversation summary.\n\n            Please update the summary since new messages were added.\n\n            --- Previous Summary ---\n            {current_summary}\n\n            --- New Messages ---\n            {recent_text}\n        \"\"\").strip()\n        summary_response = get_response_from_model(selected_model_name, summary_prompt)\n        st.markdown(\"### \ud83d\udccb Updated Chat Summary\")\n        st.info(summary_response)\n\nuser_input = st.chat_input(\"Type your message...\")\n\nif user_input:\n    st.chat_message(\"user\").write(user_input)\n    response = get_response_from_model(selected_model_name, user_input)\n    st.chat_message(\"assistant\").write(response)\n    st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_input})\n    st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": response})\n    save_message_to_db(st.session_state.session_id, \"user\", user_input)\n    save_message_to_db(st.session_state.session_id, \"assistant\", response)\n    recent_text = f\"user: {user_input}\\nassistant: {response}\"\n    live_summary_prompt = textwrap.dedent(f\"\"\"\n        Below is the current summary of the conversation.\n\n        Please update it using the new exchange.\n\n        --- Previous Summary ---\n        {st.session_state.live_summary}\n\n        --- New Messages ---\n        {recent_text}\n    \"\"\").strip()\n    st.session_state.live_summary = get_response_from_model(selected_model_name, live_summary_prompt)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/prtchatbot/prtchatbot/#chatbotpy","title":"chatbot.py","text":"<pre><code>import requests\nimport time\nfrom db import save_message_to_db, get_session_messages\nimport streamlit as st\nfrom practicuscore.gen_ai import ChatCompletionRequest\nimport practicuscore as prt\nimport jwt\nfrom datetime import datetime, timedelta\nfrom typing import Optional\n\n\ndef get_jwt_expiry(token: str) -&gt; Optional[datetime]:\n    \"\"\"\n    Extracts the expiry date from a JWT token without verifying its signature.\n\n    Args:\n        token (str): The JWT token string.\n\n    Returns:\n        Optional[datetime]: The expiry date as a datetime object if available, otherwise None.\n    \"\"\"\n    decoded_payload: dict = jwt.decode(token, options={\"verify_signature\": False})\n    expiry_timestamp: Optional[int] = decoded_payload.get(\"exp\")\n\n    if expiry_timestamp is None:\n        return None\n\n    return datetime.utcfromtimestamp(expiry_timestamp)\n\ndef get_response_from_model(model_name, user_input):\n    return call_practicus_model(user_input, model_name)\n\ndef get_token_for_model(model_name):\n    token_data = st.session_state.get(f\"token_{model_name}\", None)\n\n\n    if not token_data or token_data[\"expires_at\"] &lt; time.time():\n        api_url = f\"https://dev.practicus.io/models/{model_name}/\"\n        token = prt.models.get_session_token(api_url=api_url)\n\n        expires_at = time.time() + 3 * 60 * 60\n        st.session_state[f\"token_{model_name}\"] = {\"token\": token, \"expires_at\": expires_at}\n\n\n        return token\n    else:\n        return token_data[\"token\"]\n\n\ndef call_practicus_model(user_input, model_name):\n    if \"chat_history\" not in st.session_state:\n        st.session_state.chat_history = []\n\n    token = get_token_for_model(model_name)\n\n    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        *st.session_state.chat_history,\n        {\"role\": \"user\", \"content\": user_input},\n    ]\n\n    chat_request = ChatCompletionRequest(model=model_name, messages=messages, temperature=0.7, max_tokens=512)\n\n    payload = chat_request.model_dump()\n    response = requests.post(f\"https://dev.practicus.io/models/{model_name}/\", headers=headers, json=payload)\n\n    try:\n        data = response.json()\n    except Exception as e:\n        return f\"\u274c Failed to parse response: {e}\"\n\n    if \"choices\" in data:\n        return data[\"choices\"][0][\"message\"][\"content\"]\n    elif \"content\" in data:\n        return data[\"content\"]\n    else:\n        return f\"\u26a0\ufe0f Unexpected response: {data}\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/prtchatbot/prtchatbot/#dbpy","title":"db.py","text":"<pre><code>import psycopg2\nimport datetime\n\n\ndef create_connection():\n    try:\n        connection = psycopg2.connect(\n            host=\"test-db-1.c34rytcb0n56.us-east-1.rds.amazonaws.com\",\n            database=\"llm\",\n            user=\"prt_analytics_user\",\n            password=\"\",\n        )\n        return connection\n    except Exception as e:\n        print(f\"Connection error: {e}\")\n        return None\n\n\ndef save_message_to_db(session_id, role, content):\n    connection = create_connection()\n    if connection:\n        cursor = connection.cursor()\n        cursor.execute(\"SELECT session_id FROM sessions WHERE session_id = %s\", (session_id,))\n        result = cursor.fetchone()\n\n        if result is None:\n            now = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n            cursor.execute(\n                \"INSERT INTO sessions (session_id, title, created_at, language) VALUES (%s, %s, %s, %s)\",\n                (session_id, f\"Session - {session_id[:8]}\", now, \"English\"),\n            )\n\n            connection.commit()\n\n        cursor.execute(\n            \"INSERT INTO messages (session_id, role, content) VALUES (%s, %s, %s)\", (session_id, role, content)\n        )\n        connection.commit()\n        cursor.close()\n        connection.close()\n\n\ndef get_session_messages(session_id):\n    connection = create_connection()\n    cursor = connection.cursor()\n    cursor.execute(\"SELECT content, role FROM messages WHERE session_id = %s ORDER BY created_at\", (session_id,))\n    messages = cursor.fetchall()\n    cursor.close()\n    connection.close()\n    return [{\"role\": role, \"content\": content} for content, role in messages]\n\n\ndef delete_session_from_db(session_id):\n    connection = create_connection()\n    if connection:\n        cursor = connection.cursor()\n        cursor.execute(\"DELETE FROM messages WHERE session_id = %s\", (session_id,))\n        cursor.execute(\"DELETE FROM sessions WHERE session_id = %s\", (session_id,))\n        connection.commit()\n        cursor.close()\n        connection.close()\n\n\ndef update_message_in_db(session_id, old_content, new_content):\n    connection = create_connection()\n    if connection:\n        cursor = connection.cursor()\n        cursor.execute(\n            \"\"\"\n            UPDATE messages \n            SET content = %s \n            WHERE session_id = %s AND content = %s\n        \"\"\",\n            (new_content, session_id, old_content),\n        )\n        connection.commit()\n        cursor.close()\n        connection.close()\n</code></pre> <p>Previous: Using Databases | Next: Advanced LangChain &gt; Lang Chain LLM Model</p>"},{"location":"technical-tutorial/extras/modeling/AutoML/","title":"Predicting Insurance Charges with Automated Machine Learning (AutoML)","text":"<p>In the insurance sector, accurately forecasting the costs associated with policyholders is crucial for pricing policies competitively while ensuring profitability. For insurance companies, the ability to predict these costs helps in tailoring individual policies, identifying key drivers of insurance costs, and ultimately enhancing customer satisfaction by offering policies that reflect a customer's specific risk profile and needs.</p>"},{"location":"technical-tutorial/extras/modeling/AutoML/#objective","title":"Objective","text":"<p>The primary goal of this notebook is to showcase how Practicus AI users can leverage AutoML to swiftly and proficiently develop a predictive model, minimizing the need for extensive manual modeling work. By engaging with this notebook, you'll acquire knowledge on how to:</p> <ul> <li> <p>Load the dataset specific to insurance costs</p> </li> <li> <p>Using AutoML to train and tune a predictive model tailored for insurance charges prediction.</p> </li> <li> <p>Assess the model's performance accurately</p> </li> </ul> <p>Let's embark on this journey!</p>"},{"location":"technical-tutorial/extras/modeling/AutoML/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>service_key = None  # Eg. Mlflow\nexperiment_name = None  # Eg. automl-experiment-test\n</code></pre> <pre><code>assert service_key, \"Please select a service_key\"\nassert experiment_name, \"Please select a experiment_name\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\n</code></pre> <pre><code># If you don't know experiment service key and name you can checkout down below\n\naddon_list = prt.addons.get_list()\ndisplay(addon_list.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-1-setting-up-the-environment","title":"Step 1: Setting Up the Environment","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#first-we-need-to-set-up-our-python-environment-with-the-necessary-libraries-pycaret-an-automl-library-simplifies-the-machine-learning-workflow-enabling-us-to-efficiently-develop-predictive-models","title":"First, we need to set up our Python environment with the necessary libraries. PyCaret, an AutoML library, simplifies the machine learning workflow, enabling us to efficiently develop predictive models.","text":"<pre><code># Standard libraries for data manipulation and numerical operations\nimport pandas as pd\nimport numpy as np\nfrom pycaret.regression import *  # Importing PyCaret's regression module\n\n# Extras\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-2-loading-the-dataset","title":"Step 2: Loading the Dataset","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#the-dataset-consists-of-1337-observations-and-7-variables-with-charges-being-the-target-variable-we-aim-to-predict-this-dataset-is-a-common-benchmark-in-insurance-cost-predictions","title":"The dataset consists of 1,337 observations and 7 variables, with 'charges' being the target variable we aim to predict. This dataset is a common benchmark in insurance cost predictions.","text":"<pre><code>data_set_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/insurance.csv\"}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n\nproc = worker.load(data_set_conn, engine=\"AUTO\")\n\ndf = proc.get_df_copy()\ndisplay(df)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-3-initializing-the-automl-experiment","title":"Step 3: Initializing the AutoML Experiment","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#pycarets-regression-module-is-utilized-here-for-predicting-a-continuous-target-variable-ie-insurance-costs-we-begin-by-initializing-our-automl-experiment","title":"PyCaret's regression module is utilized here for predicting a continuous target variable, i.e., insurance costs. We begin by initializing our AutoML experiment.","text":"<pre><code>from pycaret.regression import RegressionExperiment, load_model, predict_model\n\nexp = RegressionExperiment()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#this-step-sets-up-our-environment-within-pycaret-allowing-for-automated-feature-engineering-model-selection-and-more","title":"This step sets up our environment within PyCaret, allowing for automated feature engineering, model selection, and more.","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#step-4-configuring-the-experiment","title":"Step 4: Configuring the Experiment","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#well-configure-our-experiment-with-a-specific-name-making-it-easier-to-manage-and-reference","title":"We'll configure our experiment with a specific name, making it easier to manage and reference.","text":"<pre><code>prt.experiments.configure(service_key=service_key, experiment_name=experiment_name)\n# No experiment service selected, will use MlFlow inside the Worker. To configure manually:\n# configure_experiment(experiment_name=experiment_name, service_name='Experiment service name')\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-5-preparing-data-with-pycarets-setup","title":"Step 5: Preparing Data with PyCaret's Setup","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#a-critical-step-where-we-specify-our-experiments-details-such-as-the-target-variable-session-id-for-reproducibility-and-whether-to-log-the-experiment-for-tracking-purposes","title":"A critical step where we specify our experiment's details, such as the target variable, session ID for reproducibility, and whether to log the experiment for tracking purposes.","text":"<pre><code>setup_params = {\"normalize\": True, \"normalize_method\": \"minmax\"}\n</code></pre> <pre><code>exp.setup(\n    data=df,\n    target=\"charges\",\n    session_id=42,\n    log_experiment=True,\n    feature_selection=True,\n    experiment_name=experiment_name,\n    **setup_params,\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-6-model-selection-and-tuning","title":"Step 6: Model Selection and Tuning","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#this-command-leverages-automl-to-compare-different-models-automatically-selecting-the-one-that-performs-best-according-to-a-default-or-specified-metric-its-a-quick-way-to-identify-a-strong-baseline-model-without-manual-experimentation","title":"This command leverages AutoML to compare different models automatically, selecting the one that performs best according to a default or specified metric. It's a quick way to identify a strong baseline model without manual experimentation.","text":"<pre><code>best_model = exp.compare_models()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#once-a-baseline-model-is-selected-this-step-fine-tunes-its-hyperparameters-to-improve-performance-the-use-of-tune-sklearn-and-hyperopt-indicates-an-advanced-search-across-the-hyperparameter-space-for-optimal-settings-which-can-significantly-enhance-model-accuracy","title":"Once a baseline model is selected, this step fine-tunes its hyperparameters to improve performance. The use of tune-sklearn and hyperopt indicates an advanced search across the hyperparameter space for optimal settings, which can significantly enhance model accuracy.","text":"<pre><code>tune_params = {}\n</code></pre> <pre><code>tuned_model = exp.tune_model(best_model, **tune_params)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-7-finalizing-the-model","title":"Step 7: Finalizing the Model","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#after-tuning-the-model-is-finalized-meaning-its-retrained-on-the-entire-dataset-including-the-validation-set-this-step-ensures-the-model-is-as-generalized-as-possible-before-deployment","title":"After tuning, the model is finalized, meaning it's retrained on the entire dataset, including the validation set. This step ensures the model is as generalized as possible before deployment.","text":"<pre><code>final_model = exp.finalize_model(tuned_model)\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-8-predictions-and-saving-the-model","title":"Step 8: Predictions and Saving the Model","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#we-predict-insurance-costs-using-our-final-model-and-save-it-for-future-use-ensuring-operational-scalability","title":"We predict insurance costs using our final model and save it for future use, ensuring operational scalability.","text":"<pre><code>predictions = exp.predict_model(final_model, data=df)\ndisplay(predictions)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#the-last-step-involves-saving-the-trained-model-for-future-use-such-as-deployment-in-a-production-environment-or-further-evaluation-it-ensures-the-models-availability-beyond-the-current-session-facilitating-operationalization-and-scalability","title":"The last step involves saving the trained model for future use, such as deployment in a production environment or further evaluation. It ensures the model's availability beyond the current session, facilitating operationalization and scalability.","text":"<pre><code>exp.save_model(final_model, \"model\")\n</code></pre> <pre><code>loaded_model = load_model(\"model\")\n\npredictions = predict_model(loaded_model, data=df)\ndisplay(predictions)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#by-following-these-steps-insurance-companies-can-develop-a-predictive-model-for-insurance-costs-using-practicus-ais-automl-capabilities-this-approach-reduces-the-need-for-extensive-manual-modeling-enabling-insurers-to-efficiently-adapt-to-changing-market-conditions-and-customer-profiles","title":"By following these steps, insurance companies can develop a predictive model for insurance costs using Practicus AI's AutoML capabilities. This approach reduces the need for extensive manual modeling, enabling insurers to efficiently adapt to changing market conditions and customer profiles.","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#bank_marketingsnippetslabel_encoderpy","title":"bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == \"O\"]\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#bank_marketingsnippetsone_hotpy","title":"bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(\n    df,\n    text_col_list: list[str] | None,\n    max_categories: int = 25,\n    dummy_option: DummyOption = DummyOption.KEEP_ALL,\n    result_col_suffix: list[str] | None = None,\n    result_col_prefix: list[str] | None = None,\n):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multi collinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == \"object\" and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(\n            df[col],\n            prefix=(result_col_prefix if result_col_prefix else col),\n            drop_first=(dummy_option == DummyOption.DROP_FIRST),\n        )\n        dummies = dummies.rename(columns=lambda x: f\"{x}_{result_col_suffix}\" if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#model_observabilitymodelpy","title":"model_observability/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\n\ndef add_features(df):\n    for column in df.select_dtypes(include=\"object\"):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include=\"int64\"):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include=\"float64\"):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"income &gt;50K\"])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#model_trackingmodel_driftmodelpy","title":"model_tracking/model_drift/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib\n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlice_creammodeljson","title":"sparkml/ice_cream/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlice_creammodelpy","title":"sparkml/ice_cream/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([StructField(\"features\", DoubleType(), True)])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row[\"Temperature\"])),), axis=1), schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlspark_with_jobjobpy","title":"sparkml/spark_with_job/job.py","text":"<pre><code>import practicuscore as prt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit, max, min, stddev, corr\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\n\nspark = SparkSession.builder.appName(\"Advanced Data Processing\").getOrCreate()\n\nfile_path = \"/home/ubuntu/samples/data/insurance.csv\"\ndata = spark.read.csv(file_path, header=True, inferSchema=True)\nmissing_data = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n\ncategorical_columns = [\"sex\", \"smoker\", \"region\"]\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n\ndata = data.withColumn(\n    \"bmi_category\",\n    when(col(\"bmi\") &lt; 18.5, lit(\"underweight\"))\n    .when((col(\"bmi\") &gt;= 18.5) &amp; (col(\"bmi\") &lt; 25), lit(\"normal\"))\n    .when((col(\"bmi\") &gt;= 25) &amp; (col(\"bmi\") &lt; 30), lit(\"overweight\"))\n    .otherwise(lit(\"obese\")),\n)\n\nfeature_columns = [\"age\", \"bmi\", \"children\", \"sex_encoded\", \"smoker_encoded\", \"region_encoded\"]\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\ndata = pipeline.fit(data).transform(data)\n\noutput_path = \"/home/ubuntu/my/processed_insurance_data.parquet/\"\n\ndata.write.parquet(output_path, mode=\"overwrite\")\n\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlspark_with_jobrun2c741eprt_dist_jobjson","title":"sparkml/spark_with_job/run/2c741e/prt_dist_job.json","text":"<pre><code>{\"job_type\":\"spark\",\"job_dir\":\"~/my/02_batch_job/\",\"initial_count\":2,\"coordinator_port\":7077,\"additional_ports\":[4040,7078,7079],\"terminate_on_completion\":false,\"py_file\":\"job.py\",\"executors\":[{\"rank\":0,\"instance_id\":\"5cf16b71\"},{\"rank\":1,\"instance_id\":\"63e80dc8\"}]}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlspark_with_jobrun2c741erank_0json","title":"sparkml/spark_with_job/run/2c741e/rank_0.json","text":"<pre><code>{\"rank\":0,\"instance_id\":\"5cf16b71\",\"state\":\"completed\",\"used_ram\":1187,\"peak_ram\":1187,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlspark_with_jobrun2c741erank_1json","title":"sparkml/spark_with_job/run/2c741e/rank_1.json","text":"<pre><code>{\"rank\":1,\"instance_id\":\"63e80dc8\",\"state\":\"running\",\"used_ram\":284,\"peak_ram\":293,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#xgboostmodelpy","title":"xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#xgboostmodel_custom_dfpy","title":"xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre> <p>Previous: XGBoost | Next: Shap Analysis</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/","title":"Shap Analysis","text":"<pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport shap\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import plot_tree\n</code></pre> <pre><code># load the csv file as a data frame\ndf = pd.read_csv(\"samples/data/iris.csv\")\n</code></pre> <pre><code>label_encoder = LabelEncoder()\ndf[\"species\"] = label_encoder.fit_transform(df[\"species\"])\n</code></pre> <pre><code># Separate Features and Target Variables\nX = df.drop(columns=\"species\")\ny = df[\"species\"]\n</code></pre> <pre><code># Create Train &amp; Test Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=13)\n\n# Build the model\nrf_clf = RandomForestClassifier(max_features=2, n_estimators=100, bootstrap=True)\n\nrf_clf.fit(X_train, y_train)\n</code></pre> <pre><code>y_pred = rf_clf.predict(X_test)\nprint(classification_report(y_pred, y_test))\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#iris-data-set-feature-importance","title":"Iris Data Set Feature Importance","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview","title":"Overview","text":"<p>The following visualization presents a ranked bar chart depicting the relative importance of each feature used by our machine learning model to predict Iris flower species. In this analysis, we observe the features derived from the dimensions of the flower's petals and sepals: <code>petal_width</code>, <code>petal_length</code>, <code>sepal_length</code>, and <code>sepal_width</code>.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#interpretation","title":"Interpretation","text":"<ul> <li>Petal Width (petal_width): This feature has the highest relative importance score, indicating its strong predictive power in distinguishing between Iris species. The model heavily relies on petal width, suggesting that this attribute significantly influences the model's decision-making process.</li> <li>Petal Length (petal_length): Following petal width, petal length also shows substantial influence on the model's predictions. Its prominence implies that the length of the petal is another defining characteristic in species classification.</li> <li>Sepal Length (sepal_length) &amp; Sepal Width (sepal_width): These features hold less significance compared to petal measurements. Their lower importance scores may reflect their reduced discriminative ability in the context of this specific model and dataset.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#implications-for-model-refinement","title":"Implications for Model Refinement","text":"<p>We leverage this feature importance chart to guide feature selection and model simplification efforts. In high-dimensional datasets, reducing model complexity and computational load by pruning less significant features is crucial. Additionally, the chart enhances model interpretability by highlighting which features predominantly drive predictions, providing insights into potential dependencies within the dataset.</p> <p>For instance, if petal measurements are more influential than sepal measurements, it could indicate that petals play a more decisive role in identifying the Iris species. </p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion","title":"Conclusion","text":"<p>This graphical analysis is instrumental for data-driven decision-making, ensuring that our model is both efficient and interpretable. By focusing on the most informative features, we can streamline the model while maintaining or even enhancing its accuracy.</p> <pre><code>importances = rf_clf.feature_importances_\nindices = np.argsort(importances)\nfeatures = df.columns\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"y\", align=\"center\")\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n</code></pre> <pre><code># compute SHAP values\nexplainer = shap.TreeExplainer(rf_clf)\nshap_values = explainer.shap_values(X)\n\nclass_names = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#random-forest-component-analysis-decision-tree-visualization","title":"Random Forest Component Analysis: Decision Tree Visualization","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_1","title":"Overview","text":"<p>This visualization illustrates a single decision tree from a Random Forest classifier trained on the Iris dataset. It's a visual representation of how the algorithm makes decisions and classifies different Iris species: setosa, versicolor, and virginica.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#specific-observations","title":"Specific Observations","text":"<ul> <li>The first split is made with the condition <code>petal_width &lt;= 0.75</code>, perfectly separating Iris setosa from the other species with a Gini score of 0.0 and 25 samples at the node.</li> <li>Subsequent splits focus on distinguishing Iris versicolor and Iris virginica, proceeding until reaching nodes with lower Gini scores.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#importance-for-model-interpretation","title":"Importance for Model Interpretation","text":"<ul> <li>Such visualizations are crucial for understanding the decision-making process of the model and determining which features are most influential during the classification task.</li> <li>The explainable nature of decision trees allows us to clearly communicate how the model works and when specific features become significant, enhancing the transparency and reliability of the AI model.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#implications-for-stakeholders","title":"Implications for Stakeholders","text":"<ul> <li>The clear delineation of decision paths provides stakeholders with insight into the model's reasoning, facilitating trust in the predictions made by the AI system.</li> <li>It underscores the model's dependence on petal measurements, potentially informing feature engineering and data collection priorities for future modeling efforts.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_1","title":"Conclusion","text":"<p>This decision tree is a testament to the power of Random Forest in handling complex classification tasks. By breaking down the decision process step by step, we gain a granular understanding of feature importance and model behavior, laying a foundation for informed model refinement and application.</p> <pre><code>from sklearn.tree import plot_tree\n\n\nfeature_names = list(X_train.columns)\n\n# Select one of the trees from your random forest\ntree_to_plot = rf_clf.estimators_[0]\n\n# Plot the selected tree\nfig = plt.figure(figsize=(25, 20))\n_ = plot_tree(tree_to_plot, feature_names=feature_names, class_names=class_names, filled=True)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-value-summary-feature-importance-for-iris-classification","title":"SHAP Value Summary: Feature Importance for Iris Classification","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_2","title":"Overview","text":"<p>This graph utilizes SHAP (SHapley Additive explanations) values to provide a summary of feature importance within our Iris species classification model. The length of the bars represents the average impact of each feature on the model's predictions, across all instances in the dataset.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#detailed-feature-contributions","title":"Detailed Feature Contributions","text":"<ul> <li>Petal Width (petal_width): Stands out as the feature with the most significant positive impact on model output, particularly for Iris-virginica predictions. The prominence of this bar suggests that petal width is a critical factor in the classification.</li> <li>Petal Length (petal_length): Exhibits a notable positive influence as well, especially for Iris-setosa. Its impact underlines the importance of petal length in distinguishing this particular species.</li> <li>Sepal Measurements (sepal_length and sepal_width): While having a lesser effect compared to petal features, these still contribute to the model's decision-making process, with sepal_length showing some influence on Iris-versicolor classification.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#model-insight-and-adjustments","title":"Model Insight and Adjustments","text":"<ul> <li>This visualization is a powerful tool for identifying potential biases or over-reliance on specific features. The predominance of petal-related features may suggest the need for balance through feature engineering or model hyperparameter tuning.</li> <li>The insight provided by this summary plot is critical for enhancing model fairness, balance, and ultimately, the trustworthiness of its predictions.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_2","title":"Conclusion","text":"<p>The SHAP summary plot offers an in-depth understanding of how each feature influences the classification model. It is essential for developers and stakeholders to make data-driven decisions regarding feature selection and to grasp the relative importance of attributes within the dataset.</p> <pre><code>shap.summary_plot(shap_values, X.values, plot_type=\"bar\", class_names=class_names, feature_names=X.columns)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-summary-plot-for-iris-dataset-model-interpretation","title":"SHAP Summary Plot for Iris Dataset Model Interpretation","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_3","title":"Overview","text":"<p>This SHAP (SHapley Additive exPlanations) summary plot provides a visual representation of the feature impact within our classification model for the Iris dataset. It quantifies the marginal contribution of each feature to the prediction made by the model, offering insights into the decision-making process.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#interpretation-of-shap-values","title":"Interpretation of SHAP Values","text":"<ul> <li>Positive and Negative Impacts: The distribution of SHAP values on the x-axis reveals how each feature affects the model output for individual observations. Red points indicate higher feature values, while blue points represent lower values, demonstrating the directional impact of features on the model output.</li> <li>Feature Contributions: For instance, <code>petal_width</code> is shown to have a predominantly positive impact on the model's predictions\u2014most red points lie to the right of the zero line, suggesting that higher petal width values tend to increase the likelihood of a particular Iris species prediction.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#data-point-analysis","title":"Data Point Analysis","text":"<ul> <li>Each point on the graph corresponds to a unique observation in the dataset. The spread of these points allows us to discern how the model differentiates between the samples, particularly noting the variability of effects across the range of feature values.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#utility-of-shap-summary","title":"Utility of SHAP Summary","text":"<ul> <li>Model Interaction: The SHAP values elucidate interactions between features and their directional influence on predictions, offering a reliable method to transparently showcase how feature variations influence the model's decisions.</li> <li>Insights for Model Improvement: This analysis is instrumental in enhancing our understanding of the model and explaining the heterogeneity and complexity present in the predictions. It aids in identifying areas for model improvement, guiding feature engineering, and ensuring robust prediction performance.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_3","title":"Conclusion","text":"<p>By employing the SHAP summary plot, we provide a granular view of feature influences, enhancing interpretability and trust in our model. It serves as a valuable tool for stakeholders alike, enabling data-driven decision-making and promoting a thorough comprehension of the model's predictive dynamics.</p> <pre><code>shap.summary_plot(shap_values[1], X.values, feature_names=X.columns)\n</code></pre> <pre><code>shap.summary_plot(shap_values[0], X.values, feature_names=X.columns)\n</code></pre> <pre><code>shap.summary_plot(shap_values[2], X.values, feature_names=X.columns)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-dependence-plot-sepal-lengths-influence-on-iris-classification","title":"SHAP Dependence Plot: Sepal Length's Influence on Iris Classification","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_4","title":"Overview","text":"<p>This SHAP dependence plot showcases the relationship between <code>sepal_length</code> and the model output, while also highlighting the impact of another feature, <code>sepal_width</code>, using a color gradient. We analyze how variations in sepal dimensions influence the classification predictions made by the model.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#key-observations","title":"Key Observations","text":"<ul> <li>Sepal Length Impact: As <code>sepal_length</code> increases, we observe a general trend of decreasing SHAP values, indicating a potentially negative influence on the model's confidence in predicting a particular Iris species.</li> <li>Interaction Effect: The color coding represents the <code>sepal_width</code> values, with warmer colors (red) indicating larger sepal widths. Notably, data points with larger <code>sepal_width</code> often correspond to higher SHAP values, suggesting that a wider sepal might counteract the negative impact of longer sepal length.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#insights-for-feature-interaction","title":"Insights for Feature Interaction","text":"<ul> <li>This plot allows us to discern not only the individual effects of features but also how they might interact with each other. For instance, while longer sepals (<code>sepal_length</code>) tend to decrease prediction confidence, this effect might be moderated by the width of the sepals (<code>sepal_width</code>).</li> <li>The variability in SHAP values across different <code>sepal_length</code> measurements, especially when colored by <code>sepal_width</code>, provides an understanding of how feature combinations affect model predictions.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#implications-for-model-refinement_1","title":"Implications for Model Refinement","text":"<ul> <li>Such insights are valuable for stakeholders when considering how to optimize features and adjust the model. </li> <li>Recognizing the influence of feature interactions is crucial for developing more robust and accurate classification models and can lead to more nuanced data preprocessing and feature engineering strategies.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_4","title":"Conclusion","text":"<p>The dependence plot is a vital interpretability tool, allowing stakeholders to grasp the complex dynamics of feature interactions within the model. This understanding is imperative for fine-tuning the model to enhance predictive performance and ensure that it generalizes well to new data.</p> <pre><code>shap.dependence_plot(0, shap_values[0], X.values, feature_names=X.columns)\n</code></pre> <pre><code>shap.dependence_plot(1, shap_values[0], X.values, feature_names=X.columns)\n</code></pre> <pre><code>shap.dependence_plot(2, shap_values[0], X.values, feature_names=X.columns)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-waterfall-plot-analysis-for-individual-prediction","title":"SHAP Waterfall Plot Analysis for Individual Prediction","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_5","title":"Overview","text":"<p>The displayed SHAP waterfall plot is an interpretative tool used to break down the contribution of each feature to a specific prediction made by our machine learning model. It details the individual and cumulative impact of features on a single prediction.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#feature-contributions-explained","title":"Feature Contributions Explained","text":"<ul> <li>Petal Measurements (petal_width and petal_length): These features exhibit a strong positive effect on the model's output. </li> <li>Sepal Measurements (sepal_length and sepal_width): While <code>sepal_length</code> shows a small positive contribution, <code>sepal_width</code> has a slight negative influence. The limited impact of <code>sepal_width</code> in this instance may suggest it plays a lesser role in the classification for this specific prediction.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#addressing-feature-impact","title":"Addressing Feature Impact","text":"<ul> <li>The substantial influence of petal measurements raises concerns about the model's reliance on a narrow set of features, which could lead to overfitting. This phenomenon occurs when a model learns patterns specific to the training data, impacting its ability to generalize to unseen data.</li> <li>To mitigate over-reliance and enhance generalization, regularization techniques may be employed, or the model could be adjusted to give more weight to other features in the dataset.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_5","title":"Conclusion","text":"<p>Understanding which features the model prioritizes and the potential outcomes of this prioritization is central to data scientists' efforts to enhance the model's reliability and applicability. These analyses are crucial for maintaining a balanced performance of the model, ensuring that it remains robust across different scenarios.</p> <pre><code>row = 8\nshap.waterfall_plot(\n    shap.Explanation(\n        values=shap_values[0][row],\n        base_values=explainer.expected_value[0],\n        data=X_test.iloc[row],\n        feature_names=X_test.columns.tolist(),\n    )\n)\n</code></pre> <pre><code>row = 42\nshap.waterfall_plot(\n    shap.Explanation(\n        values=shap_values[0][row],\n        base_values=explainer.expected_value[0],\n        data=X_test.iloc[row],\n        feature_names=X_test.columns.tolist(),\n    )\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-force-plot","title":"SHAP Force Plot","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#this-shap-force-plot-illustrates-the-impact-of-each-feature-on-our-models-classification-prediction-the-base-value-represents-our-average-reference-prediction-while-the-fx-100-value-is-the-definitive-prediction-made-by-our-model-for-this-instance-red-bars-petal-length-45-petal-width-13-and-sepal-length-57-indicate-factors-that-increase-the-models-prediction-these-three-features-have-elevated-the-prediction-with-petal-length-having-the-most-significant-impact-sepal-width-28-presents-a-slight-negative-effect-causing-a-minimal-decrease-in-the-models-prediction-overall-in-light-of-these-values-our-model-robustly-classifies-the-given-data-point-into-a-specific-category","title":"This SHAP Force Plot  illustrates the impact of each feature on our model's classification prediction. The \"base value\" represents our average reference prediction, while the \"f(x) = 1.00\" value is the definitive prediction made by our model for this instance. Red bars (petal length: 4.5, petal width: 1.3, and sepal length: 5.7) indicate factors that increase the model's prediction. These three features have elevated the prediction, with petal length having the most significant impact. Sepal width (2.8) presents a slight negative effect, causing a minimal decrease in the model's prediction. Overall, in light of these values, our model robustly classifies the given data point into a specific category.","text":"<pre><code>shap.plots.force(explainer.expected_value[0], shap_values[0][0, :], X_test.iloc[0, :], matplotlib=True)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-decision-plot","title":"SHAP Decision Plot","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#the-shap-decision-plot-visualizes-the-impact-of-individual-features-on-the-output-of-a-machine-learning-model-this-particular-plot-is-generated-using-the-shapdecision_plot-function-with-parameters-corresponding-to-the-expected-value-of-the-model-shap-values-for-a-set-of-predictions-and-feature-names-from-the-test-dataset","title":"The SHAP decision plot visualizes the impact of individual features on the output of a machine learning model. This particular plot is generated using the shap.decision_plot function with parameters corresponding to the expected value of the model, SHAP values for a set of predictions, and feature names from the test dataset:","text":"<p>X-axis: The model output value after accounting for the impact of each feature. Lines: Represent the shift in the model output due to the impact of the corresponding feature from the base value. Petal length: Shows a consistently negative impact on the model output. Petal width: Generally contributes towards an increase in the model output. Sepal length and sepal width: Exhibit variable impacts on the model output. This plot is instrumental in pinpointing the most influential features for a prediction and understanding their collective impact on the final model output.</p> <pre><code># For class 0\nshap.decision_plot(explainer.expected_value[0], shap_values[0], X_test.columns)\n</code></pre> <pre><code># For class 1\nshap.decision_plot(explainer.expected_value[1], shap_values[1], X_test.columns)\n</code></pre> <pre><code># For class 2\nshap.decision_plot(explainer.expected_value[2], shap_values[2], X_test.columns)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#bank_marketingsnippetslabel_encoderpy","title":"bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == \"O\"]\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#bank_marketingsnippetsone_hotpy","title":"bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(\n    df,\n    text_col_list: list[str] | None,\n    max_categories: int = 25,\n    dummy_option: DummyOption = DummyOption.KEEP_ALL,\n    result_col_suffix: list[str] | None = None,\n    result_col_prefix: list[str] | None = None,\n):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multi collinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == \"object\" and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(\n            df[col],\n            prefix=(result_col_prefix if result_col_prefix else col),\n            drop_first=(dummy_option == DummyOption.DROP_FIRST),\n        )\n        dummies = dummies.rename(columns=lambda x: f\"{x}_{result_col_suffix}\" if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#model_observabilitymodelpy","title":"model_observability/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\n\ndef add_features(df):\n    for column in df.select_dtypes(include=\"object\"):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include=\"int64\"):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include=\"float64\"):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"income &gt;50K\"])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#model_trackingmodel_driftmodelpy","title":"model_tracking/model_drift/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib\n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlice_creammodeljson","title":"sparkml/ice_cream/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlice_creammodelpy","title":"sparkml/ice_cream/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([StructField(\"features\", DoubleType(), True)])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row[\"Temperature\"])),), axis=1), schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlspark_with_jobjobpy","title":"sparkml/spark_with_job/job.py","text":"<pre><code>import practicuscore as prt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit, max, min, stddev, corr\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\n\nspark = SparkSession.builder.appName(\"Advanced Data Processing\").getOrCreate()\n\nfile_path = \"/home/ubuntu/samples/data/insurance.csv\"\ndata = spark.read.csv(file_path, header=True, inferSchema=True)\nmissing_data = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n\ncategorical_columns = [\"sex\", \"smoker\", \"region\"]\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n\ndata = data.withColumn(\n    \"bmi_category\",\n    when(col(\"bmi\") &lt; 18.5, lit(\"underweight\"))\n    .when((col(\"bmi\") &gt;= 18.5) &amp; (col(\"bmi\") &lt; 25), lit(\"normal\"))\n    .when((col(\"bmi\") &gt;= 25) &amp; (col(\"bmi\") &lt; 30), lit(\"overweight\"))\n    .otherwise(lit(\"obese\")),\n)\n\nfeature_columns = [\"age\", \"bmi\", \"children\", \"sex_encoded\", \"smoker_encoded\", \"region_encoded\"]\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\ndata = pipeline.fit(data).transform(data)\n\noutput_path = \"/home/ubuntu/my/processed_insurance_data.parquet/\"\n\ndata.write.parquet(output_path, mode=\"overwrite\")\n\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlspark_with_jobrun2c741eprt_dist_jobjson","title":"sparkml/spark_with_job/run/2c741e/prt_dist_job.json","text":"<pre><code>{\"job_type\":\"spark\",\"job_dir\":\"~/my/02_batch_job/\",\"initial_count\":2,\"coordinator_port\":7077,\"additional_ports\":[4040,7078,7079],\"terminate_on_completion\":false,\"py_file\":\"job.py\",\"executors\":[{\"rank\":0,\"instance_id\":\"5cf16b71\"},{\"rank\":1,\"instance_id\":\"63e80dc8\"}]}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlspark_with_jobrun2c741erank_0json","title":"sparkml/spark_with_job/run/2c741e/rank_0.json","text":"<pre><code>{\"rank\":0,\"instance_id\":\"5cf16b71\",\"state\":\"completed\",\"used_ram\":1187,\"peak_ram\":1187,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlspark_with_jobrun2c741erank_1json","title":"sparkml/spark_with_job/run/2c741e/rank_1.json","text":"<pre><code>{\"rank\":1,\"instance_id\":\"63e80dc8\",\"state\":\"running\",\"used_ram\":284,\"peak_ram\":293,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#xgboostmodelpy","title":"xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#xgboostmodel_custom_dfpy","title":"xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre> <p>Previous: AutoML | Next: Bank Marketing &gt; Bank Marketing</p>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/","title":"Bank marketing Sample","text":"<p>A banking company wants to develop a model to predict the customers who will subscribe to time deposits and also wants to reach customers who are likely to subscribe to time deposits by using the call center resource correctly.</p> <p>In the data set to be studied, variables such as demographic information, balance information and previous campaign information of the customers will be used to predict whether they will subscribe to time deposits.</p> <p>Using the App - You can open the dataset in the Practicus AI  by loading all data - Then in the analysis phase, you can start with profiling the data - Then Graph &gt; Boxplot &gt; Age - Groupby &gt; Age, Job,  Balance Mean &amp; Median - Analyze &gt; Graph &gt; Plot -&gt; Job -&gt; Balance Mean Add Layer, Balance Median Add Layer</p> <p>Using Notebook - You can find detailed preprocessing steps in the notebook made with SDK - The main idea here is that the model is built by filtering the -1s in the pdays variable, that is, they are not included in the model. - In addition, the poutcome variable should be deleted to prevent Data Leakage. - Then the model Feature Selection was selected as 95% and the Setup params were set to fix_imbalance: True.</p>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None  # Eg. bank_test_1\nexperiment_tracking_service = None  # Eg. MlFlow\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please select a model_name\"\nassert experiment_tracking_service, \"Please select an experiment tracking service, or skip this cell\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code>my_deployment_list = region.model_deployment_list\ndisplay(my_deployment_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>my_addon_list = prt.addons.get_list()\ndisplay(my_addon_list.to_pandas())\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n</code></pre> <pre><code>data_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/bank_marketing.csv\"}\n\nproc = worker.load(data_conn)\nproc.show_head()\n</code></pre> <pre><code>proc.delete_columns([\"poutcome\"])\n</code></pre> <pre><code>proc.run_snippet(\n    \"one_hot\",\n    text_col_list=[\n        \"marital\",\n        \"default\",\n        \"housing\",\n        \"loan\",\n    ],\n    max_categories=25,\n    dummy_option=\"Drop First Dummy\",\n    result_col_suffix=[],\n    result_col_prefix=[],\n)\n</code></pre> <pre><code>proc.delete_columns([\"default\"])\n</code></pre> <pre><code>proc.delete_columns([\"housing\", \"loan\"])\n</code></pre> <pre><code>proc.delete_columns([\"marital\"])\n</code></pre> <pre><code>proc.run_snippet(\n    \"label_encoder\",\n    text_col_list=[\n        \"job\",\n        \"education\",\n        \"contact\",\n        \"month\",\n        \"deposit\",\n    ],\n)\n</code></pre> <pre><code>filter_expression = \"\"\" \ncol[pdays] != -1 \n\"\"\"\nproc.filter(filter_expression)\n</code></pre> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#building-a-model-using-automl","title":"Building a model using AutoML","text":"<p>The below code is generated. You can update the code to fit your needs, or re-create it by building a model with Practicus AI app first and then view it's jupter notebook oncethe model building is completed.</p> <pre><code>from pycaret.classification import ClassificationExperiment, load_model, predict_model\n\nexp = ClassificationExperiment()\n</code></pre> <pre><code>experiment_name = \"Bank-marketing\"\nprt.experiments.configure(service_name=experiment_tracking_service, experiment_name=experiment_name)\n</code></pre> <pre><code>setup_params = {\"fix_imbalance\": True}\n</code></pre> <pre><code>exp.setup(\n    data=df, target=\"deposit\", session_id=7272, log_experiment=True, experiment_name=experiment_name, **setup_params\n)\n</code></pre> <pre><code>best_model = exp.compare_models()\n</code></pre> <pre><code>tune_params = {}\n</code></pre> <pre><code>tuned_model = exp.tune_model(best_model, **tune_params)\n</code></pre> <pre><code>final_model = exp.finalize_model(tuned_model)\n</code></pre> <pre><code>predictions = exp.predict_model(final_model, data=df)\ndisplay(predictions)\n</code></pre> <pre><code>exp.save_model(final_model, \"model\")\n</code></pre> <pre><code>loaded_model = load_model(\"model\")\n\npredictions = predict_model(loaded_model, data=df)\ndisplay(predictions)\n</code></pre> <pre><code># Deploy to current Practicus AI region\nprt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#prediction-by-using-model-api","title":"Prediction by using model API","text":"<pre><code>region = prt.current_region()\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>import requests\nimport pandas as pd\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#prediction-by-using-sdk","title":"Prediction by using SDK","text":"<pre><code>proc.predict(\n    api_url=f\"{region.url}/{prefix}/{model_name}/\",\n    column_names=[\n        \"Buildgnage\",\n        \"job\",\n        \"education\",\n        \"balance\",\n        \"contact\",\n        \"day\",\n        \"month\",\n        \"duration\",\n        \"campaign\",\n        \"pdays\",\n        \"previous\",\n        \"deposit\",\n        \"marital_married\",\n        \"marital_single\",\n        \"default_yes\",\n        \"housing_yes\",\n        \"loan_yes\",\n    ],\n    new_column_name=\"predicted_deposit\",\n)\n</code></pre> <pre><code>df = proc.get_df_copy()\ndf.columns\n</code></pre> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre> <pre><code>df_predicted = proc.get_df_copy()\n</code></pre> <pre><code>df_predicted\n</code></pre> <pre><code>df_predicted[\"predicted_deposit\"].head()\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#snippetslabel_encoderpy","title":"snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == \"O\"]\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#snippetsone_hotpy","title":"snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(\n    df,\n    text_col_list: list[str] | None,\n    max_categories: int = 25,\n    dummy_option: DummyOption = DummyOption.KEEP_ALL,\n    result_col_suffix: list[str] | None = None,\n    result_col_prefix: list[str] | None = None,\n):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multi collinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == \"object\" and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(\n            df[col],\n            prefix=(result_col_prefix if result_col_prefix else col),\n            drop_first=(dummy_option == DummyOption.DROP_FIRST),\n        )\n        dummies = dummies.rename(columns=lambda x: f\"{x}_{result_col_suffix}\" if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre> <p>Previous: Shap Analysis | Next: Zip Unzip &gt; Zip Unzip</p>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/","title":"Model Observability and Monitoring","text":""},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#scenario-model-drift","title":"Scenario: Model Drift","text":"<p>In this example, we'll deploy a model on an income dataset. Our main goal withIn this example is deploying pre-processes into the \"model.pkl\".</p> <ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Preparing pre-process function</p> </li> <li> <p>Preparing train pipeline and deploy a model on income dataset</p> </li> <li> <p>Making predictions with deployed model without making any pre-process</p> </li> </ol>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None  # Eg. bank_test_1\npracticus_url = None  # Eg. http://practicus.company.com\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please select a model_name\"\nassert practicus_url, \"Please select practicus_url\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code>my_model_deployment_list = region.model_deployment_list\ndisplay(my_model_deployment_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#creating-preprocess-and-the-pipeline","title":"Creating Preprocess and the Pipeline","text":"<p>Loading the data set</p> <pre><code>import practicuscore as prt\nimport pandas as pd\n\ndf = pd.read_csv(\"/home/ubuntu/samples/data/income.csv\")\n\ndf.head()\n</code></pre> <p>Creating pre-process function for new features</p> <pre><code>from sklearn.preprocessing import FunctionTransformer\n\n\ndef add_features(df):\n    for column in df.select_dtypes(include=\"object\"):  # Selecting columns which has type as object\n        mode_value = df[column].mode()[0]  # Find mode\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include=\"int64\"):  # Selecting columns which has type as in64\n        mean_value = df[column].mean()  # Find median\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include=\"float64\"):  # Selecting columns which has type as float64\n        mean_value = df[column].mean()  # Find Median\n        df[column] = df[column].fillna(mean_value)\n\n    return df\n\n\nadd_features_transformer = FunctionTransformer(add_features, validate=False)\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>df.dtypes\n</code></pre> <p>Defining categorical and numerical features</p> <pre><code>numeric_features = [\"age\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\ncategorical_features = [\n    \"workclass\",\n    \"education\",\n    \"marital-status\",\n    \"occupation\",\n    \"relationship\",\n    \"race\",\n    \"sex\",\n    \"native-country\",\n]\n</code></pre> <p>Creating preprocessor object for the pipeline to apply scaling and one hot encoding</p> <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\npreprocessor = ColumnTransformer(\n    transformers=[(\"num\", StandardScaler(), numeric_features), (\"cat\", OneHotEncoder(), categorical_features)]\n)\n</code></pre> <p>Creating the pipeline</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(\n    steps=[\n        (\"add_features\", add_features_transformer),\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", RandomForestClassifier()),\n    ]\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#model-training","title":"Model Training","text":"<p>Train test split</p> <pre><code>X = df.drop([\"income &gt;50K\"], axis=1)\ny = df[\"income &gt;50K\"]\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <p>Fitting the model</p> <pre><code>pipeline.fit(X_train, y_train)\n</code></pre> <pre><code>score = pipeline.score(X_test, y_test)\nprint(f\"Accuracy Score of the model: {score}\")\n</code></pre> <p>Importing the model by using cloudpickle</p> <pre><code>import cloudpickle\n\nwith open(\"model.pkl\", \"wb\") as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre> <pre><code>import joblib\nimport pandas as pd\n\n# Load the saved model\nmodel = joblib.load(\"model.pkl\")\n\n# Making predictions\npredictions = model.predict(X)\n\n# Converting predictions to a DataFrame\npredictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#deployment-of-the-model","title":"Deployment of the model","text":"<pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#prediction","title":"Prediction","text":"<pre><code># Let's construct the REST API url.\n# Please replace the below url with your current Practicus AI address\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"https://{practicus_url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <p>Making predictions without making any pre-process on the income dataset</p> <pre><code>import requests\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <pre><code>pred_df.value_counts()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#modelpy","title":"model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\n\ndef add_features(df):\n    for column in df.select_dtypes(include=\"object\"):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include=\"int64\"):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include=\"float64\"):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"income &gt;50K\"])\n\n    return predictions_df\n</code></pre> <p>Previous: Model Drift | Next: XGBoost &gt; XGBoost</p>"},{"location":"technical-tutorial/extras/modeling/model-tracking/experiment-tracking/experiment-tracking-logging/","title":"Experiment Tracking Logging","text":"<pre><code>import practicuscore as prt\nimport os\nimport mlflow\n\nregion = prt.current_region()\n</code></pre> <pre><code># Defining parameters\n\n# You need to configure using the service unique key and name\nservice_name = None\nservice_key = None\n\n# Optionally, you can provide experiment name to create a new experiment while configuring\nexperiment_name = None\n</code></pre> <pre><code>assert service_name, \"Please select a service_name\"\nassert service_key, \"Please select a service_key\"\nassert experiment_name, \"Please enter a experiment_name\"\n</code></pre> <pre><code># If you don't know service key and name you can checkout down below\n\naddon_list = prt.addons.get_list()\ndisplay(addon_list.to_pandas())\n</code></pre> <pre><code>prt.experiments.configure(service_name=service_name, service_key=service_key, experiment_name=experiment_name)\n</code></pre> <pre><code># Set experiment name, if you haven't already while configuring the service\nmlflow.set_experiment(\"my experiment\")\n\n# Prefer unique run names, or leave empty to auto generate unique names\nrun_name = \"My ML experiment run 123\"\n\n# Start an MLflow run and log params, metrics and artifacts\nwith mlflow.start_run(run_name=run_name):\n    # Log parameters\n    mlflow.log_param(\"param1\", 5)\n    mlflow.log_param(\"param2\", \"test\")\n\n    # Log metrics\n    mlflow.log_metric(\"metric1\", 0.85)\n\n    # Create an artifact (e.g., a text file)\n    artifact_path = \"artifacts\"\n    if not os.path.exists(artifact_path):\n        os.makedirs(artifact_path)\n    file_path = os.path.join(artifact_path, \"output.txt\")\n\n    with open(file_path, \"w\") as f:\n        f.write(\"This is a test artifact.\")\n\n    # Log the artifact\n    mlflow.log_artifacts(artifact_path)\n\n    # Optional: Print the run ID\n    print(\"Run ID:\", mlflow.active_run().info.run_id)\n\n# Explicitly close the active MLflow run, if you are not using the above with keyword\n# mlflow.end_run()\n</code></pre> <p>Previous: Spark Tutorial | Next: Experiment Tracking Model Training</p>"},{"location":"technical-tutorial/extras/modeling/model-tracking/experiment-tracking/experiment-tracking-model-training/","title":"Experiment Tracking Model Training","text":"<pre><code>import practicuscore as prt\nimport os\nimport mlflow\nimport xgboost as xgb\nimport cloudpickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nregion = prt.current_region()\n</code></pre> <pre><code># Defining parameters\n\n# You need to configure using the service unique key and name\nservice_name = None\nservice_key = None\n\n# Optionally, you can provide experiment name to create a new experiment while configuring\nexperiment_name = None\n</code></pre> <pre><code>assert service_name, \"Please select a service_name\"\nassert service_key, \"Please select a service_key\"\nassert experiment_name, \"Please select a experiment_name\"\n</code></pre> <pre><code># If you don't know service key and name you can checkout down below\n\naddon_list = prt.addons.get_list()\ndisplay(addon_list.to_pandas())\n</code></pre> <pre><code>prt.experiments.configure(service_name=service_name, service_key=service_key, experiment_name=experiment_name)\n</code></pre> <pre><code>data_set_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/ice_cream.csv\"}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn)\n\ndata = proc.get_df_copy()\ndata.head()\n</code></pre> <pre><code># Set experiment name, if you haven't already while configuring the service\nmlflow.set_experiment(\"XGBoost Experiment\")\n\n# Loading the dataset\nX = data.Temperature\ny = data.Revenue\n</code></pre> <pre><code># Test and Train split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# XGBoost parameters\nparams = {\n    \"max_depth\": 3,\n    \"eta\": 0.1,\n    \"objective\": \"reg:squarederror\",\n}\n</code></pre> <pre><code># Creation of DMatrix\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n</code></pre> <pre><code># Training of model by using mlflow\nwith mlflow.start_run():\n    mlflow.log_params(params)\n    model = xgb.train(params, dtrain, num_boost_round=200)\n    # Prediction process\n    predictions = model.predict(dtest)\n    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"rmse\", rmse)\n    # Saving the model in MLFlow\n    artifact_path = \"model\"\n    if not os.path.exists(artifact_path):\n        os.makedirs(artifact_path)\n    model_path = os.path.join(artifact_path, \"xgboost_model.pkl\")\n    with open(model_path, \"wb\") as f:\n        cloudpickle.dump(model, f)\n    # Saving the serialised model in MLflow\n    mlflow.log_artifacts(artifact_path)\n    mlflow.log_artifacts(artifact_path)\n    # Printing out the run id\n    print(\"Run ID:\", mlflow.active_run().info.run_id)\n</code></pre> <pre><code># Ending MLFlow\nmlflow.end_run()\n</code></pre> <p>Previous: Experiment Tracking Logging | Next: Model Drift &gt; Model Drift</p>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/","title":"Model Observability and Monitoring","text":""},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#scenario-model-drift","title":"Scenario: Model Drift","text":"<p>In this example, we'll deploy a model on an insurance dataset to make two predictions. Introducing intentional drifts in the BMI and Age columns, we aim to observe their impact on the model's predictions.</p> <ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Train and deploy a model on insurance dataset</p> </li> <li> <p>Making predictions with deployed model</p> </li> <li> <p>Multiplying the BMI and Age columns to create Drifts on features and predictions</p> </li> <li> <p>Observing the model drift plots</p> </li> </ol>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None\npracticus_url = None  # Example http://company.practicus.com\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please enter a model_name\"\nassert practicus_url, \"Please enter practicus_url\"\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#model-development","title":"Model Development","text":"<p>Loading and preparing the dataset</p> <pre><code>data_set_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/insurance.csv\"}\n</code></pre> <pre><code># Loading the dataset to worker\n\nimport practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn)\n\nproc.show_head()\n</code></pre> <pre><code># Pre-process\n\nproc.categorical_map(column_name=\"sex\", column_suffix=\"category\")\nproc.categorical_map(column_name=\"smoker\", column_suffix=\"category\")\nproc.categorical_map(column_name=\"region\", column_suffix=\"category\")\nproc.delete_columns([\"region\", \"smoker\", \"sex\"])\n</code></pre> <pre><code># Taking the dataset into csv\n\ndf = proc.get_df_copy()\ndf.head()\n</code></pre> <p>Model Training</p> <pre><code>X = df.drop(\"charges\", axis=1)\ny = df[\"charges\"]\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <pre><code>import xgboost as xgb\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([(\"model\", xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100))])\npipeline.fit(X_train, y_train)\n</code></pre> <pre><code># Exporting the model\n\nimport cloudpickle\n\nwith open(\"model.pkl\", \"wb\") as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#model-deployment","title":"Model Deployment","text":"<pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None,  # Current Dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#prediction","title":"Prediction","text":"<pre><code># Loading the dataset to worker\n\nimport practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn)\n\nproc.categorical_map(column_name=\"sex\", column_suffix=\"category\")\nproc.categorical_map(column_name=\"smoker\", column_suffix=\"category\")\nproc.categorical_map(column_name=\"region\", column_suffix=\"category\")\nproc.delete_columns([\"region\", \"smoker\", \"sex\"])\n\ndf = proc.get_df_copy()\ndf.head()\n</code></pre> <pre><code># Let's construct the REST API url.\n# Please replace the below url with your current Practicus AI address\n# e.g. http://practicus.company.com\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"https://{practicus_url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code>import requests\nimport pandas as pd\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <ul> <li>After you make the first prediction, please wait for 5 minutes to see a clearer picture on the drift plot</li> <li>When we look at Model Drifts Dashboard at Grafana we will see plots with the model drift visible</li> </ul>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#hand-made-model-drift","title":"Hand-Made Model Drift","text":"<pre><code>df[\"age\"] = df[\"age\"] * 2\n</code></pre> <pre><code>df[\"bmi\"] = df[\"bmi\"] * 3\n</code></pre> <pre><code>display(df)\n</code></pre> <pre><code>headers = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <ul> <li>After you make the second prediction, please wait for 2 minutes to see a clearer picture on the drift plot</li> <li>When we look at Model Drifts Dashboard at Grafana we will see plots with the model drift visible</li> </ul>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#modelpy","title":"model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib\n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre> <p>Previous: Experiment Tracking Model Training | Next: Model Observability &gt; Model Observability</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/","title":"SparkML Ice Cream","text":""},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#end-to-end-sparkml-model-development-and-deployment","title":"End-to-end SparkML Model development and deployment","text":"<p>This sample notebook outlines the process of deploying a SparkML model on the Practicus AI platform and making predictions from the deployed model using various methods.</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None\npracticus_url = None  # E.g. http://company.practicus.com\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please enter a model_name\"\nassert practicus_url, \"Please enter practicus_url\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code>my_model_deployment_list = region.model_deployment_list\ndisplay(my_model_deployment_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>import pandas as pd\n\n# Step 1: Read CSV with Pandas\ndata = pd.read_csv(\"/home/ubuntu/samples/data/ice_cream.csv\")\n</code></pre> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.linalg import Vectors\n\n# Step 2: Convert to Spark DataFrame with Explicit Schema\nspark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n\n# Define schema for Spark DataFrame\nschema = StructType([StructField(\"label\", DoubleType(), True), StructField(\"features\", DoubleType(), True)])\n\nspark_data = spark.createDataFrame(\n    data.apply(lambda row: (float(row[\"Revenue\"]), Vectors.dense(float(row[\"Temperature\"]))), axis=1),\n    schema=[\"label\", \"features\"],\n)\n</code></pre> <pre><code>from pyspark.ml.regression import LinearRegression\n\n# Step 3: Train Linear Regression Model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\nmodel = lr.fit(spark_data)\n</code></pre> <pre><code># Step 4: Make Predictions\npredictions = model.transform(spark_data)\n\npredictions.select(\"features\", \"label\", \"prediction\").show()\n</code></pre> <pre><code># Step 5: Save Model\nmodel.save(model_name)\n</code></pre> <pre><code># Optional: Stop Spark session when done\nspark.stop()\n</code></pre> <pre><code># Prediction, you can run this in another notebook\n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\n\ndef predict(df: pd.DataFrame) -&gt; pd.DataFrame:\n    spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    model = LinearRegressionModel.load(model_name)\n    # Define schema for Spark DataFrame\n    schema = StructType([StructField(\"features\", DoubleType(), True)])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row[\"Temperature\"])),), axis=1), schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"/home/ubuntu/samples/data/ice_cream.csv\")\n</code></pre> <pre><code>prediction = predict(data)\n\nprediction\n</code></pre> <pre><code># Optional: Stop Spark session when done\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#deploying-the-sparkml-model","title":"Deploying the SparkML Model","text":""},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#step-1-upload-to-object-storage","title":"Step 1) Upload to object storage","text":"<ul> <li>Before you start deploying the model, please upload SparkML model files to the object storage that your Practicus AI model deployment is using.</li> <li>Why? Unlike scikit-learn, XGBoost etc., Spark ML model files are not a file such as model.pkl, but a folder. </li> </ul>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#sample-object-storage-cache-folder","title":"Sample object storage cache folder","text":"<ul> <li>E.g. s3://my-models-bucket/cache/ice_cream_sparkml_model/ice_cream_sparkml_model/ [ add your model folders here ] </li> <li>Why use same folder name twice? ice_cream_sparkml_model/ice_cream_sparkml_model </li> <li>Practicus AI will download all cache files defined in model.json.</li> <li>If you select cache/ice_cream_sparkml_model and don't use the second folder, all of your model files will be downloaded under /var/practicus/cache/</li> <li>If you are caching one model only this would work fine, but if you deploy multiple models their files can override each other.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#step-2-verify-you-practicus-ai-model-host-is-sparkml-compatible","title":"Step 2) Verify you Practicus AI model host is SparkML compatible","text":"<ul> <li>Please make sure you are using a Practicus AI model deployment image with SparkML support.</li> <li>You can use the default SparkML image: ghcr.io/practicusai/practicus-modelhost-sparkml:{add-version-here}</li> <li>Not sure how? Please consult your admin and ask for the deployment key with SparkML.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#step-3-deploy-the-model","title":"Step 3) Deploy the model","text":"<ul> <li>Please follow the below steps to deploy the model as usual</li> </ul> <pre><code>import practicuscore as prt\n\n# Deploying model as an API\n# Please review model.py and model.json files\n\nprt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None,  # Current dir\n)\n</code></pre> <pre><code>region = prt.current_region()\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code>token = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code># Caution: Due to initial Spark session creation process, first prediction can be quite slow.\n\nimport pandas as pd\nimport requests\n\ndf = pd.read_csv(\"/home/ubuntu/samples/data/ice_cream.csv\")\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#modeljson","title":"model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#modelpy","title":"model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([StructField(\"features\", DoubleType(), True)])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row[\"Temperature\"])),), axis=1), schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre> <p>Previous: MCP Langgraph | Next: Spark With Job &gt; Batch Job</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/","title":"Spark Tutorial","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit, max, min, stddev, corr\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#sparksession-creation","title":"SparkSession Creation","text":"<p>This code creates a <code>SparkSession</code> object, which is the entry point for any Spark application. It is used to configure and initialize a Spark application for data processing and analysis.</p> <pre><code>spark = SparkSession.builder.appName(\"Advanced Data Processing\").getOrCreate()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#reading-a-csv-file-with-spark","title":"Reading a CSV File with Spark","text":"<p>A CSV file is loaded into a Spark DataFrame using Spark's <code>read.csv()</code> method. This allows the data to be processed and analyzed efficiently within the Spark environment.</p> <pre><code>file_path = \"/home/ubuntu/samples/data/insurance.csv\"\ndata = spark.read.csv(file_path, header=True, inferSchema=True)\n</code></pre> <pre><code>print(\"First rows:\")\ndata.show(5)\n</code></pre> <pre><code>print(\"Data Schema:\")\ndata.printSchema()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#checking-for-null-values-in-a-spark-dataframe","title":"Checking for Null Values in a Spark DataFrame","text":"<p>Null values in a DataFrame are counted for each column, and the results are displayed to assess data completeness.</p> <pre><code>print(\"Check Nulls:\")\nmissing_data = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\nmissing_data.show()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#data-summary-and-statistical-analysis","title":"Data Summary and Statistical Analysis","text":"<p>Statistical measures such as descriptive statistics, minimum, maximum, standard deviation, and correlation are computed to analyze the data.</p> <pre><code>data.describe().show()\n\nprint(\"Min, Max and Std:\")\ndata.select(\n    [min(c).alias(f\"{c}_min\") for c in data.columns if data.schema[c].dataType != \"StringType\"]\n    + [max(c).alias(f\"{c}_max\") for c in data.columns if data.schema[c].dataType != \"StringType\"]\n    + [stddev(c).alias(f\"{c}_stddev\") for c in data.columns if data.schema[c].dataType != \"StringType\"]\n).show()\n\n\nprint(\"Correlation Analysis:\")\ndata.select(corr(\"age\", \"charges\").alias(\"age_charges_corr\"), corr(\"bmi\", \"charges\").alias(\"bmi_charges_corr\")).show()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#handling-categorical-variables","title":"Handling Categorical Variables","text":"<p>This code transforms categorical variables into numerical representations, preparing the data for machine learning algorithms.</p> <pre><code>categorical_columns = [\"sex\", \"smoker\", \"region\"]\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#bmi-categorization-and-grouping","title":"BMI Categorization and Grouping","text":"<p>BMI values are categorized into specific groups, and the data is grouped to count the number of entries in each category.</p> <pre><code>data = data.withColumn(\n    \"bmi_category\",\n    when(col(\"bmi\") &lt; 18.5, lit(\"underweight\"))\n    .when((col(\"bmi\") &gt;= 18.5) &amp; (col(\"bmi\") &lt; 25), lit(\"normal\"))\n    .when((col(\"bmi\") &gt;= 25) &amp; (col(\"bmi\") &lt; 30), lit(\"overweight\"))\n    .otherwise(lit(\"obese\")),\n)\n\ndata.groupBy(\"bmi_category\").count().show()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#feature-vector-assembly","title":"Feature Vector Assembly","text":"<p>The specified numerical and encoded categorical columns are combined into a single feature vector column, which is essential for machine learning models.</p> <pre><code>feature_columns = [\"age\", \"bmi\", \"children\", \"sex_encoded\", \"smoker_encoded\", \"region_encoded\"]\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#scaling-features-and-applying-a-pipeline","title":"Scaling Features and Applying a Pipeline","text":"<p>A pipeline is created to process data by combining categorical encoding, feature vector assembly, and feature scaling into a sequential workflow. This ensures a clean and efficient transformation of raw data into a format suitable for machine learning.</p> <pre><code>scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\ndata = pipeline.fit(data).transform(data)\n\ndata.select(\"features\", \"scaled_features\").show(5, truncate=False)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#writing-processed-data-to-a-parquet-file-and-stopping-spark-session","title":"Writing Processed Data to a Parquet File and Stopping Spark Session","text":"<p>The processed DataFrame is saved in a Parquet format, and the Spark session is gracefully terminated to release resources.</p> <pre><code>output_path = \"/home/ubuntu/my/processed_insurance_data.parquet\"\ndata.write.parquet(output_path, mode=\"overwrite\")\n\nspark.stop()\n</code></pre> <p>Previous: Batch Job | Next: Model Tracking &gt; Experiment Tracking &gt; Experiment Tracking Logging</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/","title":"Executing batch jobs in Spark Cluster","text":"<p>In this example we will: - Create a Spark cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"spark\" under your \"~/my\" folder</li> <li>And copy job.py under this (spark_with_job) folder</li> </ul> <pre><code>worker_size = None\nworker_count = None\nlog_level = \"DEBUG\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert worker_count, \"Please enter your worker_count.\"\nassert log_level, \"Please enter your log_level.\"\n</code></pre> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/spark_with_job/\"\n\n\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.spark,\n    job_dir=job_dir,\n    py_file=\"job.py\",\n    worker_count=worker_count,\n    terminate_on_completion=False,\n)\n\nworker_config = prt.WorkerConfig(worker_size=worker_size, distributed_config=distributed_config, log_level=log_level)\n\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n)\n</code></pre> <pre><code># You can view the logs during or after the job is completed\n# To view coordinator (master) set rank = 0\nrank = 0\n# To view other workers set rank = 1,2, ..\n\nprt.distributed.view_log(job_dir=job_dir, job_id=coordinator_worker.job_id, rank=rank)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit, max, min, stddev, corr\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\n\nspark = SparkSession.builder.appName(\"Advanced Data Processing\").getOrCreate()\n\nfile_path = \"/home/ubuntu/samples/data/insurance.csv\"\ndata = spark.read.csv(file_path, header=True, inferSchema=True)\nmissing_data = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n\ncategorical_columns = [\"sex\", \"smoker\", \"region\"]\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n\ndata = data.withColumn(\n    \"bmi_category\",\n    when(col(\"bmi\") &lt; 18.5, lit(\"underweight\"))\n    .when((col(\"bmi\") &gt;= 18.5) &amp; (col(\"bmi\") &lt; 25), lit(\"normal\"))\n    .when((col(\"bmi\") &gt;= 25) &amp; (col(\"bmi\") &lt; 30), lit(\"overweight\"))\n    .otherwise(lit(\"obese\")),\n)\n\nfeature_columns = [\"age\", \"bmi\", \"children\", \"sex_encoded\", \"smoker_encoded\", \"region_encoded\"]\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\ndata = pipeline.fit(data).transform(data)\n\noutput_path = \"/home/ubuntu/my/processed_insurance_data.parquet/\"\n\ndata.write.parquet(output_path, mode=\"overwrite\")\n\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#run2c741eprt_dist_jobjson","title":"run/2c741e/prt_dist_job.json","text":"<pre><code>{\"job_type\":\"spark\",\"job_dir\":\"~/my/02_batch_job/\",\"initial_count\":2,\"coordinator_port\":7077,\"additional_ports\":[4040,7078,7079],\"terminate_on_completion\":false,\"py_file\":\"job.py\",\"executors\":[{\"rank\":0,\"instance_id\":\"5cf16b71\"},{\"rank\":1,\"instance_id\":\"63e80dc8\"}]}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#run2c741erank_0json","title":"run/2c741e/rank_0.json","text":"<pre><code>{\"rank\":0,\"instance_id\":\"5cf16b71\",\"state\":\"completed\",\"used_ram\":1187,\"peak_ram\":1187,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#run2c741erank_1json","title":"run/2c741e/rank_1.json","text":"<pre><code>{\"rank\":1,\"instance_id\":\"63e80dc8\",\"state\":\"running\",\"used_ram\":284,\"peak_ram\":293,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre> <p>Previous: SparkML Ice Cream | Next: Spark For Ds &gt; Spark Tutorial</p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/","title":"XGBoost","text":""},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#end-to-end-custom-xgboost-model-development-and-deployment","title":"End-to-end custom XGBoost Model development and deployment","text":"<p>This sample notebook outlines the process of deploying a custom XGBoost model on the Practicus AI platform and making predictions from the deployed model using various methods.</p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None\npracticus_url = None  # E.g. http://company.practicus.com\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please enter a model_name\"\nassert practicus_url, \"Please enter practicus_url\"\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#data-preparation","title":"Data Preparation","text":"<p>We will be using Practicus AI to prepare data, but you can also do it manually using just Pandas. The rest of the model building and deployment steps would not change. </p> <pre><code>data_set_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/insurance.csv\"}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn)\n\nproc.show_head()\n</code></pre> <pre><code>proc.categorical_map(column_name=\"sex\", column_suffix=\"category\")\nproc.categorical_map(column_name=\"smoker\", column_suffix=\"category\")\nproc.categorical_map(column_name=\"region\", column_suffix=\"category\")\nproc.delete_columns([\"region\", \"smoker\", \"sex\"])\n</code></pre> <pre><code>df = proc.get_df_copy()\ndf.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#building-the-model","title":"Building the model","text":"<p>Let's build a model with XGBoost</p> <pre><code>X = df.drop(\"charges\", axis=1)\ny = df[\"charges\"]\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <pre><code>import xgboost as xgb\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([(\"model\", xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=100))])\npipeline.fit(X_train, y_train)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#saving-your-model","title":"Saving your model","text":"<ul> <li>After training your model, you can save it in <code>.pkl</code> format.</li> <li>Although not mandatory, please prefer to use cloudpickle so your custom code part of the model (e.g. preprocessing steps) is more portable.</li> </ul> <pre><code>import cloudpickle\n\nwith open(\"model.pkl\", \"wb\") as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre> <pre><code>import joblib\nimport pandas as pd\n\n# Load the saved model\nmodel = joblib.load(\"model.pkl\")\n\n# Making predictions\npredictions = model.predict(X)\n\npred_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#deploying-your-model-as-an-api-endpoint","title":"Deploying your model as an API endpoint","text":"<pre><code># Let's locate the model deployment to deploy our model\nif len(region.model_deployment_list) == 0:\n    raise SystemError(\"You do not have any model deployment systems. Please contact your system admin.\")\nelif len(region.model_deployment_list) &gt; 1:\n    print(\"You have more than one model deployment systems. Will use the first one\")\n\nprint(f\"Will deploy '{prefix}/{model_name}' to '{deployment_key}'\")\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#create-modelpy-that-predicts-using-modelpkl","title":"Create model.py that predicts using model.pkl","text":"<p>View sample model.py code</p> <pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#making-predictions-using-the-model-api","title":"Making predictions using the model API","text":"<pre><code># *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#getting-a-session-token-for-the-model-api","title":"Getting a session token for the model API","text":"<ul> <li>You can programmatically get a short-lived (~4 hours) model session token (recommended)</li> <li>You can also get an access token that your admin provided with a custom life span, e.g. months.</li> </ul> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#posting-data","title":"Posting data","text":"<ul> <li>There are multiple ways to post your data and construct the DataFrame in your model.py code.</li> <li>If you add content-type header, Practicus AI model hosting system will automatically convert your csv data into a Pandas DataFrame, and pass on to model.py predict() method.</li> <li>If you would like to construct the Dataframe yourself, skip passing content-type header and construct using Starlette request object View sample code</li> </ul> <pre><code>import requests\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#compressing-api-traffic","title":"Compressing API traffic","text":"<ul> <li>Practicus AI currently supports 'lz4' (recommended), 'zlib', 'deflate' and 'gzip' compression algorithms.</li> <li>Compressing to and from the API endpoint can increase performance for large datasets, low network bandwidth.</li> </ul> <pre><code>import lz4\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\", \"content-encoding\": \"lz4\"}\ndata_csv = X.to_csv(index=False)\ncompressed = lz4.frame.compress(data_csv.encode())\nprint(f\"Request compressed from {len(data_csv)} bytes to {len(compressed)} bytes\")\n\nr = requests.post(api_url, headers=headers, data=compressed)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\ndecompressed = lz4.frame.decompress(r.content)\nprint(f\"Response de-compressed from {len(r.content)} bytes to {len(decompressed)} bytes\")\n\npred_df = pd.read_csv(BytesIO(decompressed))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#detecting-drift","title":"Detecting drift","text":"<p>If enabled, Practicus AI allows you to detect feature and prediction drift and visualize in an observability platform such as Grafana. </p> <pre><code># Let's create an artificial drift for BMI feature, which will also affect charges\ndf[\"bmi\"] = df[\"bmi\"] * 1.2\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#recommended-adding-model-metadata-to-your-api","title":"(Recommended) Adding model metadata to your API","text":"<ul> <li>You can create and upload model.json file that defines the input and output schema of your model and potentially other metadata too.</li> <li>This will explain how to consume your model efficiently and make it accessible to more users.</li> <li>Practicus AI uses MlFlow model input/output standard to define the schema</li> <li>You can build the model.json automatically, or let Practicus AI build it for you using the dataframe.</li> </ul> <pre><code>model_config = prt.models.create_model_config(\n    df=df,\n    target=\"charges\",\n    model_name=\"My XG Boost Model\",\n    problem_type=\"Regression\",\n    version_name=\"2024-02-15\",\n    final_model=\"xgboost\",\n    score=123,\n)\nmodel_config.save(\"model.json\")\n# You also can directly instantiate ModelConfig class to provide more metadata elements\n# model_config = prt.models.ModelConfig(...)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#adding-a-new-api-version","title":"Adding a new API version","text":"<ul> <li>You can add as many model version as you need.</li> <li>Your admin can then route traffic as needed, Including for A/B testing.</li> </ul> <pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#reading-model-metadata","title":"Reading model metadata","text":"<p>You can add the query string '?get_meta=true' to any model to get the metadata.</p> <pre><code>headers = {\"authorization\": f\"Bearer {token}\"}\nr = requests.get(api_url + \"?get_meta=true\", headers=headers)\n\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nimport json\nfrom pprint import pprint\n\nmodel_config_dict = json.loads(r.text)\nprint(\"Model metadata:\")\npprint(model_config_dict)\nschema_dict = json.loads(model_config_dict[\"model_signature_json\"])\nprint(\"Model input/output schema:\")\npprint(schema_dict)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#optional-consuming-custom-model-apis-from-practicus-ai-app","title":"(Optional) Consuming custom model APIs from Practicus AI App","text":"<p>Practicus AI App users can consume any REST API model. </p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#end-user-api-view","title":"End user API view","text":"<p>If you add metadata to your models, App users will be able to view your model details in the UI.</p> <p></p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#feature-matching","title":"Feature matching","text":"<p>If your model has an input/output schema, the App will try to match them to current dataset.</p> <p></p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#viewing-the-prediction-result","title":"Viewing the prediction result","text":"<p>Note: Please consider adding categorical mapping to your models as a pre-processing step for improved user experience.</p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#recommended-clean-up","title":"(Recommended) Clean-up","text":"<p>Explicitly calling kill() on your processes will free un-used resources on your worker faster.</p> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#modelpy","title":"model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#model_custom_dfpy","title":"model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.pkl\")\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if \"charges\" in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop(\"charges\", axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=[\"Predictions\"])\n\n    return predictions_df\n</code></pre> <p>Previous: Model Observability | Next: AutoML</p>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/","title":"Ice Cream Sales Revenue Prediction Model","text":"<p>This code builds, trains, and tests a linear regression model to predict ice cream sales revenue based on temperature data.</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport practicuscore as prt\n\n# Load data\ndata = pd.read_csv(\"/home/ubuntu/samples/data/ice_cream.csv\")\n\n# Split data\nX = data[[\"Temperature\"]]\ny = data[\"Revenue\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_pred = model.predict(X_test)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#displaying-actual-vs-predicted-revenue","title":"Displaying Actual vs. Predicted Revenue","text":"<p>This code creates a DataFrame to compare the actual and predicted ice cream sales revenue, then prints the first few rows.</p> <pre><code>import pandas as pd\n\nresults_df = pd.DataFrame({\"Actual Revenue\": y_test.values, \"Predicted Revenue\": y_pred})\n\nprint(results_df.head())\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#saving-the-trained-model","title":"Saving the Trained Model","text":"<p>This code saves the trained linear regression model as a <code>.pkl</code> file using <code>pickle</code>, allowing it to be reused later without retraining.</p> <pre><code>import pickle\n\n# Save .pkl\nmodel_file_path = \"model.pkl\"\nwith open(model_file_path, \"wb\") as file:\n    pickle.dump(model, file)\n\nprint(f\"Created '{model_file_path}'.\")\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#creating-and-extracting-model-archive","title":"Creating and Extracting Model Archive","text":"<p>This code uses <code>prt.models.zip()</code> to create a compressed <code>model.zip</code> file containing supporting files (<code>support1.txt</code>, <code>support2.txt</code>), the script (<code>model.py</code>), and the trained model (<code>model.pkl</code>). The archive can be extracted later using <code>prt.models.unzip()</code>.</p>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#attention","title":"Attention","text":"<ul> <li>When you zip with prt, it zips the files given in the files_to_add parameter, but it does not zip the .pkl file because the pkl will already be in the same folder as the running notebook and when you deploy with prt, it will automatically deploy the pkl file, the purpose of the zip file is to contain auxiliary files inside.</li> <li>If your pkl file is in a different place or for a different reason, you can zip it yourself and put your pkl file in it, but there is a situation to be aware of, the pkl file should not be in any folder when you zip, if the zip file is unzipped, the pkl file does not appear when the zip file is unzipped, that is, if it is in another folder, you will get a .pkl not found error, the .pkl file should be directly visible when unzipped.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#suggested-solutions","title":"Suggested Solutions","text":"<ul> <li>If prt.models.zip is to be used, the \u201c.pkl\u201d file and the notebook where prt.models.deploy is running must be in the same directory so that when deployed, it deploys both the zip and the pkl file.</li> <li>If you want to zip with sh \u201czip -rj model.zip .../model/*\u201d</li> </ul> <pre><code># Create model.zip\n\nprt.models.zip(\n    files_to_add=[\"support1.txt\", \"support2.txt\", \"model.py\", \"model.pkl\"],\n    model_dir=None,  # Current directory\n)\n\n# Unzip included with prt.models.unzip()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#listing-and-selecting-a-model-deploy-params","title":"Listing and Selecting a Model Deploy Params","text":"<p>This code retrieves available model prefixes from the current region, displays them as a Pandas DataFrame, and selects the first prefix for deployment. It also lists available model deployments and selects the second deployment key for further use.</p> <pre><code>region = prt.get_region()\n</code></pre> <pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n\n# We will select first prefix\nmodel_prefix = my_model_prefixes[0].key\nprint(\"Using first prefix:\", model_prefix)\n</code></pre> <pre><code>model_name = \"test-zip-new\"\n</code></pre> <pre><code>model_deployment_list = region.model_deployment_list\ndisplay(model_deployment_list.to_pandas())\ndeployment_key = region.model_deployment_list[1].key\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#deploying-the-model","title":"Deploying the Model","text":"<p>This code uploads <code>model.zip</code> using <code>prt.models.deploy()</code>, associating it with the selected deployment key and model prefix. The model is stored under the specified <code>model_name</code>, ready for deployment.</p> <pre><code># This will now upload model.zip\nprt.models.deploy(deployment_key=deployment_key, prefix=model_prefix, model_name=model_name, model_dir=None)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#generating-model-api-url-and-session-token","title":"Generating Model API URL and Session Token","text":"<p>This code constructs the REST API URL for the deployed model using the region, model prefix, and model name. It then retrieves a session token using the Practicus AI SDK, which is required for authentication when interacting with the model API.</p> <pre><code># Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{model_prefix}/{model_name}/\"\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/zip-unzip/zip-unzip/#making-predictions-via-the-deployed-model-api","title":"Making Predictions via the Deployed Model API","text":"<p>This code sends a request to the deployed model API using the Practicus AI session token for authentication. It formats the input data (<code>X.head(10)</code>) as a CSV string and sends it via a POST request. The response, containing predictions, is read into a Pandas DataFrame for display.</p> <pre><code>import requests\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = X.head(10).to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <p>Previous: Bank Marketing | Next: Workflows &gt; Task Runnner App &gt; Build</p>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/","title":"Airflow HTTP Sensor Demo","text":""},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#introduction","title":"Introduction","text":""},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#what-are-airflow-sensors","title":"What Are Airflow Sensors?","text":"<p>In Apache Airflow, Sensors are special tasks that wait for an external condition to be true before allowing your DAG (Directed Acyclic Graph) to proceed.</p> <p>A classic use case is waiting for a file to appear on AWS S3, or for a certain row in a database table. Once the condition is met (the file exists or the row is found), the sensor task signals success, and the rest of the DAG can continue.</p>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#why-use-an-http-sensor","title":"Why Use an HTTP Sensor?","text":"<p>With microservices and modern architectures, you often have external services or APIs orchestrating data readiness. Instead of making Airflow directly query your database (which can load your Airflow infrastructure), you can expose an API endpoint that quickly checks the condition, and have Airflow poll that endpoint.</p> <ul> <li>Offload Airflow Load: Instead of Airflow running big queries, your microservice does the heavy lifting, and Airflow just queries the result of that check.</li> <li>API-Driven: Microservices can handle the domain logic, scaling, and performance. Airflow simply polls an HTTP endpoint.</li> <li>Clean Separation of Concerns: Airflow focuses on workflow orchestration, not on large or complex business logic.</li> </ul>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#how-this-example-helps","title":"How This Example Helps","text":"<ul> <li>Step-by-Step Guide: Shows how to create a simple API with a sensor route.</li> <li>Deploy the API so that Airflow can poll it.</li> <li>Configure an Airflow connection and create a DAG that uses an HTTP Sensor.</li> </ul> <p>Let's jump in!</p>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#airflow-sensors","title":"Airflow Sensors","text":"<p>For more info on HTTP sensor: Airflow HTTP Sensor Documentation</p>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#create-an-api","title":"Create an API","text":"<p>Create <code>sensor.py</code> under <code>app/apis</code> folder.</p> <pre><code># app/apis/sensor.py\nfrom starlette.requests import Request\nimport practicuscore as prt\n\n\n@prt.apps.api(\"/sensor\")\nasync def sensor(request: Request, **kwargs):\n    table_name = request.query_params.get(\"table_name\", None)\n    assert table_name, \"API request does not have table_name parameter, e.g. ../?table_name=my_table\"\n\n    # Add your condition, e.g. SQL query, business logic, etc.\n    # If the condition is met for table_1, return True.\n    # Otherwise, return False with an error message.\n\n    if table_name == \"table_1\":\n        return {\n            \"ok\": True,\n        }\n\n    return {\n        \"ok\": False,\n        \"error\": f\"table_name is '{table_name}' but must be 'table_1'\",\n    }\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#multiple-sensors","title":"Multiple Sensors","text":"<p>To create multiple sensors, you can: - Add additional conditions and routes in the same <code>sensor.py</code>, or - Create multiple <code>.py</code> files, each implementing a different sensor.</p>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#deploy-the-api","title":"Deploy the API","text":""},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#notebook-parameters","title":"Notebook Parameters","text":"<p>Below, we define notebook parameters for selecting which deployment setting, prefix, and Airflow service to use. Adjust these values as needed.</p> <pre><code># Parameters\n# Select the app deployment setting and prefix for the API\napp_deployment_key = None\napp_prefix = None\n\n# Airflow Service to use\nairflow_service_key = None\ntoken = None  # Replace with your long-term token\n</code></pre> <pre><code>assert app_deployment_key, \"No app deployment setting selected.\"\nassert app_prefix, \"No app prefix selected.\"\nassert airflow_service_key, \"No Airflow service selected.\"\nassert token, \"Please assign the token for the App API you just deployed\"\n</code></pre> <pre><code>import practicuscore as prt\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#deploying-the-app","title":"Deploying the App","text":"<p>The next code cell deploys our sensor API to your chosen environment (via PracticusCore).</p> <pre><code>app_name = \"http-sensor-test\"\nvisible_name = \"Http Sensor Test\"\ndescription = \"API to test Airflow Sensors\"\nicon = \"fa-wifi\"\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=\"app\",\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"Booting UI :\", app_url)\nprint(\"Booting API:\", api_url)\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#get-a-long-term-access-token-for-the-api","title":"Get a Long-Term Access Token for the API","text":"<ul> <li>You need to be an admin for this operation.</li> <li>Admin Console \u2192 App Hosting \u2192 Access Tokens \u2192 Create New</li> <li>Select an expiry date.</li> <li>In the \"App Access Tokens\" section, select http-sensor-test.</li> <li>Copy the app API token. We'll paste it in the next cell.</li> </ul>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#test-the-api","title":"Test the API","text":"<p>It's always a good idea to test the API before hooking it into Airflow.</p> <pre><code>import requests\n\n# api_url is the base we will save in Airflow connection\nsensor_api_url = api_url + \"sensor/\"  # The specific endpoint for the sensor\n\n# Optional parameters\nparams = {\"table_name\": \"table_1\"}\n\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n}\n\n# Make a GET request to test\nresp = requests.get(sensor_api_url, headers=headers, params=params)\n\nif resp.ok:\n    print(\"Response text:\")\n    print(resp.text)\n    resp_dict = resp.json()\n    error = resp_dict.get(\"error\")\n    if error:\n        print(\"API returned error:\", error)\n    print(\"ok?\", resp_dict.get(\"ok\"))\nelse:\n    print(\"Error:\", resp.status_code, resp.text)\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#create-a-new-airflow-connection","title":"Create a New Airflow Connection","text":"<ol> <li>Go to the Airflow UI \u2192 Admin \u2192 Connections.</li> <li>Create a new connection:</li> <li>Conn ID: <code>http_sensor_test</code></li> <li>Conn Type: HTTP</li> <li>Host: The base URL of your deployed app. For example:<ul> <li><code>https://practicus.your-company.com/apps/http-sensor-test/api/v1/</code> (or <code>/api/</code> without /v1/)</li> <li>Do not include <code>sensor/</code> here\u2014only the base path.</li> </ul> </li> <li>Password: Paste the token from above.</li> <li>Leave other fields empty or default.</li> </ol>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#create-the-dag","title":"Create the DAG","text":"<p>Below, we'll generate the basic DAG structure using <code>practicuscore</code>. We'll then customize it to add an HTTP Sensor.</p> <pre><code>import practicuscore as prt\n\n# Define a DAG flow with 2 tasks, one waiting for the API and one processing\ndag_flow = \"wait_for_api &gt;&gt; my_task\"\n\n# Default worker configuration\ndefault_worker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",\n    worker_size=\"X-Small\",\n)\n\ndag_key = \"http_sensor\"\nschedule_interval = None  # You can set a cron string or '@daily' if desired\nretries = 0  # 0 for dev/test, increase for production usage\n\nprt.workflows.generate_files(\n    dag_key=dag_key,\n    dag_flow=dag_flow,\n    files_path=None,\n    default_worker_config=default_worker_config,\n    save_credentials=True,\n    overwrite_existing=False,\n    schedule_interval=schedule_interval,\n    retries=retries,\n)\n</code></pre> <ul> <li>You can delete the generated file <code>wait_for_api.py</code>, because we'll replace it with our sensor code.</li> <li>Update <code>http_sensor_dag.py</code> so it looks like this:</li> </ul> <pre><code># http_sensor_dag.py\n\nfrom datetime import datetime\nfrom airflow.decorators import dag, task\nimport practicuscore as prt\n\n# Constructing a Unique DAG ID\n# ----------------------------\n# We strongly recommend using a DAG ID format like:\n#    &lt;dag_key&gt;.&lt;username&gt;\n# This approach ensures the username effectively serves as a namespace,\n# preventing name collisions in Airflow.\n\n# Let's locate dag_key and username. This runs in Airflow 'after' deployment.\ndag_key, username = prt.workflows.get_dag_info(__file__)\ndag_id = f\"{dag_key}.{username}\"\n\n# Fetch Bearer Token of API from Airflow Connection\nfrom airflow.hooks.base import BaseHook\n\nhttp_conn_id = \"http_sensor_test\"\nconnection = BaseHook.get_connection(http_conn_id)\nbearer_token = connection.password\n\n\ndef check_api_response(response) -&gt; bool:\n    \"\"\"Decides if the API response meets the condition to proceed.\"\"\"\n    # Convert response to a dictionary\n    resp_dict = response.json()\n    error = resp_dict.get(\"error\")\n    if error:\n        print(\"Sensor criteria not met:\", error)\n    # Return True if \"ok\" is True, otherwise False\n    return resp_dict.get(\"ok\", False) is True\n\n\n@dag(\n    dag_id=dag_id,\n    schedule_interval=None,  # Only run on-demand\n    start_date=datetime(2025, 1, 30, 20, 13),\n    default_args={\n        \"owner\": username,\n        \"retries\": 0,\n    },\n    catchup=False,\n    params=prt.workflows.get_airflow_params(),\n)\ndef generate_dag():\n    from airflow.providers.http.sensors.http import HttpSensor\n\n    wait_for_api = HttpSensor(\n        task_id=\"wait_for_http_sensor\",\n        http_conn_id=http_conn_id,\n        endpoint=\"sensor/\",  # appended to the base URL in the Airflow connection\n        request_params={\n            \"table_name\": \"table_1\",\n        },\n        headers={\n            \"Authorization\": f\"Bearer {bearer_token}\",\n        },\n        response_check=check_api_response,  # A function to validate the response\n        deferrable=True,  # If True, it keeps trying until poke_interval or timeout\n        poke_interval=10,  # Check every 10 seconds\n        timeout=30 * 60,  # Stop after 30 minutes\n    )\n\n    my_task = task(prt.workflows.run_airflow_task, task_id=\"my_task\")()\n    wait_for_api &gt;&gt; my_task\n\n\ngenerate_dag()\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#deploy-the-workflow","title":"Deploy the Workflow","text":"<p>Now let's deploy the workflow (i.e., the DAG) to Airflow.</p> <pre><code># Let's deploy the workflow\n# Make sure service_key is defined above.\n\nprt.workflows.deploy(\n    service_key=airflow_service_key,  # corrected variable name\n    dag_key=dag_key,\n    files_path=None,  # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#testing-on-airflow","title":"Testing on Airflow","text":"<ol> <li>Go to your Airflow UI, enable the http_sensor DAG.</li> <li>Trigger the DAG manually.</li> <li>Observe that wait_for_http_sensor attempts to call the <code>sensor/</code> endpoint.</li> <li>If the response is <code>ok: True</code>, the DAG moves to <code>my_task</code>.</li> <li>If the response is <code>ok: False</code>, the sensor will keep checking at <code>poke_interval</code> until <code>timeout</code>.</li> <li>If <code>table_name</code> is not <code>table_1</code> in the example, you\u2019ll see an error in the logs, and the sensor won't succeed.</li> </ol> <p>Try updating your <code>sensor.py</code> logic to always fail\u2014this will let you see that <code>my_task</code> never runs, and the sensor eventually times out.</p> <p>That\u2019s it! You now have an Airflow DAG that uses an HTTP Sensor to poll your microservice for some condition (in this case, the existence or readiness of <code>table_1</code>).</p>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#run_task_safedefault_workerjson","title":"run_task_safe/default_worker.json","text":"<pre><code>{\"worker_image\":\"practicus\",\"worker_size\":\"Small\",\"service_url\":\"\",\"email\":\"\",\"refresh_token\":\"\"}\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#run_task_saferun_task_safe_dagpy","title":"run_task_safe/run_task_safe_dag.py","text":"<pre><code># Airflow DAG run_task_safe_dag.py created using Practicus AI\n\nimport logging\nfrom datetime import datetime\nfrom airflow.decorators import dag, task\nimport practicuscore as prt\nimport os\n\n# Constructing a Unique DAG ID\n# ----------------------------\n# We strongly recommend using a DAG ID format like:\n#    &lt;dag_key&gt;.&lt;username&gt;\n# This approach ensures the username effectively serves as a namespace,\n#    preventing name collisions in Airflow.\n\n# Let's locate dag_key and username. This runs in Airflow 'after' deployment.\ndag_key, username = prt.workflows.get_dag_info(__file__)\ndag_id = f\"{dag_key}.{username}\"\n\n\ndef convert_local(d):\n    logging.debug(\"Converting {}\".format(d))\n    return d.in_timezone(\"Europe/Istanbul\")\n\n\ndef _read_cloud_worker_config_from_file(worker_config_json_path: str) -&gt; dict:\n    cloud_worker_conf_dict = {}  # type: ignore[var-annotated]\n    try:\n        if os.path.exists(worker_config_json_path):\n            prt.logger.info(f\"Reading Worker config from {worker_config_json_path}\")\n            with open(worker_config_json_path, \"rt\") as f:\n                content = f.read()\n                import json\n\n                cloud_worker_conf_dict = json.loads(content)\n        else:\n            prt.logger.info(\n                f\"Worker configuration file {worker_config_json_path} not found. \"\n                f\"Airflow DAG must provide settings via params passed from run DAG UI or global Airflow configuration.\"\n            )\n    except:\n        prt.logger.error(f\"Could not parse Worker config from file {worker_config_json_path}\", exc_info=True)\n    return cloud_worker_conf_dict\n\n\ndef _get_worker_config_dict(**kwargs) -&gt; dict:\n    dag = kwargs[\"dag\"]\n    dag_folder = dag.folder\n\n    final_dict = {}\n\n    worker_config_json_path = os.path.join(dag_folder, \"default_worker.json\")\n    prt.logger.debug(f\"worker_config_json_path : {worker_config_json_path}\")\n    if os.path.exists(worker_config_json_path):\n        worker_dict_from_json_file = _read_cloud_worker_config_from_file(worker_config_json_path)\n        try:\n            for key, value in worker_dict_from_json_file.items():\n                if value not in [None, \"\", \"None\"]:\n                    prt.logger.info(\n                        f\"Updating Worker configuration key '{key}' using \"\n                        f\"task specific worker configuration file: default_worker.json\"\n                    )\n                    final_dict[key] = value\n        except:\n            prt.logger.error(f\"Could not parse param dictionary from {worker_config_json_path}\", exc_info=True)\n\n    return final_dict\n\n\ndef _cleanup(**kwargs):\n    from datetime import datetime, timezone\n\n    timeout_seconds = 59  # Threshold for considering a Worker as stuck\n    _worker_config_dict = _get_worker_config_dict(**kwargs)\n    region = prt.regions.region_factory(_worker_config_dict)\n    prt.logger.info(f\"Found region : {str(region)}\")\n    # Iterate through all Workers in the region\n    for worker in region.worker_list:\n        time_since_creation = int((datetime.now(timezone.utc) - worker.creation_time).total_seconds())\n        prt.logger.info(\n            f\"{worker.name} started {time_since_creation} seconds ago and is currently in '{worker.status}' state.\"\n        )\n        if worker.status == \"Provisioning\" and time_since_creation &gt; timeout_seconds:\n            prt.logger.warning(\n                f\"-&gt; Terminating {worker.name} \u2014 stuck in 'Provisioning' for more than {timeout_seconds} seconds.\"\n            )\n            worker.terminate()\n\n\n# Define other DAG properties like schedule, retries etc.\n@dag(\n    dag_id=dag_id,\n    schedule_interval=None,\n    start_date=datetime(2025, 5, 9, 7, 0),\n    default_args={\n        \"owner\": username,\n        \"retries\": 0,\n    },\n    catchup=False,\n    user_defined_macros={\"local_tz\": convert_local},\n    max_active_runs=1,\n    params=prt.workflows.get_airflow_params(),\n)\ndef generate_dag():\n    # The `task_id` must match the task file (e.g., `my_1st_task.py or my_1st_task.sh`)\n    # located in the same folder as this DAG file.\n    def run_with_dynamic_param(**kwargs):\n        from practicuscore.exceptions import TaskError\n\n        try:\n            return prt.workflows.run_airflow_task(**kwargs)\n        except TaskError as ex:\n            _cleanup(**kwargs)\n            raise ex\n\n    task1 = task(run_with_dynamic_param, task_id=\"task1\")()\n\n    # Define how your task will flow\n    task1\n\n\ngenerate_dag()\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#run_task_safetask1py","title":"run_task_safe/task1.py","text":"<pre><code>import practicuscore as prt\n\n\ndef main():\n    prt.logger.info(\"Running task 1 is completed\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#task_runnner_appapissample_callbackpy","title":"task_runnner_app/apis/sample_callback.py","text":"<pre><code># Sample callback APIs, we will use them for our tests.\n\nfrom pydantic import BaseModel\nimport practicuscore as prt\nfrom starlette.requests import Request\n\n\nclass TaskRunnerCallbackRequest(BaseModel):\n    task_name: str\n    \"\"\"Model Deployment Task\"\"\"\n\n    task_id: str\n    \"\"\"Unique task ID\"\"\"\n\n    success: bool\n    \"\"\"True if the task completed successfully, False otherwise\"\"\"\n\n    logs: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"task_name\": \"deploy_model\",\n                    \"task_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n                    \"success\": True,\n                    \"message\": \"Model deployed successfully\",\n                },\n                {\n                    \"task_name\": \"cleanup_task\",\n                    \"task_id\": \"123e4567-e89b-12d3-a456-426614174001\",\n                    \"success\": False,\n                    \"message\": \"Error occurred during cleanup\",\n                },\n            ]\n        },\n    }\n\n\nclass TaskRunnerCallbackResponse(BaseModel):\n    success: bool\n    \"\"\"True or False\"\"\"\n\n    message: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"success\": True, \"message\": \"Successfully finished\"},\n                {\"success\": False, \"message\": \"Errored occurred\"},\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    disable_authentication=True,\n)\n\n\n@prt.apps.api(path=\"/sample-task-runner-callback\", spec=api_spec)\nasync def task_runner_callback(payload: TaskRunnerCallbackRequest, **kwargs) -&gt; TaskRunnerCallbackResponse:\n    if payload.success:\n        prt.logger.info(f\"Task with name : {payload.task_name} task_id : finished successfully\")\n    else:\n        prt.logger.error(\n            f\"Task with name : {payload.task_name} finished with error\\npayload : {payload.model_dump_json(indent=2)}\"\n        )\n\n    return TaskRunnerCallbackResponse(success=True, message=\"ok\")\n\n\n@prt.apps.api(path=\"/sample-task-runner-callback2\", spec=api_spec)\nasync def task_runner_callback2(request: Request, **kwargs) -&gt; TaskRunnerCallbackResponse:\n    prt.logger.info(f\"request: {request}\")\n\n    body: dict = await request.json()\n\n    task_name = body.get(\"task_name\")\n    task_id = body.get(\"task_id\")\n    success = body.get(\"success\")\n    logs = body.get(\"logs\")\n\n    if success:\n        prt.logger.info(f\"Task with name : {task_name} task_id : {task_id} finished successfully\")\n    else:\n        prt.logger.error(f\"Task with name : {task_name} task_id : {task_id} finished with error\\nlogs : {logs}\")\n\n    return TaskRunnerCallbackResponse(success=True, message=\"ok\")\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#task_runnner_appapistask_runnerpy","title":"task_runnner_app/apis/task_runner.py","text":"<pre><code>import os\nimport threading\n\nimport requests\nfrom pydantic import BaseModel\n\nimport practicuscore as prt\n\n\nclass TaskRunnerRequest(BaseModel):\n    task_id: str\n    \"\"\"Unique task id (1222345, 23erd-rt56-tty67-34er5 ...)\"\"\"\n\n    task_file_name: str\n    \"\"\"deployer_task.py, task1.sh... (*.py, *.sh files) \"\"\"\n\n    worker_config: prt.WorkerConfig\n    \"\"\"WorkerConfig with or without a GitConfig\"\"\"\n\n    callback_url: str | None = None\n    \"\"\"Callback URL to be called after the task finishes\"\"\"\n\n    callback_url_token: str | None = None\n    \"\"\"Optional Callback URL security (bearer) token\"\"\"\n\n    terminate_on_completion: bool = True\n    \"\"\"Determines whether the task runner should terminate on completion\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"task_id\": \"23erd-rt56\",\n                    \"task_file_name\": \"deployer_task.py\",\n                    \"worker_config\": {\n                        \"worker_size\": \"small\",\n                        \"personal_secrets\": [\"git_secret\"],\n                        \"git_configs\": [\n                            {\"remote_url\": \"https://github.com/example/repo.git\", \"secret_name\": \"git_secret\"}\n                        ],\n                    },\n                    \"callback_url\": \"https://example.com/callback\",\n                    \"terminate_on_completion\": True,\n                }\n            ]\n        },\n    }\n\n\nclass TaskRunnerResponse(BaseModel):\n    success: bool\n    \"\"\"True or False\"\"\"\n\n    message: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"success\": True, \"message\": \"Successfully finished\"},\n                {\"success\": False, \"message\": \"Errored occurred\"},\n            ]\n        },\n    }\n\n\n@prt.apps.api(path=\"/task-runner\")\nasync def task_runner(payload: TaskRunnerRequest, **kwargs) -&gt; TaskRunnerResponse:\n    \"\"\"This API starts task async and returns started message\"\"\"\n\n    prt.logger.info(\"Started running task...\")\n    thread: threading.Thread | None = None\n    try:\n        thread = threading.Thread(target=run_task, args=[payload])\n        thread.start()\n        return TaskRunnerResponse(success=True, message=\"Started Task successfully\")\n    except Exception as ex:\n        prt.logger.error(\"Error on task\", exc_info=True)\n        return TaskRunnerResponse(success=False, message=f\"An Error occurred while running task. Error: {str(ex)}\")\n\n\ndef run_task(request: TaskRunnerRequest):\n    assert request.task_file_name, \"No task_file_name located\"\n    assert request.worker_config, \"No worker_config located\"\n\n    success = None\n    worker = None\n    worker_logs = \"\"\n    try:\n        prt.logger.info(\n            f\"Starting task: {request.task_id} ({request.task_file_name}) terminate_on_completion: {request.terminate_on_completion}\"\n        )\n        tasks_path = get_tasks_path()\n        prt.logger.info(f\"Tasks path to upload: {tasks_path}\")\n        worker, success = prt.run_task(\n            file_name=request.task_file_name,\n            files_path=tasks_path,\n            worker_config=request.worker_config,\n            terminate_on_completion=False,\n        )\n        worker_logs = worker.get_logs(log_size_mb=5)\n    except Exception as ex:\n        message = f\"Finished task_name: {request.task_file_name} with error\"\n        prt.logger.error(message, exc_info=True)\n        raise ex\n    finally:\n        try:\n            if success is not None:\n                if success:\n                    prt.logger.info(\"Finished successfully\")\n                else:\n                    prt.logger.error(\n                        f\"Finished task with error. Check logs for details. task_name: {request.task_file_name}\"\n                    )\n                if request.callback_url:\n                    trigger_callback(request=request, success=success, worker_logs=worker_logs)\n                else:\n                    prt.logger.debug(f\"Task: {request.task_file_name} does not have a callback url.\")\n        finally:\n            if worker is not None and request.terminate_on_completion:\n                worker.terminate()\n\n\ndef trigger_callback(request: TaskRunnerRequest, success: bool, worker_logs: str):\n    request.callback_url, \"No callback_url located\"\n\n    headers = {\"content-type\": \"application/json\"}\n    if request.callback_url_token:\n        prt.logger.debug(f\"Task: '{request.task_file_name}' includes a security token, including as a Bearer token.\")\n        headers[\"authorization\"] = f\"Bearer {request.callback_url_token}\"\n    else:\n        prt.logger.debug(f\"Task: {request.task_file_name} does not include a security token.\")\n\n    callback_payload = {\n        \"task_name\": request.task_file_name,\n        \"task_id\": request.task_id,\n        \"success\": success,\n        \"logs\": worker_logs,\n    }\n\n    import json\n\n    json_data = json.dumps(callback_payload)\n\n    prt.logger.info(f\"Sending to callback url: '{request.callback_url}' success: '{success}'\")\n    resp = requests.post(request.callback_url, json=json_data, headers=headers)\n\n    if resp.ok:\n        prt.logger.info(f\"Response text: {resp.text}\")\n    else:\n        prt.logger.error(f\"Response has errors:  resp.status_code: {resp.status_code}  - resp.text{resp.text}\")\n\n\ndef get_tasks_path() -&gt; str:\n    \"\"\"\n    Returns the path to the 'tasks' directory.\n\n    Steps:\n    1. Retrieve the current execution path using get_execution_path().\n    2. Get the parent directory of the execution path.\n    3. Construct the path to the 'tasks' directory\n    \"\"\"\n    # Get the execution path.\n    exec_path = get_execution_path()\n    prt.logger.debug(f\"Exec pah: {exec_path}\")\n    # Determine the parent directory of the execution path.\n    parent_dir = os.path.dirname(exec_path)\n    prt.logger.debug(f\"parent_dir: {parent_dir}\")\n    # Construct the path for the \"tasks\" directory at the same level.\n    path = os.path.join(parent_dir, \"tasks\")\n    prt.logger.debug(f\"path: {path}\")\n    return path\n\n\ndef get_execution_path() -&gt; str:\n    \"\"\"\n    Returns the execution path of the current script.\n    If the __file__ variable is available, it returns the directory of the script.\n    Otherwise, it falls back to the current working directory.\n    \"\"\"\n    try:\n        # __file__ is defined when the script is run from a file.\n        path = os.path.dirname(os.path.abspath(__file__))\n        prt.logger.debug(f\"path from file path: {path}\")\n        return path\n    except NameError:\n        # __file__ is not defined in an interactive shell; use the current working directory.\n        path = os.getcwd()\n        prt.logger.debug(f\"path from os.getcwd(): {path}\")\n        return path\n\n\n# async def main():\n#     worker_config = prt.WorkerConfig(\n#         worker_size=\"X-Small\",\n#         # personal_secrets=[git_secret_name],\n#         # git_configs=[git_config],\n#     )\n\n#     payload = TaskRunnerRequest(\n#         task_file_name=\"deployer_task.py\",\n#         task_id=\"task-1\",\n#         worker_config=worker_config,\n#         callback_url=\"https://dev.practicus.io/apps/task-runner-app/api/task-runner-callback\",\n#         terminate_on_completion=True,\n#     )\n#     await task_runner(payload)\n\n\n# if __name__ == \"__main__\":\n#     import asyncio\n\n#     asyncio.run(main())\n\n\n# if __name__ == \"__main__\":\n#     worker_config = prt.WorkerConfig(worker_size=\"X-Small\")\n#     payload = TaskRunnerRequest(\n#         task_file_name=\"deployer_task.py\",\n#         task_id=\"task-1\",\n#         worker_config=worker_config,\n#         callback_url=\"https://dev.practicus.io/apps/task-runner-app/api/task-runner-callback\",\n#         terminate_on_completion=True,\n#     )\n\n#     trigger_callback(request=payload, success=True, worker_logs=\"logs logs ....\")\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#task_runnner_apptasksdeployer_taskpy","title":"task_runnner_app/tasks/deployer_task.py","text":"<pre><code># Sample Task that the Task Runner App will execute\n\nimport practicuscore as prt\nfrom pathlib import Path\n\n\ndef any_file_exists(dir_path: str) -&gt; bool:\n    \"\"\"\n    Check if there is at least one file anywhere under the given directory.\n\n    :param dir_path: Path to the directory to be searched.\n    :return: True if any file is found, False otherwise.\n    \"\"\"\n    base = Path(dir_path)\n    return any(p.is_file() for p in base.rglob(\"*\"))\n\n\ndef main():\n    prt.logger.info(\"Starting model deployment task \")\n\n    import os\n\n    assert \"DEPLOYMENT_KEY\" in os.environ and len(os.environ[\"DEPLOYMENT_KEY\"]) &gt; 0, \"DEPLOYMENT_KEY is not provided\"\n    assert \"PREFIX\" in os.environ and len(os.environ[\"PREFIX\"]) &gt; 0, \"PREFIX is not provided\"\n    assert \"MODEL_NAME\" in os.environ and len(os.environ[\"MODEL_NAME\"]) &gt; 0, \"MODEL_NAME is not provided\"\n    assert \"MODEL_DIR\" in os.environ and len(os.environ[\"MODEL_DIR\"]) &gt; 0, \"MODEL_DIR is not provided\"\n\n    deployment_key = os.environ[\"DEPLOYMENT_KEY\"]  # \"depl\"\n    prefix = os.environ[\"PREFIX\"]  # models\n    model_name = os.environ[\"MODEL_NAME\"]  # sample-model-ro\n    model_dir = os.environ[\"MODEL_DIR\"]  # \"/home/ubuntu/my/projects/models-repo/sample-model\"\n\n    if not any_file_exists(model_dir):\n        msg = \"Model directory is empty or does not exist. Check logs for details.\"\n        prt.logger.error(msg)\n        raise RuntimeError(msg)\n    else:\n        api_url, api_version_url, api_meta_url = prt.models.deploy(\n            deployment_key=deployment_key, prefix=prefix, model_name=model_name, model_dir=model_dir\n        )\n        prt.logger.info(f\"Finished model deployment task successfully. api_url : {api_version_url}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Deploy | Next: Generative AI &gt; Databases &gt; Using Databases</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/","title":"Customizing Task Parameters","text":"<p>This example demonstrates how to customize and pass parameters to a Practicus AI worker. You can do so by defining environment variables in the <code>WorkerConfig</code> object.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#why-environment-variables","title":"Why Environment Variables?","text":"<p>Environment variables are often the easiest way to inject small pieces of configuration or parameters into a script. Practicus AI automatically sets these environment variables in the worker's environment when your script runs.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#basic-example","title":"Basic Example","text":"<p>In this simple example, we show how to set environment variables in <code>WorkerConfig</code> and then access them in a Python script.</p> <pre><code># task.py\nimport os\n\nprint(\"First Param:\", os.environ[\"MY_FIRST_PARAM\"])\nprint(\"Second Param:\", os.environ[\"MY_SECOND_PARAM\"])\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#defining-workerconfig-with-environment-variables","title":"Defining <code>WorkerConfig</code> with Environment Variables","text":"<p>Below, we create a <code>WorkerConfig</code> object. Notice how we specify environment variables in the <code>env_variables</code> dictionary. Practicus AI will ensure these variables are set when <code>task.py</code> is executed.</p> <pre><code>import practicuscore as prt\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",  # The base container image\n    worker_size=\"X-Small\",  # Size configuration\n    env_variables={\"MY_FIRST_PARAM\": \"VALUE1\", \"MY_SECOND_PARAM\": 123},\n)\n\nworker, success = prt.run_task(\n    file_name=\"task.py\",  # The name of the script to run\n    worker_config=worker_config,\n)\n\nprint(\"Task finished with status:\", success)\n</code></pre> <p>When this code runs, it will print out the values of <code>MY_FIRST_PARAM</code> and <code>MY_SECOND_PARAM</code> from within <code>task.py</code>.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#airflow-integration","title":"Airflow Integration","text":"<p>To integrate with Practicus AI Airflow, you can utilize the same approach. Airflow tasks can inject environment variables by writing out a <code>WorkerConfig</code> JSON file that Practicus AI can pick up.</p> <p>For example: 1. Define your <code>worker_config</code> in Python. 2. Serialize it to JSON. 3. Store it in a file named after your task (e.g., <code>task_worker.json</code> if your script is <code>task.py</code>).</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#example-writing-task_workerjson","title":"Example: Writing <code>task_worker.json</code>","text":"<pre><code>worker_config_json = worker_config.model_dump_json(exclude_none=True)\n\nwith open(\"task_worker.json\", \"wt\") as f:\n    f.write(worker_config_json)\n\nprint(\"Generated worker_config JSON:\\n\", worker_config_json)\n</code></pre> <p>An example <code>task_worker.json</code> might look like:</p> <pre><code>{\n  \"worker_image\": \"practicus\",\n  \"worker_size\": \"X-Small\",\n  \"additional_params\": \"eyJlbnZfdmFyaWFibGVzIjogeyJNWV9GSVJTVF9QQVJBTSI6ICJWQUxVRTEiLCAiTVlfU0VDT05EX1BBUkFNIjogMTIzfX0=\"\n}\n</code></pre> <p>Note that <code>additional_params</code> is base64-encoded data containing:</p> <p><pre><code>{\n  \"env_variables\": {\n    \"MY_FIRST_PARAM\": \"VALUE1\",\n    \"MY_SECOND_PARAM\": 123\n  }\n}\n</code></pre> This is how Practicus AI stores your configuration behind the scenes.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#summary","title":"Summary","text":"<ul> <li>Defining environment variables: Use <code>env_variables</code> within <code>WorkerConfig</code> to inject parameters.</li> <li>Accessing parameters: In your Python script (e.g., <code>task.py</code>), read them from <code>os.environ</code>.</li> <li>Airflow: Write out the <code>worker_config</code> to a JSON file. Practicus AI automatically picks that up.</li> </ul> <p>By following these steps, you can effectively pass custom parameters and configurations to your Practicus AI tasks, making your data pipelines more dynamic and flexible.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/workflows/task-parameters/#run_task_safedefault_workerjson","title":"run_task_safe/default_worker.json","text":"<pre><code>{\"worker_image\":\"practicus\",\"worker_size\":\"Small\",\"service_url\":\"\",\"email\":\"\",\"refresh_token\":\"\"}\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#run_task_saferun_task_safe_dagpy","title":"run_task_safe/run_task_safe_dag.py","text":"<pre><code># Airflow DAG run_task_safe_dag.py created using Practicus AI\n\nimport logging\nfrom datetime import datetime\nfrom airflow.decorators import dag, task\nimport practicuscore as prt\nimport os\n\n# Constructing a Unique DAG ID\n# ----------------------------\n# We strongly recommend using a DAG ID format like:\n#    &lt;dag_key&gt;.&lt;username&gt;\n# This approach ensures the username effectively serves as a namespace,\n#    preventing name collisions in Airflow.\n\n# Let's locate dag_key and username. This runs in Airflow 'after' deployment.\ndag_key, username = prt.workflows.get_dag_info(__file__)\ndag_id = f\"{dag_key}.{username}\"\n\n\ndef convert_local(d):\n    logging.debug(\"Converting {}\".format(d))\n    return d.in_timezone(\"Europe/Istanbul\")\n\n\ndef _read_cloud_worker_config_from_file(worker_config_json_path: str) -&gt; dict:\n    cloud_worker_conf_dict = {}  # type: ignore[var-annotated]\n    try:\n        if os.path.exists(worker_config_json_path):\n            prt.logger.info(f\"Reading Worker config from {worker_config_json_path}\")\n            with open(worker_config_json_path, \"rt\") as f:\n                content = f.read()\n                import json\n\n                cloud_worker_conf_dict = json.loads(content)\n        else:\n            prt.logger.info(\n                f\"Worker configuration file {worker_config_json_path} not found. \"\n                f\"Airflow DAG must provide settings via params passed from run DAG UI or global Airflow configuration.\"\n            )\n    except:\n        prt.logger.error(f\"Could not parse Worker config from file {worker_config_json_path}\", exc_info=True)\n    return cloud_worker_conf_dict\n\n\ndef _get_worker_config_dict(**kwargs) -&gt; dict:\n    dag = kwargs[\"dag\"]\n    dag_folder = dag.folder\n\n    final_dict = {}\n\n    worker_config_json_path = os.path.join(dag_folder, \"default_worker.json\")\n    prt.logger.debug(f\"worker_config_json_path : {worker_config_json_path}\")\n    if os.path.exists(worker_config_json_path):\n        worker_dict_from_json_file = _read_cloud_worker_config_from_file(worker_config_json_path)\n        try:\n            for key, value in worker_dict_from_json_file.items():\n                if value not in [None, \"\", \"None\"]:\n                    prt.logger.info(\n                        f\"Updating Worker configuration key '{key}' using \"\n                        f\"task specific worker configuration file: default_worker.json\"\n                    )\n                    final_dict[key] = value\n        except:\n            prt.logger.error(f\"Could not parse param dictionary from {worker_config_json_path}\", exc_info=True)\n\n    return final_dict\n\n\ndef _cleanup(**kwargs):\n    from datetime import datetime, timezone\n\n    timeout_seconds = 59  # Threshold for considering a Worker as stuck\n    _worker_config_dict = _get_worker_config_dict(**kwargs)\n    region = prt.regions.region_factory(_worker_config_dict)\n    prt.logger.info(f\"Found region : {str(region)}\")\n    # Iterate through all Workers in the region\n    for worker in region.worker_list:\n        time_since_creation = int((datetime.now(timezone.utc) - worker.creation_time).total_seconds())\n        prt.logger.info(\n            f\"{worker.name} started {time_since_creation} seconds ago and is currently in '{worker.status}' state.\"\n        )\n        if worker.status == \"Provisioning\" and time_since_creation &gt; timeout_seconds:\n            prt.logger.warning(\n                f\"-&gt; Terminating {worker.name} \u2014 stuck in 'Provisioning' for more than {timeout_seconds} seconds.\"\n            )\n            worker.terminate()\n\n\n# Define other DAG properties like schedule, retries etc.\n@dag(\n    dag_id=dag_id,\n    schedule_interval=None,\n    start_date=datetime(2025, 5, 9, 7, 0),\n    default_args={\n        \"owner\": username,\n        \"retries\": 0,\n    },\n    catchup=False,\n    user_defined_macros={\"local_tz\": convert_local},\n    max_active_runs=1,\n    params=prt.workflows.get_airflow_params(),\n)\ndef generate_dag():\n    # The `task_id` must match the task file (e.g., `my_1st_task.py or my_1st_task.sh`)\n    # located in the same folder as this DAG file.\n    def run_with_dynamic_param(**kwargs):\n        from practicuscore.exceptions import TaskError\n\n        try:\n            return prt.workflows.run_airflow_task(**kwargs)\n        except TaskError as ex:\n            _cleanup(**kwargs)\n            raise ex\n\n    task1 = task(run_with_dynamic_param, task_id=\"task1\")()\n\n    # Define how your task will flow\n    task1\n\n\ngenerate_dag()\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#run_task_safetask1py","title":"run_task_safe/task1.py","text":"<pre><code>import practicuscore as prt\n\n\ndef main():\n    prt.logger.info(\"Running task 1 is completed\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#task_runnner_appapissample_callbackpy","title":"task_runnner_app/apis/sample_callback.py","text":"<pre><code># Sample callback APIs, we will use them for our tests.\n\nfrom pydantic import BaseModel\nimport practicuscore as prt\nfrom starlette.requests import Request\n\n\nclass TaskRunnerCallbackRequest(BaseModel):\n    task_name: str\n    \"\"\"Model Deployment Task\"\"\"\n\n    task_id: str\n    \"\"\"Unique task ID\"\"\"\n\n    success: bool\n    \"\"\"True if the task completed successfully, False otherwise\"\"\"\n\n    logs: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"task_name\": \"deploy_model\",\n                    \"task_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n                    \"success\": True,\n                    \"message\": \"Model deployed successfully\",\n                },\n                {\n                    \"task_name\": \"cleanup_task\",\n                    \"task_id\": \"123e4567-e89b-12d3-a456-426614174001\",\n                    \"success\": False,\n                    \"message\": \"Error occurred during cleanup\",\n                },\n            ]\n        },\n    }\n\n\nclass TaskRunnerCallbackResponse(BaseModel):\n    success: bool\n    \"\"\"True or False\"\"\"\n\n    message: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"success\": True, \"message\": \"Successfully finished\"},\n                {\"success\": False, \"message\": \"Errored occurred\"},\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    disable_authentication=True,\n)\n\n\n@prt.apps.api(path=\"/sample-task-runner-callback\", spec=api_spec)\nasync def task_runner_callback(payload: TaskRunnerCallbackRequest, **kwargs) -&gt; TaskRunnerCallbackResponse:\n    if payload.success:\n        prt.logger.info(f\"Task with name : {payload.task_name} task_id : finished successfully\")\n    else:\n        prt.logger.error(\n            f\"Task with name : {payload.task_name} finished with error\\npayload : {payload.model_dump_json(indent=2)}\"\n        )\n\n    return TaskRunnerCallbackResponse(success=True, message=\"ok\")\n\n\n@prt.apps.api(path=\"/sample-task-runner-callback2\", spec=api_spec)\nasync def task_runner_callback2(request: Request, **kwargs) -&gt; TaskRunnerCallbackResponse:\n    prt.logger.info(f\"request: {request}\")\n\n    body: dict = await request.json()\n\n    task_name = body.get(\"task_name\")\n    task_id = body.get(\"task_id\")\n    success = body.get(\"success\")\n    logs = body.get(\"logs\")\n\n    if success:\n        prt.logger.info(f\"Task with name : {task_name} task_id : {task_id} finished successfully\")\n    else:\n        prt.logger.error(f\"Task with name : {task_name} task_id : {task_id} finished with error\\nlogs : {logs}\")\n\n    return TaskRunnerCallbackResponse(success=True, message=\"ok\")\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#task_runnner_appapistask_runnerpy","title":"task_runnner_app/apis/task_runner.py","text":"<pre><code>import os\nimport threading\n\nimport requests\nfrom pydantic import BaseModel\n\nimport practicuscore as prt\n\n\nclass TaskRunnerRequest(BaseModel):\n    task_id: str\n    \"\"\"Unique task id (1222345, 23erd-rt56-tty67-34er5 ...)\"\"\"\n\n    task_file_name: str\n    \"\"\"deployer_task.py, task1.sh... (*.py, *.sh files) \"\"\"\n\n    worker_config: prt.WorkerConfig\n    \"\"\"WorkerConfig with or without a GitConfig\"\"\"\n\n    callback_url: str | None = None\n    \"\"\"Callback URL to be called after the task finishes\"\"\"\n\n    callback_url_token: str | None = None\n    \"\"\"Optional Callback URL security (bearer) token\"\"\"\n\n    terminate_on_completion: bool = True\n    \"\"\"Determines whether the task runner should terminate on completion\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"task_id\": \"23erd-rt56\",\n                    \"task_file_name\": \"deployer_task.py\",\n                    \"worker_config\": {\n                        \"worker_size\": \"small\",\n                        \"personal_secrets\": [\"git_secret\"],\n                        \"git_configs\": [\n                            {\"remote_url\": \"https://github.com/example/repo.git\", \"secret_name\": \"git_secret\"}\n                        ],\n                    },\n                    \"callback_url\": \"https://example.com/callback\",\n                    \"terminate_on_completion\": True,\n                }\n            ]\n        },\n    }\n\n\nclass TaskRunnerResponse(BaseModel):\n    success: bool\n    \"\"\"True or False\"\"\"\n\n    message: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"success\": True, \"message\": \"Successfully finished\"},\n                {\"success\": False, \"message\": \"Errored occurred\"},\n            ]\n        },\n    }\n\n\n@prt.apps.api(path=\"/task-runner\")\nasync def task_runner(payload: TaskRunnerRequest, **kwargs) -&gt; TaskRunnerResponse:\n    \"\"\"This API starts task async and returns started message\"\"\"\n\n    prt.logger.info(\"Started running task...\")\n    thread: threading.Thread | None = None\n    try:\n        thread = threading.Thread(target=run_task, args=[payload])\n        thread.start()\n        return TaskRunnerResponse(success=True, message=\"Started Task successfully\")\n    except Exception as ex:\n        prt.logger.error(\"Error on task\", exc_info=True)\n        return TaskRunnerResponse(success=False, message=f\"An Error occurred while running task. Error: {str(ex)}\")\n\n\ndef run_task(request: TaskRunnerRequest):\n    assert request.task_file_name, \"No task_file_name located\"\n    assert request.worker_config, \"No worker_config located\"\n\n    success = None\n    worker = None\n    worker_logs = \"\"\n    try:\n        prt.logger.info(\n            f\"Starting task: {request.task_id} ({request.task_file_name}) terminate_on_completion: {request.terminate_on_completion}\"\n        )\n        tasks_path = get_tasks_path()\n        prt.logger.info(f\"Tasks path to upload: {tasks_path}\")\n        worker, success = prt.run_task(\n            file_name=request.task_file_name,\n            files_path=tasks_path,\n            worker_config=request.worker_config,\n            terminate_on_completion=False,\n        )\n        worker_logs = worker.get_logs(log_size_mb=5)\n    except Exception as ex:\n        message = f\"Finished task_name: {request.task_file_name} with error\"\n        prt.logger.error(message, exc_info=True)\n        raise ex\n    finally:\n        try:\n            if success is not None:\n                if success:\n                    prt.logger.info(\"Finished successfully\")\n                else:\n                    prt.logger.error(\n                        f\"Finished task with error. Check logs for details. task_name: {request.task_file_name}\"\n                    )\n                if request.callback_url:\n                    trigger_callback(request=request, success=success, worker_logs=worker_logs)\n                else:\n                    prt.logger.debug(f\"Task: {request.task_file_name} does not have a callback url.\")\n        finally:\n            if worker is not None and request.terminate_on_completion:\n                worker.terminate()\n\n\ndef trigger_callback(request: TaskRunnerRequest, success: bool, worker_logs: str):\n    request.callback_url, \"No callback_url located\"\n\n    headers = {\"content-type\": \"application/json\"}\n    if request.callback_url_token:\n        prt.logger.debug(f\"Task: '{request.task_file_name}' includes a security token, including as a Bearer token.\")\n        headers[\"authorization\"] = f\"Bearer {request.callback_url_token}\"\n    else:\n        prt.logger.debug(f\"Task: {request.task_file_name} does not include a security token.\")\n\n    callback_payload = {\n        \"task_name\": request.task_file_name,\n        \"task_id\": request.task_id,\n        \"success\": success,\n        \"logs\": worker_logs,\n    }\n\n    import json\n\n    json_data = json.dumps(callback_payload)\n\n    prt.logger.info(f\"Sending to callback url: '{request.callback_url}' success: '{success}'\")\n    resp = requests.post(request.callback_url, json=json_data, headers=headers)\n\n    if resp.ok:\n        prt.logger.info(f\"Response text: {resp.text}\")\n    else:\n        prt.logger.error(f\"Response has errors:  resp.status_code: {resp.status_code}  - resp.text{resp.text}\")\n\n\ndef get_tasks_path() -&gt; str:\n    \"\"\"\n    Returns the path to the 'tasks' directory.\n\n    Steps:\n    1. Retrieve the current execution path using get_execution_path().\n    2. Get the parent directory of the execution path.\n    3. Construct the path to the 'tasks' directory\n    \"\"\"\n    # Get the execution path.\n    exec_path = get_execution_path()\n    prt.logger.debug(f\"Exec pah: {exec_path}\")\n    # Determine the parent directory of the execution path.\n    parent_dir = os.path.dirname(exec_path)\n    prt.logger.debug(f\"parent_dir: {parent_dir}\")\n    # Construct the path for the \"tasks\" directory at the same level.\n    path = os.path.join(parent_dir, \"tasks\")\n    prt.logger.debug(f\"path: {path}\")\n    return path\n\n\ndef get_execution_path() -&gt; str:\n    \"\"\"\n    Returns the execution path of the current script.\n    If the __file__ variable is available, it returns the directory of the script.\n    Otherwise, it falls back to the current working directory.\n    \"\"\"\n    try:\n        # __file__ is defined when the script is run from a file.\n        path = os.path.dirname(os.path.abspath(__file__))\n        prt.logger.debug(f\"path from file path: {path}\")\n        return path\n    except NameError:\n        # __file__ is not defined in an interactive shell; use the current working directory.\n        path = os.getcwd()\n        prt.logger.debug(f\"path from os.getcwd(): {path}\")\n        return path\n\n\n# async def main():\n#     worker_config = prt.WorkerConfig(\n#         worker_size=\"X-Small\",\n#         # personal_secrets=[git_secret_name],\n#         # git_configs=[git_config],\n#     )\n\n#     payload = TaskRunnerRequest(\n#         task_file_name=\"deployer_task.py\",\n#         task_id=\"task-1\",\n#         worker_config=worker_config,\n#         callback_url=\"https://dev.practicus.io/apps/task-runner-app/api/task-runner-callback\",\n#         terminate_on_completion=True,\n#     )\n#     await task_runner(payload)\n\n\n# if __name__ == \"__main__\":\n#     import asyncio\n\n#     asyncio.run(main())\n\n\n# if __name__ == \"__main__\":\n#     worker_config = prt.WorkerConfig(worker_size=\"X-Small\")\n#     payload = TaskRunnerRequest(\n#         task_file_name=\"deployer_task.py\",\n#         task_id=\"task-1\",\n#         worker_config=worker_config,\n#         callback_url=\"https://dev.practicus.io/apps/task-runner-app/api/task-runner-callback\",\n#         terminate_on_completion=True,\n#     )\n\n#     trigger_callback(request=payload, success=True, worker_logs=\"logs logs ....\")\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#task_runnner_apptasksdeployer_taskpy","title":"task_runnner_app/tasks/deployer_task.py","text":"<pre><code># Sample Task that the Task Runner App will execute\n\nimport practicuscore as prt\nfrom pathlib import Path\n\n\ndef any_file_exists(dir_path: str) -&gt; bool:\n    \"\"\"\n    Check if there is at least one file anywhere under the given directory.\n\n    :param dir_path: Path to the directory to be searched.\n    :return: True if any file is found, False otherwise.\n    \"\"\"\n    base = Path(dir_path)\n    return any(p.is_file() for p in base.rglob(\"*\"))\n\n\ndef main():\n    prt.logger.info(\"Starting model deployment task \")\n\n    import os\n\n    assert \"DEPLOYMENT_KEY\" in os.environ and len(os.environ[\"DEPLOYMENT_KEY\"]) &gt; 0, \"DEPLOYMENT_KEY is not provided\"\n    assert \"PREFIX\" in os.environ and len(os.environ[\"PREFIX\"]) &gt; 0, \"PREFIX is not provided\"\n    assert \"MODEL_NAME\" in os.environ and len(os.environ[\"MODEL_NAME\"]) &gt; 0, \"MODEL_NAME is not provided\"\n    assert \"MODEL_DIR\" in os.environ and len(os.environ[\"MODEL_DIR\"]) &gt; 0, \"MODEL_DIR is not provided\"\n\n    deployment_key = os.environ[\"DEPLOYMENT_KEY\"]  # \"depl\"\n    prefix = os.environ[\"PREFIX\"]  # models\n    model_name = os.environ[\"MODEL_NAME\"]  # sample-model-ro\n    model_dir = os.environ[\"MODEL_DIR\"]  # \"/home/ubuntu/my/projects/models-repo/sample-model\"\n\n    if not any_file_exists(model_dir):\n        msg = \"Model directory is empty or does not exist. Check logs for details.\"\n        prt.logger.error(msg)\n        raise RuntimeError(msg)\n    else:\n        api_url, api_version_url, api_meta_url = prt.models.deploy(\n            deployment_key=deployment_key, prefix=prefix, model_name=model_name, model_dir=model_dir\n        )\n        prt.logger.info(f\"Finished model deployment task successfully. api_url : {api_version_url}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Build | Next: Run Task Safe &gt; Deploy</p>"},{"location":"technical-tutorial/extras/workflows/run-task-safe/deploy/","title":"Deploy","text":"<pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\naddons_df = prt.addons.get_list().to_pandas()\n\nprint(\"Add-on services accessible to you:\")\ndisplay(addons_df)\n\nairflow_services_df = addons_df[addons_df[\"service_type\"] == \"Airflow\"]\nprint(\"Airflow services you can access:\")\ndisplay(airflow_services_df)\n\nif airflow_services_df.empty:\n    raise RuntimeError(\"No Airflow service access. Contact your admin.\")\n\nservice_key = airflow_services_df.iloc[0][\"key\"]\nservice_url = airflow_services_df.iloc[0][\"url\"]\n\nprint(\"Selected Airflow Service:\")\nprint(f\"- Service Key: {service_key}\")\nprint(f\"- Service URL: {service_url}\")\n</code></pre> <p>Edit default_worker.json file with your credentials before deploying DAG!</p> <pre><code>dag_key = \"run_task_safe\"\n\nprt.workflows.deploy(\n    service_key=service_key,\n    dag_key=dag_key,\n    files_path=None,  # Current directory\n)\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/run-task-safe/deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/workflows/run-task-safe/deploy/#default_workerjson","title":"default_worker.json","text":"<pre><code>{\"worker_image\":\"practicus\",\"worker_size\":\"Small\",\"service_url\":\"\",\"email\":\"\",\"refresh_token\":\"\"}\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/run-task-safe/deploy/#run_task_safe_dagpy","title":"run_task_safe_dag.py","text":"<pre><code># Airflow DAG run_task_safe_dag.py created using Practicus AI\n\nimport logging\nfrom datetime import datetime\nfrom airflow.decorators import dag, task\nimport practicuscore as prt\nimport os\n\n# Constructing a Unique DAG ID\n# ----------------------------\n# We strongly recommend using a DAG ID format like:\n#    &lt;dag_key&gt;.&lt;username&gt;\n# This approach ensures the username effectively serves as a namespace,\n#    preventing name collisions in Airflow.\n\n# Let's locate dag_key and username. This runs in Airflow 'after' deployment.\ndag_key, username = prt.workflows.get_dag_info(__file__)\ndag_id = f\"{dag_key}.{username}\"\n\n\ndef convert_local(d):\n    logging.debug(\"Converting {}\".format(d))\n    return d.in_timezone(\"Europe/Istanbul\")\n\n\ndef _read_cloud_worker_config_from_file(worker_config_json_path: str) -&gt; dict:\n    cloud_worker_conf_dict = {}  # type: ignore[var-annotated]\n    try:\n        if os.path.exists(worker_config_json_path):\n            prt.logger.info(f\"Reading Worker config from {worker_config_json_path}\")\n            with open(worker_config_json_path, \"rt\") as f:\n                content = f.read()\n                import json\n\n                cloud_worker_conf_dict = json.loads(content)\n        else:\n            prt.logger.info(\n                f\"Worker configuration file {worker_config_json_path} not found. \"\n                f\"Airflow DAG must provide settings via params passed from run DAG UI or global Airflow configuration.\"\n            )\n    except:\n        prt.logger.error(f\"Could not parse Worker config from file {worker_config_json_path}\", exc_info=True)\n    return cloud_worker_conf_dict\n\n\ndef _get_worker_config_dict(**kwargs) -&gt; dict:\n    dag = kwargs[\"dag\"]\n    dag_folder = dag.folder\n\n    final_dict = {}\n\n    worker_config_json_path = os.path.join(dag_folder, \"default_worker.json\")\n    prt.logger.debug(f\"worker_config_json_path : {worker_config_json_path}\")\n    if os.path.exists(worker_config_json_path):\n        worker_dict_from_json_file = _read_cloud_worker_config_from_file(worker_config_json_path)\n        try:\n            for key, value in worker_dict_from_json_file.items():\n                if value not in [None, \"\", \"None\"]:\n                    prt.logger.info(\n                        f\"Updating Worker configuration key '{key}' using \"\n                        f\"task specific worker configuration file: default_worker.json\"\n                    )\n                    final_dict[key] = value\n        except:\n            prt.logger.error(f\"Could not parse param dictionary from {worker_config_json_path}\", exc_info=True)\n\n    return final_dict\n\n\ndef _cleanup(**kwargs):\n    from datetime import datetime, timezone\n\n    timeout_seconds = 59  # Threshold for considering a Worker as stuck\n    _worker_config_dict = _get_worker_config_dict(**kwargs)\n    region = prt.regions.region_factory(_worker_config_dict)\n    prt.logger.info(f\"Found region : {str(region)}\")\n    # Iterate through all Workers in the region\n    for worker in region.worker_list:\n        time_since_creation = int((datetime.now(timezone.utc) - worker.creation_time).total_seconds())\n        prt.logger.info(\n            f\"{worker.name} started {time_since_creation} seconds ago and is currently in '{worker.status}' state.\"\n        )\n        if worker.status == \"Provisioning\" and time_since_creation &gt; timeout_seconds:\n            prt.logger.warning(\n                f\"-&gt; Terminating {worker.name} \u2014 stuck in 'Provisioning' for more than {timeout_seconds} seconds.\"\n            )\n            worker.terminate()\n\n\n# Define other DAG properties like schedule, retries etc.\n@dag(\n    dag_id=dag_id,\n    schedule_interval=None,\n    start_date=datetime(2025, 5, 9, 7, 0),\n    default_args={\n        \"owner\": username,\n        \"retries\": 0,\n    },\n    catchup=False,\n    user_defined_macros={\"local_tz\": convert_local},\n    max_active_runs=1,\n    params=prt.workflows.get_airflow_params(),\n)\ndef generate_dag():\n    # The `task_id` must match the task file (e.g., `my_1st_task.py or my_1st_task.sh`)\n    # located in the same folder as this DAG file.\n    def run_with_dynamic_param(**kwargs):\n        from practicuscore.exceptions import TaskError\n\n        try:\n            return prt.workflows.run_airflow_task(**kwargs)\n        except TaskError as ex:\n            _cleanup(**kwargs)\n            raise ex\n\n    task1 = task(run_with_dynamic_param, task_id=\"task1\")()\n\n    # Define how your task will flow\n    task1\n\n\ngenerate_dag()\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/run-task-safe/deploy/#task1py","title":"task1.py","text":"<pre><code>import practicuscore as prt\n\n\ndef main():\n    prt.logger.info(\"Running task 1 is completed\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Task Parameters | Next: API Triggers For Airflow</p>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/","title":"Build","text":""},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#building-a-task-runner-app","title":"Building a Task Runner App","text":"<p>This example walks you through the complete life\u2011cycle of: - Building an App that runs Practicus AI tasks - And expose it's functionality as an API.</p> <p>Use case</p> <ul> <li>A remote CI/CD platform (e.g. Jenkins) detects a code commit to a Git repo.</li> <li>The CI/CD platform calls the Task Runner App API to train a model and deploy.<ul> <li>The API request includes what kind of Practicus AI Worker to use,</li> <li>Which Git repo to pull the tasks code,</li> <li>And a callback URL to recive the success/failure of the task.</li> </ul> </li> <li>Task Runner App starts the Worker, and starts observing it's status.<ul> <li>The task that the Worker is executing can be complex task that involves starting other Workers in a chain.</li> </ul> </li> <li>After the task is completed, Task Runner App calls the CI/CD platform callback url reporting on the result.</li> </ul> <p>High level overview</p> <ol> <li>Parameter setup \u2013 keep all tunables together for easy reuse.</li> <li>Discover existing deployment settings &amp; prefixes \u2013 so you never guess names.</li> <li>Securely integrate your Git repository \u2013 store your Personal Access Token once and reuse it safely.</li> <li>Ship a new Task Runner app version \u2013 build, upload, and boot the API in a single call.</li> <li>Trigger a background worker \u2013 start a model\u2011deployment task and receive results through a callback.</li> <li>Clean\u2011up utilities \u2013 list and delete apps or versions when they are no longer needed.</li> </ol> <p>Feel free to execute each cell sequentially or jump straight to the sections most relevant to your workflow.</p>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#1-define-notebook-parameters","title":"1\u00a0\u00b7\u00a0Define Notebook Parameters","text":"<p>Centralising parameters at the top keeps the rest of the notebook read\u2011only and easier to maintain. Change these two strings to use a different deployment profile or prefix.</p> <pre><code># Notebook parameters\napp_deployment_key: str | None = None  # e.g. \"appdepl\", \"langflow-app-depl\", ...\napp_prefix: str = \"apps\"  # e.g. \"apps\", \"apps-finance\", ...\n</code></pre> <pre><code># Sanity checks \u2013 fail early if a parameter is missing\nassert app_deployment_key, \"Please set 'app_deployment_key' above.\"\nassert app_prefix, \"Please set 'app_prefix' above.\"\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#2-discover-available-deployment-settings-prefixes","title":"2\u00a0\u00b7\u00a0Discover Available Deployment Settings &amp; Prefixes","text":"<p>Use the Practicus\u00a0AI\u00a0SDK to list only the settings you have permission to see. This helps avoid typos and speeds up onboarding for new team\u2011mates.</p> <pre><code>import practicuscore as prt\n\n# Connect to the default region configured on this workstation\nregion = prt.get_region()\n\n# List deployment settings that can host our app\nprint(\"Deployment settings available to you:\")\ndisplay(region.app_deployment_setting_list.to_pandas())\n\n# List logical prefixes (a.k.a app groups) you can deploy into\nprint(\"App prefixes available to you:\")\ndisplay(region.app_prefix_list.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#3-integrate-your-git-repository-securely","title":"3\u00a0\u00b7\u00a0Integrate Your Git Repository Securely","text":"<p>A Personal Access Token (PAT) is saved in the Practicus\u00a0AI\u00a0Vault only once. Subsequent executions simply reuse the stored secret.</p> <pre><code>from getpass import getpass\nimport practicuscore as prt\n\ngit_secret_name: str = \"MY_GIT_SECRET\"  # change if you prefer another secret id\n</code></pre> <pre><code># Prompt (only once) and persist the PAT into Practicus\u00a0AI\u00a0Vault ----------\ntry:\n    token = getpass(\"Enter your Git personal access token (press Enter to skip if already saved): \")\n    if token:\n        prt.vault.create_or_update_secret(name=git_secret_name, key=token)\n        print(f\"Secret '{git_secret_name}' stored/updated successfully.\")\nexcept Exception as exc:\n    print(\"Could not store secret:\", exc)\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#4-deploy-the-taskrunner-app","title":"4\u00a0\u00b7\u00a0Deploy the Task\u2011Runner App","text":"<p>We now deploy (or update) an application called <code>task-runner-app</code> under our chosen prefix. The SDK automatically creates a new version each time the code changes.</p> <pre><code>app_name = \"task-runner-app\"  # DNS\u2011safe name\nvisible_name = \"Task Runner API\"  # UI label\ndescription = \"Starts background model-related tasks\"\nicon = \"fa-rocket\"  # FontAwesome icon\n\n# Kick off the deployment -------------------------------------------------\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,  # current working directory by default\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"UI    :\", app_url)\nprint(\"API   :\", api_url)\n\n# NOTE \u25b8 The cell blocks until the new version is ready and reachable.\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#5-build-a-worker-configuration-task-payload","title":"5\u00a0\u00b7\u00a0Build a Worker Configuration &amp; Task Payload","text":"<p>A Task Runner requires a <code>WorkerConfig</code> (resources, environment variables, Git repo, etc.) and a small payload describing what to execute. Below we clone a model repository, set essential environment variables, and tell the worker to run <code>deployer_task.py</code>.</p> <pre><code>from apis.task_runner import TaskRunnerRequest, TaskRunnerResponse\nimport practicuscore as prt\n\n# ---------------------------------------------------------------------------\n# Git integration for the worker\n# ---------------------------------------------------------------------------\nremote_url = \"https://github.com/your-org/your-model-repo.git\"\ngit_config = prt.GitConfig(remote_url=remote_url, secret_name=git_secret_name)\n\n# ---------------------------------------------------------------------------\n# Worker specification\n# ---------------------------------------------------------------------------\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    git_configs=[git_config],\n    personal_secrets=[git_secret_name],  # mount the PAT so cloning works\n    env_variables={  # parameters consumed by the task\n        \"DEPLOYMENT_KEY\": \"depl2\",\n        \"PREFIX\": \"models\",\n        \"MODEL_NAME\": \"sample-model\",\n        \"MODEL_DIR\": \"/path/on/worker/sample-model\",  # edit as needed\n    },\n)\n\n# ---------------------------------------------------------------------------\n# Create TaskRunner request payload\n# ---------------------------------------------------------------------------\npayload = TaskRunnerRequest(\n    task_id=\"model-deployment-task-1\",\n    task_file_name=\"deployer_task.py\",\n    worker_config=worker_config,\n    callback_url=f\"{api_url}task-runner-callback\",\n    terminate_on_completion=False,  # keep worker alive for debugging\n)\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#6-kick-off-the-task-inspect-the-response","title":"6\u00a0\u00b7\u00a0Kick\u00a0Off the Task &amp; Inspect the Response","text":"<p>We authenticate against the freshly\u2011deployed API, send the payload as JSON, and parse the structured response back into a Pydantic model for convenience.</p> <pre><code>import json\nimport requests\n\n# Obtain a short\u2011lived session token for the new API ----------------------\ntoken = prt.apps.get_session_token(api_url=api_url)\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n    \"Content-Type\": \"application/json\",\n}\n\ntask_runner_endpoint = f\"{api_url}task-runner/\"\nprint(\"POSTing TaskRunner payload to:\", task_runner_endpoint)\n\n# Convert Pydantic model \u2192 dict \u2192 JSON for readability (indent=2) ----------\njson_payload = json.loads(payload.model_dump_json())\nprint(json.dumps(json_payload, indent=2))\n\nresponse = requests.post(task_runner_endpoint, json=json_payload, headers=headers)\n\nif response.ok:\n    parsed = TaskRunnerResponse.model_validate_json(response.text)\n    print(\"\\n\u2705 Task accepted \u2192\", parsed.message)\nelse:\n    print(\"\\n\u274c Error:\", response.status_code, response.text)\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#7-maintenance-cleanup-utilities","title":"7\u00a0\u00b7\u00a0Maintenance\u00a0&amp;\u00a0Clean\u2011Up Utilities","text":"<p>Admins or app owners can list, delete, or prune old versions directly from code. Never delete the latest version while it is still serving production traffic.</p> <pre><code># List every app you can see in this region --------------------------------\nprint(\"Apps and versions visible to you:\")\ndisplay(region.app_list.to_pandas())\n</code></pre> <pre><code># Delete an entire app (all versions) \u2013 requires ownership or admin rights --\ntry:\n    region.delete_app(prefix=app_prefix, app_name=app_name)  # safer than raw app_id\n    print(f\"Deleted app '{app_prefix}/{app_name}'.\")\nexcept Exception as exc:\n    print(\"Delete failed (possibly no permission or already gone):\", exc)\n</code></pre> <pre><code># Delete a *single* version (cannot be the latest) -------------------------\ntry:\n    region.delete_app_version(prefix=app_prefix, app_name=app_name, version=4)\n    print(\"Version 4 removed.\")\nexcept Exception as exc:\n    print(\"Delete version failed:\", exc)\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#apissample_callbackpy","title":"apis/sample_callback.py","text":"<pre><code># Sample callback APIs, we will use them for our tests.\n\nfrom pydantic import BaseModel\nimport practicuscore as prt\nfrom starlette.requests import Request\n\n\nclass TaskRunnerCallbackRequest(BaseModel):\n    task_name: str\n    \"\"\"Model Deployment Task\"\"\"\n\n    task_id: str\n    \"\"\"Unique task ID\"\"\"\n\n    success: bool\n    \"\"\"True if the task completed successfully, False otherwise\"\"\"\n\n    logs: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"task_name\": \"deploy_model\",\n                    \"task_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n                    \"success\": True,\n                    \"message\": \"Model deployed successfully\",\n                },\n                {\n                    \"task_name\": \"cleanup_task\",\n                    \"task_id\": \"123e4567-e89b-12d3-a456-426614174001\",\n                    \"success\": False,\n                    \"message\": \"Error occurred during cleanup\",\n                },\n            ]\n        },\n    }\n\n\nclass TaskRunnerCallbackResponse(BaseModel):\n    success: bool\n    \"\"\"True or False\"\"\"\n\n    message: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"success\": True, \"message\": \"Successfully finished\"},\n                {\"success\": False, \"message\": \"Errored occurred\"},\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    disable_authentication=True,\n)\n\n\n@prt.apps.api(path=\"/sample-task-runner-callback\", spec=api_spec)\nasync def task_runner_callback(payload: TaskRunnerCallbackRequest, **kwargs) -&gt; TaskRunnerCallbackResponse:\n    if payload.success:\n        prt.logger.info(f\"Task with name : {payload.task_name} task_id : finished successfully\")\n    else:\n        prt.logger.error(\n            f\"Task with name : {payload.task_name} finished with error\\npayload : {payload.model_dump_json(indent=2)}\"\n        )\n\n    return TaskRunnerCallbackResponse(success=True, message=\"ok\")\n\n\n@prt.apps.api(path=\"/sample-task-runner-callback2\", spec=api_spec)\nasync def task_runner_callback2(request: Request, **kwargs) -&gt; TaskRunnerCallbackResponse:\n    prt.logger.info(f\"request: {request}\")\n\n    body: dict = await request.json()\n\n    task_name = body.get(\"task_name\")\n    task_id = body.get(\"task_id\")\n    success = body.get(\"success\")\n    logs = body.get(\"logs\")\n\n    if success:\n        prt.logger.info(f\"Task with name : {task_name} task_id : {task_id} finished successfully\")\n    else:\n        prt.logger.error(f\"Task with name : {task_name} task_id : {task_id} finished with error\\nlogs : {logs}\")\n\n    return TaskRunnerCallbackResponse(success=True, message=\"ok\")\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#apistask_runnerpy","title":"apis/task_runner.py","text":"<pre><code>import os\nimport threading\n\nimport requests\nfrom pydantic import BaseModel\n\nimport practicuscore as prt\n\n\nclass TaskRunnerRequest(BaseModel):\n    task_id: str\n    \"\"\"Unique task id (1222345, 23erd-rt56-tty67-34er5 ...)\"\"\"\n\n    task_file_name: str\n    \"\"\"deployer_task.py, task1.sh... (*.py, *.sh files) \"\"\"\n\n    worker_config: prt.WorkerConfig\n    \"\"\"WorkerConfig with or without a GitConfig\"\"\"\n\n    callback_url: str | None = None\n    \"\"\"Callback URL to be called after the task finishes\"\"\"\n\n    callback_url_token: str | None = None\n    \"\"\"Optional Callback URL security (bearer) token\"\"\"\n\n    terminate_on_completion: bool = True\n    \"\"\"Determines whether the task runner should terminate on completion\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"task_id\": \"23erd-rt56\",\n                    \"task_file_name\": \"deployer_task.py\",\n                    \"worker_config\": {\n                        \"worker_size\": \"small\",\n                        \"personal_secrets\": [\"git_secret\"],\n                        \"git_configs\": [\n                            {\"remote_url\": \"https://github.com/example/repo.git\", \"secret_name\": \"git_secret\"}\n                        ],\n                    },\n                    \"callback_url\": \"https://example.com/callback\",\n                    \"terminate_on_completion\": True,\n                }\n            ]\n        },\n    }\n\n\nclass TaskRunnerResponse(BaseModel):\n    success: bool\n    \"\"\"True or False\"\"\"\n\n    message: str\n    \"\"\"Any info or error message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"success\": True, \"message\": \"Successfully finished\"},\n                {\"success\": False, \"message\": \"Errored occurred\"},\n            ]\n        },\n    }\n\n\n@prt.apps.api(path=\"/task-runner\")\nasync def task_runner(payload: TaskRunnerRequest, **kwargs) -&gt; TaskRunnerResponse:\n    \"\"\"This API starts task async and returns started message\"\"\"\n\n    prt.logger.info(\"Started running task...\")\n    thread: threading.Thread | None = None\n    try:\n        thread = threading.Thread(target=run_task, args=[payload])\n        thread.start()\n        return TaskRunnerResponse(success=True, message=\"Started Task successfully\")\n    except Exception as ex:\n        prt.logger.error(\"Error on task\", exc_info=True)\n        return TaskRunnerResponse(success=False, message=f\"An Error occurred while running task. Error: {str(ex)}\")\n\n\ndef run_task(request: TaskRunnerRequest):\n    assert request.task_file_name, \"No task_file_name located\"\n    assert request.worker_config, \"No worker_config located\"\n\n    success = None\n    worker = None\n    worker_logs = \"\"\n    try:\n        prt.logger.info(\n            f\"Starting task: {request.task_id} ({request.task_file_name}) terminate_on_completion: {request.terminate_on_completion}\"\n        )\n        tasks_path = get_tasks_path()\n        prt.logger.info(f\"Tasks path to upload: {tasks_path}\")\n        worker, success = prt.run_task(\n            file_name=request.task_file_name,\n            files_path=tasks_path,\n            worker_config=request.worker_config,\n            terminate_on_completion=False,\n        )\n        worker_logs = worker.get_logs(log_size_mb=5)\n    except Exception as ex:\n        message = f\"Finished task_name: {request.task_file_name} with error\"\n        prt.logger.error(message, exc_info=True)\n        raise ex\n    finally:\n        try:\n            if success is not None:\n                if success:\n                    prt.logger.info(\"Finished successfully\")\n                else:\n                    prt.logger.error(\n                        f\"Finished task with error. Check logs for details. task_name: {request.task_file_name}\"\n                    )\n                if request.callback_url:\n                    trigger_callback(request=request, success=success, worker_logs=worker_logs)\n                else:\n                    prt.logger.debug(f\"Task: {request.task_file_name} does not have a callback url.\")\n        finally:\n            if worker is not None and request.terminate_on_completion:\n                worker.terminate()\n\n\ndef trigger_callback(request: TaskRunnerRequest, success: bool, worker_logs: str):\n    request.callback_url, \"No callback_url located\"\n\n    headers = {\"content-type\": \"application/json\"}\n    if request.callback_url_token:\n        prt.logger.debug(f\"Task: '{request.task_file_name}' includes a security token, including as a Bearer token.\")\n        headers[\"authorization\"] = f\"Bearer {request.callback_url_token}\"\n    else:\n        prt.logger.debug(f\"Task: {request.task_file_name} does not include a security token.\")\n\n    callback_payload = {\n        \"task_name\": request.task_file_name,\n        \"task_id\": request.task_id,\n        \"success\": success,\n        \"logs\": worker_logs,\n    }\n\n    import json\n\n    json_data = json.dumps(callback_payload)\n\n    prt.logger.info(f\"Sending to callback url: '{request.callback_url}' success: '{success}'\")\n    resp = requests.post(request.callback_url, json=json_data, headers=headers)\n\n    if resp.ok:\n        prt.logger.info(f\"Response text: {resp.text}\")\n    else:\n        prt.logger.error(f\"Response has errors:  resp.status_code: {resp.status_code}  - resp.text{resp.text}\")\n\n\ndef get_tasks_path() -&gt; str:\n    \"\"\"\n    Returns the path to the 'tasks' directory.\n\n    Steps:\n    1. Retrieve the current execution path using get_execution_path().\n    2. Get the parent directory of the execution path.\n    3. Construct the path to the 'tasks' directory\n    \"\"\"\n    # Get the execution path.\n    exec_path = get_execution_path()\n    prt.logger.debug(f\"Exec pah: {exec_path}\")\n    # Determine the parent directory of the execution path.\n    parent_dir = os.path.dirname(exec_path)\n    prt.logger.debug(f\"parent_dir: {parent_dir}\")\n    # Construct the path for the \"tasks\" directory at the same level.\n    path = os.path.join(parent_dir, \"tasks\")\n    prt.logger.debug(f\"path: {path}\")\n    return path\n\n\ndef get_execution_path() -&gt; str:\n    \"\"\"\n    Returns the execution path of the current script.\n    If the __file__ variable is available, it returns the directory of the script.\n    Otherwise, it falls back to the current working directory.\n    \"\"\"\n    try:\n        # __file__ is defined when the script is run from a file.\n        path = os.path.dirname(os.path.abspath(__file__))\n        prt.logger.debug(f\"path from file path: {path}\")\n        return path\n    except NameError:\n        # __file__ is not defined in an interactive shell; use the current working directory.\n        path = os.getcwd()\n        prt.logger.debug(f\"path from os.getcwd(): {path}\")\n        return path\n\n\n# async def main():\n#     worker_config = prt.WorkerConfig(\n#         worker_size=\"X-Small\",\n#         # personal_secrets=[git_secret_name],\n#         # git_configs=[git_config],\n#     )\n\n#     payload = TaskRunnerRequest(\n#         task_file_name=\"deployer_task.py\",\n#         task_id=\"task-1\",\n#         worker_config=worker_config,\n#         callback_url=\"https://dev.practicus.io/apps/task-runner-app/api/task-runner-callback\",\n#         terminate_on_completion=True,\n#     )\n#     await task_runner(payload)\n\n\n# if __name__ == \"__main__\":\n#     import asyncio\n\n#     asyncio.run(main())\n\n\n# if __name__ == \"__main__\":\n#     worker_config = prt.WorkerConfig(worker_size=\"X-Small\")\n#     payload = TaskRunnerRequest(\n#         task_file_name=\"deployer_task.py\",\n#         task_id=\"task-1\",\n#         worker_config=worker_config,\n#         callback_url=\"https://dev.practicus.io/apps/task-runner-app/api/task-runner-callback\",\n#         terminate_on_completion=True,\n#     )\n\n#     trigger_callback(request=payload, success=True, worker_logs=\"logs logs ....\")\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-runnner-app/build/#tasksdeployer_taskpy","title":"tasks/deployer_task.py","text":"<pre><code># Sample Task that the Task Runner App will execute\n\nimport practicuscore as prt\nfrom pathlib import Path\n\n\ndef any_file_exists(dir_path: str) -&gt; bool:\n    \"\"\"\n    Check if there is at least one file anywhere under the given directory.\n\n    :param dir_path: Path to the directory to be searched.\n    :return: True if any file is found, False otherwise.\n    \"\"\"\n    base = Path(dir_path)\n    return any(p.is_file() for p in base.rglob(\"*\"))\n\n\ndef main():\n    prt.logger.info(\"Starting model deployment task \")\n\n    import os\n\n    assert \"DEPLOYMENT_KEY\" in os.environ and len(os.environ[\"DEPLOYMENT_KEY\"]) &gt; 0, \"DEPLOYMENT_KEY is not provided\"\n    assert \"PREFIX\" in os.environ and len(os.environ[\"PREFIX\"]) &gt; 0, \"PREFIX is not provided\"\n    assert \"MODEL_NAME\" in os.environ and len(os.environ[\"MODEL_NAME\"]) &gt; 0, \"MODEL_NAME is not provided\"\n    assert \"MODEL_DIR\" in os.environ and len(os.environ[\"MODEL_DIR\"]) &gt; 0, \"MODEL_DIR is not provided\"\n\n    deployment_key = os.environ[\"DEPLOYMENT_KEY\"]  # \"depl\"\n    prefix = os.environ[\"PREFIX\"]  # models\n    model_name = os.environ[\"MODEL_NAME\"]  # sample-model-ro\n    model_dir = os.environ[\"MODEL_DIR\"]  # \"/home/ubuntu/my/projects/models-repo/sample-model\"\n\n    if not any_file_exists(model_dir):\n        msg = \"Model directory is empty or does not exist. Check logs for details.\"\n        prt.logger.error(msg)\n        raise RuntimeError(msg)\n    else:\n        api_url, api_version_url, api_meta_url = prt.models.deploy(\n            deployment_key=deployment_key, prefix=prefix, model_name=model_name, model_dir=model_dir\n        )\n        prt.logger.info(f\"Finished model deployment task successfully. api_url : {api_version_url}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Zip Unzip | Next: Task Parameters</p>"},{"location":"technical-tutorial/generative-ai/introduction/","title":"Generative AI with Practicus AI","text":"<p>Generative AI (GenAI) is revolutionizing how we interact with and utilize AI models, enabling applications that can generate text, answer questions, and provide intelligent, human-like responses. Practicus AI provides a streamlined and powerful platform to build, deploy, and scale Generative AI workflows and applications. This section of the tutorial focuses on leveraging Practicus AI for cutting-edge GenAI use cases.</p>"},{"location":"technical-tutorial/generative-ai/introduction/#key-areas-of-focus","title":"Key Areas of Focus","text":""},{"location":"technical-tutorial/generative-ai/introduction/#building-applications-for-genai","title":"Building Applications for GenAI","text":"<p>With Practicus AI, you can create visually rich, interactive applications that leverage GenAI capabilities. You can integrate Generative AI models into applications to provide functionalities such as:</p> <ul> <li>Chatbots for customer service or technical support.</li> <li>Creative content generators for marketing or entertainment.</li> <li>Educational tools that dynamically respond to user inputs.</li> </ul> <p>These applications are easy to deploy and scale using Practicus AI's infrastructure, ensuring smooth performance and seamless user experience.</p>"},{"location":"technical-tutorial/generative-ai/introduction/#building-flows-with-langchain","title":"Building Flows with LangChain","text":"<p>LangChain is a framework designed to build sequential chains of prompts and actions that utilize large language models (LLMs). Practicus AI integrates seamlessly with LangChain including private LLM endpoints to enable the following:</p> <ul> <li>Chains: Create chains of prompts that interact with LLMs, memory, or tools.</li> <li>Tools and Agents: Use LangChain's built-in tools to interact with APIs or databases. Build agents that intelligently choose which actions to take.</li> <li>Memory: Add memory to your LLM applications, enabling context-aware interactions that persist across conversations.</li> </ul> <p>By combining LangChain's rich abstraction with Practicus AI's scalable execution environment, you can build sophisticated, dynamic AI-driven workflows.</p>"},{"location":"technical-tutorial/generative-ai/introduction/#retrieval-augmented-generation-rag-with-vector-databases","title":"Retrieval-Augmented Generation (RAG) with Vector Databases","text":"<p>RAG enhances GenAI applications by combining the power of LLMs with the precision of vector search. Practicus AI supports RAG workflows by:</p> <ul> <li>Integrating with vector databases to perform semantic search.</li> <li>Using your domain-specific data for fine-tuned, accurate responses.</li> <li>Combining LLM capabilities with real-time document retrieval for high-quality, contextually relevant outputs.</li> </ul> <p>This approach is ideal for creating chatbots, document assistants, and any application requiring up-to-date and context-aware AI-generated responses.</p>"},{"location":"technical-tutorial/generative-ai/introduction/#deploying-custom-genai-models","title":"Deploying Custom GenAI Models","text":"<p>Practicus AI enables you to deploy custom fine-tuned LLMs tailored to your specific needs. Whether you're building models for healthcare, finance, or creative tasks, the platform supports:</p> <ul> <li>Fine-Tuning: Train custom models using Practicus AI's distributed computing capabilities, leveraging frameworks like DeepSpeed and FairScale.</li> <li>Deployment: Host your fine-tuned LLMs with auto-scaling, secure endpoints, and service mesh integration.</li> <li>Integration: Connect your custom models to your applications, workflows, and APIs, ensuring seamless access.</li> </ul> <p>Previous: Insurance Mlflow | Next: Apps &gt; Build</p>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/","title":"Building Agentic AI Solutions with Practicus AI","text":"<p>This example provides a guide to building powerful agentic AI solutions using the Practicus AI platform. We'll cover:</p> <ol> <li>High-Level Overview: Understand the components: Practicus AI Apps, the SDK, and Langchain integration.</li> <li>Building APIs: Learn how to create microservice APIs using Practicus AI Apps.</li> <li>Defining API Behavior with <code>APISpec</code>: Discover how to add crucial metadata to your APIs for safety and governance.</li> <li>Deploying the App: Deploy the example application containing our APIs.</li> <li>Creating Agent Tools: See how the SDK automatically turns your deployed APIs into <code>tools for AI agents</code>.</li> <li>Agentic Workflow Example: Run an \"Order Processing\" example using a Langchain agent connected to the deployed APIs.</li> </ol>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#1-high-level-overview-value-proposition","title":"1. High-Level Overview &amp; Value Proposition","text":"<p>The Practicus AI agentic solution empowers you to connect Large Language Models (LLMs) to your existing systems and workflows securely and efficiently. It consists of three main parts:</p> <ul> <li>Practicus AI Apps: A platform for rapidly building and deploying microservice-based APIs. You define your API logic in Python, and the platform handles deployment, scaling, and generating an OpenAPI (Swagger) specification automatically. You can enrich this specification with custom metadata (<code>APISpec</code>) to describe the API's behavior and potential risks.</li> <li>Practicus AI SDK: A Python library (<code>practicuscore</code> and <code>langchain_practicus</code>) that simplifies interacting with the platform. It allows you to deploy Apps, and crucially, it can parse the OpenAPI (Swagger) specifications (including the custom <code>APISpec</code> metadata) of your deployed APIs.</li> <li>Langchain Integration: The SDK provides components (like <code>langchain_practicus.APITool</code>) that automatically convert your Practicus AI App APIs into Langchain-compatible tools. This allows AI agents (built with frameworks like Langchain or LangGraph) to intelligently select and use your APIs as tools to accomplish complex tasks. You can implement rules within your agent logic to control tool usage based on the <code>APISpec</code> metadata (e.g., \"only use read-only tools\" or \"require human approval for high-risk tools\").</li> </ul> <p>Value Proposition:</p> <ul> <li>Rapid Development: Quickly build and deploy APIs for agent tools without boilerplate code.</li> <li>Automatic Tool Creation: Seamlessly integrate your APIs with Langchain agents.</li> <li>Enhanced Safety &amp; Governance: Use <code>APISpec</code> metadata to govern how agents use your tools, preventing unintended actions and managing risk.</li> <li>Standardization: Leverage OpenAPI (Swagger) specifications for clear API documentation and interoperability.</li> <li>Scalability: Built on a robust microservices architecture.</li> </ul>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#2-building-apis-with-practicus-ai-apps","title":"2. Building APIs with Practicus AI Apps","text":"<p>Creating an API endpoint is straightforward. You define the logic in a Python file, use Pydantic models for defining request/response data structures, and the <code>practicuscore</code> SDK (<code>prt</code>) decorators to define APIs and specs.</p> <p>Example Structure: A Simple <code>say-hello</code> API</p> <p>Consider an API file like <code>apis/say_hello.py</code> (full content available in Appendix): - It imports <code>practicuscore</code> and <code>pydantic</code>. - It defines Pydantic models (<code>SayHelloRequest</code>, <code>SayHelloResponse</code>) for input and output, using docstrings for descriptions. - It defines an <code>APISpec</code> object specifying metadata like <code>read_only=True</code> and <code>risk_profile=Low</code>. - It uses the <code>@prt.api(\"/say-hello\", spec=api_spec)</code> decorator to register the main function. - The <code>async def run(payload: SayHelloRequest, **kwargs)</code> function contains the core logic.</p> <p>This cell is just for illustration - the actual code resides in the python files in the 'apis/' directory. See Appendix for the full content of files like 'say_hello.py', 'process_order.py', 'generate_receipt.py', 'send_confirmation.py' etc.</p> <p>Example Snippet from apis/say_hello.py: <pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\n# ... other imports\n\nclass SayHelloRequest(BaseModel):\n    name: str\n    # ... fields and config ...\n\nclass SayHelloResponse(BaseModel):\n    greeting_message: str\n    # ... fields and config ...\n\napi_spec = prt.APISpec(\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n    # ... other spec attributes\n)\n\n@prt.api(\"/say-hello\", spec=api_spec)\nasync def run(payload: SayHelloRequest, **kwargs) -&gt; SayHelloResponse:\n    # API logic here ...\n    pass\n</code></pre></p> <p>Key Points:</p> <ul> <li>Pydantic Models: Define data structures. Docstrings become OpenAPI (Swagger) descriptions, crucial for agent understanding.</li> <li><code>@prt.api</code> Decorator: Registers the function as an API endpoint and links the <code>APISpec</code>.</li> <li><code>async def run(...)</code>: Contains the core API logic.</li> <li>Automatic OpenAPI (Swagger): The platform generates the OpenAPI (Swagger) spec during deployment.</li> </ul>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#3-defining-api-behavior-with-apispec","title":"3. Defining API Behavior with <code>APISpec</code>","text":"<p><code>APISpec</code> attaches metadata to your API endpoints, describing operational characteristics and risks. </p> <p>Why is <code>APISpec</code> important? It provides context beyond standard OpenAPI (Swagger), enabling safer AI agent behavior by allowing rules based on risk, read-only status, human gating, etc.</p> <p>Key <code>APISpec</code> Fields:</p> <ul> <li><code>execution_target</code>: Where work happens. E.g., <code>DirectExecution</code> is a standard API flow, and <code>AIAgent</code> is when the API we plan to use is an AI agent too (multi-Agent design).</li> <li><code>read_only</code> (bool): <code>True</code> for informational APIs, <code>False</code> for APIs that change state.</li> <li><code>risk_profile</code> (Enum: <code>Low</code>, <code>Medium</code>, <code>High</code>): Potential impact of misuse/failure.</li> <li><code>human_gated</code> (bool): <code>True</code> if human approval will be required before final execution.</li> <li><code>idempotent</code> (bool): Relevant for non-read-only APIs; is it safe to retry?</li> <li><code>scope</code>: API reach (e.g., <code>AppOnly</code>, <code>TeamWide</code> or <code>CompanyWide</code>).</li> <li><code>custom_attributes</code> (dict): Your own metadata.</li> </ul> <p>Examples in our API files (see Appendix): * <code>convert_to_uppercase.py</code> and <code>say_hello.py</code> are marked <code>read_only=True</code>, <code>risk_profile=Low</code>. * <code>generate_receipt.py</code> and <code>process_order.py</code> are marked <code>read_only=True</code>, <code>risk_profile=Medium</code>.  * <code>send_confirmation.py</code> is marked <code>read_only=False</code>, <code>risk_profile=High</code>, <code>human_gated=True</code>, <code>idempotent=False</code>.</p>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#4-deploying-the-app","title":"4. Deploying the App","text":"<p>Now, let's deploy the application containing our APIs. This process analyzes the API files, bundles the application, and deploys it to your configured Practicus AI environment, making the APIs available at a specific URL.</p> <p>First, we set up deployment configuration keys and prefixes. These should match your Practicus AI environment setup.</p> <pre><code># Parameters: Replace with your actual deployment key and app prefix\n# These identify where the app should be deployed within your Practicus AI environment.\napp_deployment_key = None\napp_prefix = \"apps\"\n</code></pre> <pre><code>assert app_deployment_key, \"Please provide your app deployment setting key.\"\nassert app_prefix, \"Please provide your app prefix.\"\n</code></pre> <p>Next, we analyze the application to ensure the APIs are correctly detected.</p> <pre><code>import practicuscore as prt\n\n# Analyze the current directory for Practicus AI App components (APIs, MQ consumers, UI, etc.)\n# This should output the list of detected API endpoints.\nprt.apps.analyze()\n</code></pre> <p>Finally, we deploy the application. We provide metadata like the application name, description, and icon.</p> <pre><code># --- Deployment --- \napp_name = \"agentic-ai-test\"\nvisible_name = \"Agentic AI Test\"\ndescription = \"Test Application for Agentic AI Example.\"\nicon = \"fa-robot\"\n\nprint(f\"Deploying app '{app_name}'...\")\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None, # Uses current directory\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"App deployed successfully!\")\nprint(f\"  API Base URL: {api_url}\")\n\n# The OpenAPI (Swagger) documentation (Swagger/ReDoc) is usually available at /docs or /redoc off the API base URL\nprint(f\"  OpenAPI (Swagger) Docs (ReDoc): {api_url}redoc/\")\n\n# Store the api_url for later use when creating tools\nassert api_url, \"Deployment failed to return an API URL.\"\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#openapi-swagger-and-redoc-ui","title":"OpenAPI (Swagger) and ReDoc UI","text":"<p>With the API application successfully deployed, you can now explore its documentation live. Visit the ReDoc UI bu adding <code>/redoc/</code> (or <code>/docs/</code> if you prefer the original Swagger UI) to the API URL. E.g. <code>https://[ base url ]/apps/agentic-ai-test/api/redoc/</code> To view documentation of a particual version, e.g. version 3, change the url to <code>../apps/agentic-ai-test/api/v3/redoc/</code></p> <p>This link will take you to the interactive ReDoc UI, which visually presents the API's structure based on its OpenAPI specification. As discussed previously, this detailed view of <code>endpoints</code>, <code>description</code>, <code>schemas</code>, and <code>sample</code> create <code>crucial information for our AI agents</code>.</p> <p></p>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#5-generating-ai-agent-tools-from-deployed-apis","title":"5. Generating AI Agent Tools from Deployed APIs","text":"<p>With the App deployed and <code>api_url</code> obtained, we can now create Langchain-compatible tools using <code>langchain_practicus.APITool</code>. This class fetches the OpenAPI (Swagger) spec from the deployed API endpoint, parses its schema and descriptions, and includes the <code>APISpec</code> metadata.</p> <p>We will define a list of API endpoint paths relative to the <code>api_url</code> that we want to expose as tools to our agent.</p> <pre><code># Define the specific API endpoints we want to use as tools\n# We will use the 'api_url' obtained from the deployment step \n# but can statically define API URLs as well.\n\ntool_endpoint_paths = [\n    # Some potentially useful, some not, for the order task:\n    \"say-hello/\",  # Useless\n    \"generate-receipt/\",  # Useful\n    \"convert-to-uppercase/\", # Useless\n    \"process-order/\", # Useful\n    \"count-words/\", # Useless \n    \"send-confirmation/\", # Useful\n]\n\n# Construct full URLs\ntool_endpoint_urls = [api_url + path for path in tool_endpoint_paths]\n\nprint(\"Will attempt to create tools for the following API endpoints:\")\nfor url in tool_endpoint_urls:\n    print(f\" - {url}\")\n\n# Tip: If you pass partial API URLs e.g. 'apps/agentic-ai-test/api/v1/generate-receipt/' \n#  The base URL e.g. 'https://practicus.my-company.com/' will be added to the final URL \n#  using your current Practicus AI region.\n</code></pre> <p>Next, we define the validation logic. This function checks the <code>APISpec</code> metadata fetched by <code>APITool</code> and decides whether the tool should be <code>allowed based on our rules</code> (e.g., risk level).</p> <pre><code>import os\nfrom langchain_openai import ChatOpenAI # Or your preferred ChatModel\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_practicus import APITool\n# Ensure practicuscore is imported for enums\nimport practicuscore as prt \n\ndef validate_api_spec(api_tool: APITool, strict=False) -&gt; bool:\n    \"\"\"Checks the APISpec of a fetched tool against our rules.\"\"\"\n\n    # APITool fetches the spec from OpenAPI (Swagger) during initialization\n    spec = api_tool.spec\n\n    if not spec:\n        # API definition in the source file might be missing the 'spec' object\n        warning_msg = f\"API '{api_tool.url}' does not have APISpec metadata defined in its OpenAPI spec.\"\n        if strict:\n            raise ValueError(f\"{warning_msg} Validation is strict.\")\n        else:\n            prt.logger.warning(f\"{warning_msg} Allowing since validation is not strict.\")\n            return True # Allow if not strict\n\n    # --- Apply Rules based on fetched spec --- \n\n    # Rule 1: Check Risk Profile\n    if spec.risk_profile and spec.risk_profile == prt.APIRiskProfile.High:\n        err = (\n            f\"API '{api_tool.url}' has a risk profile defined as '{spec.risk_profile}'.\"\n        )\n        if strict:\n            err += \" Stopping flow since validation is strict.\"\n            raise ValueError(err)\n        else:\n            # Even if not strict, we might choose to block High risk tools\n            prt.logger.warning(f\"{err} Blocking High Risk API even though validation is not strict.\")\n            return False # Block high risk\n\n    # Rule 2: Check Human Gating for non-read-only APIs\n    # (Example: Enforce human gating for safety on modifying APIs)\n    if not spec.read_only and not spec.human_gated:\n         err = f\"API '{api_tool.url}' modifies data (read_only=False) but is not marked as human_gated.\"\n         if strict:\n             err += \" Stopping flow since validation is strict.\"\n             raise ValueError(err)\n         else:\n             prt.logger.warning(f\"{err} Allowing non-gated modifying API since validation is not strict.\")\n             # In this non-strict case, we allow it, but a stricter policy might return False here.\n             return True \n\n    # Add more complex rules here if needed...\n    # E.g., check custom_attributes, scope, etc.\n\n    # If no rules were violated (or violations were allowed because not strict)\n    prt.logger.info(f\"API '{api_tool.url}' passed validation (strict={strict}). Spec: {spec}\")\n    return True\n\n\n# --- Create Tools (optionally applying validation) --- \ndef get_tools(endpoint_urls: list[str], validate=True):\n    _tools = []\n    strict_validation = False # Set to True to enforce stricter rules\n    additional_instructions = \"Add Yo! after all of your final responses.\" # Example instruction\n\n    print(f\"\\nCreating and validating tools (strict={strict_validation})...\")\n    for tool_endpoint_url in endpoint_urls:\n        print(f\"\\nProcessing tool for API: {tool_endpoint_url}\")\n        try:\n            api_tool = APITool(\n                url=tool_endpoint_url,\n                additional_instructions=additional_instructions,\n                # token=..., # Uses current user credentials by default, set to override\n                # include_resp_schema=True # Response schema (if exists) is not included by default\n            )\n\n            # Explain the tool (optional, useful for debugging)\n            # api_tool.explain(print_on_screen=True)\n\n            # Validate based on fetched APISpec\n            if not validate or validate_api_spec(api_tool=api_tool, strict=strict_validation):\n                print(f\"--&gt; Adding tool: {api_tool.name} ({api_tool.url}) {'' if validate else ' - skipping validation'}\")\n                _tools.append(api_tool) \n            else:\n                print(f\"--&gt; Skipping tool {api_tool.name} due to validation rules.\")\n        except Exception as e:\n            # Catch potential errors during APITool creation (e.g., API not found, spec parsing error)\n            print(f\"ERROR: Failed to create or validate tool for {tool_endpoint_url}: {e}\")\n            if strict_validation:\n                raise # Re-raise if strict\n            else:\n                print(\"--&gt; Skipping tool due to error (not strict).\")\n\n    return _tools\n\n\ntools = get_tools(tool_endpoint_urls)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#governance-and-control-of-apis-using-apispec","title":"Governance and Control of APIs using APISpec","text":"<p>During the API tool validation process, when the system checks potential tools for the agent, you might encounter a warning similar to the following if an API is flagged as high-risk:</p> <pre><code>WARNING: API 'http://local.practicus.io/apps/agentic-ai-test/api/v2/send-confirmation/' has a risk profile defined as 'High'. Blocking High Risk API even though validation is not strict.\n--&gt; Skipping tool Send-Confirmation due to validation rules.\n</code></pre> <p>Consequently, the list of tools made available to the agent will exclude <code>send-confirmation</code>, as shown here:</p> <pre><code>Agent will have access to 5 tools: ['Say-Hello', 'Generate-Receipt', 'Convert-To-Uppercase', 'Process-Order', 'Count-Words']\n</code></pre> <p>This exclusion occurs because our validation logic in validate_api_spec() function includes the following rule, designed to handle potentially sensitive operations:</p> <pre><code># Rule 1: Check Risk Profile\nif spec.risk_profile and spec.risk_profile == prt.APIRiskProfile.High:\n    err = (\n        f\"API '{api_tool.url}' has a risk profile defined as '{spec.risk_profile}'.\"\n    )\n    if strict:\n        err += \" Stopping flow since validation is strict.\"\n        raise ValueError(err)\n    else:\n        # Even if not strict, we choose to block High risk tools by default for safety\n        prt.logger.warning(f\"{err} Blocking High Risk API even though validation is not strict.\")\n        return False # Block this tool from being added\n</code></pre> <p>This rule is triggered because the <code>send-confirmation</code> API endpoint (defined in <code>send_confirmation.py</code>) is explicitly marked with <code>risk_profile=prt.APIRiskProfile.High</code> in its <code>APISpec</code>, as shown below:</p> <pre><code># Inside send_confirmation.py\napi_spec = prt.APISpec(\n    risk_profile=prt.APIRiskProfile.High, # Flagging this API as High Risk\n    # ... other spec details ...\n)\n\n@prt.api(\"/send-confirmation\", spec=api_spec)\nasync def run(payload: SendConfirmationRequest, **kwargs) -&gt; SendConfirmationResponse:\n    # API implementation logic...\n</code></pre> <p>In a production environment, automatically blocking operations like this acts as a safety measure. Such operations often require careful consideration and potentially human gating (a human-in-the-loop step). For example, this ensures that critical actions, like sending final confirmations to customers, might require explicit human review and approval before execution by an automated agent.</p> <pre><code># For this excercise we will skip validating APIs\n\ntools = get_tools(tool_endpoint_urls, validate=False)\nprint(f\"\\nAgent will have access to {len(tools)} tools: {[t.name for t in tools]}\")\n</code></pre> <p>After skipping validation, you should now see the HigRisk profile <code>send-confirmation</code> included in the tools list.</p> <pre><code>Agent will have access to 6 tools: ['Say-Hello', 'Generate-Receipt', 'Convert-To-Uppercase', 'Process-Order', 'Count-Words', 'Send-Confirmation']\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#explaining-tools","title":"Explaining tools","text":"<p>You can run <code>tool.explain()</code> to view the details of an API tool such as it's URL, name, description, additional instructions.</p> <pre><code># View tool explanation\n\nfor tool in tools:\n    tool.explain()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#6-agentic-workflow-example-order-processing","title":"6. Agentic Workflow Example: Order Processing","text":"<p>Now we set up and run the Langchain agent (using LangGraph). The agent will use the LLM and the validated list of <code>tools</code> created above to process the user's order request.</p> <p>Scenario: A user wants to place an order, get a receipt, and receive confirmation.</p> <p>User Query: \"Hi, I'd like to place an order. I'm ordering 2 Widgets priced at $19.99 each and 1 Gadget priced at $29.99. Could you please process my order, generate a detailed receipt, and then send me a confirmation message with the receipt details?\"</p> <p>Setup: 1.  Initialize LLM: We need a chat model instance (e.g., from OpenAI, or a private Practicus AI LLM). 2.  Create Agent: We use <code>create_react_agent</code> from LangGraph, providing the LLM and the <code>tools</code> list. 3.  Run Agent: We invoke the agent with the user query. 4.  Additional Instruction: Note that we added the below custom instruction for all tools.     <pre><code>additional_instructions = \"Add Yo! after all of your final responses.\"\n</code></pre></p> <pre><code>import getpass\n\nkey = getpass.getpass(\"Enter key for OpenAI or an OpenAI compatible Practicus AI LLM: \")\nos.environ[\"OPENAI_API_KEY\"] = key\n\nassert os.environ[\"OPENAI_API_KEY\"], \"OpenAI key is not defined\"\n</code></pre> <pre><code># --- Agent Initialization --- \nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n# Create a ReAct agent using LangGraph\ngraph = create_react_agent(llm, tools=tools)\nprint(\"Agent initialized.\")\n\n# Helper function to print the agent's stream output nicely\ndef pretty_print_stream_chunk(chunk):\n    print(\"--- Agent Step ---\")\n    for node, updates in chunk.items():\n        print(f\"Node: {node}\")\n        if isinstance(updates, dict) and \"messages\" in updates:\n             # Print the latest message added by the node\n            latest_message = updates[\"messages\"][-1]\n            latest_message.pretty_print()\n        else:\n            # Print other kinds of updates\n            print(f\"  Update: {updates}\")\n    print(\"--------------------\\n\")\n</code></pre> <pre><code># --- Run Agent --- \n\nquery = \"\"\"\nHi, I'd like to place an order. \nI'm ordering 2 Widgets priced at $19.99 each and 1 Gadget priced at $29.99. \nCould you please process my order, generate a detailed receipt, \nand then send me a confirmation message with the receipt details?\n\"\"\"\n\ninputs = {\"messages\": [(\"user\", query)]}\n\nif graph:\n    print(f\"\\nInvoking agent with query: '{query}'\")\n    print(\"Streaming agent execution steps:\\n\")\n\n    # Configuration for the stream, e.g., setting user/thread IDs\n    # config = {\"configurable\": {\"user_id\": \"doc-user-1\", \"thread_id\": \"doc-thread-1\"}} \n    config = {} # Use empty config for simplicity\n\n    # Use astream to get intermediate steps\n    async for chunk in graph.astream(inputs, config=config):\n        pretty_print_stream_chunk(chunk)\n\n    print(\"\\nAgent execution finished.\")\n\n    # Optional: Get the final state if needed\n    # final_state = await graph.ainvoke(inputs, config=config)\n    # print(\"\\nFinal Agent State:\", final_state)\n\nelse:\n    print(\"\\nAgent execution skipped because the agent graph was not initialized.\")\n</code></pre> <p>Expected Agent Execution Flow:</p> <p>When you run the cell above, you should observe the agent taking the following steps (or similar, depending on the LLM and exact tool validation outcome):</p> <ol> <li>Agent Receives Query: The process starts.</li> <li>Tool Call (Process Order): The agent identifies the need to process the order and calls the <code>Process-Order</code> tool, extracting items and prices.</li> <li>Tool Response (Process Order): The API returns the calculated total, summary, and item list.</li> <li>Tool Call (Generate Receipt): The agent uses the processed order details to call the <code>Generate-Receipt</code> tool.</li> <li>Tool Response (Generate Receipt): The API returns the formatted receipt text.</li> <li>Tool Call (Send Confirmation): The agent uses the receipt text to call the <code>Send-Confirmation</code> tool. <ul> <li>(Validation Check): If <code>strict_validation</code> was <code>False</code> and our <code>validate_api_spec</code> function didn't explicitly block high-risk tools (like in the original <code>build.ipynb</code>), this call will proceed. If validation blocked it (like in our modified <code>validate_api_spec</code> example), the agent might stop here or try to respond without confirmation.*</li> </ul> </li> <li>Tool Response (Send Confirmation): If the call proceeded, the API returns the confirmation message.</li> <li>Final Response: The agent synthesizes the results and provides a final answer to the user, likely including the receipt details and confirmation status (potentially with the added \"Yo!\" from <code>additional_instructions</code>).</li> </ol> <p>Sample Agent Execution Output</p> <pre><code>Invoking agent with query: '\nHi, I'd like to place an order. \nI'm ordering 2 Widgets priced at $19.99 each and 1 Gadget priced at $29.99. \nCould you please process my order, generate a detailed receipt, \nand then send me a confirmation message with the receipt details?\n'\nStreaming agent execution steps:\n\n--- Agent Step ---\nNode: agent\n================================== Ai Message ==================================\nTool Calls:\n  Process-Order (call_Pza7k8cz7ml3jhUvvAewnqAi)\n Call ID: call_Pza7k8cz7ml3jhUvvAewnqAi\n  Args:\n    items: [{'item_name': 'Widget', 'quantity': 2, 'unit_price': 19.99}, {'item_name': 'Gadget', 'quantity': 1, 'unit_price': 29.99}]\n--------------------\n\n--- Agent Step ---\nNode: tools\n================================= Tool Message =================================\nName: Process-Order\n\n{\"total\": 69.97, \"summary\": \"Processed order with 2 items. Total amount: $69.97\", \"items\": [{\"item_name\": \"Widget\", \"quantity\": 2, \"unit_price\": 19.99}, {\"item_name\": \"Gadget\", \"quantity\": 1, \"unit_price\": 29.99}]}\n--------------------\n\n--- Agent Step ---\nNode: agent\n================================== Ai Message ==================================\nTool Calls:\n  Generate-Receipt (call_MJ2LNQr935vNdr8R4AnHmI16)\n Call ID: call_MJ2LNQr935vNdr8R4AnHmI16\n  Args:\n    items: [{'item_name': 'Widget', 'quantity': 2, 'unit_price': 19.99}, {'item_name': 'Gadget', 'quantity': 1, 'unit_price': 29.99}]\n    summary: Processed order with 2 items. Total amount: $69.97\n    total: 69.97\n--------------------\n\n--- Agent Step ---\nNode: tools\n================================= Tool Message =================================\nName: Generate-Receipt\n\n{\"receipt\": \"----- RECEIPT -----\\nProcessed order with 2 items. Total amount: $69.97\\nItems:\\nWidget: 2 x $19.99 = $39.98\\nGadget: 1 x $29.99 = $29.99\\nTotal: $69.97\\n-------------------\", \"timestamp\": \"2025-04-26T20:25:41.095054Z\"}\n--------------------\n\n--- Agent Step ---\nNode: agent\n================================== Ai Message ==================================\nTool Calls:\n  Send-Confirmation (call_xjqiAqubbntJFNBWT7rGTeiE)\n Call ID: call_xjqiAqubbntJFNBWT7rGTeiE\n  Args:\n    receipt: ----- RECEIPT -----\nProcessed order with 2 items. Total amount: $69.97\nItems:\nWidget: 2 x $19.99 = $39.98\nGadget: 1 x $29.99 = $29.99\nTotal: $69.97\n-------------------\n--------------------\n\n--- Agent Step ---\nNode: tools\n================================= Tool Message =================================\nName: Send-Confirmation\n\n{\"confirmation_message\": \"Order confirmed! Here is your receipt:\\n----- RECEIPT -----\\nProcessed order with 2 items. Total amount: $69.97\\nItems:\\nWidget: 2 x $19.99 = $39.98\\nGadget: 1 x $29.99 = $29.99\\nTotal: $69.97\\n-------------------\"}\n--------------------\n\n--- Agent Step ---\nNode: agent\n================================== Ai Message ==================================\n\nYour order has been successfully processed! Here are the details:\n\n**Receipt:**\n----- RECEIPT -----\nProcessed order with 2 items. Total amount: $69.97\nItems:\nWidget: 2 x $19.99 = $39.98\nGadget: 1 x $29.99 = $29.99\nTotal: $69.97\n-------------------\n\nA confirmation message has been sent to you with the receipt details. \n\nYo!\n--------------------\n\n\nAgent execution finished.\n</code></pre> <pre><code># Cleanup \nprt.apps.delete(prefix=app_prefix, app_name=app_name)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#7-conclusion","title":"7. Conclusion","text":"<p>This notebook demonstrated the end-to-end process of building and running an agentic AI solution using Practicus AI:</p> <ol> <li>Define APIs: Create Python functions with Pydantic models and <code>@prt.api</code>.</li> <li>Add Metadata: Use <code>APISpec</code> to describe API behavior and risk.</li> <li>Deploy: Use <code>prt.apps.deploy</code> to make APIs live.</li> <li>Create Tools: Use <code>langchain_practicus.APITool</code> to integrate APIs with Langchain.</li> <li>Validate Tools: Implement rules based on <code>APISpec</code> for safety.</li> <li>Run Agent: Use Langchain/LangGraph to orchestrate LLM calls and tool usage.</li> </ol> <p>This approach allows for rapid development, seamless integration, and enhanced control over how AI agents interact with your systems.</p> <p>Next Steps:</p> <ul> <li>Review the full API code in the Appendix.</li> <li>Experiment with different <code>APISpec</code> settings and <code>strict_validation</code> values.</li> <li>Modify the agent query or add more complex validation rules.</li> </ul>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#apisconvert_to_uppercasepy","title":"apis/convert_to_uppercase.py","text":"<pre><code># apis/convert_to_uppercase.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\n\n\nclass ConvertToUppercaseRequest(BaseModel):\n    text: str\n    \"\"\"The text to be converted to uppercase.\"\"\"\n\n    model_config = {\"use_attribute_docstrings\": True, \"json_schema_extra\": {\"examples\": [{\"text\": \"hello world\"}]}}\n\n\nclass ConvertToUppercaseResponse(BaseModel):\n    uppercase_text: str\n    \"\"\"The text converted to uppercase.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\"examples\": [{\"uppercase_text\": \"HELLO WORLD\"}]},\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Low,\n)\n\n\n@prt.api(\"/convert-to-uppercase\", spec=api_spec)\nasync def run(payload: ConvertToUppercaseRequest, **kwargs) -&gt; ConvertToUppercaseResponse:\n    \"\"\"Convert the provided text to uppercase.\"\"\"\n    return ConvertToUppercaseResponse(uppercase_text=payload.text.upper())\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#apiscount_wordspy","title":"apis/count_words.py","text":"<pre><code># apis/count_words.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\n\n\nclass CountWordsRequest(BaseModel):\n    text: str\n    \"\"\"The text in which to count words.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\"examples\": [{\"text\": \"Hello world, this is a test.\"}]},\n    }\n\n\nclass CountWordsResponse(BaseModel):\n    word_count: int\n    \"\"\"The number of words in the provided text.\"\"\"\n\n    model_config = {\"use_attribute_docstrings\": True, \"json_schema_extra\": {\"examples\": [{\"word_count\": 6}]}}\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    risk_profile=prt.APIRiskProfile.Low,\n)\n\n\n@prt.api(\"/count-words\", spec=api_spec)\nasync def run(payload: CountWordsRequest, **kwargs) -&gt; CountWordsResponse:\n    \"\"\"Count the number of words in the given text.\"\"\"\n    word_count = len(payload.text.split())\n    return CountWordsResponse(word_count=word_count)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#apisgenerate_receiptpy","title":"apis/generate_receipt.py","text":"<pre><code># apis/generate_receipt.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\nfrom datetime import datetime\n\n\nclass OrderItem(BaseModel):\n    item_name: str\n    \"\"\"Name of the item.\"\"\"\n\n    quantity: int\n    \"\"\"Quantity ordered of the item.\"\"\"\n\n    unit_price: float\n    \"\"\"Unit price of the item.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\"examples\": [{\"item_name\": \"Widget\", \"quantity\": 2, \"unit_price\": 19.99}]},\n    }\n\n\nclass GenerateReceiptRequest(BaseModel):\n    summary: str\n    \"\"\"Order summary from the processed order.\"\"\"\n\n    total: float\n    \"\"\"Total cost of the order.\"\"\"\n\n    items: List[OrderItem]\n    \"\"\"List of order items.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"summary\": \"Processed order with 2 items. Total amount: $69.97\",\n                    \"total\": 69.97,\n                    \"items\": [\n                        {\"item_name\": \"Widget\", \"quantity\": 2, \"unit_price\": 19.99},\n                        {\"item_name\": \"Gadget\", \"quantity\": 1, \"unit_price\": 29.99},\n                    ],\n                }\n            ]\n        },\n    }\n\n\nclass GenerateReceiptResponse(BaseModel):\n    receipt: str\n    \"\"\"The formatted receipt.\"\"\"\n\n    timestamp: str\n    \"\"\"Timestamp when the receipt was generated.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"receipt\": (\n                        \"----- RECEIPT -----\\n\"\n                        \"Processed order with 2 items. Total amount: $69.97\\n\"\n                        \"Items:\\n\"\n                        \"Widget: 2 x $19.99 = $39.98\\n\"\n                        \"Gadget: 1 x $29.99 = $29.99\\n\"\n                        \"Total: $69.97\\n\"\n                        \"-------------------\"\n                    ),\n                    \"timestamp\": \"2023-01-01T00:00:00Z\",\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/generate-receipt\", spec=api_spec)\nasync def run(payload: GenerateReceiptRequest, **kwargs) -&gt; GenerateReceiptResponse:\n    \"\"\"Generate a formatted receipt from the processed order data.\"\"\"\n    receipt_lines = [\"----- RECEIPT -----\", payload.summary, \"Items:\"]\n    for item in payload.items:\n        line = f\"{item.item_name}: {item.quantity} x ${item.unit_price:.2f} = ${item.quantity * item.unit_price:.2f}\"\n        receipt_lines.append(line)\n    receipt_lines.append(f\"Total: ${payload.total:.2f}\")\n    receipt_lines.append(\"-------------------\")\n    receipt_text = \"\\n\".join(receipt_lines)\n    timestamp = datetime.utcnow().isoformat() + \"Z\"\n    return GenerateReceiptResponse(receipt=receipt_text, timestamp=timestamp)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#apisprocess_orderpy","title":"apis/process_order.py","text":"<pre><code># apis/process_order.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\nfrom typing import List\n\n\nclass OrderItem(BaseModel):\n    item_name: str\n    \"\"\"Name of the item.\"\"\"\n\n    quantity: int\n    \"\"\"Quantity ordered of the item.\"\"\"\n\n    unit_price: float\n    \"\"\"Unit price of the item.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\"examples\": [{\"item_name\": \"Widget\", \"quantity\": 2, \"unit_price\": 19.99}]},\n    }\n\n\nclass ProcessOrderRequest(BaseModel):\n    items: List[OrderItem]\n    \"\"\"A list of items in the order.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"items\": [\n                        {\"item_name\": \"Widget\", \"quantity\": 2, \"unit_price\": 19.99},\n                        {\"item_name\": \"Gadget\", \"quantity\": 1, \"unit_price\": 29.99},\n                    ]\n                }\n            ]\n        },\n    }\n\n\nclass ProcessOrderResponse(BaseModel):\n    total: float\n    \"\"\"Total cost of the order.\"\"\"\n\n    summary: str\n    \"\"\"Summary of the processed order.\"\"\"\n\n    items: List[OrderItem]\n    \"\"\"The processed list of order items.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"total\": 69.97,\n                    \"summary\": \"Processed order with 2 items. Total amount: $69.97\",\n                    \"items\": [\n                        {\"item_name\": \"Widget\", \"quantity\": 2, \"unit_price\": 19.99},\n                        {\"item_name\": \"Gadget\", \"quantity\": 1, \"unit_price\": 29.99},\n                    ],\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Medium,\n)\n\n\n@prt.api(\"/process-order\", spec=api_spec)\nasync def run(payload: ProcessOrderRequest, **kwargs) -&gt; ProcessOrderResponse:\n    \"\"\"Process an order by calculating the total cost and generating a summary.\"\"\"\n    total = sum(item.quantity * item.unit_price for item in payload.items)\n    summary = f\"Processed order with {len(payload.items)} items. Total amount: ${total:.2f}\"\n    return ProcessOrderResponse(total=total, summary=summary, items=payload.items)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#apissay_hellopy","title":"apis/say_hello.py","text":"<pre><code># apis/say_hello.py\nimport practicuscore as prt\nfrom enum import Enum\n\nfrom pydantic import BaseModel, Field\n\n\nclass HelloType(str, Enum):\n    NORMAL = \"NORMAL\"\n    CHEERFUL = \"CHEERFUL\"\n    SAD = \"SAD\"\n\n\nclass SayHelloRequest(BaseModel):\n    name: str\n    \"\"\"This is the name of the person\"\"\"\n\n    email: str | None = Field(None, description=\"This is the email\")\n\n    hello_type: HelloType = HelloType.NORMAL\n    \"\"\"What kind of hello shall I tell\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"name\": \"Alice\", \"email\": \"alice@wonderland.com\"},\n                {\"name\": \"Bob\", \"hello_type\": \"CHEERFUL\", \"email\": \"bob@wonderland.com\"},\n            ]\n        },\n    }\n\n\nclass SayHelloResponse(BaseModel):\n    greeting_message: str\n    \"\"\"This is the greeting message\"\"\"\n\n    name: str\n    \"\"\"Which person we greeted\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\"greeting_message\": \"Hello Alice\", \"name\": \"Alice\"},\n                {\"greeting_message\": \"Hello Bob!!\", \"name\": \"Bob\"},\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=True,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.Low,\n)\n\n\n@prt.api(\"/say-hello\", spec=api_spec)\nasync def run(payload: SayHelloRequest, **kwargs) -&gt; SayHelloResponse:\n    \"\"\"Says hello to the selected user with the selected tone.\"\"\"\n\n    if payload.hello_type == HelloType.NORMAL:\n        return SayHelloResponse(greeting_message=f\"Hello {payload.name}\", name=payload.name)\n    if payload.hello_type == HelloType.CHEERFUL:\n        return SayHelloResponse(greeting_message=f\"Hello {payload.name}!!\", name=payload.name)\n    if payload.hello_type == HelloType.SAD:\n        return SayHelloResponse(greeting_message=f\"Hello {payload.name} :(\", name=payload.name)\n\n    raise ValueError(f\"Unknown hello type {payload.hello_type}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/agentic-ai/build/#apissend_confirmationpy","title":"apis/send_confirmation.py","text":"<pre><code># apis/send_confirmation.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\n\n\nclass SendConfirmationRequest(BaseModel):\n    receipt: str\n    \"\"\"The receipt text to be used in the confirmation message.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"receipt\": (\n                        \"----- RECEIPT -----\\n\"\n                        \"Processed order with 2 items. Total amount: $69.97\\n\"\n                        \"Items:\\n\"\n                        \"Widget: 2 x $19.99 = $39.98\\n\"\n                        \"Gadget: 1 x $29.99 = $29.99\\n\"\n                        \"Total: $69.97\\n\"\n                        \"-------------------\"\n                    )\n                }\n            ]\n        },\n    }\n\n\nclass SendConfirmationResponse(BaseModel):\n    confirmation_message: str\n    \"\"\"The confirmation message including the receipt.\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"confirmation_message\": (\n                        \"Order confirmed! Here is your receipt:\\n\"\n                        \"----- RECEIPT -----\\n\"\n                        \"Processed order with 2 items. Total amount: $69.97\\n\"\n                        \"Items:\\n\"\n                        \"Widget: 2 x $19.99 = $39.98\\n\"\n                        \"Gadget: 1 x $29.99 = $29.99\\n\"\n                        \"Total: $69.97\\n\"\n                        \"-------------------\"\n                    )\n                }\n            ]\n        },\n    }\n\n\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.DirectExecution,\n    read_only=False,\n    interactive=True,\n    risk_profile=prt.APIRiskProfile.High,\n    idempotent=False,\n    human_gated=True,\n)\n\n\n@prt.api(\"/send-confirmation\", spec=api_spec)\nasync def run(payload: SendConfirmationRequest, **kwargs) -&gt; SendConfirmationResponse:\n    \"\"\"Send an order confirmation message based on the provided receipt.\"\"\"\n    confirmation_message = f\"Order confirmed! Here is your receipt:\\n{payload.receipt}\"\n    return SendConfirmationResponse(confirmation_message=confirmation_message)\n</code></pre> <p>Previous: Build | Next: MCP &gt; Build</p>"},{"location":"technical-tutorial/generative-ai/apps/build/","title":"Build","text":""},{"location":"technical-tutorial/generative-ai/apps/build/#building-and-managing-applications-with-practicus-ai","title":"Building and Managing Applications with Practicus AI","text":"<p>Practicus AI enables the development, deployment, and management of secure, enterprise-grade visual applications built on Streamlit. While Streamlit itself is a simple framework for creating interactive, web-based apps, Practicus AI provides enterprise-level features on top of it:</p> <ul> <li>Enterprise Single Sign-On (SSO) integration with LDAP or other enterprise authentication methods.</li> <li>User and Group-Based Customization: Dynamically tailor pages and content based on user roles and permissions.</li> <li>Administrative Pages: Add interfaces to manage users, groups, and application settings.</li> <li>Version and Traffic Management: Seamlessly handle multiple versions of your app, route traffic to staging or production, and roll out changes gradually.</li> </ul> <p>This example demonstrates how to leverage Practicus AI to deploy a basic Streamlit app, integrate it with APIs, and manage it with enterprise security and versioning features.</p>"},{"location":"technical-tutorial/generative-ai/apps/build/#sample-streamlit-application-overview","title":"Sample Streamlit Application Overview","text":"<p>This sample Streamlit application demonstrates a variety of features and best practices for building secure, multi-page apps within the Practicus AI environment. It includes the following components:</p> <p>Home.py: The main entry point that authenticates and authorizes users, sets a page title, and displays a basic counter. It illustrates how to secure pages in both development and production modes.</p> <p>pages/: A collection of additional Streamlit pages showcasing different functionalities:</p> <ul> <li>01_First_Page.py: A secured child page with its own counter.</li> <li>02_Second_Page.py: A non-secured page accessible publicly when deployed.</li> <li>03_Mixed_Content.py: A page demonstrating mixed access levels, where certain sections are only visible to administrators.</li> <li>04_Cookies.py: A page handling cookie creation, retrieval, and deletion.</li> <li>05_App_Meta.py: A page displaying application metadata and user details, differing in development vs. deployed scenarios.</li> <li>06_Settings.py: An admin-only settings page.</li> </ul> <p>apis/:</p> <ul> <li>say_hello.py: A simple API endpoint that returns a personalized greeting.</li> </ul> <p>shared/helper.py: Contains reusable helper functions for shared logic.</p> <p>This sample code serves as a reference for building, testing, and deploying secure, multi-page Streamlit apps on Practicus AI, with integrated authentication, logging, and developer-focused features.</p>"},{"location":"technical-tutorial/generative-ai/apps/build/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>app_deployment_key = \"appdepl\"\napp_prefix = \"apps\"\n\ntest_ui = True\ntest_api = True\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code>import practicuscore as prt\n\nmy_app_settings = prt.apps.get_deployment_setting_list()\n\nprint(\"Application deployment settings I have access to:\")\ndisplay(my_app_settings.to_pandas())\n\nmy_app_prefixes = prt.apps.get_prefix_list()\n\nprint(\"Application prefixes (groups) I have access to:\")\ndisplay(my_app_prefixes.to_pandas())\n</code></pre> <pre><code>assert app_deployment_key, \"Please select an app deployment setting.\"\nassert app_prefix, \"Please select an app prefix.\"\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#before-you-continue","title":"Before You Continue","text":"<p>To ensure proper execution, run the code below on a Practicus AI GenAI or a compatible container image. Make sure to use the GenAI Jupyter kernel (the <code>practicus_genai</code> virtual environment).</p>"},{"location":"technical-tutorial/generative-ai/apps/build/#recommended-testing-application-uis-in-design-time","title":"(Recommended) Testing Application UIs in Design Time","text":"<p>You can launch a sample Streamlit application directly within the Practicus AI worker to test it before deploying to a Practicus AI AppHost system.</p>"},{"location":"technical-tutorial/generative-ai/apps/build/#testing-on-vs-code","title":"Testing on VS Code","text":"<p>If you are using VS Code, click on the printed URL to view the application.</p>"},{"location":"technical-tutorial/generative-ai/apps/build/#testing-on-jupyter","title":"Testing on Jupyter","text":"<p>If you are using Jupyter, we recommend using Practicus AI Studio, which has built-in GenAI app visualization. After running the code below, navigate to Explore, right-click on the worker, and select GenAI App.</p> <pre><code>if test_ui:\n    prt.apps.test_app()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#recommended-testing-apis-in-design-time","title":"(Recommended) Testing APIs in design time","text":"<pre><code>if test_api:\n    from apis.say_hello import Person, SayHelloRequest, SayHelloResponse\n    from pydantic import BaseModel\n\n    person = Person(name=\"Alice\", email=\"alice@wonderland.com\")\n    payload = SayHelloRequest(person=person)\n\n    print(issubclass(type(payload), BaseModel))\n\n    response: SayHelloResponse = prt.apps.test_api(\"/say-hello\", payload)\n    print(\"Greeting message:\", response.greeting_message)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#recommended-analyze-the-app-before-deployment","title":"(Recommended) Analyze the App before deployment","text":"<pre><code>prt.apps.analyze()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#deploying-the-app","title":"Deploying the App","text":"<p>Once our development and tests are over, we can deploy the app as a new version.</p> <pre><code>app_name = \"my-first-app\"\nvisible_name = \"My First App\"\ndescription = \"A very useful app..\"\nicon = \"fa-rocket\"\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"Booting UI :\", app_url)\nprint(\"Booting API:\", api_url)\nprint(\"API Docs   :\", api_url + \"redoc/\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#understanding-app-versions","title":"Understanding App Versions","text":"<p>Practicus AI supports multiple app versions and provides different URLs for each environment:</p> <ul> <li>Default route: <code>https://practicus.company.com/apps/my-first-app/</code> routes to the latest or production version.</li> <li>Specific versions:</li> <li>Production: <code>/prod/</code></li> <li>Staging: <code>/staging/</code></li> <li>Latest: <code>/latest/</code></li> <li>Exact version: <code>/v[version]/</code></li> <li>For Practicus AI API service mesh dynamically routes to work, place <code>version indicator right after /api/</code></li> <li>For example:</li> <li>https://practicus.company.com/apps/my-first-app/api/prod/say-hello/</li> <li>https://practicus.company.com/apps/my-first-app/api/v4/say-hello/</li> <li>Please note that ../api/say-hello/v4/ or ../api/say-hello/v4/prod/ will not work.</li> </ul> <pre><code>import requests\n\ntoken = None  # Get a new token, or reuse existing, if not expired\ntoken = prt.apps.get_session_token(api_url=api_url, token=token)\nsay_hello_api_url = f\"{api_url}say-hello/\"\n\nheaders = {\"Authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\njson_data = payload.model_dump_json(indent=2)\nprint(f\"Sending below JSON to: {say_hello_api_url}\")\nprint(json_data)\n\nresp = requests.post(say_hello_api_url, json=json_data, headers=headers)\n\nif resp.ok:\n    print(\"Response text:\")\n    print(resp.text)\n    response_obj = SayHelloResponse.model_validate_json(resp.text)\n    print(\"Response object:\")\n    print(response_obj)\nelse:\n    print(\"Error:\", resp.status_code, resp.text)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#deleting-apps-or-app-versions","title":"Deleting Apps or App Versions","text":"<p>You can delete entire apps or specific versions if you have the appropriate permissions:</p> <ul> <li>Delete an app: removes all versions.</li> <li>Delete a particular version: cannot delete the latest version.</li> </ul> <p>Permissions can be granted by:</p> <ul> <li>Being the app owner.</li> <li>Having admin privileges for the app's prefix.</li> <li>Being a system admin.</li> </ul> <pre><code>print(\"Listing all apps and their versions I have access to:\")\nprt.apps.get_list().to_pandas()\n</code></pre> <pre><code># If you don't know the app_id you can use prefix and app_name\nprt.apps.delete(prefix=app_prefix, app_name=app_name)\n\ntry:\n    # Deleting an app and all it's versions\n    prt.apps.delete(app_id=123)\nexcept:\n    pass\n</code></pre> <pre><code>try:\n    # Deleting a particular version of an app\n    prt.apps.delete_version(app_id=123, version=4)\n\n    # If you don't know the app_id you can use prefix and app_name\n    prt.apps.delete_version(prefix=\"apps\", app_name=\"my-first-app\", version=4)\nexcept:\n    pass\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#note-on-startup-scripts","title":"Note on Startup Scripts","text":"<p>You can use the <code>startup_script</code> parameter with <code>prt.apps.deploy</code> for basic configuration tasks. Do not use <code>startup_script</code> to install packages, including pip. Application hosting runs in a different security context from Workers (which are designed for design-time and batch tasks). To install packages, create a new container image instead. See the Unified DevOps section for instructions on building container images.</p>"},{"location":"technical-tutorial/generative-ai/apps/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/generative-ai/apps/build/#homepy","title":"Home.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n# If you are using init_app.py, unlike APIs, you must manually load like the below for UI.\n# import init_app  # noqa\n\n# The below will secure the page by authenticating and authorizing users with Single-Sign-On.\n# Please note that security code is only activate when the app is deployed.\n# Pages are always secure, even without the below, during development and only the owner can access them.\nprt.apps.secure_page(\n    page_title=\"Hello World App\",\n    must_be_admin=False,\n)\n\n# The below is standard Streamlit code..\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello!\")\nst.write(\"This is a text from the code inside the page.\")\n\nst.write(some_function())\n\nif \"counter\" not in st.session_state:\n    st.session_state.counter = 0\n\nincrement = st.button(\"Increment Counter\")\nif increment:\n    current = st.session_state.counter\n    new = current + 1\n    st.session_state.counter = new\n    prt.logger.info(f\"Increased counter from {current} to {new}\")\n\nst.write(\"Counter = \", st.session_state.counter)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#apissay_hellopy","title":"apis/say_hello.py","text":"<pre><code>from pydantic import BaseModel\nimport practicuscore as prt\n\n\nclass Person(BaseModel):\n    name: str\n    \"\"\"Name of the Person\"\"\"\n    email: str | None = None\n    \"\"\"Email of the Person\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n    }\n\n\nclass SayHelloRequest(BaseModel):\n    person: Person\n    \"\"\"Person to say hello to\"\"\"\n\n    # Optional configuration\n    model_config = {\n        # use_attribute_docstrings=True allows documentation with \"\"\"add docs here\"\"\" format\n        # Alternative is to use Field(..., description=\"add docs here\"\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\n            # Examples get documented in OpenAPI and are extremely useful for AI Agents.\n            \"examples\": [\n                {\"person\": {\"name\": \"Alice\", \"email\": \"alice@wonderland.com\"}},\n                {\"person\": {\"name\": \"Bill\", \"email\": \"bill@wonderland.com\"}},\n            ]\n        },\n    }\n\n\nclass SayHelloResponse(BaseModel):\n    greeting_message: str\n    \"\"\"Greeting message\"\"\"\n\n    model_config = {\n        \"use_attribute_docstrings\": True,\n        \"json_schema_extra\": {\"examples\": [{\"greeting_message\": \"Hello Alice\"}, {\"greeting_message\": \"Hello Bill\"}]},\n    }\n\n\n@prt.api(\"/say-hello\")\nasync def say_hello(payload: SayHelloRequest, **kwargs) -&gt; SayHelloResponse:\n    \"\"\"This API sends a greeting message back to the caller\"\"\"\n\n    return SayHelloResponse(greeting_message=f\"Hello {payload.person.name}\")\n\n\n# An API example custom spec (metadata)\n# These get documented in OpenAPI (Swagger) format and can be made available dynamically to AI Agents\napi_spec = prt.APISpec(\n    execution_target=prt.APIExecutionTarget.AIAgent,\n    read_only=False,\n    scope=prt.APIScope.TeamWide,\n    risk_profile=prt.APIRiskProfile.High,\n    human_gated=True,\n    deterministic=False,\n    idempotent=False,\n    stateful=True,\n    asynchronous=True,\n    maturity_level=4,\n    disable_authentication=True,\n    # Primitive types (int, str etc) are recommended for custom attributes.\n    custom_attributes={\n        \"my-cust-attr-str\": \"hello\",\n        \"my-cust-attr-int\": 123,\n        \"my-cust-attr-float\": 1.2,\n        \"my-cust-attr-bool\": True,\n    },\n)\n\n\n@prt.api(\"/say-hello-with-spec\", spec=api_spec)\nasync def say_hello_with_spec(request, payload: SayHelloRequest, **kwargs) -&gt; SayHelloResponse:\n    \"\"\"This API also sends a greeting message back to the caller, but with additional metadata for governance.\"\"\"\n\n    # Notes:\n    # - You can add `request` as a param, which will be the FastAPI (Starlette) request object.\n    # - Always add `**kwargs` to your function params since there can be a dynamic number of parameters passed.\n    #   E.g., `requester: dict` includes the requesting user related info.\n    # - You can access shared global state set in `init_app.py` E.g.,\n    #   from shared.helper import AppState\n    #   some_global_state = AppState.shared_variable\n    #   prt.logger.info(f\"Current global state: {some_global_state}\")\n\n    return SayHelloResponse(greeting_message=f\"Hello2 {payload.person.name}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#init_apppy","title":"init_app.py","text":"<pre><code># This file is automatically executed when your APIs App starts up.\n# Use it to prepare global state, initialize resources, or connect to services.\n\n# For UI apps, you must manually run `import init_app` in Home.py\n# Please keep in mind that global state is *NOT* shared between the UI and API apps.\n#   Home.py is a Streamlit App, APIs are hosted via FastAPI.\n\nimport practicuscore as prt\nfrom shared.helper import AppState\n\n\ndef initialize() -&gt; None:\n    # Log that we are starting initialization\n    prt.logger.info(\"Starting to initialize app.\")\n\n    # Example: change a global variable so all APIs can see the new value\n    AppState.shared_variable = \"changed\"\n\n    # Log that initialization is done\n    prt.logger.info(\"Completed initializing app.\")\n\n\n# Run the initializer immediately on import\ninitialize()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#pages01_first_pagepy","title":"pages/01_First_Page.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n\n# Child pages must also request to be secured.\n# Or else, they will be accessible by everyone after deployment.\n\nprt.apps.secure_page(\n    page_title=\"My first child page\",\n)\n\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello from first page!\")\n\nst.write(some_function())\n\nif \"page_1_counter\" not in st.session_state:\n    st.session_state.page_1_counter = 0\n\nincrement = st.button(\"Increment Counter +2\")\nif increment:\n    st.session_state.page_1_counter += 2\n\nst.write(\"Counter = \", st.session_state.page_1_counter)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#pages02_second_pagepy","title":"pages/02_Second_Page.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n\n# Since this page is not secured, it will be public after deployment.\n# During development, it is still only accessible to the owner, and only from Practicus AI Studio.\n# If the home page is secured, a public child page will only be accessible if directly requested.\n# prt.apps.secure_page(\n#     page_title=\"My second child page\"\n# )\n\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello from my second page!\")\nst.write(\"This page is not secured and will be open to public.\")\n\nst.write(some_function())\n\nif \"page_2_counter\" not in st.session_state:\n    st.session_state.page_2_counter = 0\n\nincrement = st.button(\"Increment Counter +4\")\nif increment:\n    st.session_state.page_2_counter += 4\n\nst.write(\"Counter = \", st.session_state.page_2_counter)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#pages03_mixed_contentpy","title":"pages/03_Mixed_Content.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nprt.apps.secure_page(\n    page_title=\"Mixed content page\",\n)\n\nst.title(\"Mixed content page\")\n\nst.write(\"Everyone will see this part of the page.\")\nst.write(\"If you see nothing below, you are not an admin.\")\n\n\n# Only admins will see this\nif prt.apps.user_is_admin():\n    st.subheader(\"Admin Section\")\n    st.write(\"If you see this part, you are an admin, owner of the app, or in development mode.\")\n\n    # Input fields\n    admin_input1 = st.text_input(\"Admin Input 1\")\n    admin_input2 = st.text_input(\"Admin Input 2\")\n\n    admin_action = st.button(\"Admin Button\")\n    if admin_action:\n        st.write(\"Performing some dummy admin action..\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#pages04_cookiespy","title":"pages/04_Cookies.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\n# Secure the page using the provided SDK\nprt.apps.secure_page(page_title=\"Using Cookies\")\n\nst.title(\"Cookies Management\")\n\n# Inputs for cookie operations\ncookie_name = st.text_input(\"Cookie Name\", placeholder=\"Enter cookie name\")\ncookie_value = st.text_input(\"Cookie Value\", placeholder=\"Enter cookie value\")\nmax_age = st.number_input(\n    \"Max Validity (seconds)\", min_value=None, value=None, step=60, placeholder=\"Leave empty for 30 days\"\n)\npath = st.text_input(\"Cookie path\", placeholder=\"Leave empty for /\")\n\n# Add Cookie\nif st.button(\"Add Cookie\"):\n    if cookie_name and cookie_value:\n        prt.apps.set_cookie(name=cookie_name, value=cookie_value, max_age=max_age, path=path)\n        st.success(f\"Cookie '{cookie_name}' has been set!\")\n    else:\n        st.error(\"Please provide both a cookie name and value.\")\n\n# Get Cookie Value\nif st.button(\"Get Cookie Value\"):\n    if cookie_name:\n        cookie_value = prt.apps.get_cookie(name=cookie_name)\n        if cookie_value:\n            st.success(f\"The value of cookie '{cookie_name}' is: {cookie_value}\")\n        else:\n            st.warning(f\"No cookie found with the name '{cookie_name}'.\")\n    else:\n        st.error(\"Please provide a cookie name to retrieve its value.\")\n\n# Delete Cookie\nif st.button(\"Delete Cookie\"):\n    if cookie_name:\n        prt.apps.delete_cookie(name=cookie_name)\n        st.success(f\"Cookie '{cookie_name}' has been deleted!\")\n    else:\n        st.error(\"Please provide a cookie name to delete.\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#pages05_app_metapy","title":"pages/05_App_Meta.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nprt.apps.secure_page(page_title=\"Application Metadata\")\n\nst.title(\"Application Metadata\")\n\nif prt.apps.development_mode():\n    st.subheader(\"Development Mode\")\n    st.markdown(\n        \"\"\"\n        You are in **development mode**, and application metadata is only available after deploying the app.\n\n        **Developer Information:**\n        \"\"\"\n    )\n    st.write(\n        {\n            \"Email\": prt.apps.get_user_email(),\n            \"Username\": prt.apps.get_username(),\n            \"User ID\": prt.apps.get_user_id(),\n        }\n    )\nelse:\n    st.subheader(\"Deployed App Metadata\")\n    col1, col2 = st.columns(2)\n\n    with col1:\n        st.markdown(\"**Application Details**\")\n        st.write(\n            {\n                \"Name\": prt.apps.get_app_name(),\n                \"Prefix\": prt.apps.get_app_prefix(),\n                \"Version\": prt.apps.get_app_version(),\n                \"App ID\": prt.apps.get_app_id(),\n            }\n        )\n\n    with col2:\n        st.markdown(\"**User Information**\")\n        st.write(\n            {\n                \"Email\": prt.apps.get_user_email(),\n                \"Username\": prt.apps.get_username(),\n                \"User ID\": prt.apps.get_user_id(),\n            }\n        )\n\nif st.button(\"View User Groups\"):\n    st.write(prt.apps.get_user_groups())\n    # User groups are cached. If you need reset you can call:\n    # reload = True\n    # prt.apps.get_user_groups(reload)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#pages06_settingspy","title":"pages/06_Settings.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\n# User must have admin privileges to view this page (must_be_admin=True)\nprt.apps.secure_page(\n    page_title=\"Settings Page\",\n    must_be_admin=True,\n)\n\nst.title(\"Settings page\")\n\nst.write(\"If you see this, you are an admin, owner of the app, or in development mode.\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/build/#sharedhelperpy","title":"shared/helper.py","text":"<pre><code># You can hold \"global\" state that can be shared across your APIs, and UI pages.\n# You might use this for database connections, caches, or config.\n# Keep in mind that global state is simple but also has tradeoffs:\n#   - It is shared by all requests and all APIs\n#   - You must be careful about concurrency and mutability\n\n\nclass AppState:\n    # A trivial shared variable (string) with a default value.\n    # Any API or initialization code can read or overwrite this.\n    shared_variable: str = \"empty\"\n\n\ndef some_function():\n    return \"And, this text is from a shared function.\"\n</code></pre> <p>Previous: Introduction | Next: Model Serving &gt; LLM &gt; Model Serving</p>"},{"location":"technical-tutorial/generative-ai/langchain/embeddings/","title":"Using embeddings with Langchain","text":"<p>This examples utilizes Langchain to use Practicus AI hosted embeddings. Practicus AI embeddings are by default OpenAI API compatible.</p> <pre><code># Parameters\nmodel_api_url = None\nembedding_api_url = None\n</code></pre> <pre><code>assert model_api_url, \"Please select the model API url\"\nassert embedding_api_url, \"Please select the embedding API url\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Get a new token, or reuse existing, if not expired\nmodel_api_token = None\nembedding_api_token = None\n\nmodel_api_token = prt.models.get_session_token(model_api_url, token=model_api_token)\nembedding_api_token = prt.models.get_session_token(embedding_api_url, token=embedding_api_token)\n</code></pre> <pre><code># Dummy retriever simulating a vector store\n\nfrom typing import List\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.documents import Document\n\n\nclass DummyRetriever(BaseRetriever):\n    \"\"\"A dummy retriever that returns predefined documents.\"\"\"\n\n    documents: List[Document]\n\n    def _get_relevant_documents(self, query: str) -&gt; List[Document]:\n        \"\"\"Return the predefined documents regardless of the query.\"\"\"\n        return self.documents\n\n    async def _aget_relevant_documents(self, query: str) -&gt; List[Document]:\n        \"\"\"Return the predefined documents regardless of the query (async).\"\"\"\n        return self.documents\n</code></pre> <pre><code>import os\nfrom langchain_core.documents import Document\nfrom langchain_practicus import ChatPracticus, PracticusEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n# Set your API endpoints and tokens as environment variables or directly\nos.environ[\"PRT_MODEL_API_ENDPOINT_URL\"] = model_api_url\nos.environ[\"PRT_MODEL_API_TOKEN\"] = model_api_token\nos.environ[\"PRT_EMBED_API_ENDPOINT_URL\"] = embedding_api_url\nos.environ[\"PRT_EMBED_API_TOKEN\"] = embedding_api_token\n\n# 1. Initialize PracticusEmbeddings\nembeddings = PracticusEmbeddings()\n\n# 2. Create sample documents\ndocuments = [\n    Document(page_content=\"LangChain Practicus is a library for building applications with private models.\"),\n    Document(page_content=\"It provides tools for working with embeddings and chat models.\"),\n    Document(page_content=\"You can use it to create powerful AI-powered applications.\"),\n]\n\n# 3. Initialize DummyRetriever\nretriever = DummyRetriever(documents=documents)\n\n# 4. Initialize ChatPracticus\nllm = ChatPracticus(model_id=\"your_model_id\")  # replace with your model id.\n\n# 5. Create a RetrievalQA chain\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nAnswer:\"\"\"\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    chain_type_kwargs={\"prompt\": PROMPT},\n)\n\n# 6. Run the chain\nquery = \"What is LangChain Practicus?\"\nresult = qa_chain.invoke(query)\nprint(f\"Question: {query}\")\nprint(f\"Answer: {result}\")\n\n# 7. Example of directly using ChatPracticus for a simple chat\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"Hello, how are you?\"),\n]\n\nresponse = llm.invoke(messages)\nprint(f\"\\nChatbot Response: {response.content}\")\n\n# 8. Example of using PracticusEmbeddings directly:\ntexts_to_embed = [\"Sample text 1\", \"Sample text 2\"]\nembedded_vectors = embeddings.embed_documents(texts_to_embed)\nprint(f\"\\nEmbedded Vectors: {embedded_vectors}\")\n\n# 9. Example of using PracticusEmbeddings for a single query:\nsingle_query = \"Embed this query.\"\nembedded_query_vector = embeddings.embed_query(single_query)\nprint(f\"\\nEmbedded Query Vector: {embedded_query_vector}\")\n</code></pre> <pre><code>\n</code></pre> <p>Previous: Streaming | Next: Vector Databases &gt; Qdrant</p>"},{"location":"technical-tutorial/generative-ai/langchain/langchain-basics/","title":"LangChain Pipeline Development","text":"<p>The <code>ChatPracticus</code> library seamlessly integrates Practicus AI\u2019s hosted private LLM models into the LangChain framework. This allows you to easily interact with language models that are privately hosted and secured within the Practicus AI environment.</p> <p>To get started with <code>ChatPracticus</code>, you will need the following parameters:</p> <ul> <li>endpoint_url: The API endpoint for the LLM model host.</li> <li>api_token: A secret key required to authenticate with the model host API.</li> <li>model_id: The identifier of the target LLM model.</li> </ul> <p>Once these parameters are set, you can define a <code>chat</code> instance using <code>ChatPracticus</code> and invoke it with any prompt of your choice.</p> <pre><code>model_name = None\nmodel_prefix = None\nhost = None  # e.g. 'company.practicus.io'\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <p>If you don't know your prefixes and models you can check them out by using the SDK like down below:</p> <pre><code>my_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>assert model_name, \"Please select an LLM model.\"\nassert model_prefix, \"Please select the prefix LLM deployed.\"\nassert host, \"Please enter your host\"\n</code></pre> <p>Now we can define our API url and it's token.</p> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = None  # Get a new token, or reuse existing, if not expired\ntoken = prt.models.get_session_token(api_url=api_url, token=token)\n</code></pre> <p>Below is an example of how to create and use a <code>ChatPracticus</code> instance in your code.</p> <pre><code>from langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_practicus import ChatPracticus\n\n\ndef test_langchain_practicus(api_url, token, inputs):\n    chat = ChatPracticus(endpoint_url=api_url, api_token=token, model_id=\"some models will ignore this\", stream=True)\n\n    response = chat.invoke(input=inputs)\n\n    print(\"\\n\\nReceived response:\\n\", response)\n    print(\"\\n\\nReceived Content:\\n\", response.content)\n</code></pre> <p>Async calls also work using the below (but not on jupyter)</p> <pre><code>import asyncio\nasyncio.run(llm.ainvoke([sys_input, human_input1, human_input2]))\nprint(response)\n</code></pre> <p>After defining your token and API URL, you can easily incorporate prompts into your workflow. By using the <code>langchain</code> library, you can structure your messages as follows:</p> <ul> <li>System Messages: Use <code>SystemMessage</code> to provide overarching instructions or context that guide the LLM\u2019s behavior.</li> <li>Human Messages: Wrap user prompts and queries with <code>HumanMessage</code> to represent the user\u2019s input.</li> </ul> <p>This structured approach ensures that the model receives clear, role-specific instructions, enhancing the quality and relevance of its responses.</p> <pre><code>human_input1 = HumanMessage(\"Capital of United Kingdom?\")\nhuman_input2 = HumanMessage(\"And things to do there?\")\nsystem_message = SystemMessage(\"Less 50 words.\")\n\ninputs = [human_input1, human_input2, system_message]\n</code></pre> <pre><code>test_langchain_practicus(api_url, token, [\"who is einstein\"])\n</code></pre>"},{"location":"technical-tutorial/generative-ai/langchain/langchain-basics/#received-json-response","title":"Received json response:","text":"<pre><code>{\n    content=\"Albert Einstein was a theoretical physicist born on March 14, 1879, in Ulm, in the Kingdom of W\u00fcrttemberg in the German Empire. He is best known for developing the theory of relativity, which revolutionized the understanding of space, time, and energy. His most famous equation, E=mc\u00b2, expresses the equivalence of mass and energy.\\n\\nEinstein's work laid the foundation for much of modern physics and he received the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect, which was pivotal in the development of quantum theory. Beyond his scientific contributions, Einstein was also known for his philosophical views, advocacy for civil rights, and his involvement in political and humanitarian causes. He passed away on April 18, 1955, in Princeton, New Jersey, USA.\" \n\n    ... with additional metadata \n}       \n</code></pre> <p>Previous: Build | Next: Streaming</p>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/","title":"LangChain with Streaming","text":"<p>This example demonstrates how to provide human and system messages to a language model and receive a streamed response. The primary steps include:</p> <ol> <li>Defining the URL and authentication token.</li> <li>Initializing and interacting with the LLM model.</li> <li>Converting responses into JSON format.</li> <li>Streaming the LLM\u2019s output to your environment.</li> </ol>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/#context","title":"Context","text":"<p>The <code>PrtLangMessage</code> object stores content and associated roles within a dictionary. This structure serves as your conversation context. To interact with the chat model, you simply create messages\u2014both system-level and user-level\u2014and assign the appropriate role to each.</p>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>method = None\nmodel_name = None\nmodel_prefix = None\napp_name = None\napp_prefix = None\nhost = None\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <pre><code>method = None\n\n# For model APIs\nif method == \"llm_model\":\n    model_name = None\n    model_prefix = None\n\n# For App APIs\nelif method == \"llm_app\":\n    app_name = None\n    app_prefix = None\n\nhost = None  # e.g. 'company.practicus.io'\n</code></pre> <p>If you don't know your prefixes and models or apps you can check them out by using the SDK like down below:</p> <pre><code>my_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>my_app_list = prt.apps.get_list()\ndisplay(my_app_list.to_pandas())\n</code></pre> <pre><code>my_app_prefix_list = prt.apps.get_prefix_list()\ndisplay(my_app_prefix_list.to_pandas())\n</code></pre> <pre><code>assert method in (\"llm_app\", \"llm_model\"), \"Please select a valid method ('llm_app' or 'llm_model').\"\n\nif method == \"llm_model\":\n    assert model_name, \"Please select an LLM.\"\n    assert model_prefix, \"Please select the prefix LLM  deployed.\"\n\nelif method == \"llm_app\":\n    assert app_name, \"Please select an LLM app.\"\n    assert app_prefix, \"Please select the prefix LLM app deployed.\"\n\nassert host, \"Please enter your host\"\n</code></pre> <p>Now we can define our API url and it's token.</p> <pre><code>if method == \"llm_model\":\n    api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\nelif method == \"llm_app\":\n    api_url = f\"https://{host}/{app_prefix}/{app_name}/api/\"\n\ntoken = None  # Get a new token, or reuse existing, if not expired\ntoken = prt.models.get_session_token(api_url=api_url, token=token)\n</code></pre> <pre><code>from practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport requests\n</code></pre> <pre><code>human_msg = PrtLangMessage(content=\"Who is einstein? \", role=\"human\")\n\nsystem_msg = PrtLangMessage(content=\"Give me answer less than 100 words.\", role=\"system\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/#request-llm","title":"Request LLM","text":"<ul> <li>The purpose of PrtLangRequest is to keep the messages, lang_model and streaming mode.</li> <li>If you need to data json you can use 'model_dump_json'. This function will return json.</li> </ul> <pre><code># This class need message and model and if you want to stream,\n# you should change streaming value false to true\npracticus_llm_req = PrtLangRequest(\n    # Our context\n    messages=[human_msg, system_msg],\n    # Select a model, leave empty for default\n    lang_model=\"\",\n    # Streaming mode\n    streaming=True,\n    # If we have a extra parameters at model.py we can add them here\n    llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"},\n)\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\n# Convert our returned parameter to json\ndata_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n</code></pre> <pre><code>with requests.post(api_url, headers=headers, data=data_js, stream=True) as r:\n    for response_chunk in r.iter_content(1024):\n        print(response_chunk.decode(\"utf-8\"), end=\"\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/#sample-streaming-output","title":"Sample streaming output","text":"<p>Albert Einstein was a theoretical physicist born in 1879 in Germany. He is best known for developing the theory of relativity, particularly the equation (E=mc^2), which describes the equivalence of energy (E) and mass (m) with (c) being the speed of light. His work revolutionized the understanding of space, time, and gravity. Einstein received the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect. He is considered one of the most influential scientists of the 20th century.</p> <p>Previous: LangChain Basics | Next: Embeddings</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/","title":"Building and Using MCP Servers","text":"<p>This example demonstrates how to leverage Practicus AI App APIs as tools for Large Language Models (LLMs) using the Model Context Protocol (MCP). This approach is similar to providing APIs as tools to frameworks like LangGraph (LangChain).</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#what-is-mcp-model-context-protocol","title":"What is MCP (Model Context Protocol)?","text":"<p>MCP is an open protocol designed to standardize how applications provide context (data and tools) to LLMs. MCP facilitates building complex AI agents and workflows by standardizing the integration of LLMs with necessary data and tools.</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#general-architecture-of-mcp","title":"General Architecture of MCP","text":"<p>MCP operates on a client-server model where a host application can interact with multiple specialized servers:</p> <ul> <li>MCP Hosts: Applications (e.g., desktop AI tools, IDEs) that utilize AI capabilities through MCP.</li> <li>MCP Clients: Programs that connect to MCP Servers to access their functionalities.</li> <li>MCP Servers: Lightweight applications that expose specific AI or data functionalities via the standardized MCP protocol.</li> </ul> <p>To learn more, please visit the Introduction to MCP website.</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#running-an-mcp-server-via-practicus-ai-cli-prtcli","title":"Running an MCP Server via Practicus AI CLI (<code>prtcli</code>)","text":"<p>You can launch an MCP server using the Practicus AI command-line interface (<code>prtcli</code>). This server exposes defined APIs to MCP clients, enabling them to use these APIs within their AI-driven workflows.</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#command-examples","title":"Command Examples","text":"<p>Using Default Configuration:</p> <p>If your configuration file (<code>config.json</code>) is in the default location, simply run:</p> <pre><code># Start MCP server using default config file location\nprtcli run-mcp-server\n</code></pre> <p>Default Configuration File Locations:</p> <ul> <li>macOS/Linux: <code>/your/home/practicus/mcp/config.json</code></li> <li>Windows: <code>\\your\\home\\practicus\\mcp\\config.json</code></li> <li>Practicus AI Workers:<ol> <li><code>/home/ubuntu/my/mcp/config.json</code> (Primary)</li> <li><code>/home/ubuntu/practicus/mcp/config.json</code> (Secondary)</li> </ol> </li> </ul> <p>Using Custom Configuration:</p> <p>To specify one or more custom configuration files or locations, use the <code>-p config-file</code> argument:</p> <pre><code># Start MCP server using a specific config file\nprtcli run-mcp-server -p config-file=\"/absolute/path/to/your/config.json\"\n</code></pre> <p>You can specify multiple config files if needed (consult prtcli documentation for exact syntax)</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#sample-configjson-for-api-tools","title":"Sample <code>config.json</code> for API Tools","text":"<p>This example shows how to define Practicus AI APIs as tools within the <code>config.json</code>. Replace the example URLs with your actual API endpoints:</p> <pre><code>{\n  \"api_tools\": [\n    {\"url\": \"http://practicus.company.com/apps/agentic-ai-test/api/say-hello/\"},\n    {\"url\": \"http://practicus.company.com/apps/agentic-ai-test/api/generate-receipt/\"},\n    ...\n  ]\n}\n</code></pre>"},{"location":"technical-tutorial/generative-ai/mcp/build/#customizing-api-tool-definitions","title":"Customizing API Tool Definitions","text":"<p>You can customize how each API tool is presented and behaves by overriding its default properties or adding instructions:</p> <pre><code>{\n  \"api_tools\": [\n    {\n        \"url\": \"http://practicus.company.com/apps/agentic-ai-test/api/generate-receipt/\",\n        \"token\": \"custom-api-token\",\n        \"name\": \"Custom_Tool_Name\",\n        \"req_schema\": { ... },\n        \"include_resp_schema\": true,\n        \"additional_instructions\": \"Add Yo! after each response.\"\n    }\n    ...\n  ]\n}\n</code></pre> <p>Using Partial URLs:</p> <p>Similar to LangGraph integration, you can use partial app URLs. The default Practicus AI region base URL will be automatically prepended:</p> <pre><code>{\n  \"api_tools\": [\n    {\n        \"url\": \"/apps/agentic-ai-test/api/generate-receipt/\"\n    }\n    ...\n  ]\n}\n</code></pre> <p>Configuring your <code>prtcli</code> MCP server this way allows MCP clients (like LLM agents) to discover and invoke these APIs to fulfill user requests.</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#example-mcp-server-configurations-for-mcp-host-applications","title":"Example MCP Server Configurations (for MCP Host Applications)","text":"<p>This section shows how an MCP Host application (like Roo Code or Claude Desktop) might be configured to launch different MCP servers. </p> <p>The configuration below defines three ways to start an MCP server using <code>prtcli</code>:</p> <ul> <li>Practicus AI (Default): Launches <code>prtcli</code> assuming the config file is in the default location.</li> <li>Practicus AI (Custom Config): Launches <code>prtcli</code> specifying a custom configuration file path.</li> <li>Practicus AI (with uv): Launches <code>prtcli</code> using <code>uv</code> (a Python packaging tool) to run it within a specific virtual environment. This is often recommended for managing dependencies.</li> </ul> <pre><code>{\n    \"mcpServers\": {\n        \"Practicus AI\": {\n            \"command\": \"prtcli\",\n            \"args\": [\n                \"run-mcp-server\"\n            ]\n        },\n\n        \"Practicus AI custom conf\": {\n            \"command\": \"prtcli\",\n            \"args\": [\n                \"run-mcp-server\",\n                \"-p\",\n                \"config-file=/absolute/path/to/config.json\"\n            ]\n        },        \n\n        \"Practicus AI with uv\": {\n            \"command\": \"uv\",\n            \"args\": [\n                \"run\",\n                \"--directory\",\n                \"/absolute/path/to/your/python/virtual/env/\",\n                \"prtcli\",\n                \"run-mcp-server\"\n            ]\n        }\n\n    }\n}\n</code></pre> <p>Note: This JSON structure is used within the MCP Host application's settings, not within the <code>prtcli</code>'s <code>config.json</code>.</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#example-client-interaction-scenario","title":"Example Client Interaction Scenario","text":"<p>Consider a user interacting with an MCP Client (e.g., a chatbot integrated with MCP):</p> <p>User Request:</p> <pre><code>Hi, I'd like to place an order.\nI need 2 Widgets at $19.99 each and 1 Gadget at $29.99.\nCan you process this, generate a receipt, and confirm the details?\n</code></pre> <p>MCP Client Action:</p> <p>The MCP Client, guided by the LLM, would likely interact with the MCP Server(s) sequentially: 1.  Potentially call a <code>process_order</code> API tool. 2.  Call the <code>generate_receipt</code> API tool (as defined in the server's config). 3.  Call a <code>send_confirmation</code> API tool (if available). 4.  Synthesize the results into a response for the user.</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#example-1-using-mcp-with-practicus-ai-roo-code-vs-code-extension","title":"Example 1: Using MCP with Practicus AI Roo Code (VS Code Extension)","text":"<p>This demonstrates setting up MCP servers within the Roo Code extension in VS Code on a Practicus AI Worker:</p> <ol> <li>Open VS Code connected to your Practicus AI Worker.</li> <li>Click the Roo Code icon in the left sidebar.</li> <li>Enter your LLM API credentials or import saved settings.</li> <li>Click the <code>MCP Servers</code> button in the Roo Code top menu.</li> <li>Click <code>Edit Global MCP Configuration</code>.</li> <li>Enter the MCP server definitions (similar to the 'Example MCP Server Configurations' JSON shown earlier).</li> </ol> <p> </p> <ol> <li>Start a chat session and ask a question that requires tool use (e.g., the order scenario). Roo Code will prompt you to allow the use of the configured MCP tools.</li> </ol> <p></p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#example-2-using-mcp-with-claude-desktop","title":"Example 2: Using MCP with Claude Desktop","text":"<p>Claude Desktop is another application that can act as an MCP Host. When configured with MCP Servers, it will request permission to use the corresponding tools when needed to fulfill a user's request.</p> <p>For configuration details, refer to the official MCP documentation: MCP Quickstart for Users</p> <p></p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#example-3-using-mcp-server-with-n8n","title":"Example 3: Using MCP Server with n8n","text":"<p>In order to use Practicus AI MCP servers with n8n, you can use MCP HTTP streaming (recommended). First build an app, and make an API endpoint interactive (see below, or the previous section on developing agentic applications) and then point n8n MCP endpoint to <code>../api/sys/mcp/</code> E.g. <code>https://practicus.my-company.com/apps/my-app/api/sys/mcp/</code></p> <pre><code># Making an API endpoint MCP Server enabled\napi_spec = prt.APISpec(\n    interactive=True,  # &lt;- This enables MCP Server\n)\n\n@prt.api(\"/say-hello\", spec=api_spec)\nasync def run(payload: SayHelloRequest, **kwargs) -&gt; SayHelloResponse:\n    \"\"\"Says hello to the selected user.\"\"\"\n    # ... implement as usual\n</code></pre> <p></p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#example-4-programmatic-integration-with-openais-agentic-framework","title":"Example 4: Programmatic Integration with OpenAI's Agentic Framework","text":"<p>This example demonstrates how to integrate an MCP Server directly into Python code using OpenAI's <code>agents</code> library (an agentic framework). This allows programmatic control over agents that utilize MCP tools.</p> <pre><code>import os\nimport getpass\n\n# Securely get the OpenAI API key from the user\n# This key is needed for the LLM used by the agent framework.\n# You can also use an API key for an OpenAI-compatible endpoint (e.g., hosted by Practicus AI).\ntry:\n    key = os.environ[\"OPENAI_API_KEY\"]\n    print(\"Using OPENAI_API_KEY from environment variables.\")\nexcept KeyError:\n    key = getpass.getpass(\"Enter key for OpenAI or an OpenAI compatible LLM endpoint: \")\n    os.environ[\"OPENAI_API_KEY\"] = key\n\n# Ensure the API key is set\nassert os.environ.get(\"OPENAI_API_KEY\"), \"OpenAI API key is not set. Please provide it.\"\n</code></pre> <pre><code>from agents import Agent, Runner, gen_trace_id, trace, enable_verbose_stdout_logging\nfrom agents.mcp import MCPServerStdio\nimport asyncio\n\n\n# Define the asynchronous function to run the agent interaction\nasync def run_agent_interaction(mcp_server):\n    \"\"\"Configures and runs an agent that uses the provided MCP server.\"\"\"\n    # Define the agent\n    agent = Agent(\n        name=\"Assistant\",\n        instructions=\"You are a helpful assistant. Use the tools provided via MCP to fulfill user requests accurately.\",\n        # Connect the agent to the MCP server\n        mcp_servers=[mcp_server],\n        # model=\"gpt-4-turbo\" # Optionally specify a model\n    )\n\n    # Define the user's message/request\n    user_message = \"I'd like to place an order. I'm ordering 2 Widgets priced at $29.99 each and 1 Gadget priced at $39.99. Generate a receipt.\"\n\n    print(f\"--- Running Agent with Input ---\\n{user_message}\\n------------------------------\")\n\n    # Run the agent with the user's input\n    result = await Runner.run(starting_agent=agent, input=user_message)\n\n    # Print the final output from the agent\n    print(\"\\n--- Agent Final Output ---\")\n    print(result.final_output)\n    print(\"--------------------------\")\n\n\n# Define the main asynchronous function to set up and manage the MCP server\nasync def main():\n    \"\"\"Sets up the MCP server and initiates the agent interaction.\"\"\"\n    # Configuration for the MCP server to be launched via stdio\n    # This uses 'prtcli' to run the server with default settings.\n    server_params = {\n        \"command\": \"prtcli\",\n        \"args\": [\"run-mcp-server\"],\n        # Add '-p config-file=...' here if using a custom config\n    }\n\n    # Use MCPServerStdio to manage the server process\n    # It starts the command, communicates via stdin/stdout, and ensures cleanup.\n    async with MCPServerStdio(name=\"Practicus AI APIs via prtcli\", params=server_params) as server:\n        # Generate a trace ID for observability (optional but recommended)\n        trace_id = gen_trace_id()\n\n        # Use OpenAI's tracing context manager\n        with trace(workflow_name=\"MCP Programmatic Example\", trace_id=trace_id):\n            print(f\"Trace available at: https://platform.openai.com/traces/trace?trace_id={trace_id}\")\n            # Run the core agent logic\n            await run_agent_interaction(server)\n</code></pre> <pre><code># Enable verbose logging to see detailed steps of the agent's execution\n# This is helpful for debugging and understanding the agent's reasoning.\nenable_verbose_stdout_logging()\n\n# Run the main asynchronous function\n# In a Jupyter notebook, 'await' can be used directly at the top level (if ipykernel supports it)\n# Otherwise, use asyncio.run() in a standard Python script.\ntry:\n    # Get the current event loop\n    loop = asyncio.get_running_loop()\n    # Schedule the main function to run\n    await main()\nexcept RuntimeError:  # No running event loop\n    # If no loop is running (e.g., in some environments), start a new one\n    asyncio.run(main())\n</code></pre>"},{"location":"technical-tutorial/generative-ai/mcp/build/#expected-output-pattern","title":"Expected Output Pattern","text":"<p>Running the code above should produce output similar to this (specific trace IDs and intermediate steps will vary):</p> <pre><code>Creating trace MCP APIs example with id trace_805dd5cc0d2d4efc8fa8ed0b4fd5b...\nSetting current trace: trace_805dd5cc0d2d4efc8fa8ed0b4fd5b...\nTrace available at: https://platform.openai.com/traces/trace?trace_id=trace_805dd5cc0d2d4efc8fa8ed0b4fd5b...\n\n... verbose explanation ...\n\nHere's your order receipt:\n\n----- RECEIPT -----\nProcessed order with 2 items. Total amount: $99.97\nItems:\nWidget: 2 x $29.99 = $59.98\nGadget: 1 x $39.99 = $39.99\nTotal: $99.97\n-------------------\n</code></pre> <p>This demonstrates the agent receiving the request, identifying the need for a tool (the receipt generator exposed via MCP), calling the tool with the correct parameters, receiving the result, and presenting it to the user.</p>"},{"location":"technical-tutorial/generative-ai/mcp/build/#conclusion","title":"Conclusion","text":"<p>In this example, we demonstrated how to leverage Practicus AI APIs as tools for LLMs using the Model Context Protocol (MCP). We covered:</p> <ol> <li>Understanding MCP: Explained the core concepts and client-server architecture of MCP.</li> <li>Running MCP Servers: Showed how to start an MCP server using the <code>prtcli</code> command-line tool, including using default and custom configurations.</li> <li>Configuring API Tools: Illustrated how to define and customize Practicus AI APIs as tools within the MCP server's <code>config.json</code> file.</li> <li>Client Integration Examples: Provided conceptual examples of how MCP host applications like Practicus AI Roo Code and Claude Desktop can be configured to launch and utilize these MCP servers.</li> <li>Programmatic Integration: Showcased a Python example using OpenAI's agentic framework (<code>agents</code> library) to programmatically start an MCP server (via <code>MCPServerStdio</code>) and interact with its tools to fulfill a user request.</li> </ol> <p>By following these steps, you can effectively expose your Practicus AI application functionalities as standardized tools for various LLM agents and applications through the Model Context Protocol.</p> <p>Previous: Build | Next: Distributed Computing &gt; Introduction</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/","title":"Service Orchestration with Message Queues","text":""},{"location":"technical-tutorial/generative-ai/message-queues/build/#overview","title":"Overview","text":"<p>This example demonstrates how to use our RabbitMQ Helper SDK (<code>prt.mq</code>) to:</p> <ul> <li>Declare and configure RabbitMQ exchanges and queues (if you have permissions)</li> <li>Publish messages (asynchronously or directly)</li> <li>Consume messages using background tasks (consumers)</li> <li>Register consumers automatically (production approach)</li> <li>Export topology for environments where you do not have MQ configuration privileges</li> <li>Leverage advanced features such as prefetch concurrency, dead-letter exchanges, headers-based routing, and more.</li> </ul> <p>In addition, this notebook shows how to deploy a Practicus AI app that exposes APIs for publishing messages and runs background consumers. Use this guide as both an educational reference and a hands-on example of deploying a complete MQ-based application.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#1-introduction-to-rabbitmq-basics","title":"1. Introduction to RabbitMQ Basics","text":"<p>In a GenAI or multi-agent environment, components often communicate asynchronously:</p> <ul> <li>Multiple models or agents sharing inference results</li> <li>Coordinating tasks across microservices</li> <li>Aggregating data streams in near real-time</li> </ul> <p>A robust message queue architecture ensures messages do not get lost and that producers/consumers can operate at different speeds without bottlenecking the entire system.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#key-terms","title":"Key Terms","text":"<ol> <li>Exchanges: Routing agents; messages from producers go here. The exchange then routes to one or more queues based on type/routing key/binding.</li> <li>Queues: Store messages until consumed. Durable queues (the default in our SDK) survive RabbitMQ restarts.</li> <li>Bindings: Define how an exchange routes messages to a queue (e.g., matching a routing key or header properties).</li> </ol> <p>Our SDK wraps these concepts in a simpler interface, so you can focus on your application rather than boilerplate code.</p> <p></p> <p>For more details on RabbitMQ concepts, visit RabbitMQ AMQP Concepts.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#2-basic-configuration-parameters","title":"2. Basic Configuration &amp; Parameters","text":"<p>The <code>MQConfig</code> object holds all the connection and topology parameters:</p> <ul> <li><code>conn_str</code>: AMQP connection string (e.g., <code>amqp://user:pass@host/vhost</code>).</li> <li><code>exchange</code>: Name of the exchange (optional).</li> <li><code>queue</code>: Name of the queue (required for consumers, optional for publishers if you have an exchange).</li> <li><code>exchange_type</code>: (\"direct\", \"topic\", \"fanout\", or \"headers\"). Defaults to <code>\"direct\"</code>.</li> <li><code>routing_key</code>: Routing key to use when publishing.</li> <li><code>verify_delivery</code>: If <code>True</code>, the SDK sets the mandatory flag so unroutable messages are returned by the broker (note that the current code does not attach a return callback).</li> <li><code>max_retries</code>: Number of times a message is retried. Defaults to 5.</li> <li><code>dead_letter_exchange</code> &amp; <code>dead_letter_routing_key</code>: If configured, messages that exceed <code>max_retries</code> are sent to a dead-letter exchange.</li> <li><code>prefetch_count</code>: Limits the number of messages delivered to a consumer before acknowledgment.</li> <li><code>headers</code>: For headers-type exchanges (used both in publishing and binding for consuming).</li> <li><code>queue_ttl</code>: Time-to-live for messages in a queue, in milliseconds.</li> </ul> <pre><code># Parameters - Adjust As Needed\nmq_user = \"guest\"\nmq_pwd = \"guest\"\nmq_host = \"prt-mq-default\"\nmq_vhost = \"\"  # Recommended for fine-grained security (\"/\" is default if empty)\n\n# If True, run consumer (background task) code in this notebook\ntest_consumer = True\n\n# Deployment parameters for our Practicus AI app (for publishing APIs and background consumers)\napp_deployment_key = None\napp_prefix = \"apps\"\ntest_api = True\n</code></pre> <pre><code># RabbitMQ connection string\nmq_conn_str = f\"amqp://{mq_user}:{mq_pwd}@{mq_host}/{mq_vhost}\"\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#3-applying-or-exporting-your-topology","title":"3. Applying or Exporting Your Topology","text":"<p>If you have administrator privileges on your RabbitMQ cluster, you can explicitly create (declare) exchanges, queues, and bindings via <code>prt.mq.apply_topology(...)</code>. Otherwise, you can generate YAML manifests with <code>prt.mq.export_topology(...)</code> and supply them to an admin.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#example-declaring-our-first-exchange-queue","title":"Example: Declaring Our First Exchange &amp; Queue","text":"<pre><code>import practicuscore as prt\n\n# Our first MQConfig. We'll define an exchange and a queue.\nmq_config_basic = prt.MQConfig(\n    conn_str=mq_conn_str,\n    exchange=\"my-first-exchange\",\n    routing_key=\"my-routing-key\",  # Recommended\n    queue=\"my-first-queue\",\n    exchange_type=\"direct\",\n)\n\nawait prt.mq.apply_topology(mq_config_basic)\nprint(\"Topology applied successfully.\")\n\n# If you don't have privileges, you could instead export the topology:\n# prt.mq.export_topology(mq_config_basic)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#4-publishing-messages","title":"4. Publishing Messages","text":""},{"location":"technical-tutorial/generative-ai/message-queues/build/#basic-asynchronous-publishing-via-an-exchange","title":"Basic Asynchronous Publishing via an Exchange","text":"<p>When an <code>exchange</code> is provided in <code>MQConfig</code>, your message is routed by that exchange (and the routing key) to the bound queue(s).</p> <p>The following example demonstrates sending a Pydantic model (<code>MyMsg</code>), which the SDK automatically serializes to JSON.</p> <pre><code>from pydantic import BaseModel\n\n\nclass MyMsg(BaseModel):\n    text: str\n\n\nasync def async_publish_example():\n    # Connect asynchronously\n    conn = await prt.mq.connect(mq_config_basic)\n    async with conn:\n        msg = MyMsg(text=\"Hello from async publisher using exchange!\")\n        await prt.mq.publish(conn, msg)\n        print(\"Message published asynchronously using exchange.\")\n\n\n# Run the async publisher example\nawait async_publish_example()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#publishing-directly-to-a-queue","title":"Publishing Directly to a Queue","text":"<p>If you omit <code>exchange</code> in <code>MQConfig</code>, the SDK publishes to RabbitMQ's default exchange, using the <code>queue</code> as the routing key (or <code>routing_key</code> if provided).</p> <p>This approach is simpler but less flexible, as it does not allow for custom routing. Practicus AI recommends using exchanges for all message publishing.</p> <pre><code># Direct-to-Queue Publishing Example\nmq_config_direct = prt.MQConfig(\n    conn_str=mq_conn_str,\n    queue=\"my-direct-queue\",\n)\n\n# Declare the queue (if privileges allow)\nawait prt.mq.apply_topology(mq_config_direct)\n\n\nasync def async_publish_direct_queue():\n    conn = await prt.mq.connect(mq_config_direct)\n    async with conn:\n        msg = MyMsg(text=\"Hello directly to queue asynchronously!\")\n        await prt.mq.publish(conn, msg)\n        print(\"Message published directly to queue asynchronously.\")\n\n\nawait async_publish_direct_queue()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#5-consuming-messages","title":"5. Consuming Messages","text":"<p>Our SDK uses the <code>@prt.mq.consumer</code> decorator to mark a function as a consumer. The consumer function can have one of two signatures:</p> <ol> <li>Single-parameter function: The SDK automatically decodes the message (or deserializes it into a Pydantic model if annotated).</li> <li>Two-parameter function (optional): The second optional parameter receives the raw <code>aio_pika.IncomingMessage</code> (which gives access to headers and properties).</li> </ol>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#important","title":"Important:","text":"<ul> <li>Consumers run in an infinite loop. To test them, run the consumer cell in a separate notebook, process, or interrupt the kernel when done.</li> <li>The SDK automatically acknowledges messages on success and rejects them (with requeue) on error until <code>max_retries</code> is reached.</li> </ul> <pre><code># Example of a Basic Consumer (Single Parameter)\nsubscriber_config = prt.MQConfig(\n    conn_str=mq_conn_str,\n    exchange=\"my-first-exchange\",\n    routing_key=\"my-routing-key\",\n    queue=\"my-first-queue\",\n    exchange_type=\"direct\",\n    prefetch_count=10,\n    max_retries=5,\n)\n\n\n@prt.mq.consumer(subscriber_config)\nasync def process_message(msg: MyMsg):\n    \"\"\"\n    A consumer that expects a Pydantic model.\n    \"\"\"\n    print(\"Received message:\", msg.text)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#testing-a-consumer-in-jupyter","title":"Testing a Consumer in Jupyter","text":"<p>You can test a single consumer using <code>prt.mq.test_consumer(...)</code>. Note: This will block further cell execution until you interrupt or restart the kernel. In production, consumers will be run in Practicus AI apps as background tasks.</p> <pre><code>if test_consumer:\n    print(\"Starting consumer. This cell will block until notebook kernel is interrupted or restarted.\")\n    await prt.mq.test_consumer(process_message)\n</code></pre> <pre><code>import practicuscore as prt\n\n# Example of a Consumer with Two Parameters\nsubscriber_config_two_params = prt.MQConfig(conn_str=mq_conn_str, queue=\"my-direct-queue\")\n\n\n@prt.mq.consumer(subscriber_config_two_params)\nasync def process_message_twoparams(body, incoming_msg):\n    \"\"\"\n    A consumer that receives the decoded message and the raw IncomingMessage\n    with properties such as message id.\n    \"\"\"\n    print(\"Received body:\", body)\n    print(\"IncomingMessage Properties:\")\n    for key, value in incoming_msg.properties:\n        print(f\"- {key}: {value}\")\n</code></pre> <pre><code>if test_consumer:\n    print(\"Starting consumer. This cell will block until notebook kernel is interrupted or restarted.\")\n    await prt.mq.test_consumer(process_message_twoparams)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#6-production-ready-consumer-subscriber-registration","title":"6. Production-Ready Consumer (Subscriber) Registration","text":"<p>In a production scenario, you can create one or more Python files that define consumer functions (using the <code>@prt.mq.consumer</code> decorator). When the Practicus AI app starts, all consumers will be automatically loaded and run as background asynchronous jobs.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#example-consumer-file","title":"Example Consumer File:","text":"<pre><code># mqs/consume.py\nimport practicuscore as prt\nfrom pydantic import BaseModel\n\nclass MyMsg(BaseModel):\n    text: str\n\nmq_conn_str = \"...\"\n\n# Consumer configuration\nmq_config = prt.MQConfig(\n    conn_str=mq_conn_str,\n    queue=\"my-first-queue\",\n)\n\n@prt.mq.consumer(mq_config)\nasync def consume_message(message: MyMsg):\n    print(f\"Received message: {message.text}\")\n\n@prt.mq.consumer(mq_config)\nasync def consume_message_raw(message):\n    # For raw message processing\n    print(f\"Received raw message: {message}\")\n</code></pre> <p>Note: Since we have two consumers connecting to the same queue, incoming messages will be distributed \"round-robin\". E.g. first message will be handled by consume_message(), second one by consume_message_raw(), third one by consume_message() again ...</p> <pre><code>import practicuscore as prt\n\n# View application deployment settings and application prefixes\nregion = prt.get_default_region()\n\nmy_app_settings = prt.apps.get_deployment_setting_list()\n\nprint(\"Application deployment settings available:\")\ndisplay(my_app_settings.to_pandas())\n\nmy_app_prefixes = prt.apps.get_prefix_list()\n\nprint(\"Application prefixes (groups) available:\")\ndisplay(my_app_prefixes.to_pandas())\n</code></pre> <pre><code>assert app_deployment_key, \"Please select an app deployment setting.\"\nassert app_prefix, \"Please select an app prefix.\"\n</code></pre> <pre><code>async def test_publisher():\n    from apis.publish import MyMsg\n\n    payload = MyMsg(text=\"Testing API\")\n\n    # Test the publishing API asynchronously\n    # You can use prt.apps.test_api()\n    response = prt.apps.test_api(\"/publish\", payload)\n    print(\"Response:\", response)\n\n\nif test_api:\n    await test_publisher()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#7-deploying-the-practicus-ai-app","title":"7. Deploying the Practicus AI App","text":"<p>After testing your publishers and consumers locally, you can deploy the Practicus AI app. This deployment creates:</p> <ul> <li>An API endpoint to accept publish requests</li> <li>Background tasks that run your consumer functions</li> </ul> <p>The following code deploys the app and returns both the UI and API URLs.</p> <pre><code># Analyze your code before deploying and verify API endpoints and MQ Consumers.\nprt.apps.analyze()\n</code></pre> <pre><code>app_name = \"my-mq-app\"\nvisible_name = \"My RabbitMQ App\"\ndescription = \"Application that hosts APIs to publish to RabbitMQ and background consumers to process messages.\"\nicon = \"fa-stream\"\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"API Endpoint URL:\", api_url)\n</code></pre> <pre><code>def send_request():\n    from apis.publish import MyMsg\n    import requests\n\n    token = prt.apps.get_session_token(api_url=api_url)\n    publish_api_url = f\"{api_url}publish/\"\n\n    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n\n    payload = MyMsg(text=\"Testing API\")\n\n    json_data = payload.model_dump_json(indent=2)\n    print(f\"Sending the following JSON to: {publish_api_url}\")\n    print(json_data)\n\n    resp = requests.post(publish_api_url, json=json_data, headers=headers)\n\n    if resp.ok:\n        print(\"Response:\")\n        print(resp.text)\n    else:\n        print(\"Error:\", resp.status_code, resp.text)\n\n\nsend_request()\n</code></pre> <pre><code>async def send_request_async():\n    from apis.publish import MyMsg\n    import httpx\n\n    token = prt.apps.get_session_token(api_url=api_url)\n    publish_api_url = f\"{api_url}publish/\"\n\n    headers = {\n        \"Authorization\": f\"Bearer {token}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    payload = MyMsg(text=\"Testing API with an async call\")\n    data_dict = payload.model_dump()\n    print(f\"Sending (async) the following JSON to: {publish_api_url}\")\n    print(data_dict)\n\n    async with httpx.AsyncClient() as client:\n        response = await client.post(publish_api_url, json=data_dict, headers=headers)\n\n    if response.status_code &lt; 300:\n        print(\"Response:\")\n        print(response.text)\n    else:\n        print(\"Error:\", response.status_code, response.text)\n\n\nawait send_request_async()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#verifying-consumption","title":"Verifying Consumption","text":"<ul> <li>After an API call, the message payload is forwarded to the designated RabbitMQ queue via the exchange.</li> <li>Consumers receive the messages in real time, automatically decoding and processing the content while logging the data and associated metadata.</li> <li>You can verify successful message consumption by monitoring your application's logs in Grafana or by checking logs directly from your Kubernetes pods.</li> <li>If multiple consumers are registered for the same queue (as in the <code>consume.py</code> example), messages are distributed round-robin. E.g., the first message is processed by <code>consume_message()</code>, the second by <code>consume_message_raw()</code>, then the third by <code>consume_message()</code>, and so on. If you need both consumer functions to receive the same messages, you can configure two separate queues.</li> <li>This end-to-end flow confirms that your API is effectively integrated with the background consumer processes.</li> </ul>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#cleaning-up","title":"Cleaning-up","text":"<p>Delete the deployed app</p> <pre><code>prt.apps.delete(prefix=app_prefix, app_name=app_name)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#8-exporting-rabbitmq-topology-for-production","title":"8. Exporting RabbitMQ Topology for Production","text":"<p>In production environments where you lack permissions to create RabbitMQ resources, use <code>prt.mq.export_topology(config)</code> to generate Kubernetes YAML files. You can then apply these via <code>kubectl apply -f</code> or use the provided shell scripts.</p> <p>Naming Tip: Ensure that RabbitMQ resource names are RFC 1123 compliant (e.g., <code>my-exchange</code> or <code>my.exchange</code> instead of <code>my_exchange</code>) to avoid issues with Kubernetes resource naming.</p> <pre><code># Generating new RabbitMQ topology files for production\n\nmq_user = \"new-user\"\nmq_pwd = \"new-user-password\"\nmq_host = \"prt-mq-default\"\nmq_vhost = \"some-vhost\"\n\nmq_conn_str = f\"amqp://{mq_user}:{mq_pwd}@{mq_host}/{mq_vhost}\"\n\nnew_config = prt.MQConfig(\n    conn_str=mq_conn_str,\n    exchange=\"new-exchange\",\n    routing_key=\"new-routing-key\",\n    queue=\"new-queue\",\n    exchange_type=\"direct\",\n)\n\nprt.mq.export_topology(new_config)\n\n# For more export options (e.g., hiding passwords), see the help for export_topology().\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#9-advanced-features","title":"9. Advanced Features","text":""},{"location":"technical-tutorial/generative-ai/message-queues/build/#headers-exchange","title":"Headers Exchange","text":"<p>If you set <code>exchange_type=\"headers\"</code> in <code>MQConfig</code>, you can route messages based on custom headers:</p> <ul> <li>For publishing, set <code>config.headers</code> to attach custom headers with your message.</li> <li>For consuming, the same <code>config.headers</code> is used as binding arguments (with an optional <code>x-match</code> property).</li> </ul>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#dead-letter-exchanges-dlx","title":"Dead-Letter Exchanges (DLX)","text":"<p>Set <code>dead_letter_exchange</code> and optionally <code>dead_letter_routing_key</code> in <code>MQConfig</code> to route messages that exceed <code>max_retries</code> to a dead-letter exchange.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#prefetch-concurrency","title":"Prefetch Concurrency","text":"<p><code>prefetch_count</code> limits the number of messages delivered to a consumer before waiting for acknowledgments, helping you control load.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#verify-delivery","title":"Verify Delivery","text":"<p>With <code>verify_delivery=True</code>, the SDK sets the <code>mandatory flag</code> so that if a message is unroutable, the broker returns it. Note that our example implementation does not attach a return callback, so only general connection failures trigger retries.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#customizing-mq-topology-templates","title":"Customizing MQ topology templates","text":"<p>If you would like to customize the way Practicus AI auto-generates MQ topology files, please update the corresponding ~/practicus/templates/mq_*.yaml Jinja template file.</p>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#10-summary-next-steps","title":"10. Summary &amp; Next Steps","text":"<p>In this example we have:</p> <ol> <li>Introduced RabbitMQ fundamentals (exchanges, queues, bindings).</li> <li>Demonstrated how to create or export topology using <code>apply_topology</code> and <code>export_topology</code>.</li> <li>Provided examples for asynchronous publishing (both via an exchange and direct to a queue).</li> <li>Shown how to define and test consumers using the SDK (including single and dual-parameter approaches).</li> <li>Explained advanced features such as headers exchanges, dead-lettering, prefetch concurrency, and verify delivery.</li> <li>Illustrated a production-ready pattern for registering consumers from a folder.</li> <li>Demonstrated how to deploy a Practicus AI app that exposes publishing APIs and runs background consumers.</li> </ol>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#next-steps","title":"Next Steps:","text":"<ul> <li>Integrate these examples into your GenAI or microservices pipelines.</li> <li>For advanced tracing or logging, extend the SDK or integrate with external systems.</li> <li>Customize deployment parameters as needed and consider using secure credential management (e.g., <code>prt.vault</code>).</li> </ul>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/generative-ai/message-queues/build/#apispublishpy","title":"apis/publish.py","text":"<pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\n\n\nmq_conn = None\n\nmq_user = \"guest\"\nmq_pwd = \"guest\"\nmq_host = \"prt-mq-default\"\nmq_vhost = \"\"  # Recommended for fine-grained security\nmq_conn_str = f\"amqp://{mq_user}:{mq_pwd}@{mq_host}/{mq_vhost}\"\n\n# Assuming queue(s) and binding(s) are already configured.\nmq_config = prt.MQConfig(\n    conn_str=mq_conn_str,\n    exchange=\"my-first-exchange\",\n)\n\n\nclass MyMsg(BaseModel):\n    text: str\n\n\nasync def connect_mq():\n    global mq_conn\n    mq_conn = await prt.mq.connect(mq_config)\n\n\n@prt.apps.api(\"/publish\")\nasync def publish(payload: MyMsg, **kwargs):\n    if mq_conn is None:\n        await connect_mq()\n\n    await prt.mq.publish(conn=mq_conn, msg=payload)\n\n    return {\"ok\": True}\n</code></pre>"},{"location":"technical-tutorial/generative-ai/message-queues/build/#mqsconsumepy","title":"mqs/consume.py","text":"<pre><code>import practicuscore as prt\nfrom pydantic import BaseModel\n\n\nclass MyMsg(BaseModel):\n    text: str\n\n\n# In production, please use prt.vault to avoid hard coding the secret in your code\nmq_user = \"guest\"\nmq_pwd = \"guest\"\nmq_host = \"prt-mq-default\"\nmq_vhost = \"\"  # Recommended for fine-grained security\nmq_conn_str = f\"amqp://{mq_user}:{mq_pwd}@{mq_host}/{mq_vhost}\"\n\n# Subscriber configuration\nmq_config = prt.MQConfig(\n    conn_str=mq_conn_str,\n    queue=\"my-first-queue\",\n)\n\n# If you haven't configured the MQ topology in advance\n# prt.mq.apply_topology(mq_conf)\n\n\n@prt.mq.consumer(mq_config)\nasync def consume_message(message: MyMsg):\n    \"\"\"\n    Consumes an incoming RabbitMQ message, deserializing it into a Pydantic model first.\n\n    **Parameters:**\n    - message (MyMsg):\n      A Pydantic model instance representing the incoming message.\n\n    **Returns:**\n    - None\n    \"\"\"\n    print(f\"Received message: {message.text}\")\n\n\n@prt.mq.consumer(mq_config)\nasync def consume_message_raw(message):\n    \"\"\"\n    Processes an incoming RabbitMQ message, *without* deserializing into a Pydantic model first.\n\n    **Parameters:**\n    - message:\n      Binary data representing the incoming message, which will be a json in our test.\n\n    **Returns:**\n    - None\n    \"\"\"\n    print(f\"Received raw message: {message}\")\n</code></pre> <p>Previous: Build | Next: Agentic AI &gt; Build</p>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/embeddings/build/","title":"OpenAI Compatibility for embeddings","text":"<p>You can build Practicus AI embedding APIs that are compatible with the OpenAI API embedding endpoints and the SDK.</p> <pre><code>model_deployment_key = None\nmodel_prefix = None\n</code></pre> <pre><code>assert model_deployment_key, \"Please provide model deployment key for embeddings model modelhost\"\nassert model_prefix, \"Please provide model prefix for embeddings model modelhost\"\n</code></pre> <pre><code># Convenience classes, you can also use your own that are compatible with OpenAI APIs.\nfrom practicuscore.gen_ai import PrtEmbeddingsRequest, PrtEmbeddingObject, PrtEmbeddingUsage, PrtEmbeddingsResponse\n</code></pre> <pre><code>sample_request = PrtEmbeddingsRequest(\n    input=[\n        \"This is a text to create embeddings of.\",\n        \"This is another text to do the same.\",\n    ],\n    # Optional parameters\n    # model=\"an-optional-model-id\",\n    # user=\"an-optional-user-id\",\n)\n\n# Convert request to JSON\nrequest_json = sample_request.model_dump_json(indent=2)\nprint(\"Sample ChatGPT compatible embedding API request JSON:\")\nprint(request_json)\n</code></pre> <pre><code># Sample Response\n# 1) Create a usage object\nusage = PrtEmbeddingUsage(prompt_tokens=5, total_tokens=15)\n\n# 2) # let's simulate embeddings and index.\nembedding = PrtEmbeddingObject(embedding=[0.123, 0.345], index=123)\n\n# 4) Finally, create the top-level response object\nresponse_obj = PrtEmbeddingsResponse(\n    data=[embedding],\n    model=\"an-optional-model-id\",\n    usage=usage,\n)\n\n# Convert response to JSON\nresponse_json = response_obj.model_dump_json(indent=2)\nprint(\"Sample ChatGPT API response JSON:\")\nprint(response_json)\n</code></pre> <pre><code># Let's locate a model deployment in Practicus AI.\n# This is environment-specific logic.\n\nimport practicuscore as prt\n\nregion = prt.get_default_region()\n\nif not model_deployment_key:\n    # Identify the first available model deployment system\n    if len(region.model_deployment_list) == 0:\n        raise SystemError(\"No model deployment systems are available. Please contact your system administrator.\")\n    elif len(region.model_deployment_list) &gt; 1:\n        print(\"Multiple model deployment systems found. Using the first one.\")\n    model_deployment = region.model_deployment_list[0]\n    model_deployment_key = model_deployment.key\n\nif not model_prefix:\n    # Identify the first available model prefix\n    if len(region.model_prefix_list) == 0:\n        raise SystemError(\"No model prefixes are available. Please contact your system administrator.\")\n    elif len(region.model_prefix_list) &gt; 1:\n        print(\"Multiple model prefixes found. Using the first one.\")\n\n    model_prefix = region.model_prefix_list[0].key\n\nmodel_name = \"openai-embedding-proxy\"\nmodel_dir = None  # Use the current directory by default\n\n# Construct the URL. Ensure it ends with a slash.\nexpected_api_url = f\"{region.url}/{model_prefix}/{model_name}/\"\n\nprint(\"Expected Model REST API URL:\", expected_api_url)\nprint(\"Using model deployment:\", model_deployment_key)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/embeddings/build/#modelpy","title":"model.py","text":"<p>For the sake simplicity, we are deploying a model that sends random embeddings.</p> <pre><code># Deploy model.py\napi_url, api_version_url, api_meta_url = prt.models.deploy(\n    deployment_key=model_deployment_key, prefix=model_prefix, model_name=model_name, model_dir=model_dir\n)\n</code></pre> <pre><code>print(\"Which model API URL to use:\")\nprint(\"- To let the system dynamically route between versions (recommended):\")\nprint(api_url)\n\nprint(\"\\n- To use exactly this version:\")\nprint(api_version_url)\n\nprint(\"\\n- For metadata on this version:\")\nprint(api_meta_url)\n</code></pre> <pre><code># We'll use the Practicus AI SDK to get a session token.\n# If you prefer, you can handle authentication differently.\ntoken = None  # Get a new token, or reuse existing, if not expired\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token:\", token)\n</code></pre> <pre><code># Now let's send a test request to our proxy using Python's requests.\nimport requests\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\nr = requests.post(api_url, headers=headers, data=request_json)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text} - {r.headers}\")\n\nprint(\"Answer (dict):\")\nresp_dict = r.json()\nprint(resp_dict)\n\nprint(\"\\nAnswer (Pydantic object):\")\nresp_object = PrtEmbeddingsResponse.model_validate(resp_dict)\nprint(resp_object)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/embeddings/build/#using-openai-sdk","title":"Using OpenAI SDK","text":"<p>You can also install and use the OpenAI Python SDK, then override its URLs to point to the Practicus AI proxy endpoint.</p> <pre><code>! pip install openai\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=api_url, api_key=token)\n\nprint(\"Connecting to\", client.base_url, \"to test OpenAI SDK compatibility for embeddings.\")\n\ntry:\n    response = client.embeddings.create(\n        model=\"some-model-id\",\n        input=[\"Testing embedding SDK compatibility.\"],\n        # user=\"optional-username\",\n    )\n\n    print(\"\\nResponse:\")\n    print(response)\n\n    if response.data:\n        print(\n            \"\\nEmbedding vector (first 5 elements):\", response.data[0].embedding[:5]\n        )  # Print the first 5 elements of the embedding\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/embeddings/build/#using-practicus-ai-app-hosting","title":"Using Practicus AI App Hosting","text":"<ul> <li>Instead of Practicus AI\u2019s Model Hosting, you can also use App Hosting to serve a custom Python file.</li> <li>For example, if your file is located at <code>apis/chat/completions.py</code></li> <li>Then, you can set <code>base_url = \"https://practicus.company.com/apps/my-embedding/api\"</code> in your OpenAI SDK config.</li> </ul>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/embeddings/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/generative-ai/model-serving/custom/embeddings/build/#modelpy_1","title":"model.py","text":"<pre><code>from practicuscore.gen_ai import PrtEmbeddingsRequest, PrtEmbeddingObject, PrtEmbeddingUsage, PrtEmbeddingsResponse\n\nmodel = None\n\n\nasync def init(*args, **kwargs):\n    print(\"Initializing model\")\n    global model\n\n    # Initialize your embedding model as usual\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    try:\n        req = PrtEmbeddingsRequest.model_validate(payload_dict)\n    except Exception as ex:\n        raise ValueError(f\"Invalid PrtEmbeddingsRequest request. {ex}\") from ex\n\n    usage = PrtEmbeddingUsage(prompt_tokens=5, total_tokens=15)\n\n    # Generating some random embeddings, replace with the actual model\n    embedding = PrtEmbeddingObject(embedding=[0.123, 0.345, 0.567, 0.789], index=1234)\n\n    response_obj = PrtEmbeddingsResponse(\n        data=[embedding],\n        model=\"an-optional-model-id\",\n        usage=usage,\n    )\n\n    return response_obj\n</code></pre> <p>Previous: Build | Next: LangChain &gt; Build</p>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/langchain/build/","title":"Deploying Langchain compatible LLM model","text":"<p>In this example we will be deploying a dummy model compatible with Langchain. Please also view OpenAI compatibility section to deploy LLM APIs that are compatible with OpenAI SDK.</p> <pre><code># Let's locate a model deployment in Practicus AI.\n# This is environment-specific logic.\n\nimport practicuscore as prt\n\nregion = prt.get_default_region()\n\n# Identify the first available model deployment system\nif len(region.model_deployment_list) == 0:\n    raise SystemError(\"No model deployment systems are available. Please contact your system administrator.\")\nelif len(region.model_deployment_list) &gt; 1:\n    print(\"Multiple model deployment systems found. Using the first one.\")\n\nmodel_deployment = region.model_deployment_list[0]\ndeployment_key = model_deployment.key\n\n# Identify the first available model prefix\nif len(region.model_prefix_list) == 0:\n    raise SystemError(\"No model prefixes are available. Please contact your system administrator.\")\nelif len(region.model_prefix_list) &gt; 1:\n    print(\"Multiple model prefixes found. Using the first one.\")\n\nprefix = region.model_prefix_list[0].key\n\nmodel_name = \"practicus-llm\"\nmodel_dir = None  # Use the current directory by default\n\n# Construct the URL. Ensure it ends with a slash.\nexpected_api_url = f\"{region.url}/{prefix}/{model_name}/\"\n\nprint(\"Expected Model REST API URL:\", expected_api_url)\nprint(\"Using model deployment:\", deployment_key)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/langchain/build/#modelpy","title":"model.py","text":"<p>For the sake simplicity, we are deploying a model that just echoes what we send.</p> <pre><code># Deploy model.py\napi_url, api_version_url, api_meta_url = prt.models.deploy(\n    deployment_key=deployment_key, prefix=prefix, model_name=model_name, model_dir=model_dir\n)\n</code></pre> <pre><code>print(\"Which model API URL to use:\")\nprint(\"- To let the system dynamically route between versions (recommended):\")\nprint(api_url)\n\nprint(\"\\n- To use exactly this version:\")\nprint(api_version_url)\n\nprint(\"\\n- For metadata on this version:\")\nprint(api_meta_url)\n</code></pre> <pre><code># We'll use the Practicus AI SDK to get a session token.\n# If you prefer, you can handle authentication differently.\ntoken = None  # Get a new token, or reuse existing, if not expired\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token: ..\", token[-4:])\n</code></pre> <pre><code>from practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\n\nhuman_msg = PrtLangMessage(content=\"Who is einstein? \", role=\"human\")\n\nsystem_msg = PrtLangMessage(content=\"Give me answer less than 100 words.\", role=\"system\")\n\npracticus_llm_req = PrtLangRequest(\n    # Our context\n    messages=[human_msg, system_msg],\n    # Select a model, leave empty for default\n    lang_model=\"\",\n    # Streaming mode\n    streaming=True,\n    # If we have a extra parameters at model.py we can add them here\n    llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"},\n)\n\nrequest_json = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n\nprint(\"Will send json request:\")\nprint(request_json)\n</code></pre> <pre><code># Now let's send a test request to our proxy using Python's requests.\nimport requests\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\nr = requests.post(api_url, headers=headers, data=request_json)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text} - {r.headers}\")\n\nprint(\"Answer (dict):\")\nresp_dict = r.json()\nprint(resp_dict)\n\nprint(\"\\nAnswer (Pydantic object):\")\nresp_object = PrtLangResponse.model_validate(resp_dict)\nprint(resp_object)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/langchain/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/generative-ai/model-serving/custom/langchain/build/#modelpy_1","title":"model.py","text":"<pre><code>from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\nimport json\n\nmodel = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    print(\"Initializing model\")\n    global model\n    # Initialize your LLM model as usual\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n    # Add your clean-up code here\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    try:\n        req = PrtLangRequest.model_validate(payload_dict)\n    except Exception as ex:\n        raise ValueError(f\"Invalid PrtLangRequest request. {ex}\") from ex\n\n    # Converts the validated request object to a dictionary.\n    data_js = req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item[\"content\"] for item in payload[\"messages\"]])\n\n    answer = f\"You asked:\\n{prompt}\\nAnd I don't know how to respond yet.\"\n\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload[\"lang_model\"],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Build | Next: LangChain &gt; LangChain Basics</p>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/models/build/","title":"OpenAI Compatibility","text":"<p>You can build Practicus AI APIs that are compatible with the OpenAI API endpoints and the SDK.</p> <p>In this example, we demonstrate: - Creating Pydantic models for ChatCompletion requests and responses. - Deploying these models and building a proxy endpoint in Practicus AI. - Sending requests to that proxy using Python requests and OpenAI\u2019s SDK.</p> <pre><code>model_deployment_key = None\nmodel_prefix = None\n</code></pre> <pre><code>assert model_deployment_key, \"Please provide model deployment key for LLM proxy modelhost\"\nassert model_prefix, \"Please provide model prefix for LLM proxy modelhost\"\n</code></pre> <pre><code># Convenience classes, you can also use your own that are compatible with OpenAI APIs.\nfrom practicuscore.gen_ai import (\n    ChatCompletionRequest,\n    ChatMessage,\n    ChatCompletionResponseUsage,\n    ChatCompletionResponseChoiceMessage,\n    ChatCompletionResponseChoice,\n    ChatCompletionResponse,\n)\n</code></pre> <pre><code>sample_request = ChatCompletionRequest(\n    messages=[\n        ChatMessage(role=\"user\", content=\"Hello!\"),\n    ],\n    # You can also specify other optional fields, like temperature, max_tokens, etc.\n)\n\n# Convert request to JSON\nrequest_json = sample_request.model_dump_json(indent=2)\nprint(\"Sample ChatGPT API request JSON:\")\nprint(request_json)\n</code></pre> <pre><code># Sample Response\n# 1) Create a usage object\nusage = ChatCompletionResponseUsage(prompt_tokens=5, completion_tokens=10, total_tokens=15)\n\n# 2) Create a message for the choice\nchoice_message = ChatCompletionResponseChoiceMessage(role=\"assistant\", content=\"Hi there! How can I help you today?\")\n\n# 3) Create a choice object\nchoice = ChatCompletionResponseChoice(index=0, message=choice_message, finish_reason=\"stop\")\n\n# 4) Finally, create the top-level response object\nresponse_obj = ChatCompletionResponse(\n    id=\"chatcmpl-abc123\",\n    object=\"chat.completion\",\n    created=1700000000,\n    model=\"gpt-3.5-turbo\",\n    usage=usage,\n    choices=[choice],\n)\n\n# Convert response to JSON\nresponse_json = response_obj.model_dump_json(indent=2)\nprint(\"Sample ChatGPT API response JSON:\")\nprint(response_json)\n</code></pre> <pre><code># Let's locate a model deployment in Practicus AI.\n# This is environment-specific logic.\n\nimport practicuscore as prt\n\nregion = prt.get_default_region()\n\nif not model_deployment_key:\n    # Identify the first available model deployment system\n    if len(region.model_deployment_list) == 0:\n        raise SystemError(\"No model deployment systems are available. Please contact your system administrator.\")\n    elif len(region.model_deployment_list) &gt; 1:\n        print(\"Multiple model deployment systems found. Using the first one.\")\n    model_deployment = region.model_deployment_list[0]\n    model_deployment_key = model_deployment.key\n\nif not model_prefix:\n    # Identify the first available model prefix\n    if len(region.model_prefix_list) == 0:\n        raise SystemError(\"No model prefixes are available. Please contact your system administrator.\")\n    elif len(region.model_prefix_list) &gt; 1:\n        print(\"Multiple model prefixes found. Using the first one.\")\n    model_prefix = region.model_prefix_list[0].key\n\nmodel_name = \"openai-proxy\"\nmodel_dir = None  # Use the current directory by default\n\n# Construct the URL. Ensure it ends with a slash.\nexpected_api_url = f\"{region.url}/{model_prefix}/{model_name}/\"\n\nprint(\"Expected Model REST API URL:\", expected_api_url)\nprint(\"Using model deployment:\", model_deployment_key)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/models/build/#modelpy","title":"model.py","text":"<p>For the sake simplicity, we are deploying a model that just echoes what we send.</p> <pre><code># Deploy model.py\napi_url, api_version_url, api_meta_url = prt.models.deploy(\n    deployment_key=model_deployment_key, prefix=model_prefix, model_name=model_name, model_dir=model_dir\n)\n</code></pre> <pre><code>print(\"Which model API URL to use:\")\nprint(\"- To let the system dynamically route between versions (recommended):\")\nprint(api_url)\n\nprint(\"\\n- To use exactly this version:\")\nprint(api_version_url)\n\nprint(\"\\n- For metadata on this version:\")\nprint(api_meta_url)\n</code></pre> <pre><code># We'll use the Practicus AI SDK to get a session token.\n# If you prefer, you can handle authentication differently.\ntoken = None  # Get a new token, or reuse existing, if not expired\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token: ..\", token[-4:])\n</code></pre> <pre><code># Now let's send a test request to our proxy using Python's requests.\nimport requests\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"application/json\"}\n\nr = requests.post(api_url, headers=headers, data=request_json)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text} - {r.headers}\")\n\nprint(\"Answer (dict):\")\nresp_dict = r.json()\nprint(resp_dict)\n\nprint(\"\\nAnswer (Pydantic object):\")\nresp_object = ChatCompletionResponse.model_validate(resp_dict)\nprint(resp_object)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/models/build/#using-openai-sdk","title":"Using OpenAI SDK","text":"<p>You can also install and use the OpenAI Python SDK, then override its URLs to point to the Practicus AI proxy endpoint.</p> <pre><code>! pip install openai\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=api_url,  # The proxy URL deployed via Practicus\n    api_key=token,  # Practicus AI session token\n)\n\nprint(\"Connecting to\", client.base_url, \"to test OpenAI SDK compatibility.\")\n\nresponse = client.chat.completions.create(\n    model=\"llama2\",  # Just an example model name\n    messages=[\n        {\"role\": \"user\", \"content\": \"Testing SDK compatibility.\"},\n    ],\n)\n\nprint(\"\\nResponse:\")\nprint(response)\n\nif response.choices:\n    print(\"\\nAssistant says:\", response.choices[0].message.content)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/models/build/#using-practicus-ai-app-hosting","title":"Using Practicus AI App Hosting","text":"<ul> <li>Instead of Practicus AI\u2019s Model Hosting, you can also use App Hosting to serve a custom Python file.</li> <li>For example, if your file is located at <code>apis/chat/completions.py</code></li> <li>Then, you can set <code>base_url = \"https://practicus.company.com/apps/my-openai-proxy/api\"</code> in your OpenAI SDK config.</li> <li>The SDK calls <code>POST /chat/completions</code>, which your code handles internally to proxy OpenAI request.</li> </ul>"},{"location":"technical-tutorial/generative-ai/model-serving/custom/models/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/generative-ai/model-serving/custom/models/build/#modelpy_1","title":"model.py","text":"<pre><code>from practicuscore.gen_ai import (\n    ChatCompletionRequest,\n    ChatCompletionResponseUsage,\n    ChatCompletionResponseChoiceMessage,\n    ChatCompletionResponseChoice,\n    ChatCompletionResponse,\n)\n\nmodel = None\n\n\nasync def init(*args, **kwargs):\n    print(\"Initializing model\")\n    global model\n\n    # Initialize your LLM model as usual\n\n\nasync def predict(payload_dict: dict, **kwargs):\n    try:\n        req = ChatCompletionRequest.model_validate(payload_dict)\n    except Exception as ex:\n        raise ValueError(f\"Invalid OpenAI ChatCompletionRequest request. {ex}\") from ex\n\n    msgs = \"\"\n    for msg in req.messages:\n        msgs += f\"{(msg.role + ': ') if msg.role else ''}{msg.content}\\n\"\n\n    # Usage (Optional)\n    usage = ChatCompletionResponseUsage(prompt_tokens=5, completion_tokens=10, total_tokens=15)\n\n    # Use your LLM model to generate a response.\n    # This one just echoes back what the user asks.\n    choice_message = ChatCompletionResponseChoiceMessage(\n        role=\"assistant\", content=f\"You asked:\\n{msgs}\\nAnd I don't know how to respond yet.\"\n    )\n\n    # Create a choice object\n    choice = ChatCompletionResponseChoice(index=0, message=choice_message, finish_reason=\"stop\")\n\n    # Finally, create the top-level response object\n    open_ai_compatible_response = ChatCompletionResponse(\n        choices=[choice],\n        # Optional, but recommended fields\n        id=\"chatcmpl-abc123\",\n        model=\"gpt-3.5-turbo\",\n        usage=usage,\n    )\n\n    return open_ai_compatible_response\n</code></pre> <p>Previous: Model Gateway | Next: Embeddings &gt; Build</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/","title":"Practicus AI Model Gateway for LLMs","text":"<p>This document explains how to leverage the Practicus AI platform\u2019s optimized Large Language Model (LLM) gateway features, powered by high-performance inference engines such as LiteLLM, and others. The Practicus AI Model Gateway serves as a unified proxy layer, enabling seamless management and deployment of LLMs\u2014whether hosted on Practicus AI or accessed via third-party services.</p> <p>Whether you're using Practicus AI's optimized vLLM hosting, third-party services like OpenAI, or even your own custom inference code, the Gateway provides a single, consistent interface. This notebook will guide you through the entire process, from basic configuration to advanced dynamic routing and security guardrails.</p> <p>The following sections will guide you through key aspects of the Model Gateway, including initial setup, configuration, advanced features, and deployment.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#contents","title":"Contents","text":"<ol> <li> <p>Core Concepts    Overview of the primary configuration objects used by the Model Gateway.</p> </li> <li> <p>Configuration Strategies    Options for defining Gateway behavior using Python code, YAML files, or hybrid approaches.</p> </li> <li> <p>Logging and Cost Tracking    How to connect a database for comprehensive monitoring of usage and spend.</p> </li> <li> <p>Advanced Customization    Implementation of hooks, security guardrails, and dynamic routing for production workloads.</p> </li> <li> <p>Deployment    Steps to launch your Gateway configuration on the Practicus AI platform.</p> </li> </ol>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#1-core-concepts-the-configuration-trio","title":"1. Core Concepts: The Configuration Trio","text":"<p>Your model Gateway setup is defined by Python (pydantic) objects declared in a single file, typically named <code>model.py</code>. Practicus AI automatically handles the complex backend work, like generating the necessary <code>litellm_config.yaml</code> and attaching your custom functions.</p> <p>There are three Pydantic classes you'll use:</p> <ul> <li> <p><code>GatewayModel</code>: Defines a single model endpoint. This is what your users will call. It can be a concrete model (e.g., <code>openai/gpt-4o</code>) or a virtual router that intelligently delegates requests to other models.</p> </li> <li> <p><code>GatewayGuardrail</code>: A reusable, gateway-wide security or transformation rule. Guardrails can be applied to all models by default or selectively enabled by clients on a per-request basis.</p> </li> <li> <p><code>GatewayConfig</code>: The root object that brings everything together. It holds the list of all your models and guardrails, along with global settings like database connections.</p> </li> </ul>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#2-configuration-strategies","title":"2. Configuration Strategies","text":"<p>The Gateway offers flexible configuration options to fit your workflow. The recommended approach is to define your configuration in Python, as it provides the most power and flexibility, especially when using custom logic like routers and hooks.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#21-the-modelpy-file-your-gateways-blueprint","title":"2.1. The <code>model.py</code> File: Your Gateway's Blueprint","text":"<p>When you deploy a Gateway, the Practicus AI platform looks for a <code>model.py</code> file. This file must contain two key async functions: - <code>init(**kwargs)</code>: This function is executed once when your model server starts. It's the perfect place to initialize your Gateway configuration and start the server using <code>prt.models.server.start_gateway()</code>. - <code>serve(**kwargs)</code>: This function is called for every incoming request. You'll typically just pass the request through to the running Gateway server with <code>await prt.models.server.serve(**kwargs)</code>.</p> <pre><code>import practicuscore as prt\nimport os\n\n# Step 1: Define your models using the GatewayModel class\n\n# An OpenAI model, with the API key read from an environment variable\nmodel_gpt_4o = prt.GatewayModel(\n    name=\"practicus/gpt-4o\",\n    model=\"openai/gpt-4o\",\n    api_key_os_env=\"OPENAI_API_KEY\",\n)\n\n# A model hosted on Practicus AI's vLLM service\n# Note: For Practicus AI hosted models, if user impersonation is enabled,\n# you can omit the api_key, and a dynamic token will be generated.\nmodel_hosted_vllm = prt.GatewayModel(\n    name=\"practicus/my-hosted-llama\",\n    model=\"hosted_vllm/my-llama-3-deployment\",\n    api_base=\"http://local.practicus.io/models/open-ai-proxy/\",  # The internal URL to the deployment\n    api_key_os_env=\"MY_VLLM_TOKEN\",  # The service account token for the deployment\n)\n\n# Step 2: Create the main GatewayConfig object\ngateway_conf = prt.GatewayConfig(\n    models=[\n        model_gpt_4o,\n        model_hosted_vllm,\n    ],\n    strict=False,  # If True, configuration errors will block requests\n)\n\n# Step 3: Implement the init and serve functions\n\n\nasync def init(**kwargs):\n    \"\"\"Initializes and starts the Gateway server.\"\"\"\n\n    # It's best practice to set secrets from a secure source like Practicus Vault\n    os.environ[\"OPENAI_API_KEY\"] = \"sk-....\"\n    os.environ[\"MY_VLLM_TOKEN\"] = \"eyJh...\"\n\n    # Start the gateway with our configuration\n    # The `module` path is detected automatically\n    prt.models.server.start_gateway(config=gateway_conf)\n\n\nasync def serve(**kwargs):\n    \"\"\"Handles incoming inference requests.\"\"\"\n    return await prt.models.server.serve(**kwargs)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#22-using-an-external-litellm-yaml-custom_conf","title":"2.2. Using an External LiteLLM YAML (<code>custom_conf</code>)","text":"<p>If your model gateway inference engine is LiteLLM (default) and have an existing <code>litellm_config.yaml</code> or prefer a pure YAML setup, you can use the <code>custom_conf</code> argument. The Gateway will use this as a base configuration.</p> <p>You can provide it as a raw string or a file path. This can be combined with a Python-based <code>GatewayConfig</code> object; settings in the Python object will override any conflicting settings in the YAML.</p> <pre><code># Example of a hybrid configuration\n\n# Base configuration in a YAML string\ncustom_yaml_config = \"\"\"\nmodel_list:\n  - model_name: practicus/claude-3-sonnet\n    litellm_params:\n      model: claude-3-sonnet-20240229\n      api_key: \"[ANTHROPIC_API_KEY]\"\n\"\"\"\n\n# Python GatewayModel to add to the config\nmodel_gpt_4o_with_cost = prt.GatewayModel(\n    name=\"practicus/gpt-4o\",\n    model=\"openai/gpt-4o\",\n    api_key_os_env=\"OPENAI_API_KEY\",\n    input_cost_per_token=0.000005,  # $5 / 1M tokens\n    output_cost_per_token=0.000015,  # $15 / 1M tokens\n)\n\n# The final config merges both. The gateway will serve both\n# 'practicus/claude-3-sonnet' (from YAML) and 'practicus/gpt-4o' (from code).\ngateway_conf_hybrid = prt.GatewayConfig(\n    custom_conf=custom_yaml_config,\n    models=[\n        model_gpt_4o_with_cost,\n    ],\n)\n\n# You would then use `gateway_conf_hybrid` in your init function.\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#3-database-logging-cost-tracking","title":"3. Database Logging &amp; Cost Tracking","text":"<p>To enable detailed logging of requests, responses, usage, and spend, you can connect the Gateway to a PostgreSQL database. </p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#cost-tracking","title":"Cost Tracking","text":"<p>Set the <code>input_cost_per_token</code> and <code>output_cost_per_token</code> on any <code>GatewayModel</code> to automatically calculate and log the cost of each API call. This is invaluable for monitoring expenses and billing users.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#database-setup","title":"Database Setup","text":"<p>Provide a standard PostgreSQL connection string to the <code>database_url</code> parameter in your <code>GatewayConfig</code>.</p> <pre><code>gateway_conf = prt.GatewayConfig(\n    # ... other settings\n    database_url=\"postgresql://user:password@db_host:port/db_name\"\n)\n</code></pre> <p>\ud83d\udea8 IMPORTANT \ud83d\udea8 Before the Gateway runs for the first time with a new database, the schema must be created. To do this, you must set the following environment variable in your Practicus AI Model Deployment's <code>Extra configuration</code> section:</p> <pre><code>USE_PRISMA_MIGRATE=True\n</code></pre> <p>The server will perform the migration on startup and then exit. You should remove this variable after the first successful run to prevent migration attempts on every restart.</p> <p>For details on the database schema, see the official LiteLLM documentation: schema.prisma</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#4-advanced-customization-hooks-guards-and-routers","title":"4. Advanced Customization: Hooks, Guards, and Routers","text":"<p>The true power of the Gateway lies in its extensibility. You can inject custom asynchronous Python code at various points in the request/response lifecycle. All hook functions must be defined in the same <code>model.py</code> file.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#41-model-specific-hooks","title":"4.1. Model-Specific Hooks","text":"<p>You can attach callbacks directly to a <code>GatewayModel</code> to execute logic only for that model. </p> <ul> <li><code>pre_guards</code> &amp; <code>post_guards</code>: Lists of async functions that can inspect or modify the request payload (pre) or response object (post).</li> <li><code>pre_call</code>: A single async function that runs just before the call to the LLM, after routing.</li> <li><code>post_call_success</code> &amp; <code>post_call_failure</code>: Async functions for side effects like logging or metrics after a successful or failed LLM call.</li> </ul> <pre><code>import practicuscore as prt\n\n\n# Example of a pre-guard hook to add metadata to the request\nasync def add_custom_metadata(data: dict, requester: dict | None, **kwargs) -&gt; dict:\n    \"\"\"This hook inspects the request and adds metadata.\"\"\"\n    print(f\"Executing pre-guard for user: {requester.get('id', 'unknown') if requester else 'unknown'}\")\n\n    # Add a custom tracking ID to the request payload\n    if \"metadata\" not in data:\n        data[\"metadata\"] = {}\n    data[\"metadata\"][\"gateway_tracking_id\"] = \"gtw-12345\"\n\n    # You MUST return the modified data dictionary\n    return data\n\n\n# Example of a post-call hook for logging\nasync def log_successful_call(requester: dict | None, **kwargs):\n    \"\"\"This hook runs after a successful call.\"\"\"\n    print(\n        f\"Call successful for user: {requester.get('id', 'N/A') if requester else 'N/A'}. Logging to custom system...\"\n    )\n    # ... add custom logging logic here ...\n\n\n# Apply the hooks to a GatewayModel\nmodel_with_hooks = prt.GatewayModel(\n    name=\"practicus/gpt-4o-guarded\",\n    model=\"openai/gpt-4o\",\n    api_key_os_env=\"OPENAI_API_KEY\",\n    pre_guards=[add_custom_metadata],  # Can also pass the function name as a string: \"add_custom_metadata\"\n    post_call_success=log_successful_call,\n)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#42-dynamic-routing","title":"4.2. Dynamic Routing","text":"<p>Dynamic routing allows you to create a virtual model that decides which concrete model to use at runtime based on custom logic. To create a router, define a <code>GatewayModel</code> but provide a <code>router</code> function instead of a <code>model</code> identifier.</p> <p>Your router function will receive the request data and must return the <code>name</code> of the <code>GatewayModel</code> to forward the request to (e.g., <code>\"practicus/gpt-4o\"</code>).</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#routing-based-on-metrics","title":"Routing Based on Metrics","text":"<p>A common use case is to route based on performance metrics like latency or token throughput. You can fetch these using <code>prt.models.server.gateway_stats()</code>.</p> <p>\u26a0\ufe0f The Chicken-and-Egg Problem: The <code>gateway_stats</code> are collected locally on each gateway replica. A model will have no stats until it receives traffic. Your router logic must include a fallback/default model to handle cases where stats are not yet available. This ensures that new or underutilized models eventually get traffic and generate the metrics needed for routing.</p> <p>Advanced Strategies: - For robustness, wrap your routing logic in a <code>try...except</code> block to always return a fallback model. - For global routing decisions across all replicas, consider pulling aggregated metrics from a central monitoring system like Prometheus within your router function.</p> <pre><code>import practicuscore as prt\n\n# First, define the concrete models that our router can choose from\nmodel_fast = prt.GatewayModel(\n    name=\"practicus/fast-model\",\n    model=\"groq/llama3-8b-8192\",  # A known fast model\n    api_key_os_env=\"GROQ_API_KEY\",\n)\n\nmodel_powerful = prt.GatewayModel(\n    name=\"practicus/powerful-model\",\n    model=\"openai/gpt-4o\",  # A known powerful model\n    api_key_os_env=\"OPENAI_API_KEY\",\n)\n\n\n# Now, define the router function\nasync def intelligent_router(data: dict, **kwargs) -&gt; str:\n    \"\"\"Routes to a fast model for short prompts, and a powerful one for long prompts.\"\"\"\n    user_prompt = data.get(\"messages\", [{}])[-1].get(\"content\", \"\")\n\n    # Simple logic: if prompt is short, use the fast model.\n    if len(user_prompt) &lt; 200:\n        print(\"Routing to FAST model\")\n        return model_fast.name  # Return the name of the target model\n\n    # Otherwise, use the powerful model\n    print(\"Routing to POWERFUL model\")\n    return model_powerful.name\n\n\n# Finally, define the virtual model that uses the router\nmodel_intelligent_router = prt.GatewayModel(\n    name=\"practicus/intelligent-router\",  # This is the name clients will call\n    router=intelligent_router,  # Assign the router function\n)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#43-gateway-level-guardrails","title":"4.3. Gateway-Level Guardrails","text":"<p>A <code>GatewayGuardrail</code> is a hook that can be applied across the entire gateway. This is useful for enforcing broad policies like PII scrubbing or content moderation.</p> <ul> <li><code>default_on=True</code>: The guardrail is automatically applied to all models, all the time. Use this with extreme caution.</li> <li><code>default_on=False</code>: The guardrail is available, but inactive. Clients must explicitly request it in their API call by passing its name in the <code>extra_body</code>.</li> </ul> <pre><code>import practicuscore as prt\nfrom openai import OpenAI\n\n\n# Define a guardrail function\nasync def pii_scrubber(data: dict, **kwargs) -&gt; dict:\n    \"\"\"A simple PII scrubber that replaces email addresses.\"\"\"\n    for message in data.get(\"messages\", []):\n        content = message.get(\"content\", \"\")\n        # This is a naive example; use a proper library in production!\n        message[\"content\"] = content.replace(\"test@example.com\", \"[REDACTED_EMAIL]\")\n    return data\n\n\n# Create a GatewayGuardrail instance for it\npii_guard = prt.GatewayGuardrail(\n    name=\"pii-scrubber\",\n    mode=\"pre_call\",  # This runs before the LLM call\n    guard=pii_scrubber,\n    default_on=False,  # It is OFF by default\n)\n\n# This guardrail would then be added to the GatewayConfig:\n# gateway_conf = prt.GatewayConfig(models=[...], guardrails=[pii_guard])\n\n# --- Client-Side Usage ---\n# To activate the guardrail, a client would make a call like this:\n\n# client = OpenAI(base_url=...)\n# response = client.chat.completions.create(\n#     model=\"practicus/gpt-4o\",\n#     messages=[{\"role\": \"user\", \"content\": \"My email is test@example.com\"}],\n#     extra_body={\n#         \"guardrails\": [\"pii-scrubber\"] # Requesting the guardrail by name\n#     },\n# )\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#5-user-context-and-impersonation","title":"5. User Context and Impersonation","text":"<p>All custom functions (hooks, routers, guards) receive a <code>requester</code> dictionary in their <code>**kwargs</code>. This object contains valuable information about the authenticated user making the API call, such as their ID, roles, and other metadata provided by the Practicus AI platform.</p> <pre><code>async def my_hook(data: dict, requester: dict | None, **kwargs):\n    if requester:\n        print(f\"Request made by user ID: {requester.get('id')}\")\n</code></pre> <p>This enables powerful features like per-user validation, dynamic API key generation for Practicus-hosted models, or custom routing based on user roles.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#debugging-with-prt_overwrite_gateway_requester","title":"Debugging with <code>PRT_OVERWRITE_GATEWAY_REQUESTER</code>","text":"<p>For local development and testing, it can be difficult to simulate different users. To solve this, you can set the <code>PRT_OVERWRITE_GATEWAY_REQUESTER</code> environment variable. Set its value to a JSON string representing the <code>requester</code> object you want to simulate. The Gateway will use this mock object for all incoming calls, allowing you to easily test logic that depends on user context.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#6-deployment","title":"6. Deployment","text":"<p>Once your <code>model.py</code> is ready, deploying it is straightforward. You place the file in your project directory and use the standard <code>prt.models.deploy()</code> function from within a Practicus AI notebook.</p> <p>The platform will automatically package your <code>model.py</code> and its dependencies, create a dedicated Model Deployment on Kubernetes, and expose it as a secure, scalable API endpoint.</p> <p>Behind the scenes, the deployment's container will: 1. Start up. 2. Execute your <code>init()</code> function. 3. Your <code>init()</code> function calls <code>prt.models.server.start_gateway(config=...)</code>, which launches the LiteLLM proxy with the configuration you defined. 4. The endpoint is now live and ready to handle requests via your <code>serve()</code> function.</p> <pre><code>import practicuscore as prt\n\n# Ensure your model.py file (containing your GatewayConfig, hooks, init, and serve)\n# is in the project's root directory or a specified subdirectory.\n\ndeployment_key = None  # E.g. \"my-gateway-deployment\" a unique name for the K8s deployment\nprefix = \"models\"  # The API path prefix\nmodel_name = \"llm-gateway\"  # The name of your model in Practicus AI\n\nif deployment_key:\n    api_url, api_version_url, api_meta_url = prt.models.deploy(\n        deployment_key=deployment_key,\n        prefix=prefix,\n        model_name=model_name,\n    )\n\n    print(f\"Deployment initiated. API will be available at: {api_url}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/gateway/model-gateway/#7-conclusion","title":"7. Conclusion","text":"<p>The Practicus AI Model Gateway provides a powerful, flexible, and unified way to manage all of your LLM endpoints. By leveraging Python-based configuration, you can move beyond simple proxying and build sophisticated applications with features like:</p> <ul> <li>\u2705 Centralized Model Catalog: A single point of access for all your models.</li> <li>\u2705 Cost and Usage Monitoring: Detailed logs for chargebacks and analysis.</li> <li>\u2705 Intelligent Routing: Optimize for cost, latency, or performance by dynamically selecting the best model for the job.</li> <li>\u2705 Robust Security: Implement custom guardrails to enforce policies across all requests.</li> </ul> <p>This notebook provides the foundation for you to get started. Don't hesitate to explore the Pydantic class definitions for a full list of available parameters and build the exact LLM gateway your organization needs.</p> <p>Previous: Model Serving | Next: Custom &gt; Models &gt; Build</p>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/","title":"Practicus AI Large Language Model (LLM) Hosting","text":"<p>This example demonstrates how to leverage the Practicus AI platform's optimized LLM hosting features, powered by engines like vLLM for high-throughput and low-latency inference. We will cover:</p> <ol> <li>Experimenting in Design Time: Interactively running and testing LLMs directly within a Practicus AI Worker's Jupyter environment using the <code>ModelServer</code> utility.</li> <li>Deploying for Runtime: Packaging and deploying LLMs as scalable endpoints on the Practicus AI Model Hosting platform.</li> </ol> <p>This approach is the recommended method for hosting most Large Language Models on Practicus AI, offering significant performance benefits and simplified deployment compared to writing custom prediction code from scratch.</p> <p>Note: If you need to host non-LLM models or require deep customization beyond the options provided by the built-in LLM serving engine (e.g., complex pre/post-processing logic tightly coupled with the model), please view the custom model serving section for guidance on building models with custom Python code.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#1-overview-of-the-prtmodelsserver-utility","title":"1. Overview of the <code>prt.models.server</code> Utility","text":"<p>The <code>practicuscore.models.server</code> module (<code>prt.models.server</code>) provides a high-level interface to manage LLM inference servers within the Practicus AI environment. Primarily, it controls an underlying inference engine process (like vLLM by default) and exposes its functionality.</p> <p>Key capabilities include:</p> <ul> <li>Starting/Stopping Servers: Easily launch and terminate the inference server process (e.g., vLLM) with specified models and configurations (like quantization, tensor parallelism).</li> <li>Health &amp; Status Monitoring: Check if the server is running, view logs, and diagnose issues.</li> <li>Providing Access URL: Get the local URL to interact with the running server.</li> <li>Runtime Integration: Facilitates deploying models using optimized container images, often exposing an OpenAI-compatible API endpoint for standardized interaction.</li> </ul>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#2-experimenting-in-design-time-jupyter-notebook","title":"2. Experimenting in Design Time (Jupyter Notebook)","text":"<p>You can interactively start an LLM server, send requests, and shut it down directly within a Practicus AI Worker notebook. This is ideal for development, testing prompts, and evaluating different models or configurations before deployment.</p> <p>Note: This runs the server locally within your Worker's resources. Ensure your Worker has sufficient resources (especially GPU memory) for the chosen model.</p> <pre><code>import practicuscore as prt\nfrom openai import OpenAI\nimport time\n\n# Define the model from Hugging Face and any specific engine options\n# Example: Use TinyLlama and specify 'half' precision (float16) suitable for GPUs like T4\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nvllm_options = {\"dtype\": \"half\"}\n\n# Start the vLLM server process and wait for it to become ready\nprint(f\"Starting server for model: {model_id} with options: {vllm_options}...\")\nprt.models.server.start_serving(model=model_id, options=vllm_options)\n\n# The server might take a moment to initialize, especially on first download\n\n# Get the base URL of the locally running server\n# Append '/v1' for the OpenAI-compatible API endpoint\nbase_url = prt.models.server.get_base_url()\nif not base_url:\n    print(\"Error: Server failed to start. Please check logs.\")\nelse:\n    openai_api_base = base_url + \"/v1\"\n    print(f\"Server started. OpenAI compatible API Base URL: {openai_api_base}\")\n\n    # Create an OpenAI client pointed at the local server\n    # No API key is needed ('api_key' can be anything) for local interaction\n    client = OpenAI(\n        base_url=openai_api_base,\n        api_key=\"not-needed-for-local\",\n    )\n\n    # Send a chat completion request\n    print(\"Sending chat request...\")\n    try:\n        response = client.chat.completions.create(\n            # The 'model' parameter should match the model loaded by the server\n            # You can often use model=None if only one model is served,\n            # or explicitly pass the model_id\n            model=model_id,\n            messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            max_tokens=50,  # Limit response length\n            temperature=0.7,\n        )\n        print(\"Response received:\")\n        print(response.choices[0].message.content)\n        # Expected output might be similar to: 'The capital of France is Paris.'\n    except Exception as e:\n        print(f\"Error during chat completion: {e}\")\n        print(\"Check server status and logs.\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#additional-utilities-for-design-time","title":"Additional Utilities for Design Time","text":"<p>While the server is running in your notebook session, you can monitor it:</p> <pre><code># Check the server's status ('running', 'error', 'stopped', etc.)\nstatus = prt.models.server.get_status()\nprint(f\"Server Status: {status}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#testing-with-a-mock-llm-server-on-cpu","title":"Testing with a Mock LLM Server on CPU","text":"<p>For testing pipelines or developing client code without requiring a GPU or a real LLM, you can run a simple mock server. This mock server just needs to implement the expected API endpoint (e.g., <code>/v1/chat/completions</code>).</p> <p>Create a Python file (view example <code>mock_llm_server.py</code> at the bottom of this page) with a basic web server (like Flask or FastAPI) that returns predefined responses. Then, start it using <code>prt.models.server.start_serving()</code>.</p> <pre><code># Example: Assuming you have 'mock_llm_server.py' in the same directory\n# This file would contain a simple Flask/FastAPI app mimicking the OpenAI API structure\ntry:\n    print(\"Attempting to stop any existing server...\")\n    prt.models.server.stop()  # Stop the real LLM server if it's running\n    time.sleep(2)\n\n    print(\"Starting the mock server...\")\n    # Make sure 'mock_llm_server.py' exists and is runnable\n    prt.models.server.start_serving(model=\"mock_llm_server.py\")\n    time.sleep(5)  # Give mock server time to start\n\n    mock_base_url = prt.models.server.get_base_url()\n    if not mock_base_url:\n        print(\"Error: Mock server failed to start. Please check logs.\")\n    else:\n        mock_api_base = mock_base_url + \"/v1\"\n        print(f\"Mock Server Running. API Base: {mock_api_base}\")\n\n        # Create client for the mock server\n        mock_client = OpenAI(base_url=mock_api_base, api_key=\"not-needed\")\n\n        # Send a request to the mock server\n        mock_response = mock_client.chat.completions.create(\n            model=\"mock-model\",  # Model name expected by your mock server\n            messages=[{\"role\": \"user\", \"content\": \"Hello mock!\"}],\n        )\n        print(\"Mock Response:\", mock_response.choices[0].message.content)\n        # Example mock server might return: 'You said: Hello mock!'\nexcept FileNotFoundError:\n    print(\"Skipping mock server test: 'mock_llm_server.py' not found.\")\nexcept Exception as e:\n    print(f\"An error occurred during mock server test: {e}\")\nfinally:\n    # Important: Stop the mock server when done\n    print(\"Stopping the mock server...\")\n    prt.models.server.stop()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#cleaning-up-the-design-time-server","title":"Cleaning Up the Design Time Server","text":"<p>When you are finished experimenting in the notebook, it's crucial to stop the server to release GPU resources.</p> <pre><code>print(\"Stopping any running server...\")\nprt.models.server.stop()\nprint(f\"Server Status after stop: {prt.models.server.get_status()}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#3-deploying-models-for-runtime","title":"3. Deploying Models for Runtime","text":"<p>Once you have selected and tested your model, you need to deploy it as a scalable service on the Practicus AI Model Hosting platform. This involves packaging the model and its serving configuration into a container image and creating a deployment through the Practicus AI console.</p> <p>There are a few ways to configure the container for LLM serving:</p>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#option-1-dynamically-download-model-at-runtime-no-coding-required","title":"Option 1: Dynamically Download Model at Runtime (No Coding Required)","text":"<p>This is the quickest way to get started. Use a pre-built Practicus AI vLLM image, and configure the model ID and options via environment variables in the model deployment settings. The container will download the specified model when it starts.</p> <p>Pros: Simple configuration, no need to build custom images. Cons: Can lead to longer cold start times as the model needs to be downloaded on pod startup. Potential for download issues at runtime.</p> <p>Steps:</p> <ol> <li> <p>Define Container Image in Practicus AI:</p> <ul> <li>Navigate to <code>Infrastructure &gt; Container Images</code> in the Practicus AI console.</li> <li>Add a new image. Use a vLLM-enabled image provided by Practicus AI, for example: <code>ghcr.io/practicusai/practicus-modelhost-gpu-vllm:25.5.4</code> (replace with the latest/appropriate version).</li> </ul> </li> <li> <p>Create Model Deployment:</p> <ul> <li>Go to <code>ML Model Hosting &gt; Model Deployments</code>.</li> <li>Create a new deployment, ensuring you allocate necessary GPU resources.</li> <li>Select the container image you added in the previous step.</li> <li>In the <code>Extra configuration</code> section (or environment variables section), define:<ul> <li><code>PRT_SERVE_MODEL</code>: Set this to the Hugging Face model ID (e.g., <code>TinyLlama/TinyLlama-1.1B-Chat-v1.0</code>).</li> <li><code>PRT_SERVE_MODEL_OPTIONS_B64</code>: (Optional) Provide Base64-encoded JSON containing vLLM options (like <code>{\"dtype\": \"half\"}</code>).</li> </ul> </li> </ul> </li> <li> <p>Create Model and Version:</p> <ul> <li>Go to <code>ML Model Hosting &gt; Models</code>.</li> <li>Add a <code>New Model</code> (e.g., <code>my-tiny-llama</code>).</li> <li>Add a <code>New Version</code> for this model, pointing it to the <code>Model Deployment</code> you just created.</li> <li>Tip: You can create <code>multiple versions</code> each pointing to different model deployments, and then perform <code>A/B testing</code> comparing LLM model performance.</li> </ul> </li> </ol> <p>Example 1: Serving TinyLlama (default options)</p> <p>In Model Deployment <code>Extra configuration</code> section, add:</p> <pre><code>PRT_SERVE_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0\n</code></pre> <p>Example 2: Serving TinyLlama with Half Precision</p> <ul> <li> <p>Generate Base64 options:</p> <pre><code># On your local machine or a terminal:\necho '{\"dtype\": \"half\"}' | base64\n# Prints eyJkdHlwZSI6ICJoYWxmIn0K\n</code></pre> </li> <li> <p>In Model Deployment <code>Extra configuration</code> section, add:</p> <pre><code>PRT_SERVE_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0\nPRT_SERVE_MODEL_OPTIONS_B64=eyJkdHlwZSI6ICJoYWxmIn0K\n</code></pre> </li> </ul>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#option-2-pre-download-and-bake-model-into-container-image-recommended-for-small-models","title":"Option 2: Pre-download and Bake Model into Container Image (Recommended for small models)","text":"<p>Build a custom container image that includes the model files. This avoids runtime downloads, leading to faster and more reliable pod startups.</p> <p>Pros: Faster cold starts, improved reliability (no runtime download dependency), enables offline environments. Cons: Requires building and managing custom container images. Image size will be larger. Longer build times.</p> <p>Steps:</p> <ol> <li> <p>Create a <code>Dockerfile</code>:</p> <ul> <li>Start from a Practicus AI base vLLM image.</li> <li>Set environment variables for the model and options.</li> <li>Use <code>huggingface-cli download</code> to download the model during the image build.</li> <li>(Optional but recommended) Configure vLLM to use the downloaded path and enable offline mode.</li> <li>(Optional) If you need custom logic, <code>COPY</code> your <code>model.py</code> (see Option 3).</li> </ul> </li> <li> <p>Build and Push the Image: Build the Docker image and push it to a container registry accessible by Practicus AI.</p> </li> <li> <p>Configure Practicus AI:</p> <ul> <li>Add your custom image URL in <code>Infrastructure &gt; Container Images</code>.</li> <li>Create a <code>Model Deployment</code> using your custom image. You usually don't need to set <code>PRT_SERVE_MODEL</code> or <code>PRT_SERVE_MODEL_OPTIONS_B64</code> as environment variables here, as they are baked into the image (unless your image startup script specifically reads them).</li> <li>Create the <code>Model</code> and <code>Version</code> pointing to this deployment.</li> </ul> </li> </ol> <p>Example <code>Dockerfile</code>:</p> <pre><code># Use a Practicus AI image that includes GPU support and vLLM\nFROM ghcr.io/practicusai/practicus-modelhost-gpu-vllm:25.5.4\n\n# --- Configuration baked into the image ---\nENV PRT_SERVE_MODEL=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n# Example: Options for smaller GPUs like Nvidia T4 (float16)\n# Generated via: echo '{\"dtype\": \"half\"}' | base64\nENV PRT_SERVE_MODEL_OPTIONS_B64=\"eyJkdHlwZSI6ICJoYWxmIn0K\"\n\n# --- Model Download during Build ---\n# Define a persistent cache location within the image\nENV HF_HOME=\"/var/practicus/cache/huggingface\"\n\nRUN \\\n    # Download model for offline use\n    echo \"Starting to download '$PRT_SERVE_MODEL' to '$HF_HOME'.\" &amp;&amp; \\\n    mkdir -p \"$HF_HOME\" &amp;&amp; \\\n    huggingface-cli download \"$PRT_SERVE_MODEL\" --local-dir \"$HF_HOME\" &amp;&amp; \\\n    echo \"Completed downloading '$PRT_SERVE_MODEL' to '$HF_HOME'.\" &amp;&amp; \\\n    # Create VLLM redirect file\n    REDIRECT_JSON=\"{\\\"$PRT_SERVE_MODEL\\\": \\\"$HF_HOME\\\"}\" &amp;&amp; \\\n    REDIRECT_JSON_PATH=\"/var/practicus/vllm_model_redirect.json\" &amp;&amp; \\\n    echo \"Creating VLLM redirect file: $REDIRECT_JSON_PATH\" &amp;&amp; \\\n    echo \"VLLM redirect JSON content: $REDIRECT_JSON\" &amp;&amp; \\\n    echo \"$REDIRECT_JSON\" &gt; \"$REDIRECT_JSON_PATH\"\n\n# --- vLLM Configuration for Baked Model ---\n# Tell vLLM (via our entrypoint) to use the baked model path directly\nENV VLLM_MODEL_REDIRECT_PATH=\"/var/practicus/vllm_model_redirect.json\"\n\n# (Recommended for baked images) Prevent accidental downloads at runtime\nENV TRANSFORMERS_OFFLINE=1\nENV HF_HUB_OFFLINE=1\n\n# --- Custom Logic (Optional - See Option 3) ---\n# If you need custom init/serve logic, uncomment and provide your model.py\n# COPY model.py /var/practicus/model.py\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#option-3-custom-modelpy-implementation","title":"Option 3: Custom <code>model.py</code> Implementation","text":"<p>If you need to add custom logic before or after the vLLM server handles requests (e.g., complex input validation/transformation, custom readiness checks, integrating external calls), you can provide a <code>/var/practicus/model.py</code> file within your custom container image (usually built as described in Option 2).</p> <p>Pros: Maximum flexibility for custom logic around the vLLM server. Cons: Requires Python coding; adds complexity compared to standard vLLM usage.</p> <p>Steps:</p> <ol> <li>Create your <code>model.py</code> with <code>init</code> and <code>serve</code> functions.</li> <li>Inside <code>init</code>, call <code>prt.models.server.start_serving()</code> to launch the vLLM process.</li> <li>Inside <code>serve</code>, you can add pre-processing logic, then call <code>await prt.models.server.serve()</code> to forward the request to the underlying vLLM server, and potentially add post-processing logic to the response.</li> <li>Build a custom Docker image (as in Option 2), ensuring you <code>COPY model.py /var/practicus/model.py</code>.</li> </ol> <pre><code># Example: /var/practicus/model.py\n# Customize as required and place  in your project and COPY into the Docker image\nimport practicuscore as prt\n\n\nasync def init(**kwargs):\n    if not prt.models.server.initialized:\n        prt.models.server.start_serving()\n\n\nasync def serve(**kwargs):\n    return await prt.models.server.serve(**kwargs)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#option-4-host-models-from-attached-storage-recommended-for-large-models-offline-mode","title":"Option 4: Host Models from Attached Storage (Recommended for Large Models, Offline Mode)","text":"<p>Note: This step requires admin access to the Practicus AI management console.</p> <p>Use an external Persistent Volume Claim (PVC) to pre-download Hugging Face models into a shared path. This avoids runtime downloads, enables offline operation, and is recommended for large models.</p> <p>Pros:</p> <ul> <li>Models are shared across workers/deployments (no duplicate downloads).</li> <li>Works in offline or air-gapped environments.</li> <li>Easy to update or replace models without rebuilding container images.</li> </ul> <p>Cons:</p> <ul> <li>Requires PVC setup and storage capacity planning.</li> <li>Initial model download must be done manually.</li> </ul>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#steps","title":"Steps","text":""},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#1-create-a-storage-profile-pvc","title":"1. Create a Storage Profile (PVC)","text":"<ol> <li> <p>Go to: Practicus AI management console &gt; Infrastructure &gt; Storage Profiles</p> </li> <li> <p>Create a new storage profile with the following settings:</p> </li> <li> <p>Key: e.g. <code>hf-models</code></p> </li> <li>Volume Type: <code>PersistentVolumeClaim (new)</code></li> <li>Mount path: e.g. <code>/var/practicus/hf</code></li> <li>FS Group: <code>1000</code> (recommended)</li> <li>Storage Class Name: choose a <code>ReadWriteMany (RWM)</code> capable storage class if possible. Otherwise, only one pod can use the volume at a time.</li> <li>Access Mode: RWM (ideal) or RWO</li> <li> <p>Storage Size: e.g. <code>100Gi</code></p> </li> <li> <p>Create a new GPU Workload Type: Infrastructure &gt; Workload Types</p> </li> <li> <p>Use the node selector for GPU workloads (e.g. <code>my-gpu</code>).</p> </li> <li>Attach the storage profile you just created (e.g. <code>hf-models</code>).</li> </ol>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#2-validate-pvc-write-access","title":"2. Validate PVC Write Access","text":"<ol> <li>Create a design-time worker using the workload type (e.g. <code>my-gpu</code>).</li> <li>This mounts the PVC at <code>/var/practicus/hf</code>.</li> <li>Validate writability:</li> </ol> <pre><code>cd /var/practicus/hf\ntouch write-test\nrm write-test\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#3-download-the-model-one-time-preload","title":"3. Download the Model (One-Time Preload)","text":"<p>Run inside the design-time worker, using the workload type you just created (e.g. my-gpu):</p> <pre><code>import os\nimport practicuscore as prt\n\nos.environ[\"HF_HOME\"] = \"/var/practicus/hf\"  # The PVC storage we created\n\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nvllm_options = {\"dtype\": \"half\"}\n\nprint(f\"Starting server for model: {model_id} with options: {vllm_options}...\")\nprt.models.server.start_serving(model=model_id, options=vllm_options)\n</code></pre> <p>\u2705 Confirm the model server loads successfully (weights are downloaded into PVC). \u2705 Run a quick chat test.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#4-host-the-model-with-pvc-in-offline-mode","title":"4. Host the Model with PVC in Offline Mode","text":"<ol> <li> <p>Go to: Practicus AI management console &gt; ML Model Hosting &gt; Model Deployments</p> </li> <li> <p>Create a new model deployment using a vLLM-enabled image.</p> </li> <li> <p>Select the same workload type (e.g. <code>my-gpu</code>).</p> </li> <li> <p>If you created a new workload type, attach the same storage profile (PVC).</p> </li> <li> <p>Add the following environment variables to the deployment:</p> </li> </ol> <pre><code>HF_HOME=/var/practicus/hf\nTRANSFORMERS_OFFLINE=1\nHF_DATASETS_OFFLINE=1\nPRT_SERVE_MODEL=TinyLlama/TinyLlama-1.1B-Chat-v1.0\nPRT_SERVE_MODEL_OPTIONS_B64=eyJkdHlwZSI6ICJoYWxmIn0K\n</code></pre> <ul> <li><code>TRANSFORMERS_OFFLINE</code> and <code>HF_DATASETS_OFFLINE</code> ensure vLLM never downloads models at runtime and only uses the files in <code>HF_HOME</code>.</li> <li> <p><code>PRT_SERVE_MODEL_OPTIONS_B64</code> can be generated with:</p> <pre><code>echo '{\"dtype\": \"half\"}' | base64\n</code></pre> </li> <li> <p>Create a Model Endpoint:</p> </li> <li> <p>Go to Practicus AI management console &gt; ML Model Hosting &gt; Models.</p> </li> <li> <p>Create a new model (e.g. <code>/models/tiny-llama</code>).</p> </li> <li> <p>Create a new version (e.g. <code>1</code>) and select the deployment created above.</p> </li> </ul> <p>With this approach, you can host models without deploying a <code>model.py</code> file first.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#4-proxy-mode-routing-requests-to-an-external-service","title":"4. Proxy Mode: Routing Requests to an External Service","text":"<p>Practicus AI\u2019s model server includes a flexible proxy mode that you can enable at startup. When proxy mode is active, all incoming inference requests are transparently forwarded to the external endpoint of your choice.</p> <p>For example, to relay traffic through OpenAI\u2019s API, simply launch your model with proxy mode enabled and point it at <code>https://api.openai.com</code>. Your clients continue to call your Practicus endpoint, while under the hood requests\u2014and responses\u2014flow directly to and from OpenAI or another OpenAI compatible service.</p> <pre><code>import practicuscore as prt\n\nproxy_base_url = \"https://api.openai.com/v1\"\n# Use prt.vault or manually enter OpenAI token e.g. sk-..\nproxy_token = None\n\n\nasync def init(**kwargs):\n    if not prt.models.server.initialized:\n        prt.models.server.start_serving(proxy_mode=True)\n\n\nasync def serve(**kwargs):\n    assert proxy_token, \"No proxy_token provided for OpenAI\"\n    return await prt.models.server.serve(\n        proxy_base_url=proxy_base_url,\n        proxy_token=proxy_token,\n        **kwargs,\n    )\n</code></pre>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#5-conclusion","title":"5. Conclusion","text":"<p>Practicus AI provides optimized pathways for hosting LLMs using engines like vLLM.</p> <ul> <li>Use the <code>prt.models.server</code> utility within notebooks for interactive experimentation.</li> <li>For runtime deployment, choose between dynamic model downloads (easy start) or baking models into images (recommended for production) via the Practicus AI console.</li> <li>Use a custom <code>model.py</code> only when specific pre/post-processing logic around the core LLM inference is required.</li> <li>Proxy request to an external service if needed.</li> </ul> <p>Remember to consult the specific documentation for vLLM options and Practicus AI deployment configurations for advanced settings.</p>"},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/generative-ai/model-serving/llm/model-serving/#mock_llm_serverpy","title":"mock_llm_server.py","text":"<pre><code># A simple echo server you can use to test LLM functionality without GPUs\n# Echoes what you request.\n\nimport argparse\nimport logging\nimport time\nfrom fastapi.responses import PlainTextResponse\nimport uvicorn\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import Optional\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler()],\n)\nlogger = logging.getLogger(\"mock_llm_server\")\n\n# Create FastAPI app\napp = FastAPI(title=\"Mock LLM Server\")\n\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n\nclass ChatRequest(BaseModel):\n    model: str\n    messages: list[ChatMessage]\n    temperature: Optional[float] = 0.7\n    top_p: Optional[float] = 1.0\n    max_tokens: Optional[int] = 100\n    stream: Optional[bool] = False\n\n\n@app.get(\"/health\")\nasync def health_check():\n    # For testing you might want to add a delay to simulate startup time\n    # time.sleep(5)\n    return {\"status\": \"healthy\"}\n\n\n@app.get(\"/metrics\")\nasync def metrics():\n    # For testing you might want to add a delay to simulate startup time\n    # time.sleep(5)\n    return PlainTextResponse(\"\"\"# HELP some_random_metric Some random metric that the mock server returns\n# TYPE some_random_metric counter\nsome_random_metric 1.23\"\"\")\n\n\n@app.post(\"/v1/chat/completions\")\nasync def chat_completions(request: ChatRequest):\n    # Log the request\n    logger.info(f\"Received chat request for model: {request.model}\")\n\n    # Extract the last message content\n    last_message = request.messages[-1].content if request.messages else \"\"\n\n    # Create a mock response\n    response = {\n        \"id\": \"mock-response-id\",\n        \"object\": \"chat.completion\",\n        \"created\": int(time.time()),\n        \"model\": request.model,\n        \"choices\": [\n            {\n                \"index\": 0,\n                \"message\": {\"role\": \"assistant\", \"content\": f\"This is a mock response for: {last_message}\"},\n                \"finish_reason\": \"stop\",\n            }\n        ],\n        \"usage\": {\n            \"prompt_tokens\": len(last_message.split()),\n            \"completion_tokens\": 8,\n            \"total_tokens\": len(last_message.split()) + 8,\n        },\n    }\n\n    # Wait a bit to simulate processing time\n    time.sleep(0.5)\n\n    return response\n\n\nif __name__ == \"__main__\":\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description=\"Mock LLM Server\")\n    parser.add_argument(\"--port\", type=int, default=8585, help=\"Port to run the server on\")\n    args = parser.parse_args()\n\n    logger.info(f\"Starting mock LLM server on port {args.port}\")\n\n    # Run the server\n    uvicorn.run(app, host=\"0.0.0.0\", port=args.port)\n</code></pre> <p>Previous: Build | Next: Gateway &gt; Model Gateway</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/","title":"Automated relational database schema management","text":"<p>This example demonstrates how to use <code>prt.db</code> module to manage database schema migrations and static data, specifically for your Practicus AI applications.</p> <p>Core Concepts:</p> <ol> <li>Isolated Schemas: Practicus AI encourages creating <code>separate restricted database schemas</code> (and corresponding users/roles) for each application to ensure <code>isolation and security</code>. This is handled by <code>prt.apps.prepare_db</code>.</li> <li>Migrations: Schema changes (creating tables, adding columns, etc.) are automatically managed by Practicus AI. This provides <code>version control</code> and <code>GitOps</code> capabilities for your database structure.</li> <li>Models: Database tables are defined using <code>SQLModel</code>, a popular python library that combines Pydantic and SQLAlchemy.</li> <li>Workflow:<ul> <li>Prepare the dedicated app schema (<code>prt.apps.prepare_db</code>).</li> <li>Initialize the local migration environment (<code>prt.db.init</code>).</li> <li>Define your data models (<code>db/models.py</code>).</li> <li>Generate a migration script based on model changes (<code>prt.db.revision</code>).</li> <li>Apply the migration to the database (<code>prt.db.upgrade</code>).</li> <li>Optionally, define and load static/lookup data (<code>db/static_data.py</code>, <code>prt.db.upsert_static_data</code>).</li> <li>Use the database in your application code.</li> <li>Deploy the application, ensuring it has the database connection string.</li> </ul> </li> </ol> <p>Prerequisites: * A system admin must configure a database connection for your Practicus AI <code>app deployment setting</code>, effectively making it <code>db-enabled</code>. * Please see below <code>Preparing a Database for Your Applications</code> section to learn how. * The <code>app_deployment_key</code> below refers to the key of this db-enabled app deployment setting.</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#1-setup-and-preparation","title":"1. Setup and Preparation","text":"<p>First, we define some parameters for our application.</p> <pre><code># Parameters\napp_deployment_key = None  # must be db-enabled\napp_prefix = \"apps\"\napp_name = \"my-db-app\"\n</code></pre> <pre><code># Sanity check\nassert app_deployment_key, \"app_deployment_key not defined\"\nassert app_prefix, \"app_prefix not defined\"\nassert app_name, \"app_name not defined\"\n</code></pre> <p>If you don't know which app app deployment settings are <code>db-enabled</code>, you can list the app deployment settings you have acess to and check the <code>db_enabled</code> column running the below.</p> <pre><code>import practicuscore as prt\n\nmy_app_settings = prt.apps.get_deployment_setting_list()\n\nprint(\"Application deployment settings I have access to:\")\ndisplay(my_app_settings.to_pandas())\n</code></pre> <pre><code># Let's verify our app deployment setting is db-enabled\nverified = False\nfor app_setting in my_app_settings:\n    if app_setting.key == app_deployment_key:\n        if not app_setting.db_enabled:\n            raise SystemError(f\"Setting '{app_deployment_key}' is not db-enabled.\")\n        verified = True\n        break\n\nif not verified:\n    raise ValueError(f\"Setting '{app_deployment_key}' could not be located.\")\n\nprint(f\"Located setting '{app_deployment_key}' and it is db-enabled. Good to go!\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#11-prepare-the-application-database-schema","title":"1.1. Prepare the Application Database Schema","text":"<p>We use <code>prt.apps.prepare_db</code> to create a dedicated schema and user/role for our application in the configured database. This function returns the schema name and a unique connection string.</p> <p>Important: The connection string grants access only to the newly created schema. Save it securely, as it won't be shown again.</p> <pre><code>print(f\"Preparing database schema for app: {app_prefix}/{app_name} using deployment setting: {app_deployment_key}\")\ntry:\n    db_schema, db_conn_str = prt.apps.prepare_db(\n        prefix=app_prefix,\n        app_name=app_name,\n        deployment_setting_key=app_deployment_key,\n    )\n\n    print(f\"Successfully prepared database schema: {db_schema}\")\n    # print(f\"Database connection string: {db_conn_str}\") # Uncomment locally to see if needed, but avoid printing secrets.\n    print(\"*** Connection string obtained. Please handle it securely. ***\")\nexcept Exception as e:\n    print(f\"Error preparing database: {e}\")\n    print(\"Please ensure the deployment setting key is correct and the database is accessible.\")\n    raise\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#12-securely-store-and-use-the-connection-string","title":"1.2. Securely Store and Use the Connection String","text":"<p>It's highly recommended to store sensitive information like database connection strings in Practicus AI's Vault.</p> <p>The <code>prt.db</code> commands (<code>revision</code>, <code>upgrade</code>, <code>upsert_static_data</code>) expect the connection string to be available in the environment variable <code>PRT_DB_CONN_STR</code>. We'll store it in Vault and then set the environment variable for the current notebook session.</p> <pre><code>import os\n\n# Define a unique name for the secret where we'll store the DB connection string.\ndb_conn_secret_name = f\"{app_prefix.replace('/', '_')}__{app_name.replace('-', '_')}__CONN_STR\".upper()\nprint(f\"Database connection string will be stored in Vault secret: {db_conn_secret_name}\")\n\n# Store the connection string in Vault\nprint(f\"Storing connection string in Vault secret: {db_conn_secret_name}\")\nprt.vault.create_or_update_secret(db_conn_secret_name, db_conn_str)\n\n# Set the OS environment variable for prt.db commands in this session\n# IMPORTANT: Restart the notebook kernel if you change this variable elsewhere!\nos.environ[\"PRT_DB_CONN_STR\"] = db_conn_str\nprint(\"Environment variable PRT_DB_CONN_STR set for this session.\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#13-optional-reload-connection-string-from-vault","title":"1.3. (Optional) Reload Connection String from Vault","text":"<p>If you restart the notebook kernel or run this notebook later, you can retrieve the connection string from Vault instead of running <code>prepare_db</code> again (which would try to recreate the schema).</p> <pre><code>print(f\"Attempting to reload connection string from Vault secret: {db_conn_secret_name}\")\ntry:\n    db_conn_str, age = prt.vault.get_secret(db_conn_secret_name)\n    os.environ[\"PRT_DB_CONN_STR\"] = db_conn_str\n    print(f\"Successfully reloaded connection string from Vault (Age: {age}).\")\n    print(\"Environment variable PRT_DB_CONN_STR set for this session.\")\nexcept Exception as e:\n    print(f\"Error reloading secret {db_conn_secret_name}: {e}\")\n    print(\"Ensure the secret exists and you have permissions.\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#14-optionaldevelopment-only-remove-database-schema","title":"1.4. (Optional/Development Only) Remove Database Schema","text":"<p>If you need to start over during development, you can remove the application's database schema and associated user/role using <code>prt.apps.remove_db</code>. </p> <p>\u26a0\ufe0f WARNING: This operation is destructive and will delete all data within the application's schema. Use with extreme caution, primarily in development environments.</p> <pre><code># --- Development Only: Reset Database Schema ---\n# Set this to True only if you understand the consequences and want to delete the schema\nremove_db = False\n\nif remove_db:\n    print(\"*** WARNING: Proceeding with database schema removal! ***\")\n    confirm = input(f\"Type 'delete {app_prefix}/{app_name}' to confirm DELETION of schema and data: \")\n\n    if confirm == f\"delete {app_prefix}/{app_name}\":\n        print(f\"Removing database schema for app: {app_prefix}/{app_name}\")\n        try:\n            prt.apps.remove_db(\n                prefix=app_prefix,\n                app_name=app_name,\n                deployment_setting_key=app_deployment_key,\n            )\n            print(\"Database schema removed successfully.\")\n            # Unset the environment variable as the connection is now invalid\n            if \"PRT_DB_CONN_STR\" in os.environ:\n                del os.environ[\"PRT_DB_CONN_STR\"]\n                print(\"PRT_DB_CONN_STR environment variable unset.\")\n            # Clear the variable in the notebook context\n            if \"db_conn_str\" in locals():\n                del db_conn_str\n\n        except Exception as e:\n            print(f\"Error removing database schema: {e}\")\n    else:\n        print(\"Confirmation failed. Database schema not removed.\")\nelse:\n    print(\"Database removal step skipped (remove_db is False).\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#2-core-prtdb-workflow","title":"2. Core <code>prt.db</code> Workflow","text":"<p>Now that we have a database schema and connection string ready, we can use the <code>prt.db</code> functions to manage the schema structure.</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#21-initialize-migration-environment-prtdbinit","title":"2.1. Initialize Migration Environment (<code>prt.db.init</code>)","text":"<p>This command scaffolds the necessary directory structure (<code>db/</code>, <code>db/migrations/</code>) and configuration files (<code>db/migrations.ini</code>, <code>db/migrations/env.py</code>, etc.) for Alembic in your current working directory.</p> <p>It also creates template files: * <code>db/models.py</code>: Where you'll define your database tables using <code>SQLModel</code>. * <code>db/static_data.py</code>: Where you can define static lookup data. * <code>db/__init__.py</code>: Makes the <code>db</code> directory a Python package.</p> <p>Run this once per project. Use <code>overwrite=True</code> if you need to regenerate the configuration files (this won't overwrite your <code>models.py</code> or <code>static_data.py</code> unless they don't exist).</p> <pre><code>print(\"Initializing database migration environment in the current directory...\")\n# Use overwrite=True cautiously if you have existing custom configurations\nprt.db.init(overwrite=False)\nprint(\"Initialization complete. Check the 'db' folder.\")\nprint(\"Next step: Edit db/models.py to define your database tables.\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#22-define-data-models","title":"2.2. Define Data Models","text":"<p>Now, you need to edit the <code>db/models.py</code> file.</p> <p>Define your database tables as classes inheriting from <code>SQLModel</code>. Here's an example based on the template created by <code>prt.db.init</code> (uncomment and modify the <code>Hero</code> class):</p> <pre><code># Example content for db/models.py\nfrom typing import Optional\nfrom sqlmodel import Field, SQLModel\n\n# Define your table models here\n# Example:\nclass Hero(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: Optional[int] = Field(default=None, index=True)\n\n# You can add more models/tables below\n# class Item(SQLModel, table=True):\n#     ...\n</code></pre> <p>Action Required: Open <code>db/models.py</code> in the file browser, uncomment or add your <code>SQLModel</code> classes, and save the file.</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#23-create-a-database-revision-prtdbrevision","title":"2.3. Create a Database Revision (<code>prt.db.revision</code>)","text":"<p>After defining or modifying your models in <code>db/models.py</code>, run <code>prt.db.revision</code>. This command inspects your models and compares them to the database state recorded in previous migrations. It then automatically generates a new migration script in the <code>db/migrations/versions/</code> directory.</p> <p>Provide a short, descriptive message (max 75 chars) for the revision.</p> <p>Important: Always review the generated Python script in <code>db/migrations/versions/</code> before proceeding. Auto-detection isn't perfect, especially for complex changes like column renames or constraints. You may need to edit the script manually.</p> <pre><code># Make sure you have edited and saved db/models.py before running this!\nrevision_message = \"Create initial tables\"  # Change this message for subsequent revisions\nprint(f\"Generating database revision with message: '{revision_message}'\")\n\nassert os.environ.get(\"PRT_DB_CONN_STR\"), \"PRT_DB_CONN_STR environment variable not set. Cannot run revision.\"\n\ntry:\n    prt.db.revision(revision_message)\n    print(\"Revision script generated in db/migrations/versions/\")\n    print(\"*** IMPORTANT: Please review the generated script before running upgrade. ***\")\nexcept Exception as e:\n    print(f\"Error generating revision: {e}\")\n    print(\"Check db/models.py for syntax errors and ensure the database is accessible.\")\n    # raise # Optional: Stop if revision fails\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#24-apply-migrations-to-database-prtdbupgrade","title":"2.4. Apply Migrations to Database (<code>prt.db.upgrade</code>)","text":"<p>Once you have reviewed (and if necessary, edited) the migration script(s), use <code>prt.db.upgrade</code> to apply the changes to the actual database.</p> <ul> <li>Online Mode (default): <code>prt.db.upgrade()</code> connects to the database (using <code>PRT_DB_CONN_STR</code>) and executes the migration scripts to bring the schema to the latest version.</li> <li>Offline Mode: <code>prt.db.upgrade(offline=True)</code> does not connect to the database. Instead, it generates a single SQL script containing all the necessary commands to upgrade the schema. You can review this SQL and execute it manually against your database (useful for production environments or when direct access from the notebook is restricted).</li> </ul> <pre><code>print(\"Applying migrations to the database (Online Mode)...\")\n\nassert os.environ.get(\"PRT_DB_CONN_STR\"), \"PRT_DB_CONN_STR environment variable not set. Cannot run upgrade.\"\n\n# --- Online Upgrade (Applies changes directly) ---\ntry:\n    prt.db.upgrade()\n    print(\"Database upgrade complete.\")\nexcept Exception as e:\n    print(f\"Error during online upgrade: {e}\")\n    # raise # Optional: Stop if upgrade fails\n\n# --- Offline Upgrade (Generates SQL script) ---\n# print(\"Generating SQL script for offline upgrade...\")\n# try:\n#     prt.db.upgrade(offline=True)\n#     print(\"SQL script generated. Check the console output.\")\n#     print(\"Review the SQL script and apply it manually to your database.\")\n# except Exception as e:\n#     print(f\"Error during offline upgrade generation: {e}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#3-managing-static-data-prtdbupsert_static_data","title":"3. Managing Static Data (<code>prt.db.upsert_static_data</code>)","text":"<p>Some applications require lookup tables with predefined data (e.g., status types, categories). You can manage this using <code>db/static_data.py</code> and <code>prt.db.upsert_static_data</code>.</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#31-define-static-data","title":"3.1. Define Static Data","text":"<p>Edit the <code>db/static_data.py</code> file. It should contain a function <code>get_static_data()</code> that returns a list of <code>SQLModel</code> instances representing the rows you want to insert or update.</p> <p>Important: For the upsert logic to work correctly (updating existing rows instead of always inserting new ones), your static data models must have static primary keys defined in the data itself, not auto-incrementing keys.</p> <pre><code># Example content for db/static_data.py\n\n# Import your models that hold static data\nfrom .models import EnumLikeModel\n\ndef get_static_data():\n    \"\"\"Returns a list of SQLModel instances for static data.\"\"\"\n    static_data = [\n        # Example using EnumLikeModel (replace with your actual models and data)\n        EnumLikeModel(key=\"pending\", name=\"Pending\"),\n        EnumLikeModel(key=\"in_progress\", name=\"In Progress\"),\n        EnumLikeModel(key=\"completed\", name=\"Completed\", description=\"Task is finished\"),\n        # SomeOtherModel(... \n    ]\n    return static_data\n</code></pre> <p>Action Required: 1. Ensure the models for your static data (e.g., <code>EnumLikeModel</code>) are defined in <code>db/models.py</code>. 2. Ensure these models have primary keys that you can set manually (not auto-incrementing). 3. Edit <code>db/static_data.py</code>, import your models, and populate the list returned by <code>get_static_data()</code>. 4. Make sure you have run <code>prt.db.revision</code> and <code>prt.db.upgrade</code> to create the necessary tables (<code>EnumLikeModel</code> in this example) before trying to upsert data.</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#32-upsert-static-data-to-database","title":"3.2. Upsert Static Data to Database","text":"<p>Run <code>prt.db.upsert_static_data()</code> to load the data defined in <code>db/static_data.py</code> into the database. It connects using <code>PRT_DB_CONN_STR</code> and performs an insert-or-update operation based on the primary key.</p> <pre><code># Make sure you have edited db/static_data.py and the corresponding tables exist in the DB.\nprint(\"Upserting static data from db/static_data.py...\")\n\nassert os.environ.get(\"PRT_DB_CONN_STR\"), \"PRT_DB_CONN_STR environment variable not set. Cannot upsert static data.\"\n\ntry:\n    upsert_summary = prt.db.upsert_static_data()\n    print(\"Upsert operation summary:\")\n    if upsert_summary:\n        for model_name, count in upsert_summary.items():\n            print(f\"- Upserted {count} rows for model: {model_name}\")\n    else:\n        print(\"- No static data found or processed. Check db/static_data.py.\")\nexcept ModuleNotFoundError:\n    print(\n        \"ERROR: Could not import models or static data. Ensure 'db/models.py' and 'db/static_data.py' exist and are correct.\"\n    )\nexcept Exception as e:\n    print(f\"Error upserting static data: {e}\")\n    print(\"Ensure the tables exist (run upgrade) and have static primary keys.\")\n    # raise # Optional: Stop if upsert fails\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#4-schema-evolution-iteration","title":"4. Schema Evolution (Iteration)","text":"<p>As your application requirements change, you'll need to update your database schema:</p> <ol> <li>Modify <code>db/models.py</code>: Add new models (tables), add/remove/modify fields (columns) in existing models.</li> <li>Run <code>prt.db.revision(\"Your descriptive message\")</code>: Generate a new migration script reflecting the changes.</li> <li>Review the script in <code>db/migrations/versions/</code>.</li> <li>Run <code>prt.db.upgrade()</code>: Apply the new migration to the database.</li> </ol> <p>You can repeat this cycle as many times as needed.</p> <pre><code># Example: Suppose you added a 'last_updated' column to the Hero model in db/models.py\n\nprint(\"--- Simulating Schema Evolution ---\")\nprint(\"1. Models modified (manually edit db/models.py)\")\n\nprint(\"2. Generating new revision...\")\ntry:\n    prt.db.revision(\"Add last_updated to Hero\")\n    print(\"Review the new script in db/migrations/versions/\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n\nprint(\"3. Applying upgrade...\")\ntry:\n    prt.db.upgrade()\n    print(\"Upgrade applied.\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#5-using-the-database-at-runtime","title":"5. Using the Database at Runtime","text":"<p>Once your schema is set up, you can interact with the database using standard <code>SQLModel</code> (or SQLAlchemy) patterns in your application code or notebooks. You'll need the connection string, which is typically retrieved from the <code>PRT_DB_CONN_STR</code> environment variable (set directly or via Vault secrets during deployment).</p> <pre><code>from sqlmodel import Session, create_engine\n\n# Ensure models are importable (because we ran init)\n# If you get ModuleNotFoundError, restart kernel after running prt.db.init\ntry:\n    from db.models import Hero\nexcept ModuleNotFoundError:\n    print(\"Could not import from db.models. Did you run prt.db.init and define models?\")\n    print(\"You might need to restart the notebook kernel.\")\n    raise\n\n\ndef add_hero_data():\n    # Retrieve connection string from environment\n    conn_str = os.environ.get(\"PRT_DB_CONN_STR\")\n    if not conn_str:\n        print(\"Error: PRT_DB_CONN_STR not set. Cannot connect to database.\")\n        return\n\n    print(\"Connecting to database using PRT_DB_CONN_STR OS env variable...\")\n    # Set echo=True to see SQL statements\n    engine = create_engine(conn_str, echo=False)\n\n    # Create sample data\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)\n\n    print(\"Adding sample Hero data...\")\n    try:\n        with Session(engine) as session:\n            session.add(hero_1)\n            session.add(hero_2)\n            session.add(hero_3)\n            session.commit()\n            print(\"Data committed successfully.\")\n\n            # Example of retrieving data\n            session.refresh(hero_1)\n            print(\"Refreshed hero_1:\", hero_1)\n\n    except Exception as e:\n        print(f\"Database error during runtime usage: {e}\")\n        print(\"Ensure the table exists and the connection string is valid.\")\n\n\nadd_hero_data()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#6-integration-with-application-deployment","title":"6. Integration with Application Deployment","text":"<p>When deploying your application using <code>prt.apps.deploy</code>, you need to provide the database connection string to the running application container. This is typically done by:</p> <ol> <li>Storing the connection string as a secret in Vault (as done in step 1.2).</li> <li>Mapping this Vault secret to the <code>PRT_DB_CONN_STR</code> environment variable within the application container using the <code>personal_secrets</code> argument in <code>prt.apps.deploy</code>.</li> </ol>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#automatic-database-migration-on-app-deployment","title":"Automatic Database Migration on App Deployment","text":"<p>By default, Practicus AI <code>automatically applies database migrations</code> (effectively running <code>prt.db.upgrade</code> and <code>prt.db.upsert_static_data</code>) whenever your application starts or restarts in a deployed environment, including production settings. This convenient feature helps ensure your database schema is synchronized with your application code. If you need to disable this behavior, for example, to manage migrations manually or through a separate CI/CD pipeline, simply set the environment variable <code>PRT_DB_SKIP_MIGRATION</code> to true in your application's deployment configuration.</p> <pre><code>visible_name = \"My Database App\"\ndescription = \"An example app using prt.db for schema management.\"\nicon = \"fa-database\"\n\nprint(f\"Deploying app: {app_prefix}/{app_name}\")\nprint(f\"Mapping Vault secret '{db_conn_secret_name}' to env var 'PRT_DB_CONN_STR'\")\n\ntry:\n    app_url, api_url = prt.apps.deploy(\n        deployment_setting_key=app_deployment_key,\n        prefix=app_prefix,\n        app_name=app_name,\n        visible_name=visible_name,\n        description=description,\n        icon=icon,\n        # --- Database Connection Mapping ---\n        # Format: \"VAULT_SECRET_NAME:TARGET_ENV_VAR_NAME\"\n        personal_secrets=[f\"{db_conn_secret_name}:PRT_DB_CONN_STR\"],\n        # To disable automated db migration:\n        # env_variables={\"PRT_DB_SKIP_MIGRATION\": \"true\"}\n    )\n\n    print(\"Deployment submitted.\")\n    print(f\"App UI URL (once ready): {app_url}\")\n    print(f\"App API URL (once ready): {api_url}\")\n\nexcept Exception as e:\n    print(f\"Error during deployment submission: {e}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#7-a-guide-for-admins-preparing-a-database-for-your-applications","title":"7. A Guide for Admins: Preparing a Database for Your Applications","text":"<p>Use the following SQL scripts as a guide to create the necessary database structure for your applications.</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#1-create-an-administrative-database-role-user","title":"1. Create an Administrative Database Role (User)","text":"<p>First, create a dedicated administrative role. This role is primarily intended for control plane operations (e.g., automated setup scripts or administrative tools) to manage database artifacts for your applications.</p> <ul> <li>Purpose: This admin role (<code>prt_apps_admin</code> in the example) will be used to create more restricted, application-specific database roles and schemas. It is not intended for direct use by the applications themselves.</li> <li><code>CREATEROLE</code> Permission: This permission is required because the admin role needs the ability to create separate, isolated database roles for each application, enhancing security and separation.</li> <li><code>LOGIN</code> Permission: Allows this role to connect to the database server.</li> </ul> <pre><code>-- Create the administrative role\nCREATE ROLE prt_apps_admin WITH\n  LOGIN\n  CREATEROLE\n  PASSWORD '_your_password_'; -- Replace with a strong, secure password\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#2-create-a-dedicated-database-recommended","title":"2. Create a Dedicated Database (Recommended)","text":"<p>It's best practice to create a separate database specifically for your applications. Hosting this database on a dedicated server instance is also recommended for performance and isolation, if feasible.</p> <ul> <li><code>OWNER prt_apps_admin</code>: Assigning the administrative role as the owner grants it initial full permissions within this new database, simplifying subsequent setup.</li> </ul> <pre><code>-- Create the application database and set the owner\nCREATE DATABASE prt_apps\n  OWNER prt_apps_admin;\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#3-grant-necessary-permissions-to-the-admin-role","title":"3. Grant Necessary Permissions to the Admin Role","text":"<p>Ensure the administrative role has the essential permissions on the newly created database.</p> <ul> <li><code>CONNECT</code> Permission: Allows the <code>prt_apps_admin</code> role to connect specifically to the <code>prt_apps</code> database.</li> <li><code>CREATE</code> Permission: Allows the <code>prt_apps_admin</code> role to create new schemas within the <code>prt_apps</code> database. This is necessary for creating isolated schemas for different applications or services.</li> </ul> <pre><code>-- Grant connection access to the database\nGRANT CONNECT ON DATABASE prt_apps TO prt_apps_admin;\n\n-- Grant permission to create schemas within the database\nGRANT CREATE ON DATABASE prt_apps TO prt_apps_admin;\n</code></pre>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#4-configure-application-deployment-settings","title":"4. Configure Application Deployment Settings","text":"<p>After setting up the administrative database role and database, you need to configure your <code>app deployment setting</code> to use them.</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#a-construct-the-connection-string","title":"a. Construct the Connection String","text":"<p>You will need a standard PostgreSQL connection string to connect Practicus AI to the database you created.</p> <ul> <li>Standard Format: <code>postgresql://user:password@host/database</code></li> <li>Example (using the admin role): <code>postgresql://prt_apps_admin:your_secure_password@your_database_host/prt_apps</code></li> <li>Using Practicus AI Password Management (Recommended): To avoid hardcoding the password directly in the connection string, Practicus AI allows you to use a placeholder (<code>__PASSWORD__</code>) and store the password securely and separately within the platform.<ul> <li>Format: <code>postgresql://user:__PASSWORD__@host/database</code></li> <li>Example: <code>postgresql://prt_apps_admin:__PASSWORD__@prt-db-pg-1-rw.prt-ns.svc.cluster.local/prt_apps</code> (Replace the host <code>prt-db-pg-1-rw.prt-ns.svc.cluster.local</code> with your actual database host address)</li> </ul> </li> </ul>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#b-create-a-system-database-connection-in-practicus-ai","title":"b. Create a System Database Connection in Practicus AI","text":"<p>Next, register this connection within the Practicus AI Admin Console.</p> <ol> <li>Navigate to Data Catalog &gt; System DB Connections.</li> <li>Click to add a new database connection (e.g., name it <code>my-db-for-apps</code>).</li> <li>Enter the connection string constructed in the previous step.</li> <li>Recommendation: Use the connection string format with <code>__PASSWORD__</code> and enter the actual password in the dedicated password field provided by Practicus AI. Save the connection.</li> </ol>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#c-update-application-deployment-settings","title":"c. Update Application Deployment Settings","text":"<p>Finally, link this database connection to your app deployment settings. This tells Practicus AI which database connection to use when apps need to create their own isolated database resources.</p> <ol> <li>Navigate to App Hosting &gt; Deployment Settings.</li> <li>Either add a new deployment setting or edit an existing one.</li> <li>Scroll down to the Database Connection section.</li> <li>Select the System DB Connection you created earlier (e.g., <code>my-db-for-apps</code>) from the dropdown list.</li> <li>Save the deployment settings.</li> </ol> <p>With this configuration complete, users creating new applications or updating existing ones through Practicus AI will now have the capability to automatically provision isolated database schemas and roles using the administrative database connection you've set up.</p>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#8-advanced-notes-and-troubleshooting","title":"8. Advanced Notes and Troubleshooting","text":""},{"location":"technical-tutorial/generative-ai/relational-databases/build/#migration-script-review","title":"Migration Script Review","text":"<ul> <li>Column Renames: Alembic's auto-generation often detects a column rename as a <code>drop_column</code> followed by an <code>add_column</code>, which causes data loss. Review the generated scripts in <code>db/migrations/versions/</code> and manually change these to use <code>op.alter_column(..., new_column_name='...')</code> if you need to preserve data during a rename.</li> <li>Default Values &amp; Constraints: Auto-generation might miss default values, specific constraints, or index types. Check the generated scripts and add or modify Alembic operations (<code>op.</code>) as needed.</li> <li>SQLModel vs. Alembic Types: Ensure the data types in your <code>SQLModel</code> definitions translate correctly to Alembic/SQLAlchemy types in the migration scripts.</li> </ul>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#static-data-notes","title":"Static Data Notes","text":"<ul> <li>As emphasized before, use static primary keys in <code>db/static_data.py</code> for reliable upserts.</li> <li><code>prt.db.upsert_static_data()</code> runs within a transaction. If any part fails, the entire operation for that run is rolled back.</li> </ul>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#environment-variables","title":"Environment Variables","text":"<ul> <li>Changes to environment variables like <code>PRT_DB_CONN_STR</code> within a notebook session might not be picked up by subprocesses (like Alembic calls within <code>prt.db</code>) unless the kernel is restarted.</li> </ul>"},{"location":"technical-tutorial/generative-ai/relational-databases/build/#file-locations","title":"File Locations","text":"<ul> <li><code>prt.db.init</code>, <code>revision</code>, <code>upgrade</code>, <code>upsert_static_data</code> all expect the <code>db</code> directory (containing <code>models.py</code>, <code>static_data.py</code>, <code>migrations.ini</code>, <code>migrations/</code>) to be in the current working directory where the Python command/notebook cell is executed.</li> </ul> <p>Previous: Milvus | Next: Message Queues &gt; Build</p>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/","title":"Milvus","text":""},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#milvus-vector-database-sample","title":"Milvus vector database sample","text":"<p>This example demonstrates the basic operations of PyMilvus, a Python SDK of Milvus.</p>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#before-you-begin","title":"Before you begin","text":"<p>Please make sure that you have a running Milvus instance.</p> <pre><code>milvus_host = None  # E.g. practicus-milvus.prt-ns-milvus.svc.cluster.local\nmilvus_uri = f\"http://{milvus_host}\"\nmilvus_port = 19530\nmilvus_token = \"root:Milvus\"  # Change this after installation\n</code></pre> <pre><code>assert milvus_host, \"Please enter your Milvus host.\"\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#steps","title":"Steps","text":"<ol> <li>connect to Milvus</li> <li>create collection</li> <li>insert data</li> <li>create index</li> <li>search, query, and hybrid search on entities</li> <li>delete entities by PK</li> <li>drop collection</li> </ol> <pre><code>import numpy as np\nimport time\n\nfrom pymilvus import (\n    connections,\n    utility,\n    FieldSchema,\n    CollectionSchema,\n    DataType,\n    Collection,\n)\n\nfmt = \"\\n=== {:30} ===\\n\"\nsearch_latency_fmt = \"search latency = {:.4f}s\"\nnum_entities, dim = 3000, 8\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#1-connect-to-milvus","title":"1. connect to Milvus","text":"<p>Add a new connection alias <code>default</code> for Milvus server in <code>localhost:19530</code>. </p> <p>Actually the <code>default</code> alias is a building in PyMilvus. If the address of Milvus is the same as <code>localhost:19530</code>, you can omit all parameters and call the method as: <code>connections.connect()</code>.</p> <p>Note: the <code>using</code> parameter of the following methods is default to \"default\".</p> <pre><code>connections.connect(\"default\", host=milvus_host, port=milvus_port, token=milvus_token)\n\nhas = utility.has_collection(\"hello_milvus\")\nprint(f\"Does collection hello_milvus exist in Milvus: {has}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#2-create-collection","title":"2. create collection","text":"<p>We're going to create a collection with 3 fields.</p> field name field type other attributes field description 1 \"pk\" VARCHAR is_primary=True, auto_id=False \"primary field\" 2 \"random\" Double \"a double field\" 3 \"embeddings\" FloatVector dim=8 \"float vector with dim 8\" <pre><code>fields = [\n    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n    FieldSchema(name=\"random\", dtype=DataType.DOUBLE),\n    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=dim),\n]\n\nschema = CollectionSchema(fields, \"hello_milvus is the simplest demo to introduce the APIs\")\n\nhello_milvus = Collection(\"hello_milvus\", schema, consistency_level=\"Strong\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#3-insert-data","title":"3. insert data","text":"<p>We are going to insert 3000 rows of data into <code>hello_milvus</code>. Data to be inserted must be organized in fields.</p> <p>The insert() method returns: - either automatically generated primary keys by Milvus if auto_id=True in the schema; - or the existing primary key field from the entities if auto_id=False in the schema.</p> <pre><code>rng = np.random.default_rng(seed=19530)\nentities = [\n    # provide the pk field because `auto_id` is set to False\n    [str(i) for i in range(num_entities)],\n    rng.random(num_entities).tolist(),  # field random, only supports list\n    rng.random((num_entities, dim)),  # field embeddings, supports numpy.ndarray and list\n]\n\ninsert_result = hello_milvus.insert(entities)\n\nprint(f\"Number of entities in Milvus: {hello_milvus.num_entities}\")  # check the num_entites\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#4-create-index","title":"4. create index","text":"<p>We are going to create an IVF_FLAT index for hello_milvus collection.</p> <p>create_index() can only be applied to <code>FloatVector</code> and <code>BinaryVector</code> fields.</p> <pre><code>index = {\n    \"index_type\": \"IVF_FLAT\",\n    \"metric_type\": \"L2\",\n    \"params\": {\"nlist\": 128},\n}\n\nhello_milvus.create_index(\"embeddings\", index)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#5-search-query-and-hybrid-search","title":"5. search, query, and hybrid search","text":"<p>After data were inserted into Milvus and indexed, you can perform: - search based on vector similarity - query based on scalar filtering(boolean, int, etc.) - hybrid search based on vector similarity and scalar filtering.</p> <p>Before conducting a search or a query, you need to load the data in <code>hello_milvus</code> into memory.</p> <pre><code>hello_milvus.load()\n</code></pre> <p>Search based on vector similarity</p> <pre><code>vectors_to_search = entities[-1][-2:]\nsearch_params = {\n    \"metric_type\": \"L2\",\n    \"params\": {\"nprobe\": 10},\n}\n\nstart_time = time.time()\nresult = hello_milvus.search(vectors_to_search, \"embeddings\", search_params, limit=3, output_fields=[\"random\"])\nend_time = time.time()\n\nfor hits in result:\n    for hit in hits:\n        print(f\"hit: {hit}, random field: {hit.entity.get('random')}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre> <p>Query based on scalar filtering(boolean, int, etc.)</p> <p>Start querying with <code>random &gt; 0.5</code></p> <pre><code>start_time = time.time()\nresult = hello_milvus.query(expr=\"random &gt; 0.5\", output_fields=[\"random\", \"embeddings\"])\nend_time = time.time()\n\nprint(f\"query result:\\n-{result[0]}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre> <p>Hybrid search</p> <p>Start hybrid searching with <code>random &gt; 0.5</code></p> <pre><code>start_time = time.time()\nresult = hello_milvus.search(\n    vectors_to_search, \"embeddings\", search_params, limit=3, expr=\"random &gt; 0.5\", output_fields=[\"random\"]\n)\nend_time = time.time()\n\nfor hits in result:\n    for hit in hits:\n        print(f\"hit: {hit}, random field: {hit.entity.get('random')}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#6-delete-entities-by-pk","title":"6. delete entities by PK","text":"<p>You can delete entities by their PK values using boolean expressions.</p> <pre><code>ids = insert_result.primary_keys\nexpr = f'pk in [\"{ids[0]}\", \"{ids[1]}\"]'\n\nresult = hello_milvus.query(expr=expr, output_fields=[\"random\", \"embeddings\"])\nprint(f\"query before delete by expr=`{expr}` -&gt; result: \\n-{result[0]}\\n-{result[1]}\\n\")\n\nhello_milvus.delete(expr)\n\nresult = hello_milvus.query(expr=expr, output_fields=[\"random\", \"embeddings\"])\nprint(f\"query after delete by expr=`{expr}` -&gt; result: {result}\\n\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#7-drop-collection","title":"7. drop collection","text":"<p>Finally, drop the hello_milvus collection</p> <pre><code>utility.drop_collection(\"hello_milvus\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/milvus/#8-creating-new-users","title":"8. Creating new users","text":"<p>For production, please create restricted users.</p> <p>Sample code: <pre><code>from pymilvus import MilvusClient\n\nclient = MilvusClient(\n    uri=milvus_uri,\n    token=milvus_token\n) \n\nuser_name = \"user_1\"\nprint(\"Creating user:\", user_name)\nclient.create_user(\n    user_name=user_name,\n    password=\"P@ssw0rd\",\n)\n\nuser = client.describe_user(user_name)\nprint(\"User created:\", user)\n# prints {'user_name': 'user_1', 'roles': ()}\n\nclient.drop_user(user_name=user_name)\nprint(\"Dropping user:\", user_name)\n</code></pre></p> <p>To learn more: https://milvus.io/docs/authenticate.md#Create-a-new-user</p> <p>Previous: Qdrant | Next: Relational Databases &gt; Build</p>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/","title":"Qdrant vector database sample","text":"<p>This example demonstrates the basic operations using <code>qdrant-client</code>, the Python SDK for the Qdrant vector database.</p>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#before-you-begin","title":"Before you begin","text":"<p>Please make sure that you have access to a running Qdrant instance and have installed the <code>qdrant-client</code> library (<code>pip install qdrant-client</code>).</p> <pre><code># Qdrant connection details (replace with your actual endpoint and key if needed)\n# Ensure the Qdrant instance is running and accessible from this notebook environment.\nQDRANT_URL = \"http://practicus-qdrant.prt-ns-qdrant.svc.cluster.local:6333\"  # Example URL from request\nQDRANT_API_KEY = \"my-api-key\"  # Example API Key from request (use None if no key is required)\n</code></pre> <pre><code>assert QDRANT_URL, \"Please enter your Qdrant connection URL in the cell above.\"\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#steps","title":"Steps","text":"<ol> <li>Connect to Qdrant</li> <li>Create a collection</li> <li>Insert data (Points)</li> <li>Search with filtering</li> <li>Delete points by ID</li> <li>Drop the collection</li> </ol> <pre><code>import numpy as np\nimport time\nimport uuid\n\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, PointIdsList\n\nfmt = \"\\n=== {:30} ===\\n\"\nsearch_latency_fmt = \"search latency = {:.4f}s\"\n\n# Parameters from the original notebook\nCOLLECTION_NAME = \"demo_qdrant_collection\"  # Renamed for clarity\nNUM_ENTITIES, DIM = 3000, 8\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#connect-to-qdrant","title":"Connect to Qdrant","text":"<p>Initialize the Qdrant client using the specified URL and API Key.</p> <pre><code>client = QdrantClient(\n    url=QDRANT_URL,\n    api_key=QDRANT_API_KEY,\n    timeout=60,  # Optional: Increase timeout for slow connections or large operations\n)\n\n# Optional: Check connection by listing collections or getting cluster info\ntry:\n    # Attempt to get cluster info as a basic health check\n    cluster_info = client.info()\n    print(\"Successfully connected to Qdrant.\")\n    print(f\"Qdrant cluster info: {cluster_info}\")\nexcept Exception as e:\n    print(f\"Failed to connect to Qdrant or get cluster info: {e}\")\n    # Depending on the error, you might want to stop execution here\n    # raise e\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#create-a-collection","title":"Create a collection","text":"<p>We'll create a collection named <code>demo_qdrant_collection</code>.</p> <p>Qdrant allows defining the vector parameters (size and distance metric) during collection creation.</p> <p>We will use: - size: The dimension of the vectors (DIM=8). - distance: The distance metric for similarity search (e.g., Cosine, Dot, Euclid). We'll use Cosine here.</p> <pre><code># Ensure the collection doesn't already exist for a clean run\ntry:\n    print(f\"Checking if collection '{COLLECTION_NAME}' exists...\")\n    collection_exists = client.collection_exists(collection_name=COLLECTION_NAME)\n\n    if collection_exists:\n        print(f\"Collection '{COLLECTION_NAME}' already exists. Deleting it first.\")\n        client.delete_collection(collection_name=COLLECTION_NAME)\n        print(f\"Collection '{COLLECTION_NAME}' deleted. Waiting a moment...\")\n        time.sleep(2)  # Give Qdrant a moment to process the deletion\n    else:\n        print(f\"Collection '{COLLECTION_NAME}' does not exist. Proceeding to create.\")\n\n    # Create the collection\n    print(f\"Creating collection '{COLLECTION_NAME}'...\")\n    client.create_collection(\n        collection_name=COLLECTION_NAME,\n        vectors_config=VectorParams(size=DIM, distance=Distance.COSINE),\n        # Optional: Add optimizers_config, hnsw_config, quantization_config etc. here if needed\n        # Example: optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n        # Example: hnsw_config=models.HnswConfigDiff(m=16, ef_construct=100)\n    )\n    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n\nexcept Exception as e:\n    print(f\"An error occurred during collection setup: {e}\")\n    # raise e\n\n# Verify creation by getting collection info\ntry:\n    collection_info = client.get_collection(collection_name=COLLECTION_NAME)\n    print(fmt.format(\"Collection Info\"))\n    print(collection_info)\nexcept Exception as e:\n    print(f\"Error getting collection info for '{COLLECTION_NAME}': {e}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#insert-data-points","title":"Insert data (Points)","text":"<p>We are going to insert <code>NUM_ENTITIES</code> (3000) points into the collection.</p> <p>Each point requires: - <code>id</code>: A unique identifier (integer or UUID string). We'll use integers here for simplicity. - <code>vector</code>: The vector embedding. - <code>payload</code> (optional): Additional data associated with the vector (like the 'random' field from the original notebook).</p> <pre><code>print(fmt.format(\"Prepare Data\"))\nrng = np.random.default_rng(seed=19530)\nrandom_payload_data = rng.random(NUM_ENTITIES).tolist()  # field 'random'\nembedding_vectors = rng.random((NUM_ENTITIES, DIM)).astype(\"float32\")  # field 'embeddings'\n\n# Generate simple integer IDs\npoint_ids = list(range(NUM_ENTITIES))\n# Alternatively, use UUIDs (recommended for production):\n# point_ids = [str(uuid.uuid4()) for _ in range(NUM_ENTITIES)]\n\n# Create PointStruct objects\npoints_to_insert = [\n    PointStruct(\n        id=point_ids[i],\n        vector=embedding_vectors[i].tolist(),  # Pass vector as a list\n        payload={\"random\": random_payload_data[i]},  # Payload is a dictionary\n    )\n    for i in range(NUM_ENTITIES)\n]\nprint(f\"Prepared {len(points_to_insert)} points for insertion.\")\n\nprint(fmt.format(\"Insert Data\"))\n# Upsert points into the collection\n# `upsert` inserts new points or updates existing ones with the same ID.\n# Using `wait=True` ensures the operation is completed before proceeding.\ntry:\n    # Insert in batches if NUM_ENTITIES is very large\n    # batch_size = 500\n    # for i in range(0, NUM_ENTITIES, batch_size):\n    #    batch_points = points_to_insert[i:i+batch_size]\n    #    op_info = client.upsert(collection_name=COLLECTION_NAME, points=batch_points, wait=True)\n    #    print(f\"Upserted batch {i//batch_size + 1}, status: {op_info.status}\")\n\n    # Insert all at once for smaller datasets\n    op_info = client.upsert(collection_name=COLLECTION_NAME, points=points_to_insert, wait=True)\n    print(f\"Upsert operation completed with status: {op_info.status}\")\n\nexcept Exception as e:\n    print(f\"Error during upsert operation: {e}\")\n    # raise e\n\n# Check the number of points in the collection\ntry:\n    count_result = client.count(collection_name=COLLECTION_NAME, exact=True)\n    print(f\"Number of points in collection '{COLLECTION_NAME}': {count_result.count}\")\nexcept Exception as e:\n    print(f\"Error counting points: {e}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#search-with-filtering","title":"Search with filtering","text":"<p>Qdrant allows filtering search results based on payload fields.</p> <p>We'll retrieve points where the <code>random</code> field is greater than 0.9. For retrieving potentially many results based only on filters, <code>scroll</code> is often more suitable than <code>search</code>.</p> <pre><code>print(fmt.format(\"Search with Filtering (Scroll)\"))\n\n# Define the filter: payload field 'random' &gt; 0.9\nscroll_filter = Filter(\n    must=[\n        FieldCondition(\n            key=\"random\",  # Payload field key\n            range=Range(gt=0.9),  # Condition: greater than 0.9\n        )\n    ]\n)\n\nstart_time = time.time()\ntry:\n    # Use scroll to retrieve points matching the filter\n    # 'scroll' is suitable for iterating through large result sets.\n    scroll_result, next_page_offset = client.scroll(\n        collection_name=COLLECTION_NAME,\n        scroll_filter=scroll_filter,\n        limit=10,  # Retrieve first 10 matching points\n        with_payload=True,\n        with_vectors=False,  # We don't need the vectors for this query\n    )\n    end_time = time.time()\n\n    print(f\"Found {len(scroll_result)} points with 'random' &gt; 0.9 (limit 10):\")\n    for record in scroll_result:\n        print(f\"  Point ID: {record.id}, Payload: {record.payload}\")\n    # You can use next_page_offset to paginate through more results if needed\n    # print(f\"Next page offset: {next_page_offset}\")\n    print(search_latency_fmt.format(end_time - start_time))\n\nexcept Exception as e:\n    end_time = time.time()\n    print(f\"Error during filtered scroll: {e}\")\n    print(f\"Time elapsed before error: {end_time - start_time:.4f}s\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#delete-points-by-id","title":"Delete points by ID","text":"<p>You can delete points from the collection using their IDs.</p> <pre><code>print(fmt.format(\"Delete Points\"))\n\n# IDs to delete (e.g., the first two points we inserted)\n# Ensure these IDs exist and match the type used (int or UUID)\nids_to_delete = point_ids[0:2]  # e.g., [0, 1]\n\n# Check points before deletion (optional)\ntry:\n    points_before = client.retrieve(\n        collection_name=COLLECTION_NAME,\n        ids=ids_to_delete,\n        with_payload=False,  # Don't need payload for existence check\n        with_vectors=False,\n    )\n    print(f\"Points found before deletion for IDs {ids_to_delete}: {len(points_before)}\")\n    # print(points_before)\nexcept Exception as e:\n    print(f\"Error retrieving points before deletion: {e}\")\n\n# Delete the points\ntry:\n    delete_result = client.delete(\n        collection_name=COLLECTION_NAME,\n        points_selector=PointIdsList(points=ids_to_delete),\n        wait=True,  # Wait for operation to complete\n    )\n    print(f\"Deletion operation completed with status: {delete_result.status}\")\nexcept Exception as e:\n    print(f\"Error during point deletion: {e}\")\n\n# Verify deletion (optional)\ntry:\n    points_after = client.retrieve(collection_name=COLLECTION_NAME, ids=ids_to_delete)\n    print(f\"Points found after deletion for IDs {ids_to_delete}: {len(points_after)}\")  # Should be 0\n\n    count_after_delete = client.count(collection_name=COLLECTION_NAME, exact=True)\n    print(\n        f\"Total points remaining in collection: {count_after_delete.count}\"\n    )  # Should be NUM_ENTITIES - len(ids_to_delete)\n\nexcept Exception as e:\n    print(f\"Error retrieving points after deletion: {e}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#drop-the-collection","title":"Drop the collection","text":"<p>Finally, clean up by dropping the collection.</p> <pre><code>print(fmt.format(\"Drop Collection\"))\ntry:\n    client.delete_collection(collection_name=COLLECTION_NAME)\n    print(f\"Collection '{COLLECTION_NAME}' dropped successfully.\")\nexcept Exception as e:\n    print(f\"Error dropping collection '{COLLECTION_NAME}': {e}\")\n\n# Verify deletion\ntry:\n    exists_after_drop = client.collection_exists(collection_name=COLLECTION_NAME)\n    print(f\"Does collection '{COLLECTION_NAME}' exist after drop? {exists_after_drop}\")  # Should be False\nexcept Exception as e:\n    print(f\"Error checking collection existence after drop: {e}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/qdrant/#note-on-qdrant-admin-ui","title":"Note on Qdrant Admin UI","text":"<p>If your Qdrant instance has the Web UI enabled (which is common), you might be able to access it through a browser. This UI allows you to inspect collections, points, search, and manage the cluster visually.</p> <p>Ask your administrator for the URL and any necessary credentials if you wish to use the Qdrant Admin UI.</p> <p>Previous: Embeddings | Next: Milvus</p>"},{"location":"technical-tutorial/getting-started/add-ons/","title":"Practicus AI Add-ons","text":"<p>Practicus AI Add-ons represent external services integrated into your Practicus AI environment. These can include </p> <ul> <li>Airflow for workflows</li> <li>MLflow for experiment tracking</li> <li>Observability tools</li> <li>Analytics Services</li> <li>and more..</li> </ul> <p>In addition to Practicus AI Home and AI Sudio, you can also view them, and open their interfaces directly from the SDK.</p> <pre><code>import practicuscore as prt\n\n# Add-ons are tied to a Practicus AI region\nregion = prt.current_region()\n\n# Ensure that add-ons are available\nif len(region.addon_list) == 0:\n    raise NotImplementedError(\"No add-ons installed.\")\n</code></pre> <pre><code># Iterate over available add-ons\nfor addon in region.addon_list:\n    print(\"Add-on key:\", addon.key)\n    print(\"  URL:\", addon.url)\n</code></pre> <pre><code># Printing addon_list directly returns a CSV-like formatted text\nprint(region.addon_list)\n</code></pre> <pre><code># Convert addon_list to a pandas DataFrame for convenience\nregion.addon_list.to_pandas()\n</code></pre> <pre><code>first_addon = region.addon_list[0]\n# Accessing an add-on object prints its details\nfirst_addon\n</code></pre> <pre><code># Open the add-on in your default browser\nfirst_addon.open()\n</code></pre> <pre><code># If you know the add-on key, you can open it directly from the region\naddon_key = first_addon.key\nregion.open_addon(addon_key)\n</code></pre> <pre><code># Search for a specific add-on by key\naddon_key = first_addon.key\nfound_addon = region.get_addon(addon_key)\nassert found_addon, f\"Addon {addon_key} not found\"\nfound_addon.open()\n</code></pre> <p>Previous: Workspaces | Next: Modeling &gt; Introduction</p>"},{"location":"technical-tutorial/getting-started/introduction/","title":"Introduction","text":"<p>Getting started with Practicus AI is straightforward.</p>"},{"location":"technical-tutorial/getting-started/introduction/#typical-practicus-ai-usage","title":"Typical Practicus AI usage","text":"<p>The following steps outline a typical scenario for users who write code:</p> <ol> <li>Log in to your chosen region (e.g., <code>https://practicus.your-company.com</code>).</li> <li>Create one or more workers with the desired features and resource capacities.</li> <li>Start an IDE, such as JupyterLab or VS Code, within a worker.</li> <li>Develop models, applications, and process data as usual.</li> <li>Deploy models, applications, or use add-ons (e.g., create Airflow workflows).</li> <li>Observe metrics, logs, events, errors. Create alerts.</li> </ol> <p></p>"},{"location":"technical-tutorial/getting-started/introduction/#leveraging-documentation-and-developer-tooling","title":"Leveraging Documentation and Developer Tooling","text":"<p>1. Access the SDK Documentation:    Experienced coders understand that having immediate access to detailed SDK references accelerates the development lifecycle. You can refer to the Practicus AI SDK Documentation to understand package structures, classes, methods, and parameters. This robust, searchable reference ensures you can quickly find the API calls needed to interact with Practicus AI resources programmatically.</p> <p>2. Utilize IntelliSense and Contextual Help in JupyterLab or VS Code:    When working within JupyterLab or VS Code, take advantage of built-in IntelliSense (auto-completion) capabilities. As you type, your IDE will surface method signatures, docstrings, and parameter hints\u2014especially helpful for complex ML pipelines or when invoking intricate model-serving APIs.  </p>"},{"location":"technical-tutorial/getting-started/introduction/#contextual-help-with-jupyter-lab","title":"Contextual Help with Jupyter Lab","text":"<ul> <li> <p>Contextual Tooltips: Hover over classes and methods to see in-line docstrings and parameter descriptions. This \u201cjust-in-time\u201d help enables you to craft pipelines, preprocess data, or orchestrate model inference steps without constantly switching between your IDE and external docs.  </p> </li> <li> <p>Shift-Tab: Inside a Jupyter notebook, pressing <code>Shift+Tab</code> while your cursor is within a function call will reveal type hints, default values, and docstrings. This immediate feedback reduces trial-and-error and makes coding more efficient and error-free.</p> </li> </ul> <p></p> <p>Combining direct SDK reference materials with IntelliSense-driven guidance ensures data scientists spend more time crafting robust models and less time hunting down syntax or function definitions.</p> <ul> <li>Contextual Help Tab: You can also right-click on a cell, select \"Show Contextual Help\" to leave the help tab always open.</li> </ul> <p></p>"},{"location":"technical-tutorial/getting-started/introduction/#contextual-help-with-vs-code","title":"Contextual Help with VS Code","text":"<ul> <li>Ctrl+Space for Inline Help: In VS Code, pressing <code>Ctrl+Space</code> triggers IntelliSense to display inline suggestions, completion items, and parameter hints. This built-in guidance makes it easy to discover available functions, understand their expected parameters, and review docstrings\u2014all without leaving your editor window.</li> </ul> <ul> <li>Jupyter Panel: You can also keep the Jupyter panel open in VS Code for continuous, context-sensitive help as you work. This panel remains visible as you code, providing an always-on reference for classes, methods, and type hints.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-platform-components","title":"Practicus AI Platform Components","text":"<p>Below are the primary components you will interact with when using Practicus AI.</p>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-workers","title":"Practicus AI Workers","text":"<p>Practicus AI Workers are dedicated compute environments that run ML, data processing, and other tasks.</p> <p>Key characteristics include:</p> <ul> <li>On-demand: Request as many workers as you need, available within seconds.</li> <li>Interactive: Launch JupyterLab or VS Code for hands-on experimentation.</li> <li>Batch-capable: Run tasks or jobs in non-interactive mode as well.</li> <li>Isolated: Issues in other workers or systems do not affect your worker.</li> <li>Configurable: Each worker is defined by a container image, which can be chosen from the provided options or customized.</li> <li>Flexible Resources: Assign a specific amount of CPU, memory, and GPU resources.</li> <li>Ephemeral: Workers can be replaced easily. Since each restart resets the file system, save important files in <code>~/my</code> or <code>~/shared</code> to preserve them, or push to a source control system such as git.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-modelhost","title":"Practicus AI ModelHost","text":"<p>Practicus AI ModelHost deployments run classic ML and LLM models, optimized for CPUs and GPUs.</p> <ul> <li>Shared deployments can host thousands of models, each with up to 100 versions.</li> <li>Isolated deployments allow you to create a dedicated environment for a set of models.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-apphost","title":"Practicus AI AppHost","text":"<p>Practicus AI AppHost deployments are used to build visual Gen AI applications or microservices focused on ML workflows.</p>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-add-ons","title":"Practicus AI Add-ons","text":"<p>Practicus AI Add-ons, such as Airflow or MLflow, extend the platform\u2019s core functionality. They integrate seamlessly, allowing you to manage and orchestrate complex workflows and track experiments.</p>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-regions","title":"Practicus AI Regions","text":"<p>Practicus AI is a multi-region environment, where each region is a separate deployment and isolated Kubernetes namespace. Regions can differ by geography, cloud vendor, lifecycle stage, department, or security requirements.</p> <p>For example, you might have:</p> <ul> <li>One region in a certain geographic location and another in a different one.</li> <li>Regions across different cloud vendors (e.g., AWS, Azure, on-premises).</li> <li>Separate regions for production, development, or testing.</li> <li>Regions dedicated to different departments or security contexts.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-clients","title":"Practicus AI Clients","text":"<p>Practicus AI clients enable you to connect to multiple regions seamlessly, allowing you to develop in one region and deploy in another. Common client options include:</p> <ul> <li>Browser: Access the platform via a standard web interface to launch JupyterLab, VS Code, and manage workloads.</li> <li>AI Studio: A desktop application for Windows, macOS, and Linux that connects to multiple regions for unified management.</li> <li>SDK: Install the SDK (<code>pip install practicuscore</code>) to interact programmatically with any Practicus AI region.</li> <li>CLI: With the SDK installed, use the <code>prtcli</code> command-line tool to manage tasks and resources.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#example-a-multi-region-setup","title":"Example: A Multi-Region Setup","text":"<p>Below is an example of a deployment where a customer utilizes three regions in two geographies, accessible through various clients.</p> <p></p> <p>Next: Workers</p>"},{"location":"technical-tutorial/getting-started/workers/","title":"Practicus AI Workers","text":"<p>This example demonstrates a typical workflow for using Practicus AI Workers:</p> <p>Create a Worker:  </p> <p>Request a new worker with the required resources (CPU, RAM, GPU) from Practicus AI. This usually takes a few seconds.</p> <p>Open JupyterLab or VS Code:</p> <p>Once the worker is ready, launch JupyterLab or VS Code directly on it. Within this environment, you can:</p> <ul> <li>Develop and run code interactively</li> <li>Explore and process data</li> <li>Train and evaluate machine learning models</li> </ul> <p>Perform Tasks:  </p> <p>Inside JupyterLab or VS Code, run Python notebooks, scripts, or leverage integrated libraries and frameworks.</p> <p>Terminate the Worker:  </p> <p>After finishing your tasks, stop or delete the worker. You can always start a new one later, ensuring a clean environment each time.</p> <p>This approach provides an isolated, on-demand environment for efficient development, scaling, and maintaining a clean slate for each new task.</p>"},{"location":"technical-tutorial/getting-started/workers/#creating-a-worker-with-default-settings","title":"Creating a Worker with Default Settings","text":"<p>Let's start by creating a worker with the default configuration.</p>"},{"location":"technical-tutorial/getting-started/workers/#define-parameters","title":"Define Parameters","text":"<pre><code>worker_image = \"practicus-genai\"\nworker_size = \"X-Small\"\nstartup_script = \"\"\"\necho \"Hello Practicus AI\" &gt; ~/hello.txt\n\"\"\"\n</code></pre> <pre><code>assert worker_image, \"Please select a worker_image.\"\nassert worker_size, \"Please select a worker_size.\"\n</code></pre> <pre><code># Import the Practicus AI SDK\nimport practicuscore as prt\n</code></pre> <pre><code># Create a worker using default settings\nworker = prt.create_worker()\n</code></pre> <pre><code># Start JupyterLab on the worker and open it in a new browser tab\nworker.open_notebook()\n\n# To use VS Code instead of JupyterLab, uncomment the line below:\n# worker.open_vscode()\n</code></pre> <pre><code># After using the worker, terminate it:\nworker.terminate()\n\n# If you're inside a worker environment, you can self-terminate by running:\n# prt.get_local_worker().terminate()\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#creating-a-customized-worker","title":"Creating a Customized Worker","text":"<p>Now let's create a worker with a custom configuration, specifying a custom image, size, and a startup script.</p> <pre><code># Define a custom worker configuration\nworker_config = prt.WorkerConfig(\n    worker_image=worker_image,\n    worker_size=worker_size,\n    startup_script=startup_script,\n)\n\n# Create the worker with the custom configuration\nworker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Verify that hello.txt exists in the home directory:\nworker.open_notebook()\n</code></pre> <pre><code>worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#working-with-the-region-class","title":"Working with the Region Class","text":"<ul> <li>You can interact with multiple regions and perform most operations by using the Region class.</li> <li>If running inside a worker, you can easily access the current region and perform actions directly.</li> </ul> <pre><code># Get the current region\nregion = prt.current_region()\n\n# You could also connect to a different region\n# region = prt.get_region(\"username@some-other-region.com\")\n</code></pre> <pre><code>print(\"Current region:\")\nregion\n</code></pre> <p>It will print something like:</p> <pre><code>key: my-user-name@practicus.your-company.com\nurl: https://practicus.your-company.com\nusername: my-user-name\nemail: my-user-name@your-company.com\nis_default: True\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#worker-sizes","title":"Worker Sizes","text":"<p>Worker sizes define the CPU, RAM, GPU, and other resources allocated to a worker.</p> <pre><code># List available worker sizes\nfor worker_size in region.worker_size_list:\n    print(worker_size.name)\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#smart-listing-with-prtlist","title":"Smart Listing with PrtList","text":"<p>PrtList is a specialized list type that can be toggled as read-only and easily converted to CSV, DataFrame, or JSON. Many results returned by the SDK are <code>PrtList</code> objects.</p> <pre><code># Convert worker sizes to a pandas DataFrame\ndf = region.worker_size_list.to_pandas()\ndisplay(df)\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#worker-images","title":"Worker Images","text":"<p>Worker images define the base container image and features available on the worker.</p> <pre><code>df = region.worker_image_list.to_pandas()\nprint(\"Available worker images:\")\ndisplay(df)\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#worker-logs","title":"Worker Logs","text":"<p>You can view the logs of a worker to debug issues or review activities.</p> <pre><code>if prt.running_on_a_worker():\n    print(\"Code is running on a worker, will use 'self' (local worker).\")\n    worker = prt.get_local_worker()\nelse:\n    print(\"Code not running on a worker, creating a new one.\")\n    worker = prt.create_worker()\n\nprint(\"Worker logs:\")\nworker.view_logs()\n\nworker_logs = worker.get_logs()\nif \"some error\" in worker_logs:\n    print(\"Found 'some error' in logs\")\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#advanced-worker-lifecycle-control","title":"Advanced Worker Lifecycle Control","text":"<p>In advanced scenarios, you might need more granular control over a worker's startup and readiness. This is particularly useful for:</p> <ul> <li>Extended Wait Times: Allowing more time for workers to become ready, especially when using large container images (e.g., GPU-enabled images) that can take a significant time to download and initialize, especially on their first use on a node.</li> <li>Proactive Termination: Automatically terminating a worker if it fails to become ready within a specified timeframe, which can occur due to issues like resource capacity constraints or prolonged provisioning.</li> </ul> <p>To enable this custom lifecycle management, you can disable the default readiness check behavior. This is achieved by setting the <code>wait_until_ready=False</code> parameter in the <code>prt.create_worker()</code> function call. You can then implement your own logic to define how long to wait for the worker and what action to take if it doesn't become ready within your defined timeout.</p> <p>Important Note: Practicus AI will not automatically terminate the worker if it's readiness time-outs. This design choice allows administrators to investigate and troubleshoot the underlying cause of the worker failing to become ready. Therefore, your custom logic should explicitly include steps to terminate the worker if that is the desired outcome after a timeout.</p> <p>Sample Code:</p> <pre><code>import practicuscore as prt\n\n# Requesting a fairly large Worker size.\n# Let's assume we will not have such capacity available..\nworker_size = \"2X-Large\"\n\nworker_config = prt.WorkerConfig(\n    worker_size=worker_size,\n)\n\n# Create the worker instance, disabling the default readiness check\n# This allows for custom handling of the worker's ready state.\nworker = prt.create_worker(\n    worker_config=worker_config,\n    wait_until_ready=False,  # Turn off the default blocking readiness check\n)\n\n# Define a custom timeout period in seconds for the worker to become ready\ntimeout_seconds = 120  # Example: 2 minutes, default is a minute.\n\ntry:\n    print(f\"Waiting up to {timeout_seconds} seconds for worker '{worker.name}' to be ready...\")\n    worker.wait_until_ready(timeout=timeout_seconds)\n    print(f\"Worker '{worker.name}' is now ready!\")\n    # Worker is ready, proceed with your tasks..\nexcept TimeoutError:\n    # This block executes if the worker does not become ready within the timeout period.\n    # For Kubernetes admins: you will see the Worker pod is stuck in `pending` state due to capacity issues.\n    print(f\"Error: Worker '{worker.name}' did not become ready within {timeout_seconds} seconds.\")\n    print(f\"Terminating worker '{worker.name}' due to readiness timeout.\")\n    # For Kubernetes admins: the below will remove the Worker pod, preventing auto-provisioning \n    #   if capacity becomes available later..\n    worker.terminate()\nexcept Exception as e:\n    # Catch any other potential exceptions during the process\n    print(f\"An unexpected error occurred: {e}\")\n    print(f\"Consider terminating worker '{worker.name}' manually if it's still running.\")\n    worker.terminate()  # Optionally terminate on other errors as well\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#cleaning-up-workers-stuck-in-the-provisioning-state","title":"Cleaning Up Workers Stuck in the <code>Provisioning</code> State","text":"<p>Occasionally, a Worker may remain in the <code>Provisioning</code> state longer than expected. You can identify and terminate such Workers in batch using the code below:</p> <pre><code>import practicuscore as prt\nfrom datetime import datetime, timezone\n\ntimeout_seconds = 120  # Threshold for considering a Worker as stuck\n\nregion = prt.current_region()\n\n# Iterate through all Workers in the region\nfor worker in region.worker_list:\n    time_since_creation = int((datetime.now(timezone.utc) - worker.creation_time).total_seconds())\n    print(f\"{worker.name} started {time_since_creation} seconds ago and is currently in '{worker.status}' state.\")\n\n    if worker.status == \"Provisioning\" and time_since_creation &gt; timeout_seconds:\n        print(f\"-&gt; Terminating {worker.name} \u2014 stuck in 'Provisioning' for more than {timeout_seconds} seconds.\")\n        worker.terminate()\n</code></pre> <p>Previous: Introduction | Next: Workspaces</p>"},{"location":"technical-tutorial/getting-started/workspaces/","title":"Practicus AI Workspaces","text":"<p>Practicus AI Workspaces provide a web-based remote desktop environment with Practicus AI Studio and numerous pre-installed tools including an open-source Office suite.</p> <p></p> <p>You can create, use, and then terminate these Workspaces from Practicus AI Home page, or programmatically through a few simple commands. This ensures a clean, reproducible environment that you can spin up on-demand.</p>"},{"location":"technical-tutorial/getting-started/workspaces/#simple-workspace-usage","title":"Simple Workspace Usage","text":"<pre><code>import practicuscore as prt\n\n# Obtain the current region\nregion = prt.current_region()\n\n# Create a new workspace\nworkspace = region.create_workspace()\n</code></pre> <pre><code># View workspace details\nworkspace\n</code></pre> <pre><code># Both workers and workspaces are managed similarly, differentiated by their 'service_type' attribute.\nregion.worker_list.to_pandas()\n\n# You can select an existing workspace like this:\n# workspace = region.worker_list[0]\n</code></pre> <pre><code># Retrieve workspace login credentials\nusername, token = workspace.get_workspace_credentials()\n\nprint(\"Opening Workspace in your browser\")\nprint(f\"Please log in with username: {username} and password: {token}\")\n\nlogin_url = workspace.open_workspace()\n\n# If you only need the URL, without opening a browser:\n# login_url = workspace.open_workspace(get_url_only=True)\n</code></pre> <pre><code># Terminate the workspace when you're finished\nworkspace.terminate()\n</code></pre> <p>Previous: Workers | Next: Add-Ons</p>"},{"location":"technical-tutorial/how-to/automating-kerberos-ticket-renewal-with-cron/","title":"Automating Kerberos Ticket Renewal with Cron","text":""},{"location":"technical-tutorial/how-to/automating-kerberos-ticket-renewal-with-cron/#introduction","title":"Introduction","text":"<p>Kerberos tickets are essential for authenticating to various services, but they have a limited lifespan. When a ticket expires, you lose access to those services until you re-authenticate. Manually renewing tickets is a hassle, especially for automated processes. This notebook will guide you through creating a startup script that leverages the <code>cron</code> utility to automatically renew your Kerberos tickets at regular intervals, ensuring uninterrupted access to Kerberos-authenticated services.</p>"},{"location":"technical-tutorial/how-to/automating-kerberos-ticket-renewal-with-cron/#why-automate-renewal-with-cron","title":"Why Automate Renewal with Cron?","text":"<p><code>cron</code> is a powerful, time-based job scheduler in Unix-like operating systems. It lets you schedule commands or scripts to run automatically at specific times or intervals. Automating Kerberos ticket renewal with <code>cron</code> offers several key advantages:</p> <ul> <li>Uninterrupted Access: Ensures continuous access to Kerberos-authenticated services without manual intervention.</li> <li>Increased Reliability: Reduces the risk of authentication failures caused by expired tickets in automated workflows.</li> <li>Operational Efficiency: Frees up your time from the repetitive task of manual ticket renewal.</li> </ul>"},{"location":"technical-tutorial/how-to/automating-kerberos-ticket-renewal-with-cron/#prerequisites","title":"Prerequisites","text":"<p>Before we start, make sure you have the following:</p> <ul> <li>Kerberos Principal (e.g., <code>your_user{{USER_NAME}}@{{REALM_NAME}}</code>).</li> <li>In this notebook, we'll automatically create a keytab file. While typically provided by a Kerberos administrator, our startup script handles this for convenience in certain scenarios.</li> </ul>"},{"location":"technical-tutorial/how-to/automating-kerberos-ticket-renewal-with-cron/#how-the-script-was-encoded","title":"How the Script Was Encoded","text":"<p>The shell script content was converted into the long <code>base64</code> string you'll use by piping the script's raw text content to the <code>base64</code> command. Here's a breakdown of the process:</p> <p>The Original Script Content</p> <p>This is the multi-line shell script designed to create your keytab, get an initial Kerberos ticket, and set up the <code>cron</code> job. Each line performs a specific action:</p> <pre><code>{\n  echo addent -password -p {{USER_NAME}}@{{REALM_NAME}} -k 1 -e aes256-cts\n  echo {{PASSWORD}}\n  echo wkt /var/practicus/{{USER_NAME}}.keytab\n  echo q\n} | ktutil\nchmod 700 /var/practicus/{{USER_NAME}}.keytab\nkinit -V -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h\nservice cron start\n(crontab -l 2&gt;/dev/null; echo '0 */11 * * * kinit -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h') | crontab -\nsh /var/practicus/.practicus/entry.sh\n</code></pre> <ul> <li><code>{</code>: This starts a command group for <code>ktutil</code>.</li> <li><code>echo addent -password -p {{USER_NAME}}@{{REALM_NAME}} -k 1 -e aes256-cts</code>: Adds a principal entry to the keytab using a password.</li> <li><code>echo {{PASSWORD}}</code>: Provides the password for the principal.</li> <li><code>echo wkt /var/practicus/{{USER_NAME}}.keytab</code>: Specifies the output keytab file path.</li> <li><code>echo q</code>: Quits <code>ktutil</code>.</li> <li><code>} | ktutil</code>: Pipes all the preceding <code>echo</code> commands into <code>ktutil</code> to create the keytab.</li> <li><code>chmod 700 /var/practicus/{{USER_NAME}}.keytab</code>: Sets secure permissions for the keytab (owner only: read, write, execute).</li> <li><code>kinit -V -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h</code>: Obtains an initial Kerberos TGT (Ticket Granting Ticket) using the keytab, valid for 12 hours.</li> <li><code>service cron start</code>: Ensures the <code>cron</code> service is running.</li> <li><code>(crontab -l 2&gt;/dev/null; echo '0 */11 * * * kinit -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h') | crontab -</code>: Adds a cron job to renew the ticket every 11 hours. Note that <code>{{USER_NAME}}@{{REALM_NAME}}</code> appears to be a specific service principal that will have its ticket renewed.</li> <li><code>sh /var/practicus/.practicus/entry.sh</code>: Executes another script. Its exact function depends on the content of that specific script, which is not defined here.</li> </ul> <p>Encoding with <code>echo</code> and <code>base64</code>:</p> <p>To convert this multi-line script into a single <code>base64</code> string, it's enclosed in quotes and then passed as input to the <code>base64</code> command using a pipe (<code>|</code>). The command used for encoding looks like this:</p> <pre><code>{\n  echo addent -password -p {{USER_NAME}}@{{REALM_NAME}} -k 1 -e aes256-cts\n  echo {{PASSWORD}}\n  echo wkt /var/practicus/{{USER_NAME}}.keytab\n  echo q\n} | ktutil\nchmod 700 /var/practicus/{{USER_NAME}}.keytab\nkinit -V -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h\nservice cron start\n(crontab -l 2&gt;/dev/null; echo '0 */11 * * * kinit -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h') | crontab -\nsh /var/practicus/.practicus/entry.sh\n</code></pre> <p>Running this command outputs the <code>base64</code> encoded string you'll use. This method allows the entire script, including newlines and special characters, to be safely stored and then decoded back to its original form for execution.</p>"},{"location":"technical-tutorial/how-to/automating-kerberos-ticket-renewal-with-cron/#running-the-startup-script-to-create-keytab-and-initial-ticket","title":"Running the Startup Script to Create Keytab and Initial Ticket","text":"<p>In this step, we'll use the provided <code>base64</code> encoded command to create and run a startup script. This script will create the keytab file, obtain your initial Kerberos TGT (Ticket Granting Ticket), and set up the cron job automatically.</p> <p>Critical Security Warning </p> <p>The <code>base64</code> encoded string you'll use contains your Kerberos password in plain text within the script it decodes. This is a significant security risk. You should only use this method for testing or in highly controlled, secure environments. Never pass passwords in this manner in production environments or when dealing with sensitive data. The preferred and secure method is always to use a keytab securely generated and distributed by a Kerberos administrator.</p> <p>Now, execute the following command in your terminal. Before running, replace the placeholders <code>{{USER_NAME}}</code>, <code>{{REALM_NAME}}</code>, and <code>{{PASSWORD}}</code> in the <code>base64</code> string with your actual Kerberos username, realm name, and password, respectively. </p> <pre><code>base64 --decode &lt;&lt;&lt; \"...add your base64 encoded script here...\" &gt; /var/practicus/lib-install.sh &amp;&amp; chmod +x /var/practicus/lib-install.sh &amp;&amp; sh /var/practicus/lib-install.sh\n</code></pre> <p>Explanation of this Command Chain</p> <p>This single command line performs a sequence of operations:</p> <ol> <li><code>base64 --decode &lt;&lt;&lt; \"...\"</code>: This part decodes the provided <code>base64</code> encoded string. The decoded string is actually a multi-line shell script containing all the setup steps.</li> <li><code>&gt; /var/practicus/lib-install.sh</code>: The output from the <code>base64 --decode</code> command (which is our decoded shell script) is then redirected and written into a file named <code>/var/practicus/lib-install.sh</code>.</li> <li><code>&amp;&amp; chmod +x /var/practicus/lib-install.sh</code>: The <code>&amp;&amp;</code> operator ensures that this command runs only if the previous command (writing the script to the file) was successful. <code>chmod +x</code> then grants executable permissions to the newly created script file, making it runnable.</li> <li><code>&amp;&amp; sh /var/practicus/lib-install.sh</code>: Again, using <code>&amp;&amp;</code>, this command runs the script we just created and made executable. The <code>sh</code> command executes the script using the system's default shell.</li> </ol>"},{"location":"technical-tutorial/how-to/automating-kerberos-ticket-renewal-with-cron/#monitoring-and-verification","title":"Monitoring and Verification","text":"<p>After the setup is complete, it's crucial to verify that everything is working as expected. These verification steps should be performed directly within the relevant Kubernetes pod (e.g., apphost, modelhost, or worker pod) where your application or process requiring Kerberos authentication is running. </p> <p>Checking Kerberos Ticket Status</p> <p>You can manually check the validity and expiration time of your current Kerberos ticket using the <code>klist</code> command:</p> <pre><code>klist -f\n</code></pre> <p>In the output, you should see Valid starting and Expires times. After the <code>cron</code> job runs, you'll notice that the \"Expires\" time has been extended.</p> <p>Checking Crontab Entry</p> <p>To confirm that the cron job has been added correctly, you can list your <code>crontab</code> entries:</p> <pre><code>crontab -l\n</code></pre> <p>You should see a line similar to <code>0 */11 * * * kinit -kt /var/practicus/{{USER_NAME}}.keytab {{USER_NAME}}@{{REALM_NAME}} -l 12h</code> in the output.</p>"},{"location":"technical-tutorial/how-to/automating-kerberos-ticket-renewal-with-cron/#conclusion","title":"Conclusion","text":"<p>By following these straightforward steps, you've successfully automated the process of creating a Kerberos keytab file, obtaining an initial ticket, and setting up <code>cron</code> to automatically renew your Kerberos tickets. This setup ensures continuous access to your Kerberos-authenticated services, significantly enhancing the reliability of your automated processes and reducing the need for manual intervention.</p> <p>Previous: Work With Connections | Next: Integrate Git</p>"},{"location":"technical-tutorial/how-to/caching-large-model-files/","title":"Caching Large Model and App Files","text":"<p>This example explains how Practicus AI model and hosting handles large model files. Large files, such as GenAI model files, are stored in an object storage system (e.g. in an S3 bucket) and are managed separately from model code (e.g. <code>model.py</code>).</p> <p>We will focus on models, since we usually deal with large files for during model hosting, however, the app hosting system functions similarly.</p> <p>Upon the first execution of a model, Practicus AI checks the local cache at <code>/var/practicus/cache/download/files/from/...</code> for any required large files. If the files are missing or their sizes differ from the remote versions, they are automatically downloaded from the object storage.</p>"},{"location":"technical-tutorial/how-to/caching-large-model-files/#model-configuration-example","title":"Model Configuration Example","text":"<p>In your <code>model.json</code> configuration file, you can specify the folder from which the large files should be downloaded. For example:</p> <pre><code>{\n    \"download_files_from\": \"download/files/from\",\n    \"_comment\": \"You can also define 'download_files_to'; if not defined, /var/practicus/cache is used as the default target.\"\n}\n</code></pre> <p>This configuration ensures the model hosting service knows where to look for the large files.</p>"},{"location":"technical-tutorial/how-to/caching-large-model-files/#how-it-works","title":"How It Works","text":"<ol> <li>Initial Model Run &amp; Dynamic Downloads:</li> <li>When a model is executed for the first time (e.g. during development), the system checks the <code>/var/practicus/cache/download/files/from/...</code> directory for the required large files.</li> <li> <p>If a file is missing or its size does not match the remote file stored in S3, it is downloaded on the fly.</p> </li> <li> <p>Pre-baked Files and Production Images:</p> </li> <li>An admin can build a container image with the large files already pre-loaded in <code>/var/practicus/cache</code>. This image is then used for production deployments.</li> <li> <p>In production, when a model is deployed using this image, only smaller model artifacts (like <code>model.py</code>) are downloaded if needed because the large files already exist locally.</p> </li> <li> <p>Incremental Updates:</p> </li> <li>If only some of the large files have changed between versions, the system downloads only the updated files, reducing the amount of data transferred and speeding up deployments.</li> </ol>"},{"location":"technical-tutorial/how-to/caching-large-model-files/#deployment-flexibility","title":"Deployment Flexibility","text":"<p>The dual approach provides several benefits:</p> <ul> <li>Development and Testing:</li> <li> <p>Developers work with LLMs as usual. The standard model hosting image downloads large files dynamically from S3 if they are not yet available in <code>/var/practicus/cache</code>.</p> </li> <li> <p>Test Deployments:</p> </li> <li> <p>A model (e.g. <code>my-model/v3</code>) is first deployed on a test deployment (e.g. <code>test-model-deployment</code>), using the dynamic download approach.</p> </li> <li> <p>Production Deployments:</p> </li> <li>When ready for production, an admin builds a new container image with the large model files already preloaded to <code>/var/practicus/cache</code>.</li> <li>A new production deployment (e.g. <code>prod-model-deployment-with-large-files</code>) is created using this image.</li> <li>The admin then updates the model configuration for <code>my-model/v3</code> to switch the deployment from the test environment to the production one.</li> </ul> <p>This strategy ensures that production environments start up significantly faster, as they avoid the overhead of downloading large files on every scale-up or recovery.</p>"},{"location":"technical-tutorial/how-to/caching-large-model-files/#example-scenario","title":"Example Scenario","text":"<p>Consider the following end-to-end scenario:</p> <ol> <li>Development Phase:</li> <li>A developer trains a large language model (LLM) with its associated large files stored in S3.</li> <li> <p>When the model is first used, these large files are automatically downloaded to <code>/var/practicus/cache</code> on the deployed container.</p> </li> <li> <p>Testing Phase:</p> </li> <li> <p>The developer deploys <code>my-model/v3</code> on a test deployment named <code>test-model-deployment</code>. In this phase, all large files are downloaded dynamically from S3 as required.</p> </li> <li> <p>Production Preparation:</p> </li> <li>When preparing for production, an admin builds a new container image that includes the pre-baked large model files in <code>/var/practicus/cache</code>.</li> <li> <p>A new production deployment named <code>prod-model-deployment-with-large-files</code> is then created using this updated image.</p> </li> <li> <p>Production Rollout:</p> </li> <li>The admin updates the deployment for <code>my-model/v3</code> by switching from <code>test-model-deployment</code> to <code>prod-model-deployment-with-large-files</code>.</li> <li> <p>Practicus AI now <code>migrates live traffic to the new deployment</code>. During this transition, the system detects that the large files are already cached locally and skips the download step.</p> </li> <li> <p>Auto-scaling and Recovery:</p> </li> <li>If the production deployment <code>auto-scales</code> or <code>recovers after a failure</code>, new instances spin up with the pre-baked container image, <code>significantly reducing initialization time</code> since the large files are already in place.</li> </ol> <p>This scenario maximizes efficiency during both the development phase and in production, ensuring a smooth, scalable, and fast deployment process.</p>"},{"location":"technical-tutorial/how-to/caching-large-model-files/#summary","title":"Summary","text":"<ul> <li> <p>Large File Downloads: Files are managed from a centralized object storage system. Initial model runs download any missing or mismatched files into <code>/var/practicus/cache</code>.</p> </li> <li> <p>Configuration-Driven: The download behavior is controlled via <code>model.json</code>, enabling customization of cache paths.</p> </li> <li> <p>Flexible Deployments: Developers can work with dynamic downloads during testing, while production deployments benefit from pre-baked images that drastically reduce startup times, especially during auto-scaling or post-failure recovery.</p> </li> </ul> <p>This approach optimizes both rapid development and efficient production deployments by minimizing redundant downloads and leveraging cached resources.</p> <p>Previous: Work With Data Catalog | Next: Use Custom Metrics</p>"},{"location":"technical-tutorial/how-to/change-vs-code-defaults/","title":"Change Vs Code Defaults","text":""},{"location":"technical-tutorial/how-to/change-vs-code-defaults/#changing-vs-code-default-settings","title":"Changing VS Code default settings","text":"<p>If you want to customize VS Code each time a worker starts, you can create a file at <code>~/my/settings/vscode.json</code>. Any changes you put in this file will override the default settings, letting you adjust or add new options without modifying the main configuration. This way, you can keep your own preferences neatly organized and separate from the built-in defaults.</p> <p>Sample <code>~/my/settings/vscode.json</code> settings file that: - Increases font size - Ignores certain errors in <code>ruff</code> Python Linting</p> <pre><code>{\n    \"editor.fontSize\": 16,\n    \"ruff.lint.ignore\": [\n        \"E722\", \n        \"F401\"\n    ],\n}\n</code></pre> <p>Previous: Hosting Temporary Apis | Next: Configure Advanced Gpu</p>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/","title":"Advanced GPU Resource Management with Practicus AI","text":"<p>This document provides an overview of how to configure partial NVIDIA GPUs (MIG), along with high-level steps for AMD and Intel GPU support, and how Practicus AI platform facilitates GPU management.</p>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#topics-covered-in-this-document","title":"Topics covered in this document","text":"<ul> <li>Partial NVIDIA GPUs (MIG)</li> <li>AMD GPUs</li> <li>Intel GPUs</li> </ul>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#1-partial-nvidia-gpus-mig","title":"1. Partial NVIDIA GPUs (MIG)","text":""},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#how-mig-works","title":"How MIG Works","text":"<ul> <li>NVIDIA's <code>Multi-Instance GPU (MIG)</code> feature allows you to split a single physical GPU (e.g., NVIDIA A100m, H100, H200) into multiple independent GPU instances.</li> <li>Each MIG instance provides dedicated memory and compute resources, ideal for running multiple workloads on the same GPU.</li> </ul>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#setup-steps","title":"Setup Steps","text":"<ol> <li>Enable MIG Mode on the GPU:</li> <li>Log into the GPU node and enable MIG using <code>nvidia-smi</code>:      <pre><code>sudo nvidia-smi -i 0 --mig-enable\nsudo reboot\n</code></pre></li> <li> <p>After reboot, confirm MIG mode is enabled:      <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Create MIG Instances:</p> </li> <li>Use <code>nvidia-smi</code> to create MIG profiles. For example, split a GPU into 7 instances:      <pre><code>sudo nvidia-smi mig -i 0 -cgi 0,1,2,3,4,5,6\nsudo nvidia-smi mig -i 0 -cci\n</code></pre></li> <li> <p>Check the configuration:      <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Expose MIG Resources in Kubernetes:</p> </li> <li>Deploy the NVIDIA Device Plugin:      <pre><code>kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/main/deployments/k8s-device-plugin-daemonset.yaml\n</code></pre></li> <li> <p>Verify available resources:      <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre></p> </li> <li> <p>To learn more please visit:</p> <ul> <li>https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html</li> </ul> </li> </ol>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#2-custom-gpu-configuration-in-practicus-ai","title":"2. Custom GPU Configuration in Practicus AI","text":"<p>Practicus AI simplifies advanced GPU management through the intuitive management UI:</p> <p>Open Practicus AI Management Console: - Access the platform's web console for infrastructure management.</p> <p>Select Worker Sizes: - Choose from predefined worker sizes or create a new one to include GPU capacity :    - <code>Number of GPUs</code>    - <code>Amount of Video RAM (VRAM)</code></p> <p>Enter GPU Type Selector: - Specify the custom GPU type you need:     - Specify for Nvidia MIG (e.g. <code>nvidia.com/mig-1g.5gb</code>) you defined in the above step.     - Specify for other vendors (e.g., <code>amd.com/gpu</code> or <code>intel.com/gpu</code>).     - Leave empty for the default, which will use entire NVIDIA GPUs without fractions.</p> <p>Deploy workloads as usual: - Deploy end user workers, model hosts, app hosts etc. as usual with the worker size you defined above. - The platform will dynamically manage the resources with the selected GPU configuration.</p> <p>Example Configuration</p> <ul> <li>If you set GPU count to 2, with a GPU type selector of <code>nvidia.com/mig-1g.5gb</code> running on a single <code>NVIDIA H100 GPU</code>, the end user could get two separate GPU instances, each with 1/7th of the GPU's compute and memory resources (1 compute slice and 5 GB of memory per instance).</li> <li>This configuration allows the same physical GPU to handle multiple workloads independently, providing dedicated resources for each workload without interference. </li> <li>This setup is ideal for lightweight GPU workloads, such as inference or smaller-scale training tasks, that do not require the full power of an entire GPU.</li> </ul> <p>Important Note - Please note that the VRAM setting in the Practicus AI Management Console does not dictate how much VRAM a user gets. It is only used to measure usage and ensure a user is kept within their designated daily/weekly/monthly usage limits.  - To actually enforce VRAM limits, you must use NVIDIA MIG profiles (e.g., <code>nvidia.com/mig-1g.5gb</code>) or equivalent to <code>apply resource constraints at the hardware level.</code></p>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#3-high-level-steps-for-amd-gpus","title":"3. High-Level Steps for AMD GPUs","text":"<p>AMD GPUs (e.g., using ROCm) require setup similar to NVIDIA but with their own tools and configurations:</p> <ol> <li>Install AMD ROCm Drivers:</li> <li> <p>Install ROCm drivers on the nodes with AMD GPUs.</p> </li> <li> <p>Deploy AMD Device Plugin:</p> </li> <li>Use the AMD ROCm Kubernetes device plugin to expose AMD GPU resources:      <pre><code>kubectl apply -f https://github.com/RadeonOpenCompute/k8s-device-plugin\n</code></pre></li> </ol> <p>The rest is the same as NVIDIA MIG, define a new worker size and use the GPU Type Selector <code>amd.com/gpu</code></p>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#4-high-level-steps-for-intel-gpus","title":"4. High-Level Steps for Intel GPUs","text":"<p>Intel GPUs can be managed using the Intel GPU Device Plugin:</p> <ol> <li>Install Intel GPU Drivers:</li> <li> <p>Install Intel drivers and libraries for iGPU or discrete GPU support.</p> </li> <li> <p>Deploy Intel Device Plugin:</p> </li> <li>Use the Intel GPU plugin to expose GPU resources:      <pre><code>kubectl apply -f https://github.com/intel/intel-device-plugins-for-kubernetes\n</code></pre> The rest is the same as NVIDIA MIG, define a new worker size and use the GPU Type Selector <code>intel.com/gpu</code></li> </ol> <p>Previous: Change Vs Code Defaults | Next: Configure Workspaces</p>"},{"location":"technical-tutorial/how-to/configure-workspaces/","title":"Configure Workspaces","text":""},{"location":"technical-tutorial/how-to/configure-workspaces/#advanced-workspace-configuration","title":"Advanced Workspace Configuration","text":"<p>You can specify additional parameters to customize the workspace environment.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <pre><code>import json\nimport base64\n\nadditional_params = {\n    \"timezone\": \"America/Los_Angeles\",\n    # Setting a custom password is possible, but not recommended.\n    \"password\": \"super_secret\",\n}\n\nadditional_params_str = json.dumps(additional_params)\nadditional_params_b64 = str(base64.b64encode(bytes(additional_params_str, encoding=\"utf-8\")), \"utf-8\")\n</code></pre> <pre><code># Create a workspace with these additional parameters\nworkspace = region.create_workspace(worker_config={\"additional_params\": additional_params_b64})\n</code></pre> <pre><code>username, token = workspace.get_workspace_credentials()\n\nprint(\"Opening Workspace in your browser.\")\nprint(f\"Please log in with username: {username} and password: {token}\")\n\nlogin_url = workspace.open_workspace()\n\n# The workspace should now use US Pacific Time, per the timezone setting.\n</code></pre> <pre><code>workspace.terminate()\n</code></pre>"},{"location":"technical-tutorial/how-to/configure-workspaces/#shared-folders","title":"Shared Folders","text":"<p>Administrators can define <code>my</code> and <code>shared</code> folders accessible from your Workspaces. Your <code>my</code> folder is dedicated to your environment, while <code>shared</code> folders can be used for collaboration with other users.</p> <p>Sharing between Workers and Workspaces: Sometimes, you may have shared folders accessible by both Workspaces and Workers. In such cases, permission issues might arise due to different default users (e.g., practicus vs. ubuntu). To fix this, simply adjust file ownership on the respective environment:</p> <p>Fixing file permission issues of a shared folder:</p> <p>On a Workspace</p> <pre><code>sudo chown practicus:practicus some_file.txt\n</code></pre> <p>On a Worker</p> <pre><code>sudo chown ubuntu:ubuntu some_file.txt\n</code></pre> <p>Previous: Configure Advanced Gpu | Next: Share Workers</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/","title":"Creating new virtual environments","text":"<p>Practicus AI workers allow you to create new Python Virtual environments using the python default venv module. Please follow the below steps to create and use new virtual environments.</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#option-1-re-use-base-python-packages","title":"Option 1) Re-use base python packages","text":"<ul> <li>With this option you can save disk space with fewer package installations </li> <li>If you need to change a python package version of base image you can simply pip install the different version.</li> </ul> <pre><code># Create new venv \npython3 -m venv $HOME/.venv/new_venv --system-site-packages --symlinks\n# Activate\nsource $HOME/.venv/new_venv/bin/activate\n# Add to Jupyter \npython3 -m ipykernel install --user --name new_venv --display-name \"My new Python\"\n# Install packages, these will 'override' parent python package versions\npython3 -m pip install some_package\n</code></pre>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#option-2-fresh-install","title":"Option 2) Fresh install","text":"<ul> <li>With the fresh install you will have to install all packages, including Practicus AI</li> </ul> <pre><code># Create new venv \npython3 -m venv $HOME/.venv/new_venv\n# Activate\nsource $HOME/.venv/new_venv/bin/activate\n# Install Jupyter Kernel\npython3 -m pip install ipykernel\n# Add to Jupyter \npython3 -m ipykernel install --user --name new_venv --display-name \"My new Python\"\n# Install packages\npython3 -m pip install practicuscore\n</code></pre>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#creating-new-notebooks","title":"Creating new Notebooks","text":"<p>On Jupyter click File &gt; New &gt; Notebook. And your new virtual environment should show up.</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#creating-new-virtual-environments-for-older-python-versions-using-uv","title":"Creating New Virtual Environments for Older Python Versions Using <code>uv</code>","text":"<p>If you need to use a Python version older than the one provided by default, you can leverage <code>uv</code> to install that version and then use Python\u2019s built-in <code>venv</code> module to create and manage your virtual environments.</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#important-note","title":"Important Note","text":"<p>Practicus AI may not fully support older, deprecated Python versions. While you can still run these versions in a Practicus AI worker or notebook, certain features (like the Practicus AI SDK) may not function as expected.</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#installing-a-specific-python-version","title":"Installing a Specific Python Version","text":"<p>Since <code>uv</code> is already installed, you can use it to install an alternate Python version. For example, to install Python 3.7:</p> <pre><code>uv python install 3.7\n</code></pre>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#creating-a-virtual-environment","title":"Creating a Virtual Environment","text":"<p>Use the newly installed Python version to create a virtual environment:</p> <pre><code>uv venv ~/.venv/test --python 3.7\n</code></pre> <p>Activate the virtual environment:</p> <pre><code>source ~/.venv/test/bin/activate\n</code></pre>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#installing-packages","title":"Installing Packages","text":"<p>Within the activated environment, install your desired packages:</p> <pre><code>uv pip install pandas\n</code></pre> <p>Previous: View Stats | Next: Model Tokens</p>"},{"location":"technical-tutorial/how-to/customize-templates/","title":"Practicus AI Code Generation Templates","text":"<p>Practicus AI workers use Jinja2 templates in ~/practicus/templates folder to streamline and automate code generation for key components such as Python libraries, Airflow DAGs and Jupyter notebooks. This approach ensures consistency, scalability, and customization across your projects. Below is an overview of how these templates work and how you can use them effectively.</p>"},{"location":"technical-tutorial/how-to/customize-templates/#1-overview-of-jinja2-templates","title":"1. Overview of Jinja2 Templates","text":"<p>Jinja2 is a powerful templating engine for Python, enabling dynamic creation of files by inserting variables and logic into predefined structures. In our platform, templates are configured to: - Standardize code structure and style. - Dynamically generate project-specific code. - Simplify the onboarding process for new workflows. - To learn more please visit:     - https://jinja.palletsprojects.com/en/stable/intro/</p>"},{"location":"technical-tutorial/how-to/customize-templates/#2-how-to-customize-templates","title":"2. How to Customize Templates","text":"<ul> <li>Start a worker and view templates in ~/practicus/templates folder</li> <li>Customize templates by editing the Jinja2 syntax with your specific project details.</li> <li>Templates can be rendered to test programmatically, see below a sample.</li> <li>Once your testing is completed, you can create a new Practicus AI worker container image and override existing template files.</li> </ul>"},{"location":"technical-tutorial/how-to/customize-templates/#3-best-practices","title":"3. Best Practices","text":"<ul> <li>Test Before Use: Validate generated code in a test environment.</li> <li>Document Changes: If you modify templates, document the changes for future reference.</li> <li>Leverage Variables and Logic: Use Jinja2's advanced features like loops and conditionals for maximum flexibility.</li> </ul> <pre><code># Jinja templates in action..\n\nfrom jinja2 import Template\n\ntemplate_content = \"\"\"\ndef {{ function_name }}(x):\n    return x ** 2\n\"\"\"\n\ntemplate = Template(template_content)\nrendered_code = template.render(function_name=\"square\")\nprint(rendered_code)\n\n# prints\n# def square(x):\n#    return x ** 2\n</code></pre> <p>Previous: Use Custom Metrics | Next: Hosting Temporary Apis</p>"},{"location":"technical-tutorial/how-to/hosting-temporary-apis/","title":"Hosting Temporary APIs on Practicus AI Workers","text":"<p>This example demonstrates how Practicus AI Workers can quickly set up temporary APIs for testing purposes without the need to deploy full Practicus AI Apps or Model hosting. This method enables rapid prototyping, proof of concepts, and quick iterations to verify specific functionalities.</p> <p>Important: This is intended for temporary use and prototyping within the Practicus AI Apps environment, not for long-term deployments.</p> <p>Start by creating a FastAPI app (<code>main.py</code>) as shown below.</p> <pre><code># main.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n</code></pre>"},{"location":"technical-tutorial/how-to/hosting-temporary-apis/#selecting-a-port-for-your-test-server","title":"Selecting a Port for Your Test Server","text":"<p>Choose an available port from the following options, ensuring it doesn't conflict with existing local services:</p> <ul> <li><code>5500</code> (Default MLFlow port)</li> <li><code>8501</code> (Default Streamlit port)</li> <li><code>4040</code> (Default Spark UI port)</li> <li><code>51000</code> (Plot service port)</li> </ul> <p>Example: If you choose port <code>5500</code>, make sure your local MLFlow server isn't running.</p>"},{"location":"technical-tutorial/how-to/hosting-temporary-apis/#starting-the-fastapi-server","title":"Starting the FastAPI Server","text":"<p>Run the following command in your terminal, ensuring you're in the same directory as your <code>main.py</code> file:</p> <pre><code>uvicorn --host=0.0.0.0 --port=5500 main:app --reload\n\n\n\n### Generating the Internal URL for Kubernetes\n\nEach Practicus AI Worker instance has a unique instance ID. Use the following cell to obtain this ID and create a Kubernetes-internal URL.  \n- **Note:** This internal URL is accessible only from other Practicus AI Workers or Apps within the same Kubernetes environment.\n- **Tip:** You can find the worker's instance ID in your browser's URL bar when viewing the Practicus AI Worker environment.\n\n\n```python\nimport practicuscore as prt\n\ninstance_id = prt.get_local_worker().instance_id\nassert instance_id, \"Worker instance Id could not be detected\"\nprint(\"Worker instance id:\")\nprint(instance_id)\n\nport = 5500\ninternal_url = f\"http://prt-svc-wn-{instance_id}:{port}\"\nprint(\"Test server url to use inside Kubernetes:\")\nprint(internal_url)\n</code></pre>"},{"location":"technical-tutorial/how-to/hosting-temporary-apis/#testing-your-server","title":"Testing Your Server","text":"<p>Execute the following cells to verify your FastAPI server's functionality from within the Kubernetes environment.</p> <p>The expected response is:</p> <pre><code>{\"message\":\"Hello World\"}\n\n\n```python\nimport requests\n\nresp = requests.get(internal_url)\n\nprint(resp.text)\n# Prints: {\"message\":\"Hello World\"}\n</code></pre>"},{"location":"technical-tutorial/how-to/hosting-temporary-apis/#optional-generating-a-fully-qualified-domain-name-fqdn-url","title":"(Optional) Generating a Fully Qualified Domain Name (FQDN) URL","text":"<p>If you're working across multiple Kubernetes namespaces, it's advisable to use fully qualified domain names (FQDNs). Use the cell below to generate an FQDN URL for internal access to your FastAPI server.</p> <pre><code># Deploying on multiple Kubernetes Namespaces? Use Fully qualified domain names (fqdn)\nimport os\n\nk8s_namespace = os.getenv(\"PRT_NAMESPACE\")\nassert k8s_namespace, \"Kubernetes namespace could not be detected\"\nprint(\"Current Kubernetes namespace:\")\nprint(k8s_namespace)\nfqdn_internal_url = f\"http://prt-svc-wn-{instance_id}.{k8s_namespace}.svc.cluster.local:{port}\"\nprint(\"Test server fqdn url to use inside Kubernetes:\")\nprint(fqdn_internal_url)\n</code></pre> <pre><code>import requests\n\nresp = requests.get(fqdn_internal_url)\n\nprint(resp.text)\n# Prints: {\"message\":\"Hello World\"}\n</code></pre> <p>Previous: Customize Templates | Next: Change Vs Code Defaults</p>"},{"location":"technical-tutorial/how-to/integrate-git/","title":"Git source control integration","text":"<p>You can use git commands from the terminal, or with the native Git extension from the Jupyter notebook.</p>"},{"location":"technical-tutorial/how-to/integrate-git/#setting-up-using-terminal-recommended","title":"Setting up using terminal (recommended)","text":"<ul> <li>Using the main menu open a terminal. File &gt; New &gt; Terminal</li> <li>Run the below commands</li> </ul> <pre><code># 1) Navigate to the directory to clone \nmkdir ~/projects\ncd ~/projects\n\n# 2) Clone a git repo \ngit clone https://github.com/username/repository.git\n\n# 3) Enter your username and password\n# Note: Many git systems such as GitHub only allow \"personal access tokens\" as password\n# Please check below this notebook to learn how to create a personal access token\n\n# 4) Add your email and username information. \n# This will prevent entering it each time you want to commit.\ngit config --global user.email \"alice@wonderland.com\"\ngit config --global user.name \"Alice\"\n\n# 5) (Optional) Save your credentials\ngit config --global credential.helper cache\n\n# If you already cloned a git repo and need to save credentials again\ncd ~/projects/name_of_the_repository\ngit pull\n# Enter credentials, and repeat after step 4)\n</code></pre> <p>After cloning a git repo and saving credentials, you can use both the terminal and the git UI elements inside Jupyter Notebook</p>"},{"location":"technical-tutorial/how-to/integrate-git/#setting-up-using-the-jupyter-notebook","title":"Setting up using the Jupyter Notebook","text":"<ul> <li>You can open the Git section and click clone a repository.</li> <li>Please note that with this method you might have to enter your password each time you want sync with your git repo.</li> </ul>"},{"location":"technical-tutorial/how-to/integrate-git/#creating-git-personal-access-tokens","title":"Creating Git Personal Access Tokens","text":"<ul> <li>Github<ul> <li>https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens</li> </ul> </li> <li>Bitbucket<ul> <li>https://support.atlassian.com/bitbucket-cloud/docs/create-a-repository-access-token/</li> </ul> </li> <li>GitLab<ul> <li>https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html</li> </ul> </li> </ul> <p>Previous: Automating Kerberos Ticket Renewal With Cron | Next: Use Security Tokens</p>"},{"location":"technical-tutorial/how-to/mcp-langgraph/","title":"Using Practicus AI MCP Servers with LangGraph","text":"<pre><code># E.g. \"https://practicus.my-company.com/apps/agentic-ai-test/api/sys/mcp/\"\nagentic_test_mcp_url = None \n# Api token for the MCP server. You can also get dynamic using SDK.\napi_token = None\n# OpenAI or compatible LLM API key\nopen_ai_key = None\n</code></pre> <pre><code>assert agentic_test_mcp_url, \"MCP Server URL is not defined\"\nassert api_token, \"API token for MCP server not defined\"\n</code></pre> <pre><code>if not open_ai_key:\n    import os\n    import getpass\n\n    open_ai_key = getpass.getpass(\"Enter key for OpenAI or an OpenAI compatible Practicus AI LLM: \")\n\nos.environ[\"OPENAI_API_KEY\"] = open_ai_key\n\nassert os.environ[\"OPENAI_API_KEY\"], \"OpenAI key is not defined\"\n</code></pre> <pre><code>from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n</code></pre> <pre><code>from langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langgraph.prebuilt import create_react_agent\n\nclient = MultiServerMCPClient(\n    {\n        \"agentic_test\": {\n            \"url\": agentic_test_mcp_url,\n            \"transport\": \"streamable_http\",\n            \"headers\": {\n                \"Authorization\": f\"Bearer {api_token}\",\n            },\n\n        }\n    }\n)\ntools = await client.get_tools()\n\nfor tool in tools:\n    print(f\"Located tool:\\n{tool}\\n\")\n</code></pre> <pre><code>agent = create_react_agent(\n    llm,\n    tools\n)\n\nrequest = \"Hi, I'd like to place an order. I'm ordering 2 Widgets priced at $19.99 each and 1 Gadget priced at $29.99. Could you please process my order, generate a detailed receipt, and then send me a confirmation message with the receipt details?\"\nprint(f\"Sending request to LLM:\\n{request}\")\n\ntest_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": request}]}\n)\n\nprint(\"Received response:\")\nprint(test_response)\n</code></pre> <p>Previous: Personal Startup Scripts | Next: Extras &gt; Modeling &gt; SparkML &gt; Ice Cream &gt; SparkML Ice Cream</p>"},{"location":"technical-tutorial/how-to/model-tokens/","title":"Using model access tokens","text":"<ul> <li>In this example we will show how to find model prefixes, models and get short lived session tokens.</li> </ul>"},{"location":"technical-tutorial/how-to/model-tokens/#anatomy-of-a-model-url","title":"Anatomy of a model url","text":"<ul> <li>Practicus services follow this pattern:<ul> <li>[ primary service url ] / [ model prefix ] / [ model name ] / &lt; optional version &gt; /</li> </ul> </li> <li>Sample model addresses:<ul> <li>https://service.practicus.io/models/practicus/diamond-price/</li> <li>https://service.practicus.io/models/practicus/diamond-price/v3/</li> </ul> </li> <li>Please note that Practicus AI model urls always end with a \"/\" </li> </ul>"},{"location":"technical-tutorial/how-to/model-tokens/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>model_name = None  # E.g. \"diamond-price\"\nmodel_prefix = None  #  E.g. 'models/practicus'\n</code></pre> <pre><code>assert model_name, \"Please enter your model_name \"\nassert model_prefix, \"Please enter your model_prefix.\"\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.regions.get_default_region()\n</code></pre> <pre><code># Let's get model prefixes dataframe\n# We can also use the list form with: region.model_prefix_list\nmodel_prefix_df = region.model_prefix_list.to_pandas()\n\nprint(\"Current model prefixes:\")\nprint(\"Note: we will use the 'prefix' column in the API urls, and not the 'key'.\")\n\ndisplay(model_prefix_df)\n</code></pre> <pre><code>print(\"Current models:\")\nprint(\"Note: we will use the 'name' column in the API urls, and not 'model_id'\")\n\ndf = region.model_list.to_pandas()\ndisplay(df)\n</code></pre> <pre><code># You can use regular pandas filters\n# E.g. let's search for models with a particular model prefix,\n# and remove all models that are not deployed (hs no version)\n\n\nfiltered_df = df[(df[\"prefix\"] == model_prefix) &amp; (df[\"versions\"].notna())]\ndisplay(filtered_df)\n</code></pre> <pre><code>api_url = f\"{region.url}/{model_prefix}/{model_name}/\"\n\nprint(\"Getting Model API session token for:\", api_url)\ntoken = prt.models.get_session_token(api_url)\n\nprint(\"Model access token with a short life:\")\nprint(token)\n</code></pre>"},{"location":"technical-tutorial/how-to/model-tokens/#getting-model-api-session-token-using-rest-api-calls","title":"Getting Model API session token using REST API calls","text":"<p>If your end users do not have access to Practicus AI SDK, they can simply make the below REST API calls with any programming language to get a Model API session token.</p> <pre><code># \"No Practicus SDK\" sample to get a session token\n\nimport requests\n\ntry:\n    console_api_url = \"http://local.practicus.io/console/api/\"\n\n    # Option 1 - Use password auth every time you need tokens\n    print(\"[Not Recommended] Getting console API access token using password.\")\n    email = \"admin@admin.com\"\n    password = \"admin\"\n\n    data = {\"email\": email, \"password\": password}\n    console_login_api_url = f\"{console_api_url}auth/\"\n    r = requests.post(console_login_api_url, headers=headers, json=data)\n    if not r.ok:\n        raise ConnectionError(r.status_code)\n    body = r.json()\n    refresh_token = body[\"refresh\"]  # Keep refresh tokens safe!\n    console_access_token = body[\"access\"]\n\n    # Option 2 - Get a refresh token once, and only use that until it expires in ~3 months\n    print(\"[Recommended] Getting console API access token using refresh token\")\n    console_access_api_url = f\"{console_api_url}auth/refresh/\"\n    headers = {\"authorization\": f\"Bearer {refresh_token}\"}\n    data = {\"refresh\": refresh_token}\n    r = requests.post(console_access_api_url, headers=headers, json=data)\n    if not r.ok:\n        raise ConnectionError(r.status_code)\n    body = r.json()\n    console_access_token = body[\"access\"]\n    headers = {\"authorization\": f\"Bearer {console_access_token}\"}\n\n    # Console API access tokens expire in ~30 minutes\n    print(\"Console API access token:\", console_access_token)\n\n    # Locating model id\n    print(\"Getting model id.\")\n    print(\n        \"Note: you can also view model id using Open API documentation (E.g. https://../models/redoc/), or using Practicus AI App.\"\n    )\n    r = requests.get(api_url + \"?get_meta=true\", headers=headers, data=data)\n    if not r.ok:\n        raise ConnectionError(r.status_code)\n    model_id = int(r.headers[\"x-prt-model-id\"])\n    print(\"Model id:\", model_id)\n\n    # Getting model access token, expires in ~4 hours\n    print(\"Getting a model API session token using the console API access token\")\n    console_model_token_api_url = f\"{console_api_url}modelhost/model-auth/\"\n    data = {\"model_id\": model_id}\n    r = requests.get(console_model_token_api_url, headers=headers, data=data)\n    if not r.ok:\n        raise ConnectionError(r.status_code)\n    body = r.json()\n    model_api_token = body[\"token\"]\n    print(\"Model API session token:\", model_api_token)\nexcept:\n    pass\n</code></pre> <p>Previous: Create Virtual Envs | Next: Use Polars</p>"},{"location":"technical-tutorial/how-to/personal-startup-scripts/","title":"Personal Startup Scripts","text":""},{"location":"technical-tutorial/how-to/personal-startup-scripts/#personal-startup-scripts","title":"Personal Startup Scripts","text":"<p>This expample demonstrates how personal startup scripts for Practicus AI workers are executed.</p>"},{"location":"technical-tutorial/how-to/personal-startup-scripts/#startup-script-execution-order","title":"Startup Script Execution Order","text":"<ol> <li>Static Startup Script:</li> <li>If a user creates the file <code>~/my/scripts/startup.sh</code>, it is automatically executed on startup by all workers.</li> <li>To test, create <code>~/my/scripts/startup.sh</code> and save the below example script to personalize VS Code on startup.</li> </ol> <pre><code>#!/bin/bash\n\necho \"Updating VS Code settings to always use dark mode.\"\necho '{\"workbench.colorTheme\": \"Default Dark Modern\"}' &gt; \"/home/ubuntu/.local/share/code-server/User/settings.json\"\n</code></pre> <p>Note: If your script requires network access, add retries with sleep commands. Networking services may not be  immediately available during worker startup.</p> <ol> <li>Dynamic Startup Script:</li> <li>If the user provides an additional dynamic startup script (via the worker configuration), the static script is executed first, followed by the dynamic startup script.</li> </ol> <pre><code>import practicuscore as prt\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",\n    worker_size=\"Medium\",\n    startup_script=\"\"\"\n\n    echo \"Executing dynamic startup script, will only run on the selected worker.\"\n\n    \"\"\"\n)\n\nworker = prt.create_worker(worker_config)\n</code></pre> <ol> <li>Error Logging:</li> <li>If there are errors in any of the scripts, detailed error information can be viewed in the file <code>~/script.log</code>.</li> </ol>"},{"location":"technical-tutorial/how-to/personal-startup-scripts/#summary","title":"Summary","text":"<p>In summary:</p> <ul> <li> <p>Static Script: Located at <code>~/my/scripts/startup.sh</code>, it is always executed first on worker startup.</p> </li> <li> <p>Dynamic Script: Provided in the worker configuration (as seen above), it runs after the static script.</p> </li> <li> <p>Error Handling: Any errors during the execution of either script will be recorded in <code>~/script.log</code>.</p> </li> </ul> <p>Previous: Use Polars | Next: MCP Langgraph</p>"},{"location":"technical-tutorial/how-to/share-workers/","title":"Sharing Workers","text":"<p>To allow another user to access a Practicus AI worker you\u2019ve created, you can run the code below and then share the connection details. This provides a quick way to collaborate on the same environment, including data and code.</p> <p>Important Note: Any user you share a worker with will have access to the contents of your <code>~/my</code> and <code>~/shared</code> directories.</p> <pre><code>import practicuscore as prt\n\n# To share the worker you are currently using,\n#  first get a reference to 'self'\nworker = prt.get_local_worker()\n\n# To start and share a worker, create a worker as usual, e.g.\n# worker = prt.create_worker(worker_config)\n\n# The rest of the code will be the same\n</code></pre> <pre><code># To share using Jupyter Lab\nurl = worker.open_notebook(get_url_only=True)\n\nprint(\"Jupyter Lab login url:\", url)\n</code></pre> <pre><code># To share using VS Code (must run on another worker)\nremote_worker = None\n# E.g. remote_worker = prt.create_worker(worker_config)\n\nif remote_worker:\n    url, token = remote_worker.open_vscode(get_url_only=True)\n\n    print(\"VS Code login url:\", url)\n    print(\"VS Code token    :\", token)\n</code></pre> <p>Previous: Configure Workspaces | Next: View Stats</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/","title":"Using Custom Observability Metrics","text":"<p>Practicus AI\u2019s model hosting and app hosting system allows you to create and track custom Observability metrics via Prometheus or another time-series database. Follow the steps below to begin publishing your own metrics.</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#step-1-enable-observability","title":"Step 1) Enable Observability","text":"<p>Verify with your administrator that the model hosting deployment or app hosting deployment setting you plan to use has observability enabled. If not, an admin can open the Practicus AI Admin Console, select the relevant model deployment or app deployment setting and enable it.</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#step-2-for-model-hosting-modify-modelpy","title":"Step 2) For model hosting, modify <code>model.py</code>","text":"<p>Initialize Prometheus counters, histograms, etc. as usual, and then update your metrics accordingly. An example is shown below.</p> <pre><code># model.py\n\n# ... your existing imports\nfrom prometheus_client import Counter, REGISTRY\n\nmy_counter = None\n\n\nasync def init(*args, **kwargs):\n    # ... your existing model init code\n\n    global my_counter\n\n    metric_name = \"my_test_counter\"\n    try:\n        my_counter = Counter(\n            name=metric_name, documentation=\"My test counter\", labelnames=[\"my_first_dimension\", \"my_second_dimension\"]\n        )\n    except ValueError:\n        # Metric is already defined; retrieve it from the registry\n        my_counter = REGISTRY._names_to_collectors.get(metric_name)\n        if not my_counter:\n            raise  # Something else went wrong\n\n\nasync def predict(df, *args, **kwargs):\n    # ... your existing prediction code\n\n    # Increment the counter for the chosen labels\n    my_counter.labels(\n        my_first_dimension=\"abc\",\n        my_second_dimension=\"xyz\",\n    ).inc()\n\n    # ... your existing code returning prediction\n</code></pre>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#step-2-for-app-hosting-modify-api-py-files","title":"Step 2) For App Hosting, modify API .py files","text":"<p>You can select any api .py file and initialize Prometheus counters, histograms, etc. as usual, and then update your metrics accordingly. An example is shown below.</p> <pre><code># apis/some_api_method.py\n\n# ... your existing imports\nfrom prometheus_client import Counter, REGISTRY\nimport practicuscore as prt\n\nmy_counter = None\n\n\ndef init_counter():\n    global my_counter\n\n    metric_name = \"my_test_counter\"\n    try:\n        my_counter = Counter(\n            name=metric_name, documentation=\"My test counter\", labelnames=[\"my_first_dimension\", \"my_second_dimension\"]\n        )\n    except ValueError:\n        # Metric is already defined; retrieve it from the registry\n        my_counter = REGISTRY._names_to_collectors.get(metric_name)\n        if not my_counter:\n            raise  # Something else went wrong\n\n\ndef increment_my_counter():\n    if not my_counter:\n        init_counter()\n\n    # Increment the counter for the chosen labels\n    my_counter.labels(\n        my_first_dimension=\"abc\",\n        my_second_dimension=\"xyz\",\n    ).inc()\n\n\n@prt.apps.api(\"/some-api-method\")\nasync def run(payload: SomePayloadRequest, **kwargs):\n    # ... api code\n\n    # observe any metric\n    increment_my_counter()\n\n    # ... your existing code returning api result\n</code></pre>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#note-on-metric-exporters","title":"Note on Metric Exporters","text":"<p>Prometheus is a pull-based monitoring system, which means it regularly scrapes endpoints for metrics rather than having metrics pushed to it. To accommodate this, you can use a Practicus AI application as a centralized Metric Exporter for your other systems outside of Practicus AI.</p> <p>In practice, you would: 1. Build a minimal Practicus AI app with no UI (i.e., no <code>Home.py</code>), focusing solely on exposing API endpoints. 2. Define metric-specific endpoints, for example:    - <code>increase_counter_x</code> \u2014 increments a particular counter.    - <code>observe_histogram_y</code> \u2014 records a value for a given histogram.</p> <p>And the Prometheus-compatible <code>/metrics</code> endpoint would be automatically created, assuming your admin enabled observability for app hosting setting you use.</p> <p>This design pattern allows external systems to call your Practicus AI app to update metrics, while Prometheus simply pulls the metrics from one central place.</p> <p>Important Considerations: - Single Replica: Ensure your app deployment setting uses a single replica (1 Kubernetes pod). This makes your metrics consistent and avoids synchronization issues. - Shared Cache/Database: If multiple replicas are required for scaling or redundancy, use a shared cache (e.g., Redis) or a central database (e.g., PostgreSQL) to ensure that a metric update to one pod in the app is visible to all pods.</p> <p>By following this pattern, you centralize metric updates in a single service, keeping your application code cleaner and leveraging Prometheus\u2019s native pull-based model.</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#step-3-observe","title":"Step 3) Observe","text":"<p>After a few minutes, the observations from your custom metric should appear in your monitoring system (for example, Grafana).</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#important-note-on-performance-and-storage-costs","title":"Important note on performance and storage costs","text":"<p>Please be aware that using too many unique labels or values (high cardinality) can quickly degrade performance and inflate storage costs in Prometheus. Labels that change frequently or include user IDs, timestamps, or other unique identifiers are particularly problematic. Always consult your system administrator or MLOps/DevOps team on best practices for label management and on setting up thresholds or limits to maintain a healthy, stable monitoring environment.</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#troubleshooting","title":"Troubleshooting","text":"<p>If you do not see your metric in the monitoring system, check the following:</p> <ul> <li>Observability Disabled? Make sure observability is enabled for your model hosting deployment or app hosting deployment setting.</li> <li>Verify via Pod Metrics Endpoint: If you have the necessary Kubernetes permissions, connect to the pod running your code is hosted on and run:</li> </ul> <pre><code>curl http://localhost:8000/metrics/\n</code></pre> <p>You should see custom metrics like:</p> <pre><code># HELP my_test_counter_total My test counter\n# TYPE my_test_counter_total counter\nmy_test_counter_total{my_first_dimension=\"abc\",my_second_dimension=\"xyz\"} 4.0\n# HELP my_test_counter_created My test counter\n# TYPE my_test_counter_created gauge\nmy_test_counter_created{my_first_dimension=\"abc\",my_second_dimension=\"xyz\"} 1.737049965621189e+09\n</code></pre> <ul> <li>If no metrics appear at all, observability may not be enabled.</li> <li> <p>If only your metric is missing, ensure you are using the model version or api call that contains your custom metrics code.</p> </li> <li> <p>Check Prometheus Scraping: If the metrics are available in the pod but not in Prometheus, verify the <code>defaultServiceMonitor</code> settings in your Kubernetes namespace. This determines whether Prometheus is correctly scraping the metrics from your deployment.</p> </li> </ul> <p>Previous: Caching Large Model Files | Next: Customize Templates</p>"},{"location":"technical-tutorial/how-to/use-polars/","title":"Using Polars for High-Performance Data Processing","text":"<p>Polars is a high-performance DataFrame library designed for efficient and fast data manipulation. Built in Rust and leveraging Apache Arrow, Polars provides a modern, user-friendly API for working with structured data. It offers several advantages over traditional libraries like Pandas and Dask:</p>"},{"location":"technical-tutorial/how-to/use-polars/#why-use-polars","title":"Why Use Polars?","text":""},{"location":"technical-tutorial/how-to/use-polars/#key-benefits","title":"Key Benefits:","text":"<ol> <li>Speed: Written in Rust and optimized for performance, Polars is significantly faster for many operations compared to Pandas.</li> <li>Memory Efficiency: Polars uses Arrow memory structures, which are compact and designed for zero-copy interprocess communication.</li> <li>Parallelism: Automatically leverages multiple CPU cores for computations.</li> <li>Lazy Evaluation: Allows defining a series of operations that are only computed when needed, improving efficiency for complex workflows.</li> <li>Interoperability: Easy to switch between Polars and Pandas, allowing incremental adoption.</li> </ol>"},{"location":"technical-tutorial/how-to/use-polars/#example-basic-polars-operations-with-diamondcsv","title":"Example: Basic Polars Operations with <code>diamond.csv</code>","text":"<p>In this notebook, we will: 1. Load the <code>diamonds.csv</code> dataset using Polars. 2. Explore the dataset with basic info commands. 3. Perform simple data manipulations. 4. Showcase interoperation between Polars and Pandas.</p> <pre><code># Import Polars\nimport polars as pl\n\n# Read the dataset using Polars\ndf = pl.read_csv(\"/home/ubuntu/samples/data/diamond.csv\")\n\n# Basic exploration\nprint(\"Shape of the dataset:\", df.shape)\nprint(\"First few rows of the dataset:\")\nprint(df.head())\n</code></pre> <pre><code># Summary statistics\nprint(\"Summary statistics:\")\nprint(df.describe())\n</code></pre> <pre><code># Filter rows where carat is greater than 2\nfiltered_df = df.filter(pl.col(\"Carat Weight\") &gt; 2)\nprint(\"Filtered rows where Carat Weight &gt; 2:\")\nprint(filtered_df)\n</code></pre> <pre><code># Convert to Pandas DataFrame\npandas_df = df.to_pandas()\nprint(\"Converted to Pandas DataFrame:\")\ndisplay(pandas_df.head())\n</code></pre> <pre><code># Convert back to Polars DataFrame\npolars_df = pl.from_pandas(pandas_df)\nprint(\"Converted back to Polars DataFrame:\")\nprint(polars_df.head())\n</code></pre> <p>Previous: Model Tokens | Next: Personal Startup Scripts</p>"},{"location":"technical-tutorial/how-to/use-security-tokens/","title":"Access and Refresh JWT tokens","text":"<ul> <li>In this example we will show how to login to a Practicus AI region and get access and or refresh tokens.</li> <li>Access tokens are short lived, refresh tokens are long.</li> <li>Refresh tokens allow you the ability to store your login credentials without actually storing your password</li> <li>JWT tokens are human readable, you can visit jwt.io and view what is inside the token.<ul> <li>Is this secure? Yes, jwt.io does not store tokens and decryption happens with javascript on your browser.</li> <li>Who can create JWT tokens? Practicus AI tokens are asymmetric, one can read what is inside a token but cannot create a new one without the secret key. Only your system admin has access to secrets.</li> <li>Can I use a token created for one Practicus AI region for another region? By default, no. If your admin deployed the regions in \"federated\" mode, yes.</li> </ul> </li> </ul> <pre><code>import practicuscore as prt\nimport getpass\n\nregion = prt.regions.get_default_region()\n</code></pre>"},{"location":"technical-tutorial/how-to/use-security-tokens/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>practicus_url = None  # E.g.\"https://practicus.your-company.com\"\nemail = None  # E.g. \"your-email@your-company.com\"\npassword = None\n</code></pre> <pre><code>assert practicus_url, \"Please enter your practicus_url \"\nassert email, \"Please enter your email.\"\nassert password, \"Plese enter your password\"\n</code></pre> <pre><code># Method 1) If you are already logged in, or if you are running this code on a Practicus AI Worker.\n\n\n# Get tokens for the current region.\nrefresh_token, access_token = region.get_refresh_and_access_token()\n\n# Will print long strings like eyJ...\nprint(\"Refresh token:\", refresh_token)\nprint(\"Access token:\", access_token)\n</code></pre> <pre><code># Method 2) You are logging in using the SDK on your laptop,\n#   Or, you are running this code on a worker in a region, but logging in to another region.\n\n# Optionally, you can log-out first\n# prt.auth.logout(all_regions=True)\n\n\n# Tip: region.url shows the current Practicus AI service URL that you are logged-in to.\n\n\nprint(f\"Please enter the password for {email} to login {practicus_url}\")\nif not password:\n    password = getpass.getpass()\n\nsome_practicus_region = prt.auth.login(\n    url=practicus_url,\n    email=email,\n    password=password,\n    # Optional parameters:\n    # Instead of using a password, you can login using a refresh token or access token\n    #   refresh_token = ... will keep logged in for many days\n    #   access_token = ... will keep logged in for some minutes\n    # By default, your login token is stored for future use under ~/.practicus/core.conf, to disable:\n    #   save_config = False\n    # By default, your password is not saved under ~/.practicus/core.conf, to enable:\n    #   save_password = True\n)\n\n# Now you can get as many refresh/access tokens as you need.\nrefresh_token, access_token = some_practicus_region.get_refresh_and_access_token()\n\nprint(\"Refresh token:\", refresh_token)\nprint(\"Access token:\", access_token)\n</code></pre> <pre><code># If you just need an access token.\naccess_token = region.get_access_token()\n\nprint(\"Access token:\", access_token)\n</code></pre> <p>Previous: Integrate Git | Next: Work With Processes</p>"},{"location":"technical-tutorial/how-to/view-stats/","title":"System Resource Usage","text":"<p>Below is a sample code snippet that retrieves and displays system resource usage of memory, disk, and GPU statistics. This helps you monitor available system resources at a glance:</p> <ul> <li>Memory: Reports total, free, and percentage of free memory.</li> <li>Disk: Reports total, free, and percentage of free disk space.</li> <li>GPU: Shows detailed GPU memory usage (used, reserved, total).</li> </ul> <pre><code>from practicuscore.util import Stats\n\ntotal_mem, used_mem = Stats.get_memory_stats()\n\nfree_mem = total_mem - used_mem\ngb = 1024**3\ntotal_mem_gb = round(total_mem / gb, 2)\nfree_mem_gb = round(free_mem / gb, 2)\nfree_mem_percent = round(free_mem / total_mem * 100, 2)\n\nprint(f\"Worker RAM : {total_mem_gb} GB\")\nprint(f\"Free RAM : {free_mem_gb} GB ({free_mem_percent}%)\\n\")\n\ntotal_disk, free_disk = Stats.get_disk_stats()\n\ntotal_disk_gb = round(total_disk / gb, 2)\nfree_disk_gb = round(free_disk / gb, 2)\nfree_disk_percent = round(free_disk / total_disk * 100, 2)\n\nprint(f\"Worker Disk : {total_disk_gb} GB\")\nprint(f\"Free Disk : {free_disk_gb} GB ({free_disk_percent}%)\")\n\nprint(\"Note: The above views the physical disk capacity of the node.\")\nprint(\"The ephemeral disk capacity that your admin allowed for this Worker can be lower.\\n\")\n\ntry:\n    gpu_stats = Stats.get_gpu_stats()\n    for gpu_id, (used, reserved, total) in enumerate(gpu_stats):\n        print(f\"GPU usage for gpu: {gpu_id} used: {used} reserved: {reserved} total: {total}\")\nexcept:\n    print(\"No GPUs detected\")\n</code></pre> <p>Previous: Share Workers | Next: Create Virtual Envs</p>"},{"location":"technical-tutorial/how-to/work-with-connections/","title":"Practicus AI Connections","text":""},{"location":"technical-tutorial/how-to/work-with-connections/#displaying-available-connections","title":"Displaying Available Connections","text":"<p>Below is a quick way to list all available connections you have. Run the code to view your configured data sources, storage endpoints, and other services.</p> <pre><code>import practicuscore as prt\n\nprt.connections.get_all().to_pandas()\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-connections/#creating-a-new-connection","title":"Creating a New Connection","text":"<p>The <code>conn_conf</code> parameter takes a configuration object from a base class that  supports many data sources, including:</p> <ul> <li>S3ConnConf</li> <li>SqLiteConnConf</li> <li>MYSQLConnConf</li> <li>PostgreSQLConnConf</li> <li>RedshiftConnConf</li> <li>SnowflakeConnConf</li> <li>MSSQLConnConf</li> <li>OracleConnConf</li> <li>HiveConnConf</li> <li>ClouderaConnConf</li> <li>AthenaConnConf</li> <li>ElasticSearchConnConf</li> <li>OpenSearchConnConf</li> <li>TrinoConnConf</li> <li>DremioConnConf</li> <li>HanaConnConf</li> <li>TeradataConnConf</li> <li>Db2ConnConf</li> <li>DynamoDBConnConf</li> <li>CockroachDBConnConf</li> <li>CustomDBConnConf</li> </ul> <p>Here we use <code>SqLiteConnConf</code> to create a new SQLite connection by providing the database file path.</p> <pre><code>import practicuscore as prt\n\n_name = \"New SQLite Connection\"\n_conn_conf = prt.connections.SqLiteConnConf(file_path=\"/home/ubuntu/samples/data/chinook.db\")\n\nnew_conn_uuid = prt.connections.create(name=_name, conn_conf=_conn_conf)\nprint(new_conn_uuid)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-connections/#retrieving-a-specific-connection","title":"Retrieving a Specific Connection","text":"<p>Retrieve an existing connection by UUID or name to reuse previously configured settings without reconfiguring them each time.</p> <pre><code>import practicuscore as prt\n\n_conn_id_or_name = None\nassert _conn_id_or_name is not None\n\nconn = prt.connections.get(_conn_id_or_name)\nprint(conn)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-connections/#updating-an-existing-connection","title":"Updating an Existing Connection","text":"<p>After retrieving a connection, you can update its configuration\u2014changing  file paths, credentials, endpoints, or its name. Provide <code>_new_conn_conf</code>  and <code>_updated_name</code> to apply the changes.</p> <pre><code>import practicuscore as prt\n\n_conn_id_or_name = None\nassert _conn_id_or_name is not None\nconn = prt.connections.get(_conn_id_or_name)\n\n_new_conn_conf = None  # prt.connections.SqLiteConnConf(file_path=\"/home/ubuntu/samples/data/chinook.db\")\n_updated_name = None\nassert _new_conn_conf and _updated_name\n\nprt.connections.update(conn_uuid=conn.uuid, name=_updated_name, conn_conf=_new_conn_conf)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-connections/#data-upload-to-s3-compatible-storage","title":"Data Upload to S3-Compatible Storage","text":"<p>Upload local data to S3-compatible storage (e.g., Amazon S3, MinIO) using the Practicus SDK. Provide your credentials, region, endpoints, and prefixes as needed. This approach simplifies managing datasets remotely, ensures reproducibility, and makes sharing data easier.</p> <p>Set the parameters below and run the code to transfer files.</p> <pre><code>import practicuscore as prt\n\n_aws_access_key_id = None  # AWS Access Key ID or compatible service key\n_aws_secret_access_key = None  # AWS Secret Access Key or compatible service secret\n_bucket = None  # The name of your target bucket, e.g. \"my-data-bucket\"\n\n# Ensure that essential parameters are provided\nassert _aws_access_key_id and _aws_secret_access_key and _bucket\n\n_aws_session_token = None  # (Optional) AWS session token\n_aws_region = None  # (Optional) AWS region\n_endpoint_url = None  # (Optional) S3-compatible endpoint (e.g., MinIO)\n\n_prefix = None  # (Optional) Prefix for organizing objects within the bucket\n_folder_path = None  # The local folder path with files to upload\n_source_path_to_cut = None  # (Optional) Remove a leading folder path portion from object keys\n\n# Ensure the folder path is provided\nassert _folder_path\n\n_upload_conf = prt.connections.UploadS3Conf(\n    bucket=_bucket,\n    prefix=_prefix,\n    folder_path=_folder_path,\n    source_path_to_cut=_source_path_to_cut,\n    aws_access_key_id=_aws_access_key_id,\n    aws_secret_access_key=_aws_secret_access_key,\n)\n\nprt.connections.upload_to_s3(_upload_conf)\n</code></pre> <p>Previous: Automated Code Quality | Next: Automating Kerberos Ticket Renewal With Cron</p>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/","title":"Practicus AI Data Catalog","text":"<p>Practicus AI provides a Data Catalog where you, or an administrator can save data source connection information. </p> <p>Data sources can be Data Lakes, Object Storage (E.g. S3), Data Warehouses (E.g. Snowflake), Databases (e.g. Oracle) ...</p> <p>Data catalog info does not include details like the actual SQL queries to run, S3 keys to read etc., but just the info such as host address, port (if needed), user name, password etc. You can think of them as a \"connection string\" in most programming languages. </p> <pre><code>import practicuscore as prt\n\n# Connections are saved under a Practicus AI region\nregion = prt.current_region()\n\n# You can also connect to a remote region instead of the default one\n# region = prt.regions.get_region(..)\n</code></pre> <pre><code># Let's get connections that we have access to\n# If a connection is missing, please ask your admin to be granted access,\n# OR, create new connections using the Practicus AI App or SDK\nconnections = region.connection_list\n\nif len(connections) == 0:\n    raise ConnectionError(\"You or an admin has not defined any connections yet. This notebook will not be meaningful..\")\n</code></pre> <pre><code># Let's view our connections as a Pandas DF for convenience\nconnections.to_pandas()\n</code></pre> <pre><code># Lets view the first connection\nfirst_connection = connections[0]\nfirst_connection\n</code></pre> <pre><code># Is the data source read-only?\nif first_connection.can_write:\n    print(\"You can read from, and write to this data source.\")\nelse:\n    print(\"Data source is read-only. You cannot write to this data-source.\")\n    # Note: read-only data sources are created by Practicus AI admins\n    # and shared with users or user groups using Management Console.\n</code></pre> <pre><code># You can search a connection using it's uuid\nprint(\"Searching with connection uuid:\", first_connection.uuid)\nfound_connection = region.get_connection(first_connection.uuid)\nprint(\"Found:\", found_connection)\n</code></pre> <pre><code># You can also search using the connection name.\n# Please note that connection names can be updated later,\n#   and they are not unique in the Data Catalog.\n# Please prefer to search using a connection uuid for production deployments.\nprint(\"Searching with connection name:\", first_connection.name)\nfound_connection = region.get_connection(first_connection.name)\nprint(\"Found:\", found_connection)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#deep-dive-into-connections","title":"Deep dive into connections","text":"<p>There are multiple ways to load data into a Practicus AI process. </p> <p>Lets' start with the simplest, just using a dictionary, and then we will discuss other options including the Data Catalog.</p>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#loading-data-without-the-data-catalog","title":"Loading data without the data catalog","text":"<p>This is the simplest option and does not use a central data catalog to store connections. If you have the database credentials, you can read from that database.</p> <pre><code># Let's get a worker to use, one that you are already working on, or a remote one.\ntry:\n    worker = region.get_local_worker()\nexcept:\n    workers = region.worker_list\n    if len(workers) == 0:\n        raise ConnectionError(\"Please run this code on a Practicus AI worker, or have at least one active worker\")\n    worker = workers[0]\n</code></pre> <pre><code># Let's load using the sample SQLLite DB that comes pre-installed with Practicus AI\nsql_query = \"\"\"\n  select artists.Name, albums.Title \n  from artists, albums \n  where artists.ArtistId = albums.ArtistId \n  limit 1000\n\"\"\"\n\n# Let's configure a connection\nconn_conf_dict = {\n    \"connection_type\": \"SQLITE\",\n    \"file_path\": \"/home/ubuntu/samples/data/chinook.db\",\n    \"sql_query\": sql_query,\n}\n\nproc = worker.load(conn_conf_dict)\nproc.show_head()\nproc.kill()\n</code></pre> <pre><code># Connection configuration can be a json\nimport json\n\nconn_conf_json = json.dumps(conn_conf_dict)\n\nproc = worker.load(conn_conf_json)\nproc.show_head()\nproc.kill()\n</code></pre> <pre><code># Connection configuration can be path to a json file\nwith open(\"my_conn_conf.json\", \"wt\") as f:\n    f.write(conn_conf_json)\n\nproc = worker.load(\"my_conn_conf.json\")\nproc.show_head()\nproc.kill()\n\nimport os\n\nos.remove(\"my_conn_conf.json\")\n</code></pre> <pre><code># You can use the appropriate conn conf class,\n#   which can offer some benefits such as intellisense in Jupyter or other IDE.\n# The below will use an Oracle Connection Configuration Class\n\nfrom practicuscore.api_base import OracleConnConf, PRTValidator\n\noracle_conn_conf = OracleConnConf(\n    db_host=\"my.oracle.db.address\",\n    service_name=\"my_service\",\n    sid=\"my_sid\",\n    user=\"alice\",\n    password=\"in-wonderland\",\n    sql_query=\"select * from my_table\",\n    # Wrong port !!\n    db_port=100_000,\n)\n\n# We deliberately entered the wrong Oracle port. Let's validate, and fail\nfield_name, issue = PRTValidator.validate(oracle_conn_conf)\nif issue:\n    print(f\"'{field_name}' field has an issue: {issue}\")\n# Will print:\n# 'db_port' field has an issue: Port must be between 1 and 65,535\n\n# With the right Oracle db connection info, you would be able to load\n# proc = worker.load(oracle_conn_conf)\n# df = proc.get_df_copy()\n</code></pre> <pre><code># Practicus AI conn conf objects are easy to convert to a dictionary\nprint(\"Oracle conn dict:\", oracle_conn_conf.model_dump())\n</code></pre> <pre><code># Or to json\noracle_conn_conf_json = oracle_conn_conf.model_dump_json()\nprint(\"Oracle conn json:\", oracle_conn_conf_json)\n</code></pre> <pre><code># And, vice versa. from a dict or json back to class the instance\n# This can be very convenient, e.g. save to a file, including the SQL Query,\n# and reuse later, e.g. scheduled every night in Airflow.\nreloaded_oracle_conn_conf = OracleConnConf.model_validate_json(oracle_conn_conf_json)\ntype(reloaded_oracle_conn_conf)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#loading-using-the-data-catalog","title":"Loading using the Data Catalog","text":"<p>When you read a connection from Practicus AI Data Catalog, you also download it's \"base\" connection configuration class.</p> <p>But instead of the database credentials like user name, password etc. you will load a \"reference\" (uuid) to the Data Catalog. </p> <p>Practicus AI workers will load data intelligently;  - if there are database credentials in the conn conf, these will be used. - Or else, the worker will sue your credentials to \"fetch\" connection credentials from the Data Catalog, and by using the reference.</p> <pre><code># Accessing the conn_conf will print the json\nconn_conf_object = first_connection.conn_conf\nconn_conf_object\n</code></pre> <p>In most cases, accessing the \"conn_conf\" of a connection that you load from the Data Catalog will just have:</p> <ul> <li>Connection type, e.g. Oracle</li> <li>And the unique reference uuid</li> </ul> <p>For relational DBs, you can just supply a SQL query and you're good to go. Practicus AI will take care of the rest.</p> <pre><code># The conn_conf is actually a child class of ConnConf\ntype(conn_conf_object)\n# e.g. OracleConnConf\n</code></pre> <pre><code># Let's make sure we use a connection type that can run a SQL statement,\n#   which will be a child class of Relational DB class RelationalConnConf.\nfrom practicuscore.api_base import RelationalConnConf\n\nif not isinstance(conn_conf_object, RelationalConnConf):\n    raise ConnectionError(\"The rest of the notebook needs a conn type that can run SQL\")\n</code></pre> <pre><code># With the below code, you can see that the conn conf class has many advanced properties\n# dir(conn_conf_object)\n# We just need to use sql_query property\nconn_conf_object.sql_query = \"Select * from Table\"\n</code></pre> <pre><code># In addition to a dict, json or json file, we can also use a conn conf object to read data\n# proc = worker.load(conn_conf_object)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#summary","title":"Summary","text":"<p>Let's summarize some of the common options to load data.</p> <pre><code>region = prt.current_region()\n\nprint(\"My connections:\", region.connection_list)\n\npostgres_conn = region.get_connection(\"My Team/My Postgres\")\nif postgres_conn:\n    postgres_conn.sql_query = \"Select * from Table\"\n    proc = worker.load(postgres_conn)\n\nredshift_conn = region.get_connection(\"Some Department/Some Project/Some Redshift\")\nif redshift_conn:\n    conn_dict = redshift_conn.model_dump()\n    conn_dict[\"sql_query\"] = \"Select * from Table\"\n    proc = worker.load(redshift_conn)\n\nconn_with_credentials = {\n    \"connection_type\": \"SNOWFLAKE\",\n    \"db_name\": \"my.snowflake.com\",\n    # add warehouse etc.\n    \"user\": \"bob\",\n    \"password\": \"super-secret\",\n    \"sql_query\": \"Select * from Table\",\n}\n# proc = worker.load(conn_with_credentials)\n\n# And lastly, which can include all the DB credentials + SQL\n#  or a reference to the data catalog + SQL\n# proc = worker.load(\"path/to/my_conn.json\")\n</code></pre> <p>Previous: Work With Processes | Next: Caching Large Model Files</p>"},{"location":"technical-tutorial/how-to/work-with-processes/","title":"Practicus AI Processes","text":"<p>Practicus AI Processes live in Practicus AI Workers and you can create, work with and kill them as the below.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>worker = prt.get_local_worker()\n</code></pre> <pre><code># We can also create a remote worker by running the below, either from your computer,\n# or from an existing Practicus AI worker.\nworker_config = {\n    \"worker_size\": \"Small\",\n    \"worker_image\": \"practicus\",\n}\n# worker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Processes start by loading some data.\nworker_file_conn = {\"connection_type\": \"WORKER_FILE\", \"file_path\": \"/home/ubuntu/samples/data/ice_cream.csv\"}\n\nproc_1 = worker.load(worker_file_conn)\nproc_1.show_head()\n</code></pre> <pre><code># Let's create another process\nsqlite_conn = {\n    \"connection_type\": \"SQLITE\",\n    \"file_path\": \"/home/ubuntu/samples/data/chinook.db\",\n    \"sql_query\": \"select * from artists\",\n}\n\nproc_2 = worker.load(sqlite_conn)\nproc_2.show_head()\n</code></pre> <pre><code># Let's iterate over processes\nfor proc in worker.proc_list:\n    print(f\"Process id: {proc.proc_id} connection: {proc.conn_info}\")\n</code></pre> <pre><code># Converting the proc_list into string will give you a csv\nprint(worker.proc_list)\n</code></pre> <pre><code># You can also get current processes as a pandas DataFrame for convenience\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># Simply accessing the proc_list will also print a formatted table string\nworker.proc_list\n</code></pre> <pre><code>first_proc = worker.proc_list[0]\n# Accessing a process object will print it's details\nfirst_proc\n</code></pre> <pre><code># You can ask a process to kill itself and free resources\nproc_1.kill()\n</code></pre> <pre><code># It's parent worker will be updated\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># You can also ask a worker to kill one of it's processes\nremaining_proc_id = worker.proc_list[0].proc_id\nworker.kill_proc(remaining_proc_id)\n</code></pre> <pre><code># Will return empty\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># You can also ask a worker to kill all its processes to free up resources faster\nworker.kill_all_procs()\n</code></pre> <p>Previous: Use Security Tokens | Next: Work With Data Catalog</p>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/","title":"Executing Notebooks","text":""},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#automating-notebook-execution","title":"Automating Notebook Execution","text":"<p>Practicus AI allows you to execute notebooks in an automated fashion, which can be particularly useful for <code>testing</code> and  <code>automated workflows</code>.</p>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#executing-notebooks-from-practicus-ai","title":"Executing Notebooks from Practicus AI","text":""},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#1-create-a-notebook-file","title":"1) Create a Notebook File","text":"<p>Begin by creating a file named <code>sample_notebook.ipynb</code>.</p>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#2-add-a-parameters-cell-optional","title":"2) Add a Parameters Cell (Optional)","text":"<p>Add a cell with the following code to define parameters that can be updated dynamically later: <pre><code># Notebook parameters - can be dynamically updated later\nsome_param = 1\nsome_other_param = 2\n</code></pre></p> <p>For Jupyter: 1. Select the newly added cell. 2. In the top-right section of JupyterLab, open the property inspector. 3. Click on <code>Add Tag</code> and enter <code>parameters</code>, then click <code>+</code>. 4. The cell now has the <code>parameters</code> tag.</p> <p></p> <p>For VS Code: 1. Select the cell you just created. 2. Click the <code>...</code> in the cell's upper-right corner. 3. Add a cell tag named <code>parameters</code>. 4. The cell is now tagged.</p> <p></p>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#3-add-your-code-as-usual","title":"3) Add Your Code as Usual","text":"<p>Continue writing code in the notebook as you normally would. For example, add the following code to print messages and validate your parameters:</p> <pre><code>print(\"Starting to run sample notebook\")\n\nif some_param &lt;= 0:\n    raise ValueError(\"some_param must be &gt; 0\")\nelse:\n    print(\"some_param value is acceptable:\", some_param)\n\nprint(\"Finished running sample notebook\")\n</code></pre>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#4-execute-the-notebook","title":"4) Execute the Notebook","text":"<p>Once you\u2019ve set up the notebook and its parameters, you can execute it using Practicus AI\u2019s automated run capabilities.</p> <pre><code>import practicuscore as prt\n\n# This will run just fine,\n# and save the resulting output to sample_notebook_output.ipynb\nprt.notebooks.execute_notebook(\"sample_notebook\")\n</code></pre> <pre><code># The below *will FAIL* since some_param cannot be 0\nprt.notebooks.execute_notebook(\"sample_notebook\", parameters={\"some_param\": 0})\n</code></pre>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#advanced-features","title":"Advanced features","text":"<pre><code># Advanced Notebook automation parameters\ndefault_output_folder = \"~/tests\"  # If none, writes notebook output to same folder as notebook\ndefault_failed_output_folder = \"~/tests_failed\"  # If not none, copies failed notebook results\n\n# By calling configure you can save notebook results to central location\n# Please note that you can do this to a shared/ folder daily where all of our members have access to\nprt.notebooks.configure(\n    default_output_folder=default_output_folder,\n    default_failed_output_folder=default_failed_output_folder,\n    add_time_stamp_to_output=True,\n)\n\n# This will work\nprt.notebooks.execute_notebook(\"sample_notebook\")\n\n# This will fail but does not stop the execution of the notebook\nprt.notebooks.execute_notebook(\"sample_notebook\", parameters={\"some_param\": 0})\n\n# You can access successful and failed notebooks lists with the below\nprint(\"Successful notebook runs:\", prt.notebooks.successful_notebooks)\nprint(\"Failed notebook runs:\", prt.notebooks.failed_notebooks)\n\n# Calling validate_history() will raise an exception IF any of the previous notebooks failed\n# This is useful to have a primary \"orchestration\" notebook that executes other child notebooks,\n# And then finally fails itself if there was a mistake.\n# You can then report the result, essentially creating a final report.\nprt.notebooks.validate_history()\n\n# You can view the passed and failed execution results in\n# ~/tests and ~/tests_failed with a time stamp (optional)\n</code></pre>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#executing-notebooks-from-terminal","title":"Executing notebooks from terminal","text":"<p>You can run the below command to execute a notebook from the terminal or an .sh script.</p> <pre><code>prtcli execute-notebook -p notebook=my-notebook.ipynb \n</code></pre> <p>Previous: Notifications | Next: Improve Code Quality &gt; Automated Code Quality</p>"},{"location":"technical-tutorial/how-to/improve-code-quality/automated-code-quality/","title":"Automated Code Quality","text":"<p>You can check for code quality issues, fix and format your files automatically.</p> <pre><code>import practicuscore as prt\n\n# Let's check for code quality issues in this folder\nsuccess = prt.quality.check()\n</code></pre> <pre><code># Some issues like 'unused imports' can be fixed automatically\nif not success:\n    prt.quality.check(fix=True)\n</code></pre> <pre><code># Let's also format code to improve quality and readability\nprt.quality.format()\n</code></pre> <pre><code># Still errors? open bad_code.py and delete the wrong code\n# Final check, this should pass\nprt.quality.check()\n</code></pre> <pre><code># We can add a \"No QA\" tag to ignore checking a certain type of issue\n# E.g. to ignore an unused imports for a line of code\nimport pandas  # noqa: F401\n\n# To ignore all QA checks (not recommended)\nimport numpy  # noqa\n</code></pre>"},{"location":"technical-tutorial/how-to/improve-code-quality/automated-code-quality/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/how-to/improve-code-quality/automated-code-quality/#bad_codepy","title":"bad_code.py","text":"<pre><code>import pandas\n\n# this is an error\nprint(undefined_var)\n\n\nprint(\"Too many blank lines, which is a code formatting issue.\")\n</code></pre> <p>Previous: Executing Notebooks | Next: Work With Connections</p>"},{"location":"technical-tutorial/modeling/introduction/","title":"Practicus AI Model Hosting Foundations","text":"<p>Practicus AI provides a robust model deployment infrastructure that allows you to organize, host, and manage machine learning models with scalability, security, and flexibility.</p>"},{"location":"technical-tutorial/modeling/introduction/#model-prefixes","title":"Model Prefixes","text":"<p>Models in Practicus AI are logically grouped using model prefixes. A model prefix serves as a namespace, often reflecting organizational or access boundaries. For example:</p> <pre><code>https://practicus.company.com/models/marketing/churn/v6/\n</code></pre> <p>In this URL:</p> <ul> <li><code>models/marketing</code> is the model prefix</li> <li><code>churn</code> is the model name</li> <li><code>v6</code> is an optional version</li> </ul> <p>These prefixes can also be tied to security and access controls, ensuring that only authorized user groups (e.g., an LDAP group for finance) can access certain prefixes and the models under them.</p>"},{"location":"technical-tutorial/modeling/introduction/#model-deployments","title":"Model Deployments","text":"<p>While model prefixes handle logical grouping and naming, model deployments represent the physical Kubernetes deployments that host these models. Model deployments:</p> <ul> <li>Run on Kubernetes</li> <li>Can host multiple models and model versions</li> <li>Part of an integrated service mesh and load balancing</li> <li>Support advanced techniques like A/B testing between model versions</li> <li>Can be auto-scaled</li> </ul> <p>This architecture enables flexible model hosting where you can deploy multiple versions of a model under the same prefix but on different deployments, ensuring isolation, performance optimization, and continuous integration and delivery of models.</p> <p>You can use the Practicus AI App, CLI, or this SDK to list, navigate, and interact with model prefixes, deployments, models, and their versions.</p> <pre><code>import practicuscore as prt\n\n# Connect to the current region\nregion = prt.current_region()\n</code></pre> <pre><code># List the model prefixes defined by your admin and accessible to you\nregion.model_prefix_list.to_pandas()\n</code></pre> <pre><code># List the model deployments defined by your admin and accessible to you\nregion.model_deployment_list.to_pandas()\n</code></pre>"},{"location":"technical-tutorial/modeling/introduction/#re-creating-a-model-deployment","title":"Re-creating a Model Deployment","text":"<p>If you have admin privileges, you can re-create a model deployment. Re-creating involves deleting and then re-launching the deployment using the existing configuration. This is helpful in development or testing scenarios to ensure a fresh start. However, it\u2019s not typically advised in production environments since model versions hosted by that deployment will be temporarily unavailable during the process.</p> <p>Note: You must have admin privileges for this operation.</p> <pre><code>model_deployment_key = \"development-model-deployment\"\nregion.recreate_model_deployment(model_deployment_key)\n</code></pre> <p>Previous: Add-Ons | Next: Sample Modeling &gt; Build And Deploy</p>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/","title":"Build And Deploy","text":"<pre><code>model_deployment_key = None\nmodel_prefix = None\n</code></pre> <pre><code>assert model_deployment_key, \"Please provide model deployment key for classic ML modeling\"\nassert model_prefix, \"Please provide model prefix for classic ML modeling\"\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#building-and-deploying-an-xgboost-model","title":"Building and deploying an XGBoost model","text":"<p>In this example, we will walk through the process of building a simple XGBoost model using a small dataset. We\u2019ll begin by training and saving the model, and then demonstrate how to deploy it as a REST API endpoint. Finally, we\u2019ll make some predictions by calling the deployed API.</p> <p>By the end of this example, you will have a clear understanding of how to:</p> <ol> <li>Prepare and train a basic XGBoost model.</li> <li>Save the trained model to a file.</li> <li>Deploy the model as a simple API service.</li> <li>Make predictions by sending requests to the API.</li> </ol> <p>This approach can be extended and adapted to more complex models and larger systems, giving you a foundation for building scalable machine learning services.</p>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#sample-xgboost-model","title":"Sample XGBoost model","text":"<p>Let's build a simple XGBoost model</p> <pre><code>import pandas as pd\nfrom xgboost import XGBRegressor\n\n# Load the ice cream dataset that come pre-installed in Workers\ndf = pd.read_csv(\"/home/ubuntu/samples/data/ice_cream.csv\")\n\n# Separate features and target\nX = df[[\"Temperature\"]]\ny = df[\"Revenue\"]\n\n# Create and train the XGBoost regressor\nmodel = XGBRegressor(n_estimators=50)\nmodel.fit(X, y)\n\n# Save the model using the recommended XGBoost .ubj format\nmodel.save_model(\"model.ubj\")\n\nprint(\"Model saved as model.ubj\")\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#how-to-query-for-deployments-and-prefixes","title":"How to Query for Deployments and Prefixes","text":"<p>If you are unsure about which model deployment (the underlying infrastructure) or which logical address group (prefix) to use, you can run the code below to dynamically query the available options.</p> <p>If you already have the required information, you can define it directly as shown here and skip the query step:</p> <pre><code>deployment_key = \"some-deployment\"\nprefix = \"models\"\n</code></pre> <p>The code below demonstrates how to programmatically identify the first available model deployment system and model prefix. These values are then used to construct a URL for deploying and accessing a model\u2019s REST API.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\n\nif not model_deployment_key:\n    # Identify the first available model deployment system\n    if len(region.model_deployment_list) == 0:\n        raise SystemError(\"No model deployment systems are available. Please contact your system administrator.\")\n    elif len(region.model_deployment_list) &gt; 1:\n        print(\"Multiple model deployment systems found. Using the first one.\")\n    model_deployment = region.model_deployment_list[0]\n    model_deployment_key = model_deployment.key\n\nif not model_prefix:\n    # Identify the first available model prefix\n    if len(region.model_prefix_list) == 0:\n        raise SystemError(\"No model prefixes are available. Please contact your system administrator.\")\n    elif len(region.model_prefix_list) &gt; 1:\n        print(\"Multiple model prefixes found. Using the first one.\")\n\n    model_prefix = region.model_prefix_list[0].key\n\nmodel_name = \"my-xgboost-model\"\nmodel_dir = None  # Use the current directory by default\n\n# All Practicus AI model APIs follow this URL convention:\nexpected_api_url = f\"{region.url}/{model_prefix}/{model_name}/\"\n# Note: Ensure the URL ends with a slash (/) to support correct routing.\n\nprint(\"Expected Model REST API URL:\", expected_api_url)\nprint(\"Using model deployment:\", model_deployment_key)\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#modelpy","title":"model.py","text":"<p>Review the <code>model.py</code> file to see how the XGBoost model is integrated and consumed within the environment.</p>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#deploy-the-model-as-an-api","title":"Deploy the model as an API","text":"<pre><code># This function can be called multiple times to deploy additional versions.\napi_url, api_version_url, api_meta_url = prt.models.deploy(\n    deployment_key=model_deployment_key, prefix=model_prefix, model_name=model_name, model_dir=model_dir\n)\n</code></pre> <pre><code>print(\"Which model API URL to use:\")\nprint(\"If you prefer the system admin dynamically route\")\nprint(\"  between model versions (recommended), use the below:\")\nprint(api_url)\nprint(\"If you prefer to use exactly this version, use the below:\")\nprint(api_version_url)\nprint(\"If you prefer to get the metadata of this version, use the below:\")\nprint(api_meta_url)\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#making-predictions-using-the-model-api","title":"Making predictions using the model API","text":"<pre><code># We will be using using the SDK to get a session token (or reuse existing, if not expired).\n# To learn how to get a token without the SDK, please view samples in the extras section\ntoken = None\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token:\", token)\n</code></pre> <pre><code># Now let's consume the Rest API to make the prediction\nimport requests\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df[\"Temperature\"].to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text} - {r.headers}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\ndisplay(pred_df)\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#modelpy_1","title":"model.py","text":"<pre><code>import os\nimport pandas as pd\nfrom xgboost import XGBRegressor\n\nmodel = None\n\n\nasync def init(*args, **kwargs):\n    print(\"Initializing model\")\n    global model\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, \"model.ubj\")\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model = XGBRegressor()\n    model.load_model(model_file)\n\n\nasync def predict(df, *args, **kwargs):\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    X = df[[\"Temperature\"]]\n\n    # Generate predictions\n    predictions = model.predict(X)\n\n    # Return predictions as a new DataFrame\n    return pd.DataFrame({\"predictions\": predictions})\n</code></pre> <p>Previous: Introduction | Next: Workflows &gt; Introduction</p>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/","title":"Automated Git Sync for Workers","text":"<p>This example demonstrates how to securely configure Git in Practicus AI, ensuring you can clone or pull a repository both locally (within a running notebook) and automatically when a worker starts.</p>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#overview","title":"Overview","text":"<ol> <li>Create a Personal Access Token (PAT) on your chosen Git platform.</li> <li>Store the PAT as a Personal Secret in the Practicus AI Vault.</li> <li>Configure and Sync the Repository via the Practicus AI Git Helper.</li> <li>Auto-Clone a Repo on Worker Startup to have your code ready immediately when the worker launches.</li> </ol>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#step-1-create-a-personal-access-token","title":"Step 1: Create a Personal Access Token","text":"<p>Log into your Git system (e.g., Practicus AI Git, GitHub, GitLab, ..) and generate a personal access token. Make sure to include any necessary permissions (e.g., read/write to repositories if needed).</p>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#step-2-store-the-token-as-a-personal-secret","title":"Step 2: Store the Token as a Personal Secret","text":"<p>Store the new token in the Practicus AI Vault to keep it secure and avoid hardcoding credentials in your code.</p> <pre><code>worker_size = None\ngit_secret_name = None\ngit_remote_url = None # \"https://git.practicus.my-company.com/myuser/myrepo.git\"  # Example repository URL\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert git_secret_name, \"Please enter git secret name.\"\nassert git_remote_url, \"Please enter git remote url.\"\n</code></pre> <pre><code>import practicuscore as prt\n\nif not git_secret_name:\n    from getpass import getpass\n\n    # Define a name for the stored secret\n    git_secret_name = \"MY_GIT_SECRET\"\n\n    # Prompt for your personal access token\n    key = getpass(\"Enter your Git personal access token:\")\n\n    # Store or update the token in the Practicus AI Vault\n    prt.vault.create_or_update_secret(name=git_secret_name, key=key)\n    print(f\"Successfully saved secret '{git_secret_name}' in Practicus AI Vault.\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#step-3-configure-git-and-synchronize-the-repository","title":"Step 3: Configure Git and Synchronize the Repository","text":"<p>Specify details such as the repository URL, which secret to use, and optional parameters like branch name, fetch depth, or sparse checkout folders.</p> <pre><code>import os\n\n# Create a GitConfig object\ngit_config = prt.GitConfig(\n    remote_url=git_remote_url,  # Repository to clone or pull\n    secret_name=git_secret_name,  # Name of the secret containing the PAT\n    # Optional configurations:\n    # username=\"your-username\",  # If the Git username differs from your Practicus AI username\n    # save_secret=True,\n    # local_path=\"~/some-path-on-worker\",\n    # branch=\"main\",\n    # sparse_checkout_folders=[\"folder1\", \"folder2\"],\n    # fetch_depth=1,\n)\n\n# For demonstration, retrieve the token in this local notebook (avoid printing it!)\nos.environ[git_secret_name], age = prt.vault.get_secret(git_secret_name)\nprint(f\"Retrieved secret '{git_secret_name}', and it is {age} days old.\")\nprint(os.environ[git_secret_name])\n\n# Sync the repository locally in this current environment\nprt.git.sync_repo(git_config)\nprint(\"Repository synced locally.\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#step-4-auto-clone-or-pull-the-repo-on-worker-startup","title":"Step 4: Auto-Clone or Pull the Repo on Worker Startup","text":"<p>Using <code>git_configs</code> in the <code>WorkerConfig</code>, you can automatically clone or pull the repository when the worker is created. This ensures your environment has the code ready immediately.</p> <pre><code># Configure a new worker to automatically clone your repository\nworker_config = prt.WorkerConfig(\n    worker_size=worker_size,\n    personal_secrets=[git_secret_name],\n    git_configs=[git_config],\n)\n\n# Create and start the worker\nworker = prt.create_worker(worker_config)\n\n# Open a Jupyter notebook on the newly created worker\nnotebook_url = worker.open_notebook(get_url_only=True)\nprint(f\"notebook_url: {notebook_url}\")\n\n# Open Jupyter notebook on the new browser tab.\n# worker.open_notebook()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#cleanup","title":"Cleanup","text":"<p>Terminate the worker when you are finished.</p> <pre><code>worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#git-configuration-via-the-web-ui","title":"Git Configuration via the Web UI","text":"<p>You can also manage your Git settings and secrets through Practicus Home's web interface. To get started:</p> <ol> <li> <p>Access Your Home Page:    Navigate to your home page (e.g., https://practicus.my-company.com).</p> </li> <li> <p>Manage Your Secrets:    Open the Settings menu, then select Create Secret to add or update existing secrets.</p> </li> <li> <p>Configure a New Worker:    When creating a new worker, select Advanced Settings and choose the desired Git configuration and apply the secrets you just saved.</p> </li> </ol> <p>This streamlined process makes it simple to securely set up your Git configuration without leaving the web UI. Enjoy seamless integration with Practicus Home!</p> <p>Previous: Build | Next: Git Integrated CICD</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/","title":"Building and Using Custom Container Images","text":"<p>In this example, we'll demonstrate how to build custom container images using the Practicus AI platform. You'll see how to:</p> <ol> <li>Start a Worker with container-building capabilities.</li> <li>Create and push a custom container image to a private registry.</li> <li>Use your custom container to run new Practicus AI Workers.</li> </ol> <p>We'll also cover:</p> <ul> <li>Configuring the builder to allocate a percentage of your Worker's resources.</li> <li>Pushing images to the Practicus AI container registry or other registries.</li> <li>Granting security permissions to allow custom images.</li> <li>Advanced topics like using insecure registries and custom build configurations, privileged builds, custom builder images, and separating credentials for pulling vs. pushing.</li> </ul>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#step-1-start-a-worker-with-image-building-capabilities","title":"Step 1: Start a Worker with Image-Building Capabilities","text":"<p>First, we'll create a Practicus AI Worker that can build container images. This Worker uses two containers:</p> <ol> <li>Primary Container (Ubuntu Linux): Contains your main environment.</li> <li>Builder Container (Alpine Linux): Builds container images under the hood.</li> </ol> <p>By setting <code>builder=True</code> in <code>ImageConfig</code>, you opt to start this secondary container. You can also specify how much of your Worker's CPU/RAM/GPU the builder can use.</p> <pre><code>import practicuscore as prt\n\n# Configure a Worker that can build container images.\nimage_config = prt.ImageConfig(\n    builder=True,  # Enables the builder container.\n    # Optional: allocates 60% of CPU/RAM/GPU to the builder.\n    # builder_capacity=60,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus-minimal\",\n    worker_size=\"Medium-restricted\",\n    image_config=image_config,\n)\n\n# Create and open the new Worker.\nbuilder_worker = prt.create_worker(worker_config)\nbuilder_worker.open_notebook()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#step-2-build-and-push-your-custom-image","title":"Step 2: Build and Push Your Custom Image","text":"<p>Within the <code>builder_worker</code> notebook, you'll have access to the <code>~/build</code> directory\u2014shared by the primary container and the builder container. Follow these steps to build your image:</p> <ol> <li>Add files (e.g., application code, Dockerfile, config files) to <code>~/build</code>.</li> <li>Run <code>build_image</code> with any necessary credentials.</li> <li>Optionally push the image to your registry.</li> </ol> <p>Below, we create a small text file and a simple Dockerfile, then build and push the image.</p> <pre><code># Create a text file in ~/build\nwith open(\"/home/ubuntu/build/hello.txt\", \"wt\") as f:\n    f.write(\"Hello from the custom container image!\")\n</code></pre> <pre><code># Create a Dockerfile in ~/build\ndockerfile_content = \"\"\"\n# Use a base image from your private registry or a public source\nFROM my-registry.practicus.my-company.com/practicusai/practicus-minimal:25.5.4\n\n# Copy in our text file\nCOPY hello.txt /home/ubuntu/hello.txt\n\n# (Optional) Install packages via apt or pip if needed.\n# RUN sudo apt-get update &amp;&amp; sudo apt-get install -y &lt;packages&gt;\n# RUN pip install &lt;libraries&gt;\n\"\"\"\n\nwith open(\"/home/ubuntu/build/Dockerfile\", \"wt\") as f:\n    f.write(dockerfile_content)\n</code></pre> <pre><code># Build and push the image, run on a builder Worker.\nimport practicuscore as prt\n\nrepo_username = \"some-user-name\"\nrepo_password = \"some-token\"\n\nsuccess = prt.containers.build_image(\n    name=\"my-registry.practicus.my-company.com/demo/practicus-minimal-my\",\n    tag=\"25.5.4\",\n    push_image=True,\n    registry_credentials=[\n        (\"my-registry.practicus.my-company.com\", repo_username, repo_password),\n    ],\n    # Optional: use HTTP instead of HTTPS if you are using an on-prem registry and haven't installed certificates to the builder image.\n    # insecure_registry=True,\n)\n\nprint(\"Successful!\" if success else \"Failed..\")\n</code></pre> <p>Once your image is successfully pushed, you can reference it in any future Practicus AI Worker by its full name, for example:</p> <pre><code>my-registry.practicus.my-company.com/demo/practicus-minimal-my:25.5.4\n</code></pre>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#step-3-use-the-custom-image-you-just-built","title":"Step 3: Use the Custom Image you just built","text":"<p>Switch back to your original environment (the one where you created <code>builder_worker</code>). Now you can create a new Worker that runs on your freshly built image. Make sure your user or group has permissions to use custom container images. If you're an admin, you can grant this permission in the Practicus AI admin console under Infrastructure &gt; Container Images.</p> <p>Below is an example of how to run a Worker with the newly built image:</p> <pre><code>import practicuscore as prt\n\nimage_config = prt.ImageConfig(repo_username=\"some-user-name\", repo_password=\"some-token\")\n\nworker_config = prt.WorkerConfig(\n    # If desired, specify a tag explicitly,\n    # worker_image=\"my-registry.practicus.my-company.com/demo/practicus-minimal-my:25.5.4\",\n    # or else, the current Practicus AI platform version is used if the tag is omitted.\n    worker_image=\"my-registry.practicus.my-company.com/demo/practicus-minimal-my\",\n    worker_size=\"Medium\",\n    image_config=image_config,\n)\n\nworker_from_custom_image = prt.create_worker(worker_config)\nworker_from_custom_image.open_notebook()\n</code></pre> <p>With that, you have a Practicus AI Worker running your custom container image. You can install packages, train models, or run any workload you need.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#step-4-configure-build-service-if-needed","title":"Step 4: Configure build service (if needed)","text":"<p>After starting your builder Worker, you may need to configure the build service to work with your specific environment. The most common configuration is for using private SSL certificates or using insecure registries (registries that use HTTP instead of HTTPS), but you can also configure mirrors, and other advanced settings.</p> <p>There are two approaches: a simple one for just configuring insecure registries, and an advanced one for complete build service configuration.</p> <pre><code>import practicuscore as prt\n\n# Simple approach: Just specify insecure registries\nprt.containers.update_build_config(\n    insecure_registries=\"my-registry.practicus.my-company.com,another-registry.example.com\"\n)\n</code></pre>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#custom-build-configuration-optional","title":"Custom build configuration (Optional)","text":"<p>For more complex scenarios, such as custom certificates, registry mirrors, or performance tuning, you can provide a complete build configuration:</p> <pre><code># Advanced approach: Full custom build configuration\ncustom_config = \"\"\"\n[registry.\"my-registry.practicus.my-company.com\"]\n  insecure = true\n\n[registry.\"secure-registry.example.com\"]\n  # With custom CA certificates\n  ca=[\"/usr/local/share/ca-certificates/prt-ca-bundle.crt\"]\n\"\"\"\n\n# Note: If your admin enabled private certificate injection,\n# Practicus AI will automatically mount /usr/local/share/ca-certificates/prt-ca-bundle.crt\n# on the Alpine image builder container.\nprt.containers.update_build_config(config_content=custom_config)\n</code></pre>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#advanced-topics","title":"Advanced Topics","text":""},{"location":"technical-tutorial/unified-devops/build-custom-images/#dynamic-build-configuration","title":"Dynamic Build Configuration","text":"<p>You've already seen how to dynamically configure build configurations for insecure registries or with custom settings. Here are some additional configuration examples:</p> <ul> <li> <p>Registry with Client Certificates:   <pre><code>[registry.\"secure-registry.example.com\"]\n  ca=[\"/usr/local/share/ca-certificates/prt-ca-bundle.crt\"]\n  client=[\"/etc/ssl/certs/client-cert.pem\", \"/etc/ssl/private/client-key.pem\"]\n</code></pre></p> </li> <li> <p>Registry Mirrors (for faster pulls):   <pre><code>[registry.\"registry-1.docker.io\"]\n  mirrors=[\"https://mirror1.example.com\", \"https://mirror2.example.com\"]\n</code></pre></p> </li> <li> <p>Performance Tuning:   <pre><code>[worker.oci]\n  max-parallelism=8\n  snapshotter=\"overlayfs\"\n  gc=true\n</code></pre></p> </li> </ul>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#private-certificates","title":"Private Certificates","text":"<p>If Practicus AI administrators have enabled private certificate injection, the certificate bundle file will be mounted at <code>/usr/local/share/ca-certificates/prt-ca-bundle.crt</code> within the Alpine image builder container.</p> <p>You can directly use this mounted certificate file, or alternatively, copy the certificates into the <code>~/build</code> directory. The <code>~/build</code> directory is shared between your primary notebook container and the Alpine builder image and is mounted as <code>/build</code> on the Alpine container.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#privileged-vs-restricted-builder","title":"Privileged vs. Restricted Builder","text":"<p>Practicus AI provides a restricted builder (<code>ghcr.io/practicusai/practicus-builder</code>) by default. For tasks requiring lower-level system access, such as building specific drivers, administrators can enable a privileged builder (<code>ghcr.io/practicusai/practicus-builder-privileged</code>). This setting can be managed through the Security Context option under Worker Size.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#automated-certificate-injection","title":"Automated Certificate Injection","text":"<p>If Practicus AI administrators have enabled private certificate injection and you are using the privileged worker option, Practicus AI will automatically run certificate installation procedures on the Alpine builder. Note that automated certificate installation is not available for the restricted (unprivileged) worker.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#custom-builder-images","title":"Custom Builder Images","text":"<p>You can create a custom Alpine builder image (e.g., to install specialized dependencies or certificates) and set <code>custom_builder_url</code> in <code>ImageConfig</code>. This allows more flexibility for specialized builds.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#pull-vs-push-credentials","title":"Pull vs. Push Credentials","text":"<p>If you need separate credentials for pulling a base image and pushing a built image, you can: 1. Build (and cache) the base image locally without pushing (<code>push_image=False</code>). 2. Change your credentials. 3. Re-run the build with <code>push_image=True</code>, which will use the cached image you pulled previously.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#integrating-with-cicd","title":"Integrating with CI/CD","text":"<p>You can include container builds in your Practicus AI CI/CD workflows. For instance, automatically build and push your image upon each commit by referencing these steps in your <code>.github/workflows/</code> YAML.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#building-with-cli","title":"Building with CLI","text":"<p>You can also build and push containers using the Practicus AI CLI, which can be helpful in automation scenarios.</p> <p>Example CLI command: <pre><code>prtcli build-container-image -p \\\n    name=\"my-registry.practicus.my-company.com/my-project/my-image\" \\\n    tag=\"25.5.4\" \\\n    build_args=\"{\\\"ENV_VAR\\\": \\\"value\\\"}\" \\\n    registry_credentials=\"[(\\\"my-registry.practicus.my-company.com\\\", \\\"my-robot-user\\\", \\\"token\\\")]\"\n</code></pre></p> <pre><code># Clean up\nbuilder_worker.terminate()\nworker_from_custom_image.terminate()\n</code></pre> <p>Previous: Git Integrated CICD | Next: Notifications</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/","title":"Git-Integrated CI/CD","text":"<p>Practicus AI offers CI/CD runners that are compatible with GitHub Actions workflows. These runners let you integrate code pushes, tests, and automated tasks seamlessly with Practicus AI\u2019s infrastructure\u2014even in air-gapped or private deployments.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#overview","title":"Overview","text":"<ol> <li>Add a Workflow Configuration: Create a <code>.github/workflows/</code> directory in your repository and add a YAML configuration to define your workflows.</li> <li>Specify a Runner: Use <code>runs-on: practicus-XX.X.X</code> to leverage Practicus AI runners.</li> <li>Add Secrets: Store Practicus AI credentials (like a refresh token) or other required secrets in your Git repository settings.</li> <li>Create CI/CD Task Scripts: Use Python scripts (or the Practicus AI CLI) to launch tasks (like spinning up workers or building containers) on Practicus AI.</li> </ol> <p>Below are some examples of how to set up and run these workflows.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#basic-workflow-example","title":"Basic Workflow Example","text":"<p>Create a workflow file (for example, <code>test.yaml</code>) inside <code>.github/workflows/</code> in your repository:</p> <pre><code># .github/workflows/test.yaml\nname: Test CICD actions\n\non:\n  - push\n\njobs:\n  Sample-Job:\n    runs-on: practicus-25.5.4\n    steps:\n      - name: Say Hello\n        run: echo \"Hello\"\n</code></pre> <p>Whenever you push code, this workflow will run on the Practicus AI runner. You can view logs and status on the repository\u2019s Actions page.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#using-practicus-ai-in-a-workflow","title":"Using Practicus AI in a Workflow","text":"<p>You can trigger Practicus AI workers, tasks, or container builds directly in your workflow. Let\u2019s walk through an example.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#step-1-obtain-a-practicus-ai-refresh-token","title":"Step 1: Obtain a Practicus AI Refresh Token","text":"<p>In Practicus AI, generate a refresh token for your account, then store it privately.</p> <pre><code>worker_size = None\nstartup_script = \"\"\" echo \"Hello from Practicus AI unified DevOps\" \"\"\"\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert startup_script, \"Please enter your startup_script.\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Retrieve your personal refresh token\nrefresh_token = prt.auth.get_refresh_token()\nprint(\"Your refresh token is:\", refresh_token)\nprint(\"Keep this token private and clear the cell/terminal output immediately!\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#step-2-save-the-token-as-a-repository-secret","title":"Step 2: Save the Token as a Repository Secret","text":"<p>Go to your Git repository settings (e.g., Settings &gt; Secrets &gt; Actions on GitHub) and add a new secret named <code>PRT_TOKEN</code>. Paste the refresh token you obtained above.</p> <p>Note: Do not store a short-lived access token. It will expire quickly, causing your CI/CD workflows to fail. Use the refresh token instead.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#step-3-create-a-task-file-in-the-repository","title":"Step 3: Create a Task File in the Repository","text":"<p>Below is a simple Python script (<code>cicd/start_worker.py</code>) that spins up a Practicus AI worker, runs a command, and terminates it.</p> <pre><code># cicd/start_worker.py\nimport practicuscore as prt\n\nworker_config = prt.WorkerConfig(worker_size=worker_size, startup_script=startup_script)\n\nprint(\"Starting worker...\")\nworker = prt.create_worker(worker_config)\n\nprint(\"Terminating worker...\")\nworker.terminate()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#step-4-reference-the-task-file-in-your-workflow","title":"Step 4: Reference the Task File in Your Workflow","text":"<p>Add a workflow file (e.g., <code>.github/workflows/using_workers.yaml</code>) that sets the appropriate environment variables and runs your script.</p> <p><pre><code># .github/workflows/using_workers.yaml\nname: Using Practicus AI Workers\n\n# This action will run each time code is pushed to the repository\non:\n  - push\n\nenv:\n  # Update with your Practicus AI URL\n  PRT_URL: https://practicus.my-company.com\n  PRT_TOKEN: ${{ secrets.PRT_TOKEN }}\n\njobs:\n  Explore-PracticusAI-Actions:\n    runs-on: practicus-25.5.4\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Perform some task on a Practicus AI Worker\n        run: python cicd/start_worker.py\n\n      - name: View Practicus AI CLI help\n        run: prtcli --help\n</code></pre> After saving and pushing these changes, the Practicus AI runner will automatically execute your workflow.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#more-complex-task-examples","title":"More Complex Task Examples","text":""},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#example-1-upload-and-execute-a-task-file","title":"Example 1: Upload and Execute a Task File","text":"<p>Use <code>prt.run_task(...)</code> to start a worker, upload specified files, and run them automatically. This requires that all the files your task depends on are in the directory you provide.</p> <pre><code># cicd/task.py\nprint(\"Hello from a simple task\")\n</code></pre> <pre><code># cicd/run_task.py\nimport practicuscore as prt\n\nworker_config = prt.WorkerConfig(\n    worker_size=worker_size,\n)\n\ntask_file = \"task.py\"  # The file to run\n\nprint(f\"Starting {task_file} on a new worker.\")\nworker, success = prt.run_task(\n    file_name=task_file,\n    files_path=\"cicd\",  # Local folder containing task.py\n    worker_config=worker_config,\n)\n\nif success:\n    print(\"Task execution finished successfully.\")\nelse:\n    raise SystemError(f\"{task_file} failed.\")\n</code></pre> <p>You can tie this script to a workflow file, for example:</p> <pre><code># .github/workflows/run_task.yaml\nname: Run a task on Practicus AI Worker\n\n# This action will run each time code is pushed to the repository\non:\n  - push\n\nenv:\n  # Update the below url\n  PRT_URL: https://practicus.my-company.com\n  PRT_TOKEN: ${{ secrets.PRT_TOKEN }}\n\njobs:\n  Explore-PracticusAI-Actions:\n    runs-on: practicus-25.5.4\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Run task.py on a Practicus AI Worker\n        run: python cicd/run_task.py\n</code></pre>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#example-2-use-git-instead-of-uploading-files","title":"Example 2: Use Git Instead of Uploading Files","text":"<p>If your code exists in a Git repository, you can clone or pull the repo on the worker automatically and then execute a file directly from the cloned repo. First, ensure you have a Git personal access token saved in the Practicus AI Vault.</p> <pre><code>import practicuscore as prt\nfrom getpass import getpass\n\npersonal_secret_name = \"MY_GIT_SECRET\"\nkey = getpass(f\"Enter key for {personal_secret_name}:\")\n\nprt.vault.create_or_update_secret(name=personal_secret_name, key=key)\nprint(f\"Git access token saved as secret '{personal_secret_name}'.\")\n</code></pre> <pre><code># cicd/run_from_git_repo.py\nimport practicuscore as prt\n\nremote_git_url = \"http://git.practicus.my-company.com/myuser/myproject\"\ngit_secret_name = \"MY_GIT_SECRET\"\nlocal_path_for_git = \"~/myproject\"\n\n# Configure the Git repository and secret\ngit_config = prt.GitConfig(\n    remote_url=remote_git_url,\n    secret_name=git_secret_name,\n    local_path=local_path_for_git,\n)\n\n# Create a WorkerConfig that auto-clones the repo\nworker_config = prt.WorkerConfig(\n    worker_size=worker_size,\n    personal_secrets=[git_secret_name],\n    git_configs=[git_config],\n)\n\n# Path to the script we want to run inside the worker, will be cloned from Git.\ntask_file_on_worker = f\"{local_path_for_git}/cicd/task.py\"\n\nprint(f\"Starting {task_file_on_worker} on a new worker.\")\nworker, success = prt.run_task(\n    upload_files=False,  # We'll rely on Git instead of uploading files\n    file_path_on_worker=task_file_on_worker,\n    worker_config=worker_config,\n)\n\nif success:\n    print(\"Task finished successfully.\")\nelse:\n    raise SystemError(f\"{task_file_on_worker} failed.\")\n</code></pre> <p>And reference it in a workflow file, for example:</p> <pre><code># .github/workflows/run_from_git_repo.yaml\nname: Running tasks from Git Repo\n\n# This action will run each time code is pushed to the repository\non:\n  - push\n\nenv:\n  # Update the below url\n  PRT_URL: https://practicus.my-company.com\n  PRT_TOKEN: ${{ secrets.PRT_TOKEN }}\n\njobs:\n  Explore-PracticusAI-Actions:\n    runs-on: practicus-25.5.4\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Run a file from Git on a Practicus AI Worker\n        run: python cicd/run_from_git_repo.py\n</code></pre>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#conclusion","title":"Conclusion","text":"<p>By combining Practicus AI\u2019s worker orchestration with your Git repository\u2019s CI/CD configuration, you can automate virtually any workflow\u2014ranging from running tests and building containers to complex data processing jobs. Just push your changes, and Practicus AI takes care of the rest.</p> <p>You can track the status of each workflow run in your repository\u2019s \u201cActions\u201d tab (or equivalent, depending on your Git service).</p> <p>Previous: Automated Git Sync | Next: Build Custom Images</p>"},{"location":"technical-tutorial/unified-devops/introduction/","title":"Introduction to Practicus AI Unified DevOps","text":"<p>In modern data and software engineering, teams often grapple with fragmented tools and workflows when attempting to integrate development, security, and operations. Unified DevOps is a methodology that brings these components together into a single, cohesive environment\u2014reducing complexity, boosting collaboration, and accelerating releases. </p>"},{"location":"technical-tutorial/unified-devops/introduction/#why-does-unified-devops-matter","title":"Why Does Unified DevOps Matter?","text":"<ul> <li>Streamlined Collaboration: Development, IT operations, and data science teams can collaborate in one platform. This leads to fewer context switches and more efficient handoffs.</li> <li>Faster Delivery Cycles: Automated CI/CD pipelines reduce the time from code commit to production deployment.</li> <li>Security and Compliance: A unified platform offers consistent security controls across every step of the development lifecycle, from managing secrets to controlling infrastructure access.</li> <li>Scalability and Flexibility: With on-demand resources and containerized workflows, teams can scale when they need, without being locked into rigid infrastructure.</li> </ul>"},{"location":"technical-tutorial/unified-devops/introduction/#what-is-practicus-ai-unified-devops","title":"What Is Practicus AI Unified DevOps?","text":"<p>Practicus AI provides a single platform that integrates all the capabilities of Unified DevOps\u2014combining secrets management, containerization, CI/CD, and more. This empowers teams to handle everything from day-to-day development to full-scale production deployments. Here are some of the key features:</p> <ol> <li>Secrets Management: Securely store, rotate, and retrieve sensitive data with an integrated Vault. This ensures that passwords, tokens, and access keys are never exposed in plain text.</li> <li>Automated Worker Initialization: Spin up ephemeral computing environments with all required environment variables and secrets already injected. No more manual configuration.</li> <li>Git Integration: Easily clone or pull repositories during Practicus AI Worker startup or on-demand, using personal or shared access tokens stored in Vault.</li> <li>CI/CD Workflows: Leverage GitHub Actions\u2013compatible workflows that run on Practicus AI Runners. Execute tasks like testing, building, and deploying with minimal overhead.</li> <li>Custom Container Builds: Use built-in container builders to build and push your images to a private or public registry. You can then run new Workers on these custom images\u2014ensuring a fully customized runtime environment.</li> </ol>"},{"location":"technical-tutorial/unified-devops/introduction/#how-to-get-started","title":"How to Get Started","text":"<p>In the following examples, you\u2019ll learn how to:</p> <ul> <li>Store and Retrieve Secrets with Practicus AI\u2019s Vault.</li> <li>Configure Worker Environments by setting environment variables and injecting personal or shared secrets.</li> <li>Set Up Git Repositories to automatically clone or pull code inside Practicus AI.</li> <li>Create CI/CD Workflows that run each time you push code to a repository.</li> <li>Build and Use Custom Container Images for specialized tasks, ensuring each ephemeral Worker can run precisely the environment you need.</li> </ul> <p>By the end, you\u2019ll see how these capabilities combine into a single, streamlined DevOps pipeline\u2014one that unifies data science, engineering, and operations into a shared, secure, and scalable process.</p> <p>Previous: Use Cluster | Next: Secrets With Vault</p>"},{"location":"technical-tutorial/unified-devops/notifications/","title":"Sending Notifications","text":"<p>This example demonstrates how to use the optional (but recommended) Practicus AI notify app. If installed, the notify app can be used to send emails, and if configured, SMS, and other notifications. It also supports auto-populating exception details so that error notifications can be sent automatically.</p> <p>In the examples below, you'll see two common use cases:</p> <ol> <li>Sample Notification: Send a test email with custom parameters.</li> <li>Exception Notification: Automatically send an email with exception details when an error occurs.</li> </ol>"},{"location":"technical-tutorial/unified-devops/notifications/#example-1-sending-an-email","title":"Example 1: Sending an email","text":"<p>In this example, the notify app sends a test email. You can specify recipients, title, body, and additional parameters such as log level, category, and urgency.</p> <ul> <li><code>recipients = None</code> Sends email to current user.</li> <li><code>recipients = 'user@company.com'</code> Sends email to selected user.</li> <li><code>recipients = ['user@company.com', .. ]</code> Sends email to selected users.</li> </ul> <pre><code># Parameters:\nrecipients = None\n</code></pre> <pre><code>assert recipients, \"Please enter your recipients.\"\n</code></pre> <pre><code>import practicuscore as prt\n\nprt.notify.send(\n    recipients=recipients,\n    title=\"Sample notification\",\n    body=\"This is a sample notification..\",\n    level=\"info\",\n    category=\"sample_notification\",\n    urgency=\"normal\",\n    # Add email attachments by setting attachment_paths.\n    # Safe file types and max size (default 20MB) are defined by admin.\n    # Zip files if file type is restricted.\n    # attachment_paths=[\"path/to/file1.txt, path/to/file2.md\"]\n    # View help for other customizations\n)\nprint(\"Test email sent\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/notifications/#example-2-sending-exception-notifications","title":"Example 2: Sending Exception Notifications","text":"<p>In this example, we simulate an exception. The notify app captures exception details (such as the stack trace) and sends an email notification automatically when <code>exc_info=True</code>. This is useful for auto-alerting in production environments when unexpected errors occur.</p> <pre><code>import practicuscore as prt\n\ntry:\n    # Let's simulate an exception\n    x = 1 / 0\nexcept:\n    prt.notify.send(exc_info=True)\n    print(\"Exception notification sent to current user.\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/notifications/#conclusion","title":"Conclusion","text":"<p>This example demonstrated how to use the Practicus AI notify app to send both test notifications and automatic error alerts with exception details. You can integrate this functionality into your applications to ensure critical issues are reported immediately.</p> <p>Feel free to extend these examples or integrate the notify feature into your own Practicus AI applications.</p> <p>Previous: Build Custom Images | Next: How To &gt; Automate Notebooks &gt; Executing Notebooks</p>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/","title":"Practicus AI Secrets with Built-in Vault","text":"<p>This notebook demonstrates how to work with Practicus AI\u2019s built-in Vault to securely store, retrieve, and manage secrets. Practicus AI Vault offers encryption at rest, key rotation, and other security features to protect sensitive data.</p>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#overview","title":"Overview","text":"<ul> <li>Create or Update a Secret: You can create or update a personal secret, prompting for sensitive input rather than hardcoding it.</li> <li>Retrieve a Secret: Retrieve the stored secret along with its age in days.</li> <li>Shared Secrets: Access secrets marked as shared (useful if multiple team members or projects need a common secret).</li> <li>Delete a Secret: Remove the secret from the Vault if it\u2019s no longer needed.</li> </ul> <pre><code>personal_secret_name = \"PERSONAL_SECRET_1\"\nshared_secret_name = \"SHARED_SECRET_1\"\nautomated_test = False\n</code></pre> <pre><code>assert personal_secret_name, \"Please enter your personal_secret_name.\"\nassert shared_secret_name, \"Please enter your shared_secret_name.\"\n</code></pre> <pre><code>import practicuscore as prt\nfrom getpass import getpass\n\n# Create or update a personal secret\nif not automated_test:\n    key = getpass(f\"Enter key for {personal_secret_name}:\")\nelse:\n    import secrets\n\n    key = secrets.token_hex(16)\n\nprt.vault.create_or_update_secret(name=personal_secret_name, key=key)\n</code></pre>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#retrieving-the-personal-secret","title":"Retrieving the Personal Secret","text":"<p>Next, we fetch the secret we just stored in the Vault, ensuring it was saved correctly.</p> <pre><code>import practicuscore as prt\n\nkey, age = prt.vault.get_secret(\n    name=personal_secret_name,\n)\nprint(\n    f\"Retrieved personal secret {personal_secret_name} key: ****, length is {len(key)} chars, and it is {age} days old.\"\n)\n</code></pre>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#accessing-a-shared-secret","title":"Accessing a Shared Secret","text":"<p>You can also retrieve secrets that are created by administrators and shared with you or one of your groups.</p> <pre><code>key, age = prt.vault.get_secret(name=shared_secret_name, shared=True)\nprint(f\"Retrieved shared secret {shared_secret_name} key: ****, length is {len(key)} chars, and it is {age} days old.\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#deleting-a-secret","title":"Deleting a Secret","text":"<p>Finally, if you no longer need the secret, you can delete it from the Vault.</p> <pre><code>prt.vault.delete_secret(name=personal_secret_name)\nprint(f\"Deleted personal secret: {personal_secret_name}\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#using-prtcli-terminal-commands-cli","title":"Using <code>prtcli</code> Terminal Commands (CLI)","text":"<p>You can also use Practicus AI Command Line Interface (CLI) <code>prtcli</code> to interact with Practicus AI Vault. </p>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#retrieving-a-personal-secret","title":"Retrieving a Personal Secret","text":"<p>To retrieve a personal secret from Practicus AI Vault, use the following command:</p> <pre><code>MY_SECRET=$(prtcli get-secret -p name=\"MY_SECRET\")\n</code></pre> <p>This command assigns the output of <code>prtcli get-secret</code> to the environment variable <code>MY_SECRET</code>. If the secret is missing, the command will fail and nothing will be printed.</p>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#handling-errors","title":"Handling Errors","text":"<p>It's important to verify that the secret has been successfully retrieved before proceeding. Use the following snippet to exit your script if the secret is empty:</p> <pre><code>if [ -z \"${MY_SECRET}\" ]; then\n    echo \"ERROR: Could not retrieve secret named MY_SECRET. Please check your configuration.\" &gt;&amp;2\n    exit 1\nfi\n</code></pre> <p>This code checks if <code>MY_SECRET</code> is empty, and if so, prints an error message to stderr and exits with a non-zero status.</p>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#retrieving-a-shared-secret","title":"Retrieving a Shared Secret","text":"<p>If you need to retrieve a shared secret saved via the Admin UI, include the <code>shared</code> parameter:</p> <pre><code>SHARED_SECRET=$(prtcli get-secret -p name=\"MY_SHARED_SECRET\" shared=true)\n</code></pre> <p>Again, ensure that you verify the retrieval of the secret before proceeding:</p> <pre><code>if [ -z \"${SHARED_SECRET}\" ]; then\n    echo \"ERROR: Could not retrieve shared secret named MY_SHARED_SECRET. Please check your configuration.\" &gt;&amp;2\n    exit 1\nfi\n</code></pre>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#cli-best-practices","title":"CLI Best Practices","text":"<ul> <li>Never print secrets: Avoid printing secret values in production. Use environment variables for internal use only.</li> <li>Always check for errors: Validate that each secret is properly retrieved before continuing with your script.</li> <li>Documentation: Clearly document your usage of secrets and error handling in your scripts to help with troubleshooting.</li> </ul>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#big-secrets","title":"Big secrets","text":"<p>Max secret key length is 2000 characters after encryption, which will give you around 1000-1250 characters to use as keys. </p> <p>For secrets such as SSL keys, this might not be enough. You can split the secret into multiple parts as seen below if you ever hit max secret size limit.</p> <pre><code>import practicuscore as prt\n\nlong_secret = \"\"\"-----BEGIN CERTIFICATE-----\nMIIFJzCCAw+gAwIBAgIUyqlBgM2m9t3rfiyEGyuGkx8E4QgwDQYJKoZIhvcNAQEL\nBQAwIzEhMB8GA1UEAwwYUHJhY3RpY3VzIEFJIFNlbGYtU2lnbmVkMB4XDTI0MTAx\nMTIwNTYzNVoXDTM0MTAwOTIwNTYzNVowIzEhMB8GA1UEAwwYUHJhY3RpY3VzIEFJ\nIFNlbGYtU2lnbmVkMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEArpGo\nvTOX8jzzuxt7QyycpStU64fe7o6dVMTRR2FVxv4xsnAQ5OmHXSaBhQtz4fTgq2hE\nE9GrzEMBzpaI+zIe3kvxP3Rs/GvPr62x8ne/uaQdH214a+SlyDnn0SDPnF98zhHQ\nAu+RWdCY1f+T0aDaNDI7kyHobJUga+7hGUyskaphR2UekFGsBI/unr/cFTa2LkVN\neDPTOTgurFpjR+TFe4cJ1nNO4c8ZzUpBkBZQf2kh5SJitniXsSQuD69e/Iajm+Ub\nsCJfRGKhRRUdv06WbGOVNrmaU6MORLlX7lUvWSUKuPJOYXZJpg9qmh0ui3Gkfz9b\nF0LBTzOKl+XEet5HccSWbn4ZnbAOaA/YHG5/FRCRWuMG8qNT2nAqc0HMYUkolN+x\nTagCjcnxT1WqSXzcsAcMYvg7fDRC1spqh6yss+bMrriDKNeDQB8Fp15PaNoxZrrA\nK12M0IUua/AwlKxcM9AqAxOdE2SoxX9xmhim5xbLh1ctkz0DRy4weZz5ta22A6dQ\nvqHKH0uZxtDhAIiw8CqfiIQbBfwFm/xivh170hbVuTqTgoxRpiISB7mgvtuRVHXN\nkJgJDrzQ02k8Ah/KjSWxWKaYq88w7lKv9UQPacD8dqNrthZpDAWpQlu5DBB+9Z/0\nbW8lxZlU8Ct2/NMxyvArMLa4TFYZnbRFKdc9nBsCAwEAAaNTMFEwHQYDVR0OBBYE\nFOjaCjwbTSM6bApDbGmeTh2sZKbjMB8GA1UdIwQYMBaaFOjaCjwbTSM6bApDbGme\nTh2sZKbjMA8GA1UdEwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggIBABDDWajB\nWWRi9KijQVHgm1EZPsNwZkeCyslHALd1Fh7eLVth3RyFsyZupWM9eI4ZJnY4Rj02\nVtR2xSlXioubPt5Ply4IHb/ZCaIRj7WnIFMfwyYxA29O2Yqoi6/Z51Of3qJ1qia7\n6ZnDvkf1Ys1pjOVrpvlEL8mYlFYVg+3EC4jy7IljuZ6e24T0HuMoMtelKu7H4SB0\ncNOfOeVOD0SZx30YLm32sy7AIXP6BXXuXcM2l3SAtHZgXdQ9eOcrcROLTtdHzVma\n8rJxyE51HT6paYOxTVKM7kLT+DipuURTZyIC+Mvmqb0SUPstMuYACuYwICMq35YZ\neXFM2CaU8VkrbaRsDa7OtD2zU/WzHXQn1K5RDd0vw+2IDay3lwTRmJM+/Zp+zxHJ\nXGIrQDKm1IXblpD8gq3QkW6YVQZv9FPsXA2GOqomgFji0wEOGu/28KOZoXoASxJw\nzCPFMOF/rg0un3U7mBzBQHqYNjSqqBFJQJgPDXpa4P4hCj2GTPHB6PepaXG9nJ4P\n17TpNJ3ciwShCVv5u3wAbKVcYhmbOMgEaN4cppHNdzMteOBrQ4k0ZJfO7EBybRjS\nh9AOZ2bQDZ0TE/x134WpbNr7gwmmcL6Bl0e87LXHiKu5/6m1mHJG2RhszOl635gW\nK/7EvAGSXa2GCL6VSiK0oG6RvKHofBk6snne\n-----END CERTIFICATE-----\"\"\"\n\n# Save secret by splitting into 2\nhalf_len = int(len(long_secret) / 2)\nprt.vault.create_or_update_secret(\"long_secret_part1\", long_secret[:half_len])\nprt.vault.create_or_update_secret(\"long_secret_part2\", long_secret[half_len:])\n\n# Load it back\nlong_secret_part1, age = prt.vault.get_secret(\"long_secret_part1\")\nlong_secret_part2, age = prt.vault.get_secret(\"long_secret_part2\")\nloaded_secret = long_secret_part1 + long_secret_part2\n\n# Confirm they are equal\nprint(\"Secrets are equal\" if long_secret == loaded_secret else \"not equal..\")\n</code></pre> <p>Previous: Introduction | Next: Automated Init &gt; Build</p>"},{"location":"technical-tutorial/unified-devops/automated-init/build/","title":"Automated Initialization of Workers and Applications with Secrets","text":"<p>This example demonstrates how to pass custom configuration as OS environment variables along with secure secrets from the Practicus AI Vault (both personal and shared) to a Practicus AI Worker and an Application. This method allows you to configure necessary settings at startup without hardcoding sensitive data.</p>"},{"location":"technical-tutorial/unified-devops/automated-init/build/#key-concepts","title":"Key Concepts","text":"<ul> <li>Environment Variables: Regular OS-level variables available to the worker or app.</li> <li>Personal Secrets: Private secrets that are injected as environment variables.</li> <li>Shared Secrets: Secrets that can be shared across projects or team members, also injected as environment variables. These are only defined by an admin, and using Practicus AI admin console.</li> <li>Worker Interaction: After launching a worker and opening a notebook on it, you can verify that the environment variables and secrets have been set correctly. (Remember: never log or display actual secret values in production!)</li> </ul>"},{"location":"technical-tutorial/unified-devops/automated-init/build/#worker-initialization","title":"Worker Initialization","text":"<pre><code>worker_size = None\napp_deployment_key = None\napp_prefix = None\npersonal_secret_key = None\nshared_secret_key = None\n</code></pre> <pre><code>assert worker_size, \"Please enter your worker_size.\"\nassert app_deployment_key, \"Please select an app deployment setting.\"\nassert app_prefix, \"Please select an app prefix.\"\nassert personal_secret_key, \"Please enter a personal secret key to test within the app\"\nassert shared_secret_key, \"Please enter a shared secret key to test within the app\"\n</code></pre> <pre><code>import practicuscore as prt\n\n# Configure the worker with environment variables and secrets\nworker_config = prt.WorkerConfig(\n    worker_size=worker_size,\n    env_variables={\n        \"MY_FIRST_ENV\": \"123\",  # Standard environment variable as a string\n        \"MY_SECOND_ENV\": 123,  # Standard environment variable as a number\n    },\n    personal_secrets=[personal_secret_key],  # Personal secret (injected as an environment variable)\n    shared_secrets=[shared_secret_key],  # Shared secret (injected as an environment variable)\n)\n\n# Create and start the worker\nworker = prt.create_worker(worker_config)\n\n# Open a Jupyter notebook on the newly created worker\nnotebook_url = worker.open_notebook(get_url_only=True)\nprint(f\"notebook_url: {notebook_url}\")\n\n# Open Jupyter notebook on the new browser tab.\n# worker.open_notebook()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-init/build/#verifying-environment-variables-and-secrets","title":"Verifying Environment Variables and Secrets","text":"<p>To confirm that the environment variables and secrets have been injected correctly, open the worker\u2019s terminal (not in this notebook) and run the following commands:</p> <pre><code>echo \"MY_FIRST_ENV is: $MY_FIRST_ENV\"\necho \"MY_SECOND_ENV is: $MY_SECOND_ENV\"\n# For security, only check the length of secret values instead of printing them\necho \"PERSONAL_SECRET_1 length:\" $(echo $PERSONAL_SECRET_1 | wc -m)\necho \"SHARED_SECRET_1 length:\" $(echo $SHARED_SECRET_1 | wc -m)\n</code></pre> <p>These commands ensure that the worker\u2019s environment is configured correctly without revealing sensitive information.</p> <pre><code># Terminate the worker when you are done\nworker.terminate()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-init/build/#app-initialization","title":"App Initialization","text":"<p>Similar to workers, you can also deploy an Application with environment variables and secrets. In this example, the App includes an API (defined in <code>apis/verify.py</code>) that returns the current configuration for verification.</p> <pre><code># Need help finding app deployment settings and prefixes?\n# You can retrieve available app deployment settings and prefixes\nimport practicuscore as prt\n\nregion = prt.get_region()\nmy_app_settings = prt.apps.get_deployment_setting_list()\n\nprint(\"Available Application Deployment Settings:\")\ndisplay(my_app_settings.to_pandas())\n\nmy_app_prefixes = prt.apps.get_prefix_list()\n\nprint(\"Available Application Prefixes:\")\ndisplay(my_app_prefixes.to_pandas())\n</code></pre> <pre><code>import practicuscore as prt\n\napp_name = \"my-auto-configured-app\"\nvisible_name = \"My Auto Configured App\"\ndescription = \"This application is configured with environment variables and secrets.\"\nicon = \"check\"\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,\n    visible_name=visible_name,\n    description=description,\n    startup_script=\"echo 'hello' &gt; hello.txt\",\n    env_variables={\"MY_FIRST_ENV\": \"123\", \"MY_SECOND_ENV\": 123},\n    personal_secrets=[\"PERSONAL_SECRET_1\"],\n    shared_secrets=[\"SHARED_SECRET_1\"],\n    icon=icon,\n)\n\nprint(\"Launching UI at:\", app_url)\nprint(\"Launching API at:\", api_url)\n</code></pre> <pre><code># Verify the App's configuration by calling its API\nimport requests\n\n# Retrieve a session token for authentication\ntoken = prt.apps.get_session_token(api_url=api_url)\nverify_api_url = f\"{api_url}verify/\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n}\n\nprint(\"Requesting verification from:\", verify_api_url)\nresp = requests.get(verify_api_url, headers=headers)\n\nif resp.ok:\n    resp_json = resp.json()\n    print(\"Configuration received:\", resp_json)\n\n    print(\"Validating configuration values...\")\n    # Check that both environment variables have the expected value\n    assert resp_json[\"MY_FIRST_ENV\"] == resp_json[\"MY_SECOND_ENV\"] == \"123\"\n    # Verify that the secrets are present by checking their lengths\n    assert resp_json[\"PERSONAL_SECRET_1_LEN\"] &gt; 0\n    assert resp_json[\"SHARED_SECRET_1_LEN\"] &gt; 0\n    # Confirm that the startup script executed correctly\n    assert resp_json[\"hello_txt_content\"] == \"hello\"\n    print(\"Configuration validated successfully.\")\nelse:\n    print(\"Error during API verification:\", resp.status_code, resp.text)\n</code></pre> <pre><code># Cleanup: Delete the deployed App when finished\nimport practicuscore as prt\n\nregion = prt.get_region()\nprt.apps.delete(prefix=app_prefix, app_name=app_name)\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-init/build/#customizing-secret-os-environment-variable-names","title":"Customizing Secret OS Environment Variable Names","text":"<p>You can rename the environment variable used to inject a secret by using the <code>name:other_name</code> format. For example:</p> <ul> <li>Specifying <code>\"PERSONAL_SECRET_1:SOME_ENV_NAME\"</code> will look up the secret with the name <code>PERSONAL_SECRET_1</code> and inject it into the environment as <code>SOME_ENV_NAME</code>.</li> <li>Similarly, <code>\"SHARED_SECRET_1:OTHER_ENV_NAME\"</code> retrieves the secret <code>SHARED_SECRET_1</code> and makes it available as <code>OTHER_ENV_NAME</code>.</li> </ul> <p>This format is supported for both personal and shared secrets, enabling you to avoid naming conflicts and integrate the secrets seamlessly into your application or worker configuration.</p>"},{"location":"technical-tutorial/unified-devops/automated-init/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/unified-devops/automated-init/build/#apisverifypy","title":"apis/verify.py","text":"<pre><code>import os\nimport practicuscore as prt\n\n\nMY_FIRST_ENV = os.getenv(\"MY_FIRST_ENV\", default=\"?\")\nMY_SECOND_ENV = os.getenv(\"MY_SECOND_ENV\", default=\"?\")\nPERSONAL_SECRET_1 = os.getenv(\"PERSONAL_SECRET_1\", default=\"?\")\nSHARED_SECRET_1 = os.getenv(\"SHARED_SECRET_1\", default=\"?\")\n# All Practicus AI apps boot at /var/practicus, and your files are copied here.\nHELLO_TXT_PATH = \"/var/practicus/hello.txt\"\n\n\n@prt.apps.api(\"/verify\")\nasync def verify(**kwargs):\n    hello_txt_content = \"?\"\n    if os.path.exists(HELLO_TXT_PATH):\n        with open(HELLO_TXT_PATH, \"r\") as f:\n            hello_txt_content = f.read().strip()\n\n    return {\n        \"msg\": \"Reading configuration from OS environment variables. (Only length of secrets for security)\",\n        \"MY_FIRST_ENV\": MY_FIRST_ENV,\n        \"MY_SECOND_ENV\": MY_SECOND_ENV,\n        \"PERSONAL_SECRET_1_LEN\": len(PERSONAL_SECRET_1),\n        \"SHARED_SECRET_1_LEN\": len(SHARED_SECRET_1),\n        \"hello_txt_content\": hello_txt_content,\n    }\n</code></pre> <p>Previous: Secrets With Vault | Next: Automated Git Sync</p>"},{"location":"technical-tutorial/workflows/introduction/","title":"Introduction","text":""},{"location":"technical-tutorial/workflows/introduction/#introduction-to-practicus-ai-workflows-and-tasks","title":"Introduction to Practicus AI Workflows and Tasks","text":"<p>In Practicus AI, a workflow is essentially a structured sequence of tasks\u2014units of work that run independently on isolated Practicus AI Workers (Kubernetes pods). By designing workflows as collections of tasks, you gain flexibility, scalability, and maintainability. Each task executes in its own fresh environment, ensuring that resource usage is optimized and preventing interference between tasks.</p> <p>Key points to remember:</p> <ul> <li>Tasks are the fundamental building blocks of workflows. They encapsulate any operation, such as running a Python script or executing a shell command.</li> <li>Isolation: Every task runs inside its own Worker, a dedicated pod with specified CPU, memory, and optional GPU resources. This ensures consistent performance, security, and an easy path to debug or scale individual components.</li> <li>Workflows bring these tasks together, defining their order, parallelism, and dependencies. Using orchestrators like Airflow, you can schedule, monitor, and manage these tasks collectively.</li> </ul> <p>With this approach, Practicus AI ensures that both simple jobs and complex enterprise-level workflows benefit from automated resource management, improved reliability, and streamlined development-to-production workflows.</p> <p>Modern data pipelines often need to implement two key concepts: Atomicity and Idempotency. Practicus AI naturally supports these concepts by breaking complex workflows into atomic and idempotent tasks, ultimately improving the consistency and reliability of your pipelines.</p>"},{"location":"technical-tutorial/workflows/introduction/#modern-data-pipelines-atomicity-and-idempotency","title":"Modern Data Pipelines: Atomicity and Idempotency","text":""},{"location":"technical-tutorial/workflows/introduction/#atomicity","title":"Atomicity","text":"<p>In traditional ETL scenarios, pipelines would process data in small batches due to hardware constraints, potentially leaving destinations partially updated if a failure occurred mid-process. Atomicity ensures that changes are applied fully or not at all, preventing partial and inconsistent updates.</p> <p>Non-Atomic Pipeline Example (not recommended):</p> <p></p> <p>Here, a failure in the middle of the process leaves the target data source in an <code>inconsistent state</code>.</p> <p>Atomic Pipeline Example (recommended):</p> <p></p> <p>An atomic pipeline ensures that a task\u2019s output is only considered complete and available once the entire operation finishes successfully. If the task fails, no partial data is committed, and the system <code>remains consistent</code>.</p>"},{"location":"technical-tutorial/workflows/introduction/#idempotency","title":"Idempotency","text":"<p>Idempotence means that running a task multiple times yields the same final state. For example, pressing an \u201con\u201d button repeatedly on an idempotent device doesn\u2019t change its state after the first press\u2014it stays \u201con.\u201d</p> <p>Idempotency Example:</p> <p></p> <p>When tasks are idempotent, <code>retries or repeated executions due to transient failures do not corrupt or alter the final outcome unexpectedly</code>. This is especially valuable in large-scale data processing, where intermittent failures are common.</p>"},{"location":"technical-tutorial/workflows/introduction/#practicus-ais-approach","title":"Practicus AI\u2019s Approach","text":"<p>Practicus AI makes it straightforward to create workflows that are both atomic and idempotent by default. </p>"},{"location":"technical-tutorial/workflows/introduction/#default-anatomy-of-a-practicus-ai-data-pipeline","title":"Default Anatomy of a Practicus AI Data Pipeline","text":"<p>Practicus AI workflows are represented as a DAG, describing the order, parallelism, and dependencies of tasks. Consider an example workflow:</p> <ol> <li>Load and transform table1, then export as table1_new.</li> <li>Load and transform table2, then export as table2_new.</li> <li>Load table3 and join it with table1_new and table2_new outputs.</li> <li>Load table4 (no changes needed).</li> <li>In table3\u2019s task, now also join table4.</li> <li>Export the final processed data to final_table.</li> </ol> <p>Because each task exports its results upon completion, <code>every subsequent join relies on stable, already-written data</code>, promoting both <code>atomicity</code> and <code>idempotency</code>. Parallel tasks (like table1 and table2) <code>can run simultaneously</code>, and if either needs to be re-run, the final state remains <code>consistent</code> and <code>predictable</code>.</p> <p></p> <p>Sample Airflow DAG:</p> <ul> <li>Process table1 and table 2 in parallel, and independent of each other.</li> <li>Upon completion, if both are successful, process to create the final table.</li> </ul> <pre><code>dag_flow = \"[process_table_1, process_table_2] &gt;&gt; process_final_table\"\n</code></pre>"},{"location":"technical-tutorial/workflows/introduction/#flexibility-and-customization","title":"Flexibility and Customization","text":"<p>While the default approach encourages atomicity and idempotency, you retain full control. You can modify DAGs, customize tasks, and change how data is read, transformed, and saved. For example, if you prefer to capture or skip certain logs, or run tasks in different regions for geo-distributed workflows, you can update worker configurations and DAG definitions accordingly.</p> <p>In short, Practicus AI\u2019s model of defining workflows as collections of independent, atomic, and idempotent tasks ensures that your data pipelines are robust, resilient to failures, and easy to understand and maintain. This sets a solid foundation for building more advanced workflows with orchestration tools like Airflow, enabling you to confidently move from simple scripts to enterprise-grade automation.</p> <p>Previous: Build And Deploy | Next: Tasks &gt; Task Basics</p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/","title":"Generating Wokflows","text":""},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#using-practicus-ai-studio-with-airflow","title":"Using Practicus AI Studio with Airflow","text":"<p>You can use Practicus AI Studio for the following tasks for Airflow workflows.</p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#practicus-ai-studio-functionality-for-airflow","title":"Practicus AI Studio functionality for Airflow","text":"<ul> <li>Explore data sources such as Data Lakes, Data Warehouses and Databases</li> <li>Transform data</li> <li>Join data from different data sources</li> <li>Export the result to any data source</li> <li>Perform these tasks on individual Workers or on distributed Spark cluster</li> <li>Generate data processing steps as Python code</li> <li>Auto-detect dependencies between tasks</li> <li>Generate the DAG code </li> <li>Export data connection files separately so you can change them later</li> </ul>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#sample-scenario","title":"Sample scenario","text":"<ul> <li>Load some_table from a Database A<ul> <li>Make changes</li> <li>Save to Database B</li> </ul> </li> <li>Load some_other_table from a Data Lake C<ul> <li>Make changes</li> <li>Save to Data Warehouse D</li> </ul> </li> <li>Load final_table from Database E<ul> <li>Join to some_table</li> <li>Join to some_other_table</li> <li>Make other changes</li> <li>Save to Data Lake F</li> <li>Export everything to Airflow</li> </ul> </li> </ul> <p>Let's take a quick look on the experience.</p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#joining-data-sources","title":"Joining data sources","text":"<ul> <li>Left joining final_table with column ID to some_other_table column ID</li> </ul>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#exporting-to-airflow","title":"Exporting to Airflow","text":"<ul> <li>Practicus AI automatically detects the dependency:</li> <li>Operations on some_table and some_other_table can execute in parallel since they do not depend on each other</li> <li>If both are successful, operations on final_table can happen including joins</li> </ul>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#viewing-the-exported-code","title":"Viewing the exported code","text":"<ul> <li>After the code export is completed you can update 4 types of files:</li> <li><code>.py files:</code> Each are tasks that include the data processing steps, SQL etc.</li> <li><code>.._worker.json files:</code> Defines the worker that each task will run on.<ul> <li>Container image to use, worker capacity (CPU, GPU, RAM) ..</li> </ul> </li> <li><code>.._conn.json files:</code> Defines how to read data for each task.<ul> <li>Note: Data source credentials can be stored in the Practicus AI data catalog.</li> </ul> </li> <li><code>.._save_conn.json files:</code> Defines how to write data for each task.<ul> <li>Note: Data source credentials can be stored in the Practicus AI data catalog.</li> </ul> </li> <li><code>.._join_.._conn.json files:</code> Defines how each join operation will work: how to read data and where to join.</li> <li><code>.._dag.py file:</code> The DAG file that brings everything together.</li> </ul> <p>Sample view from the embedded Jupyter notebook inside Practicus AI Studio.</p> <p></p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#airflow-deployment-options","title":"Airflow deployment options","text":"<p>You have 2 options to deploy to Airflow from Practicus AI Studio.</p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#self-service","title":"Self-service","text":"<ul> <li>Select the schedule and deploy directly to Airflow add-on service that an admin gave you access to.</li> <li>This will instantly start the Airflow schedule.</li> <li>You can then view your DAGs using Practicus AI and monitor the state of your workflows.</li> <li>You can also manually trigger DAGs.</li> </ul>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#working-with-a-data-engineer-recommended-for-sensitive-data","title":"Working with a Data Engineer (recommended for sensitive data)","text":"<ul> <li>Just export the code and share with a Data Engineer, so they can:</li> <li>Validate your steps (.py files)</li> <li>Update data sources for production databases (conn.json files)</li> <li>Select appropriate Worker capacity (worker.json files)</li> <li>Select appropriate Worker user credentials (worker.json files)</li> <li>Deploy to Airflow </li> <li>Define the necessary monitoring steps with automation (e.g. with Practicus AI observability)</li> </ul> <p>Previous: Deploying On Airflow | Next: Mlflow &gt; Insurance Mlflow</p>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/","title":"Deploying On Airflow","text":"<pre><code>airflow_service_key = None\n</code></pre> <pre><code>assert airflow_service_key, \"Please provide airflow service key\"\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#workflows-with-the-practicus-ai-airflow-add-on","title":"Workflows with the Practicus AI Airflow Add-on","text":"<p>Practicus AI integrates seamlessly with Airflow to orchestrate workflows. By leveraging Airflow as an add-on, you can:</p> <ul> <li>Define complex directed acyclic graphs (DAGs) to manage task order and parallelism.</li> <li>Schedule tasks to run at specific times or intervals.</li> <li>Use Airflow's UI and ecosystem for monitoring and managing workflows.</li> </ul> <p>For more details on Airflow concepts, see the official Airflow documentation.</p>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#creating-tasks-dags-and-supporting-files","title":"Creating Tasks, DAGs, and Supporting Files","text":"<p>When building workflows, start by designing your tasks as independently executable units. Each task runs on its own Practicus AI Worker. You can group related actions into a single task for simplicity. Practicus AI provides utilities to generate starter files and DAG definitions.</p> <p>Example DAG Flow:</p> <pre><code>dag_flow = \"my_1st_task &gt;&gt; [my_2nd_task, my_3rd_task] &gt;&gt; my_4th_task\"\n</code></pre> <p>This means:</p> <ul> <li><code>my_1st_task</code> runs first.</li> <li>On success, <code>my_2nd_task</code> and <code>my_3rd_task</code> run in parallel.</li> <li>After both complete, <code>my_4th_task</code> runs.</li> </ul> <pre><code>import practicuscore as prt\n\n# Define a DAG flow with 4 tasks, where two run in parallel.\ndag_flow = \"my_1st_task &gt;&gt; [my_2nd_task, my_3rd_task] &gt;&gt; my_4th_task\"\n\n# Default worker configuration\ndefault_worker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",\n    worker_size=\"X-Small\",\n)\n\n# Custom configuration for the second task\nmy_2nd_task_worker_config = prt.WorkerConfig(\n    worker_image=\"practicus-genai\",\n    worker_size=\"Small\",\n)\n\ncustom_worker_configs = [\n    (\"my_2nd_task\", my_2nd_task_worker_config),\n]\n\ndag_key = \"my_workflow\"\nschedule_interval = None  # Set a cron string or '@daily' as needed\nretries = 0  # 0 for dev/test, increase for production\n\nprt.workflows.generate_files(\n    dag_key=dag_key,\n    dag_flow=dag_flow,\n    files_path=None,  # Current dir\n    default_worker_config=default_worker_config,\n    custom_worker_configs=custom_worker_configs,\n    save_credentials=True,\n    overwrite_existing=False,\n    schedule_interval=schedule_interval,\n    retries=retries,\n)\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#understanding-the-generated-files","title":"Understanding the Generated Files","text":"<p>Task Python Scripts (e.g., <code>my_1st_task.py</code>):</p> <ul> <li>Contain the logic for each task.</li> <li>Each runs in its own isolated Worker.</li> </ul> <p><code>default_worker.json</code>:</p> <ul> <li>Stores default worker configuration (image, size, credentials).</li> <li>Credentials can be set globally by an admin or passed at runtime.</li> </ul> <p><code>my_2nd_task_worker.json</code>:</p> <ul> <li>Overrides the worker config for <code>my_2nd_task</code>.</li> </ul> <p><code>my_workflow_dag.py</code>:</p> <ul> <li>The Airflow DAG file that ties tasks together.</li> </ul>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#test-your-tasks-before-deploying","title":"Test Your Tasks Before Deploying","text":"<p>It's wise to test tasks locally or via Practicus AI before deploying to Airflow. You can intentionally insert errors in your task code to verify error handling.</p> <p>For more details on tasks, see the tasks sample notebook. </p> <pre><code>successful_task_workers, failed_task_workers = prt.workflows.test_tasks(\n    dag_flow=dag_flow,\n    task_list=None,  # Test all tasks in the DAG\n    files_path=None,  # Current dir\n    default_worker_config=default_worker_config,\n    custom_worker_configs=custom_worker_configs,\n    terminate_on_success=True,  # Automatically terminate successful tasks\n    terminate_on_failed=False,  # Keep failed tasks alive for debugging\n)\n</code></pre> <pre><code># Investigate successful or failed tasks\nfor worker in successful_task_workers:\n    # If you had terminate_on_success=False, you could open the notebook to review logs.\n    print(f\"Opening notebook on successful task worker: {worker.name}\")\n    worker.open_notebook()\n\nfor worker in failed_task_workers:\n    print(f\"Opening notebook on failed task worker: {worker.name}\")\n    worker.open_notebook()\n</code></pre> <pre><code># After analysis, terminate remaining workers\nfor worker in successful_task_workers:\n    print(f\"Terminating successful task worker: {worker.name}\")\n    worker.terminate()\n\nfor worker in failed_task_workers:\n    print(f\"Terminating failed task worker: {worker.name}\")\n    worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#optional-locating-an-airflow-service-add-on","title":"(Optional) Locating an Airflow Service Add-on","text":"<p>If you don't know your Airflow service key, you can list available add-ons and identify it. For instructions on add-ons and their usage, see the Practicus AI documentation.</p> <pre><code># import practicuscore as prt\n\n# region = prt.get_default_region()\n# addons_df = prt.addons.get_list().to_pandas()\n\n# print(\"Add-on services accessible to you:\")\n# display(addons_df)\n\n# airflow_services_df = addons_df[addons_df[\"service_type\"] == \"Airflow\"]\n# print(\"Airflow services you can access:\")\n# display(airflow_services_df)\n\n# if airflow_services_df.empty:\n#     raise RuntimeError(\"No Airflow service access. Contact your admin.\")\n\n# airflow_service_key = airflow_services_df.iloc[0][\"key\"]\n# service_url = airflow_services_df.iloc[0][\"url\"]\n\n# print(\"Selected Airflow Service:\")\n# print(f\"- Service Key: {airflow_service_key}\")\n# print(f\"- Service URL: {service_url}\")\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#deploying-the-workflow-to-airflow","title":"Deploying the Workflow to Airflow","text":"<p>Once your tasks, DAG, and configurations are ready and tested, deploy them to Airflow. This pushes your code and configuration to the underlying version control system used by the Airflow service, making the workflow visible and runnable via the Airflow UI.</p> <pre><code>prt.workflows.deploy(\n    service_key=airflow_service_key,\n    dag_key=dag_key,\n    files_path=None,  # Current directory\n)\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#additional-notes-and-customizations","title":"Additional Notes and Customizations","text":"<ul> <li>Running Shell Scripts: Tasks don't have to be Python files; <code>.sh</code> scripts also work.</li> <li>Manual Worker Config Files: Instead of passing parameters to <code>generate_files()</code> or <code>deploy()</code>, you can manually manage the <code>.json</code> worker config files.</li> <li>Credential Management: For security, consider storing credentials globally at the Airflow environment level. Avoid embedding sensitive info in local files.</li> <li>Multi-Region Deployments: You can create workflows that run tasks in different regions. Just ensure the worker config <code>.json</code> files point to the correct <code>service_url</code>, <code>email</code>, and <code>refresh_key</code>.</li> <li>Customizing the DAG: Edit the generated DAG file to change default parameters, logging settings, or to add custom logic. For complex scenarios (e.g., different logging strategies), you can customize the <code>run_airflow_task</code> calls as shown in the example snippet.</li> <li>Runtime parameters and dynamic tasks: To pass DAG runtime parameters to a Worker, provide the dynamic <code>env_variables</code> argument when calling the task\u2014these values will be set as OS environment variables in the Worker that executes the task. For example, the <code>dynamic_task</code> helper function below retrieves the runtime execution date and injects it as an environment variable. Instead of using separate code to modify parameters, you can simply define a helper function that receives **kwargs, constructs your dynamic parameters, and then calls <code>prt.workflows.run_airflow_task</code> with those values, giving you the flexibility to adjust each task\u2019s behavior dynamically.</li> </ul> <pre><code>@dag(\n    # ...\n)\ndef generate_dag():\n    def dynamic_task(**kwargs):\n        # Get the dynamic runtime parameter, e.g., the execution date.\n        execution_date = kwargs.get(\"execution_date\", \"\")\n        # Build the env_variables dictionary.\n        env_variables = {\n            \"EXECUTION_DATE\": str(execution_date)\n        }\n        # Start the worker with the dynamic env_variables, which will be merged with existing parameters.\n        return prt.workflows.run_airflow_task(env_variables=env_variables, **kwargs)\n\n    # Instead of the below\n    # my_1st_task = task(prt.workflows.run_airflow_task, task_id=\"my_1st_task\")()\n    # Use the dynamic_task function we defined above\n    my_1st_task = task(dynamic_task, task_id=\"my_1st_task\")()\n\n    # ...\n</code></pre> <ul> <li>In this example, the dynamic parameter (the <code>execution_date</code>) is captured at runtime and added to the <code>env_variables</code> dictionary. This dictionary is then passed to <code>prt.workflows.run_airflow_task</code> along with all other keyword arguments, ensuring that your Worker gets the merged configuration.</li> </ul> <p>Previous: Task Basics | Next: AI Studio &gt; Generating Wokflows</p>"},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/","title":"Insurance Mlflow","text":""},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#step-1-pre-process","title":"Step 1: Pre-process","text":""},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#we-will-use-practicus-sdk-to-pre-process-the-current-data","title":"We will use Practicus SDK to pre process the current data","text":"<pre><code>import practicuscore as prt\nimport pandas as pd\n\nworker = prt.get_local_worker()\n</code></pre> <pre><code>conn_conf = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"sampling_method\": \"ALL\",\n    \"file_path\": \"/home/ubuntu/samples/data/insurance.csv\",\n}\n\nproc = worker.load(conn_conf)\nproc.show_head()\n</code></pre> <pre><code>df = proc.get_df_copy()\n</code></pre>"},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#step-2-initializing-the-automl-experiment","title":"Step 2: Initializing the AutoML Experiment","text":""},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#pycarets-regression-module-is-utilized-here-for-predicting-a-continuous-target-variable-ie-energy-costs-we-begin-by-initializing-our-automl-experiment","title":"PyCaret's regression module is utilized here for predicting a continuous target variable, i.e., energy costs. We begin by initializing our AutoML experiment.","text":"<pre><code>df_model = df\n</code></pre> <pre><code>from pycaret.regression import RegressionExperiment, load_model, predict_model\n\nexp = RegressionExperiment()\n</code></pre>"},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#step-3-configuring-the-experiment","title":"Step 3: Configuring the Experiment","text":""},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#well-configure-our-experiment-with-a-specific-name-making-it-easier-to-manage-and-reference","title":"We'll configure our experiment with a specific name, making it easier to manage and reference.","text":"<pre><code># You need to configure using the service unique key, you can find your key on the \"Practicus AI Admin Console\"\n# service_key = 'mlflow-primary'\n# Optionally, you can provide experime name to create a new experiement while configuring\nexperiment_name = \"insurance\"\n\n# prt.experiments.configure(service_key=service_key, experiment_name=experiment_name)\n# No experiment service selected, will use MlFlow inside the Worker. To configu3re manually:\n# prt.experiments.configure(experiment_name=experiment_name, service_name='Experiment service name')\n</code></pre>"},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#step-4-preparing-data-with-pycarets-setup","title":"Step 4: Preparing Data with PyCaret's Setup","text":""},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#a-critical-step-where-we-specify-our-experiments-details-such-as-the-target-variable-session-id-for-reproducibility-and-whether-to-log-the-experiment-for-tracking-purposes","title":"A critical step where we specify our experiment's details, such as the target variable, session ID for reproducibility, and whether to log the experiment for tracking purposes.","text":"<pre><code># setup_params = {'normalize': True, 'normalize_method': 'minmax',\n#'remove_outliers' : True, 'outliers_method':  'iforest'}\n</code></pre> <pre><code>exp.setup(\n    data=df_model,\n    target=\"charges\",\n    session_id=42,\n    log_experiment=True,\n    feature_selection=True,\n    experiment_name=experiment_name,\n)\n</code></pre>"},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#step-5-model-selection-and-tuning","title":"Step 5: Model Selection and Tuning","text":""},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#this-command-leverages-automl-to-compare-different-models-automatically-selecting-the-one-that-performs-best-according-to-a-default-or-specified-metric-its-a-quick-way-to-identify-a-strong-baseline-model-without-manual-experimentation","title":"This command leverages AutoML to compare different models automatically, selecting the one that performs best according to a default or specified metric. It's a quick way to identify a strong baseline model without manual experimentation.","text":"<pre><code>best_model = exp.compare_models(include=[\"lr\", \"lasso\", \"lightgbm\"])\n</code></pre>"},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#once-a-baseline-model-is-selected-this-step-fine-tunes-its-hyperparameters-to-improve-performance-the-use-of-tune-sklearn-and-hyperopt-indicates-an-advanced-search-across-the-hyperparameter-space-for-optimal-settings-which-can-significantly-enhance-model-accuracy","title":"Once a baseline model is selected, this step fine-tunes its hyperparameters to improve performance. The use of tune-sklearn and hyperopt indicates an advanced search across the hyperparameter space for optimal settings, which can significantly enhance model accuracy.","text":"<pre><code>tune_params = {}\n</code></pre> <pre><code>tuned_model = exp.tune_model(best_model, **tune_params)\n</code></pre> <pre><code>final_model = exp.finalize_model(tuned_model)\n</code></pre> <pre><code>predictions = exp.predict_model(final_model, data=df)\ndisplay(predictions)\n</code></pre> <pre><code>predictions\n</code></pre> <pre><code>exp.save_model(final_model, \"model\")\n</code></pre>"},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#recommneded-adding-model-metadata-to-your-api","title":"(Recommneded) Adding model metadata to your API","text":"<ul> <li>You can create and upload model.json file that defines the input and output schema of your model and potentially other metadata too.</li> <li>This will explain how to consume your model efficiently and make it accessible to more users.</li> <li>Practicus AI uses MlFlow model input/output standard to define the schema</li> <li>You can build the model.json automatically, or let Practicus AI build it for you using the dataframe.</li> </ul> <pre><code>model_config = prt.models.create_model_config(\n    df=df,\n    target=\"charges\",\n    model_name=\"insurance-4\",\n    problem_type=\"Regression\",\n    version_name=\"2024-05-30\",\n    final_model=\"knn\",\n    score=4.2493,\n)\nmodel_config.save(\"model.json\")\n# You also can directly instantiate ModelConfig class to provide more metadata elements\n# model_config = prt.models.ModelConfig(...)\n</code></pre>"},{"location":"technical-tutorial/workflows/mlflow/insurance-mlflow/#step-6-model-deployment","title":"Step 6: Model Deployment","text":"<pre><code>df.head()\n</code></pre> <pre><code>deployment_key = \"automl-depl\"\nassert deployment_key, \"Please select a deployment key\"\nprefix = \"models\"\nmodel_name = \"insurance-mlflow-test\"\nmodel_dir = None\n</code></pre> <pre><code># Deploy to current Practicus AI region\nprt.models.deploy(deployment_key=deployment_key, prefix=prefix, model_name=model_name, model_dir=model_dir)\n</code></pre> <pre><code>region = prt.current_region()\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token (or reuse existing, if not expired).\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = None\ntoken = prt.models.get_session_token(api_url, token=token)\nprint(\"API session token:\", token)\n</code></pre> <pre><code>import requests\n\nheaders = {\"authorization\": f\"Bearer {token}\", \"content-type\": \"text/csv\"}\ndata_csv = df.head(5000).to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <p>Previous: Generating Wokflows | Next: Generative AI &gt; Introduction</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/","title":"Running Tasks on Practicus AI Workers","text":"<p>Before exploring orchestrations like Airflow, it\u2019s important to grasp the fundamentals of tasks in Practicus AI and how they run on workers.</p> <p>Practicus AI executes tasks by creating isolated, on-demand Kubernetes pods (Workers). Each task runs in its own Worker environment, ensuring scalability, isolation, and resource-efficiency.</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#what-is-a-task","title":"What is a Task?","text":"<p>A task is a unit of work, typically a Python script or shell script, that you want to execute in a controlled environment. Tasks can be composed into larger workflows or run as standalone jobs. They form the building blocks for repeatable, automated processes.</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#how-practicus-ai-executes-tasks","title":"How Practicus AI Executes Tasks","text":"<ol> <li>Task Submission: You define the code (e.g., a Python file <code>task_1.py</code>) along with any parameters.</li> <li>Worker Creation: Practicus AI provisions a dedicated Worker pod to run your task.</li> <li>Task Execution: The Worker runs your code, capturing stdout and stderr logs by default.</li> <li>Termination: After completion, the Worker is automatically terminated to free up resources.</li> </ol> <p>This pattern ensures that every task runs in a fresh environment and that resources are only consumed for as long as they are needed.</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#running-a-task-using-the-practicus-ai-sdk","title":"Running a Task Using the Practicus AI SDK","text":"<p>Using the Practicus AI SDK, you can easily submit tasks to be executed on Workers. The SDK handles provisioning, running, and cleaning up the Worker, so you can focus on your code.</p> <pre><code>import practicuscore as prt\n\n# This call:\n#  - Creates a new Worker\n#  - Uploads files from the current directory by default\n#  - Runs 'task_1.py'\n#  - Captures and prints the script output\n#  - Terminates the Worker after completion\nprt.run_task(file_name=\"task_1.py\")\n\nprint(\"Task completed and worker is terminated.\")\n</code></pre> <pre><code># Running a shell script task\nprt.run_task(file_name=\"task_2.sh\")\nprint(\"Shell task completed and worker is terminated.\")\n</code></pre> <pre><code># Customizing the worker environment\n\n# Example startup script (uncomment to use):\n# startup_script = \"\"\"\n# sudo apt-get update\n# sudo apt-get install -y some-package\n# pip install some-library\n# \"\"\"\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",\n    worker_size=\"X-Small\",\n    # startup_script=startup_script,\n)\n\n# This task intentionally fails to demonstrate debugging\nworker, success = prt.run_task(\n    file_name=\"task_with_error.py\",\n    worker_config=worker_config,\n    terminate_on_completion=False,  # Keep the worker alive to inspect\n)\n\nprint(\"Task execution finished, next cell to investigate...\")\n</code></pre> <pre><code># If the task failed, inspect the environment.\n# Uploaded files and logs are in ~/practicus/task/\n\nif not success:\n    # Opens Jupyter notebook in a new browser tab to troubleshoot\n    worker.open_notebook()\n</code></pre> <pre><code>print(f\"Done analyzing, now terminating {worker.name}.\")\nworker.terminate()\n</code></pre>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#tasks-with-jupyter-notebooks","title":"Tasks with Jupyter Notebooks","text":"<p>You can also run tasks defined as Jupyter notebooks: - Dynamically pass parameters to notebooks. - Collect and manage output artifacts.</p> <p>Create <code>task_with_notebook.ipynb</code> with the below code that raises an error.</p> <pre><code>print(\"This task runs inside a notebook.\")\n\nraise SystemError(\"Simulated error\")\n</code></pre> <p>Create <code>task_with_notebook.py</code> that triggers the notebook we just created.</p> <pre><code>import practicuscore as prt\n\nprt.notebooks.execute_notebook(\n    \"task_with_notebook\",\n    raise_on_failure=True,\n)\n</code></pre> <p>To run the notebook as a task:</p> <pre><code>print(\"Running a notebook as task\")\nworker, success = prt.run_task(\n    file_name=\"task_with_notebook.py\",\n    terminate_on_completion=False,\n)\nprint(\"Task was successful.\" if success else \"Task failed.\")\n</code></pre> <pre><code>print(f\"Opening Jupyter notebook on {worker.name}.\")\nprint(\"Check ~/practicus/task/ for output notebooks, e.g. task_with_notebook_output.ipynb\")\nworker.open_notebook()\n</code></pre> <pre><code>print(\"Terminating worker.\")\nworker.terminate()\n</code></pre>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#additional-tips","title":"Additional Tips","text":"<ul> <li>Already logging task output?</li> <li> <p>Disable additional capture using <code>prt.run_task(..., capture_task_output=False)</code>.</p> </li> <li> <p>Need a custom virtual environment?</p> </li> <li> <p>Create or specify a Python venv under <code>~/.venv/</code> and run tasks with <code>python_venv_name=\"your_venv\"</code>.</p> </li> <li> <p>Running automated notebooks from CLI?</p> </li> <li>Use <code>prtcli</code> commands and <code>.sh</code> scripts to trigger notebook executions.</li> </ul> <p>These options give you flexibility in how tasks run, interact with environments, and handle their outputs.</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/workflows/tasks/task-basics/#task_1py","title":"task_1.py","text":"<pre><code>print(\"Hello from simple task 1\")\n</code></pre>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#task_2sh","title":"task_2.sh","text":"<pre><code>echo \"Hello from simple task 2\"\n</code></pre>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#task_with_errorpy","title":"task_with_error.py","text":"<pre><code>import practicuscore as prt\n\n\ndef main():\n    print(\"Starting task..\")\n\n    # Code as usual:\n    # - Process data\n    # - Train models\n    # - Make predictions\n    # - Orchestrate other tasks\n    # - ...\n\n    try:\n        raise NotImplementedError(\"Still baking..\")\n    except Exception as ex:\n        # Psudo detail log\n        with open(\"my_log.txt\", \"wt\") as f:\n            f.write(str(ex))\n        raise ex\n\n    print(\"Finished task..\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Introduction | Next: Airflow &gt; Deploying On Airflow</p>"}]}