{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Practicus AI Documentation","text":""},{"location":"#about-practicus-ai","title":"About Practicus AI","text":"<p>Practicus AI is a unified platform for Generative AI and Data Intelligence. It helps you move from initial concepts to production-grade AI and Data solutions. Whether you are an experienced engineer or new to Data and AI, the platform is designed to be both reliable and easy to navigate.</p>"},{"location":"#select-benefits","title":"Select Benefits","text":"<p>Practicus AI brings together Generative AI, self-service AI and Data tools, MLOps, Orchestration, Kubernetes infrastructure management, and Observability into a single platform. This integrated, but flexible approach simplifies the process of working with AI and Data at scale.</p> <p></p>"},{"location":"#platform-components","title":"Platform Components","text":"<p>The Practicus AI Platform is built on an enterprise-grade core, with optional add-ons to extend functionality. This structure supports consistent deployment, governance, and scalability. It also provides a unified environment for managing applications, runtimes, and advanced analytics tools.</p> <p></p>"},{"location":"#core-platform","title":"Core Platform","text":"<p>The core platform establishes the foundation for a secure, scalable, and versatile AI environment. It consists of four layers: Applications, Mesh, Runtime, and Infrastructure. Each layer offers distinct capabilities\u2014such as creating and sharing GenAI applications, hosting advanced models and microservices, managing service meshes for reliability, and ensuring efficient use of GPUs and other resources.</p> <p></p>"},{"location":"#add-ons","title":"Add-Ons","text":"<p>In addition to the core platform, Practicus AI offer a variety of add-ons that can be selectively incorporated into your environment. These add-ons are protected by enterprise single sign-on and integrate smoothly with core platform and other add-on components. As we continue to expand the add-on library, you\u2019ll have access to a growing range of tools and features, ensuring that your platform remains both adaptable and secure as your AI initiatives evolve.</p>"},{"location":"#practicus-ais-core-principles","title":"Practicus AI's Core Principles","text":"<p>While the core platform and its add-ons define what you can accomplish with Practicus AI, a set of guiding principles ensures that every feature, integration, and improvement remains aligned with our core values of security, flexibility, and usability.</p>"},{"location":"#7-core-principles-of-the-practicus-ai-platform","title":"7 Core Principles of the Practicus AI Platform","text":"<ol> <li> <p>Security &amp; Compliance First:    The platform must maintain enterprise-level security and compliance at all times, enforcing measures such as SSO with MFA/OTP and strict data governance to safeguard sensitive information.</p> </li> <li> <p>Cloud-Native &amp; Vendor-Agnostic:    The platform must remain cloud-native and adhere to open-source and CNCF standards, ensuring it can be deployed in any environment\u2014on-premises, in the cloud, or within air-gapped configurations\u2014without creating vendor lock-in.</p> </li> <li> <p>Ease of Use &amp; Extensibility:    The platform must provide a unified interface that simplifies setup, enables rapid deployment, and supports extensibility, adapting smoothly to evolving requirements.</p> </li> <li> <p>Operational Simplicity for Technical Teams:    The platform must reduce operational complexity, allowing technical users to focus on designing and refining AI solutions rather than managing infrastructure overhead.</p> </li> <li> <p>Real-Time Observability &amp; Scalability:    The platform must offer continuous insights into performance and health, alongside automatic scaling and optimized GPU utilization, ensuring efficiency as workloads grow.</p> </li> <li> <p>Accessibility for Non-Technical Users:    The platform must deliver interfaces and tools that are easy to navigate, allowing non-technical users to engage in data exploration, analysis, and insight generation without requiring advanced expertise.</p> </li> <li> <p>Comprehensive Analytics &amp; Generative AI:    The platform must integrate a full spectrum of analytics capabilities, from traditional to advanced and generative AI, supporting informed decision-making and fostering innovation across diverse use cases.</p> </li> </ol>"},{"location":"#next-steps","title":"Next Steps","text":"<p>As you continue through this documentation, you will find sections tailored to different types of users. Technical teams can explore detailed operational information, while those new to AI can learn the fundamentals. Consider this the starting point for understanding how Practicus AI supports your AI and data initiatives.</p> <p>Next: Getting Started</p> <p><sup><sup>Practicus AI docs for v24.8.3 (Built on 2025-01-23 19:35)</sup></sup></p>"},{"location":"advanced/","title":"Advanced Features","text":"<p>In this section we will look at some advanced functionality of Practicus AI. </p>"},{"location":"advanced/#production-data-pipelines","title":"Production Data Pipelines","text":"<p>Typically, an ML workflow will depend on several cycles of data preparation steps before the final ML training can be done. This is rarely a one time task, since your ML model might eventually start drifting as time goes by (not predicting accurately anymore). When this happens, the first thing to try is usually re-training with new, fresh data. Which means you have to reload data from raw data sources, and apply data preparation steps again.  Instead of preparing data manually every time you train, some teams create automated data pipelines. These pipelines typically run on a schedule, for instance daily, creating clean and ready-to-train-on data sets. </p> <p>You can easily build data pipelines with Practicus AI, and embed them into existing data integration platforms.  All you need is a platform that can run Python. Please see some performance tips below in the example code </p> <p>Achieving this is straightforward, you can simply \"chain\" data preparation steps using .dp files, and not necessarily the updated Excel files for performance reasons. </p> <p>Example data pipeline:</p> <pre><code>import practicus\n# 1) create a Python .py file for the data pipeline\n# 2) place your data loading code as usual, from databases, S3 etc. \ndf = load_data_as_usual()\n# 3) execute changes in .dp files one after another\npracticus.apply_changes(df, \"data_prep_1.dp\", inplace=True)\npracticus.apply_changes(df, \"data_prep_2.dp\", inplace=True)\n# ...\npracticus.apply_changes(df, \"data_prep_10.dp\", inplace=True)\n# pipeline is completed\nsave_to_final_dest_as_usual(df)\n\n# Performance Tips: \n# 1) Pandas Data Frame will work faster with practicus, especially for very large datasets \n# 2) You can use \"inplace=True\" updates, to avoid creating new data frames \n# 3) you can delete sorting and column reordering commands in the .dp files, since your \n# ML model training will not care. Especially sorting is a relatively expensive operation.  \n# column reordering with Pandas Data Frame will also trigger creating a new df. \n</code></pre> <p>Deployment</p> <p>Once the data pipeline Python code (.py) is ready like the above, you can then \"pip install practicus\" in the data integration platform and run the .py file as usual. Please note that Practicus AI uses Java (&gt;= v8.0) behind the scenes for some complex operations. Although it is unlikely, if there is no Java runtime (JRE) installed on the data integration platform, Practicus AI will download a local copy of the JRE, first time it runs. This local Java installation is purely a \"file download\" operation of a stripped down JRE, and will not need any root/admin privileges.  You can also manually trigger downloading the local JRE operation in advance by calling practicus.install_jre(), or install Java (&gt;= v8.0) yourself. We recommend Open\u00a0JDK.  </p>"},{"location":"advanced/#dp-file-structure","title":"DP file structure","text":"<p>Practicus AI can detect changes made inside an Excel (.xlsx) file when you call detect_changes() or apply_changes()  functions. The recorded changes are then saved to a simple text file with the extension .dp (data prep).</p> <p>The .dp file is intended to be easily consumed by users that are not programmers or data scientists. The goal is to create a happy medium so that different user personas can view the changes detected in the .dp file and collaborate on a data science project.</p> <p>The .dp file does not necessarily need to only include changes that are detected by Practicus AI. As you can read below, you can freely add your own changes or remove existing ones, and then finally ask Practicus AI to run these data prep steps on your data sets to complete data preparation. </p> <p>The below is a simple .dp file example. </p> <p></p>"},{"location":"advanced/#input_columns","title":"Input_Columns","text":"<pre><code>Input_Columns = [Column Name 1], [Column Name 2], ..\n</code></pre> <p>This is an optional section, but it is recommended to use for data validation. When Practicus AI starts analyzing an Excel (.xlsx) file, it will detect all the visible column names and add them in the Input_Columns section.  When you execute apply_changes() with a new data set, Practicus AI will compare the column names of the input data set to Input_Columns and give you a warning if they are not the same. For example, let's assume you have a data set with Col_1, Col_2, Col_3 columns, export to .xlsx, make some changes and then run detect_changes(). You will see Input_Columns = [Col_1], [Col_2], [Col_3]. Let's assume you then go ahead and delete  Col_2 in your data set, and apply the .dp file. You would get a warning that the input columns do not match with the input data set. </p> <p>Example: </p> <pre><code>Input_Columns = [CRIM], [ZN], [INDUS]\n</code></pre>"},{"location":"advanced/#drop_column","title":"Drop_Column","text":"<pre><code>Drop_Column(Col[name of the column to delete])\n</code></pre> <p>Will delete a column from the data set.  Practicus AI will add a Drop_Column command for both deleted and hidden columns in an Excel (.xlsx) file. The only difference is that deleted columns will run early on in the .dp file and hidden columns will be deleted later since formulas or filters can depend on them. It is common that a user creates a new Excel column, uses a formula and then hides the old column that the formula uses. </p> <p>Example: </p> <pre><code>Drop_Column(Col[ZN])\n</code></pre>"},{"location":"advanced/#update","title":"Update","text":"<pre><code>Update(Col[name of the column to update] from_value to to_value)\n</code></pre> <p>Updates all the rows for a certain column from one value to another. For example, updating all missing values to 0. When a manual update of a cell is detected in the Excel file, an Update command is added to the .dp file. You do not need to make the same update to all cells, just one ie enough to apply the change for that column. If the same value is updated to different values manually, only the first detected update will run. Other updates will be commented out. You can review these update commands and make changes on the .dp file as needed.</p> <p>Examples:</p> <pre><code>Update(Col[CHAS] blank to 1.0)\nUpdate(Col[text field] \"abc\" to \"def\")\nUpdate(Col[ZN] 5 to 10)\n# The below Update command is commented out automatically,\n# since the same value (5) was updated to 10 in one Excel cell and to 11 in another\n# Update(Col[ZN] 5 to 11)\n</code></pre>"},{"location":"advanced/#rename_column","title":"Rename_Column","text":"<pre><code>Rename_Column(Col[current name] to Col[new name])\n</code></pre> <p>Will rename a column. Practicus AI will only analyze top n rows to detect potential renames and can miss some of them in complex cases. Please feel free to manually detect these and add them to the .dp file. </p> <p>Example: </p> <pre><code>Rename_Column(Col[petal_width] to Col[petal width])\n</code></pre>"},{"location":"advanced/#functions-and-formulas","title":"Functions and formulas","text":"<pre><code>Col[name of the column] = EXCEL_FUNCTION( .. EMBEDDED_FUNCTIONS(..) .. ) + OPERATORS\n</code></pre> <p>Using functions to create formulas is probably one of the most powerful features of Excel. The same is true for Practicus AI data prep use case as well. Practicus AI currently interprets over 200+ Excel functions and applies them with custom created Python code to your data set to perform the data transformation.  If Practicus AI ever encounters an Excel function that it doesn't understand, it will create a Python template for you to provide the missing functionality. Please read more about this in custom functions' section below.   Practicus AI currently only supports functions and formulas to run on the same row.  For things like averages of all values for  a particular column, you can use separate sheets or pivot tables to do yor analysis, and then finally perform the data preparation steps on individual rows.  </p> <p>Examples:</p> <pre><code># Operators\nCol[A] = Col[A] + 1\nCol[A] = (Col[A] + Col[B]) / 2\n# Mathematical functions\nCol[A] = SQRT(Col[A]) + POWER(Col[B], 2)\n# Statistical\nCol[A] = MAX(Col[B], Col[C], Col[D]) / AVERAGE(Col[B], Col[C], Col[D])\n# Logical\nCol[A] = IF(Col[B] &gt; 1, \"value greater than 1\", \"it is not\")\nCol[A] = IF( AND(Col[B] &gt; 1, Col[C] &gt; 1), \"both values greater than 1\", \"not\")\n# Text\nCol[first name] = UPPER(Col[first name]) \n# splitting text, i.e. John Brown to John in one column and Brown in another \nCol[first name] = LEFT(Col[full name], SEARCH(\" \", Col[full name]) - 1) \nCol[last name] = RIGHT(Col[full name], LEN(Col[full name]) - SEARCH(\" \", Col[full name])) \n</code></pre> <p>Please note that columns with formulas are currently placed in the .dp file in order that they appear in Excel. If a formula column depends on another, but appears prior in Excel you can face execution order issues. For instance, let's assume we have A = B + 1 formula that appears in the 2nd Excel column. And B = [some existing column] + 1 appears in 3rd Excel column. In the .dp file you will see A = B + 1 first, and it's evaluation will fail since B is not defined yet.  To resolve this kind of issue, you can 1) change the order of columns in Excel so that it matches the execution order of formulas, or 2) change the order in .dp file manually.</p>"},{"location":"advanced/#filtering-with-remove_except","title":"Filtering with Remove_Except","text":"<pre><code>Remove_Except(Col[name of the column] criteria)\n</code></pre> <p>Removes (filters) all the rows that do not match the criteria. Criteria can be \"is in [values]\", logical operators like &gt;, &lt;, &gt;=, &lt;=, = and != and corresponding value, \"and\", \"or\".</p> <p>Examples:</p> <pre><code># remove all values on column RAD, and only keep 4, 5 and 6\nRemove_Except(Col[RAD] is in [4, 5, 6])\n# only keep values RAD &gt; 5 and ZN &lt; 10  \nRemove_Except(Col[RAD] &gt; 5 and Col[ZN] &lt;10)\n# remove all 5's   \nRemove_Except(Col[RAD] != 5)\n# keep only 5's   \nRemove_Except(Col[RAD] = 5)\n</code></pre>"},{"location":"advanced/#sort_columns","title":"Sort_Columns","text":"<pre><code>Sort_Columns(Col[name of first column to sort], .., ascending[True|False, ..])\n</code></pre> <p>Sort values for column(s), ascending or descending. You can sort on as many columns as needed. </p> <p>Example: </p> <pre><code>Sort_Columns(Col[AGE], Col[INDUS], ascending[True, False])\n</code></pre> <p>Sorts on column \"AGE\" from smallest to largest first, and then for the same AGE values sorts on column INDUS, largest to smallest.</p>"},{"location":"advanced/#reorder_columns","title":"Reorder_Columns","text":"<pre><code>Reorder_Columns([column name], [another column name], ..)\n</code></pre> <p>All new columns are appended to the end of the data set by default.  Reorder_Columns command changes the order of the columns. Please note that since this command has to create a new data set internally, inplace=True updates for apply_changes() will not work. inplace=False is the default behaviour for apply_changes() function. </p> <p>Example:</p> <pre><code>Reorder_Columns([CRIM], [CHAS], [NOX])\n</code></pre>"},{"location":"advanced/#custom-functions-udfs","title":"Custom Functions (UDFs)","text":"<pre><code>Col[name of the column] = MY_UDF(..)\n</code></pre> <p>Practicus AI supports defining your own User Defined Functions (UDFs) in a .dp file. If Practicus AI.apply_changes() function encounters a function name that it doesn't know, it will create a placeholder Python .py file for you to be completed later. The same will also happen if Practicus AI encounters an Excel function it doesn't recognize. </p> <p>Practicus AI custom function names follow Excel syntax: capitol letters, \"A-Z\" and numbers  \"0-9\", with the exception of an underscore  \"_\". For example MY_FUNCTION_2(). MyFunction2() is not a valid Excel or Practicus AI function name.   </p> <p>Example: </p> <p>Place the below line in \"sample.dp\" file</p> <pre><code>...\nCol[A] = MY_FUNCTION(Col[B], Col[C], 1, 2, 3)\n...\n</code></pre> <p>and run </p> <pre><code>df2 = practicus.apply_changes(df, \"sample.dp\")\n</code></pre> <p>This will create \"sample_dp.py\" file, place the missing function and raise a NotImplemented error. </p> <pre><code>def practicus_my_function(row, *args):\n    # you can define your function and then delete the below line\n    raise NotImplementedError('MY_FUNCTION')\n\n    # 'row' is a Pandas Data Frame row that you can access *any* column\n    result = row['B'] + row['C'] + args[2] + args[3] + args[4]\n    # the below does the same, without accessing row \n    result = args[0] + args[1] + args[2] + args[3] + args[4]\n\n    return result\n\n# please note that the above function template is simplified for documentation purposes\n# what you get will have more lines, including proper exception handling\n</code></pre> <p>After you fill the missing function template, you can re-run practicus.apply_changes() in your notebook, and your UDF MY_FUNCTION() will execute for all the rows on the data frame, in combination with other commands. </p> <p>You can also mix and match different UDFs and Excel functions on the same line as you wish. </p> <p>Example:</p> <pre><code>COL[A] = MY_FUNC( MY_FUNC2(Col[A]), SIN(Col[B]) )\n</code></pre> <p>The above will create UDF .py placeholders for MY_FUNC( ) and MY_FUNC2( ), and also use the standard Excel function SIN( ).  </p>"},{"location":"airflow/","title":"Airflow integration","text":"<p>Create production ready data processing code with one-click, embed into Airflow or other orchestration engines.  Choose your preferred data processing engine including Pandas, DASK, RAPIDS (GPU), RAPIDS + DASK (multi-GPU) and SPARK.</p> <p>After Airflow Code export is completed, you can see, run and stop the DAGs created with the Airflow tab. You can also run and schedule more than one task sequentially.</p> <p></p> <p>You can see and edit your Airflow code exported with Jupyter Notebook and continue your operations programmatically.You can also work on the data pipelines you have created.</p> <p></p> <p>Please view the Modern Data Pipelines section to learn more about out how Practicus AI data pipelines work by default.</p>"},{"location":"analyze/","title":"Analyze","text":"<p>Easily create charts and profile your data with one click.  Quickly understand how your data is distributed and identify hard to find correlations.</p>"},{"location":"analyze/#profile","title":"Profile","text":"<p>Generate the profile of the data you use with the profile feature in the analyze section.</p> <p></p> <p></p>"},{"location":"analyze/#graph","title":"Graph","text":"<p>Let's use the graph feature in the analysis section.</p> <p></p> <p>You can choose the graph style(Plot, Bar, Horizontal Bar, Histogram, Scatter Plot) you want to create from the dialog. Then enter the x and y columns of the graph you want to create.</p> <p></p> <p></p> <p>You can perform operations on the chart with the features in the menu bar, and you can save the chart if you want.</p>"},{"location":"analyze/#quick-stats","title":"Quick Stats","text":"<p>You can see quick statistics of the columns on data.</p> <p></p> <p>It gives you statistical data about the columns such as count, mean, std, min, 25%, 50%, 75% and max.</p> <p></p>"},{"location":"analyze/#quick-pivot","title":"Quick Pivot","text":"<p>Quick pivot allows you to quickly summarize large amounts of data in an interactive way. You can choose Sum, Median, Min, Max, Std. Dev., Variance, Product as Summary Method.</p> <p></p>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#export_data","title":"export_data","text":"<pre><code>export_data(data_set, output_path, columns=None, reindex=False)\n</code></pre> <p>Exports a dataset to an Excel file (.xlsx) to be used with Microsoft Excel, Google Sheets, LibreOffice etc. </p> <p>data_set: Pandas DataFrame (recommended), NumPy Array or Python list. For large datasets, we recommend Pandas DataFrame.</p> <p>output_path: The path where the Excel file will be written. If there is no file extension, Practicus AI will automatically add .xlsx to the file name.  </p> <p>columns: Optional, default is None. We highly recommend providing column names if the data_set does not include them. If no column names are found, Practicus AI will automatically add names for you, such as column_1, column_2 etc. These names will not lead to the best user experience.   </p> <p>reindex: Optional, default is False. If a Pandas DataFrame is used to export_data function, Practicus AI will use the index found in this data frame, or create one if none found. After several cycles of \"exporting and applying changes\", indexes might start jumping from value to value, such as 0, 5, 14 because of  filtering values. Although it is perfectly fine for Practicus AI to work with sparse index values like these, in some cases it can confuse end users. When reindex=True, Practicus AI will reset the index, so it will become like 0, 1, 2 again.</p> <pre><code># Example 1, exporting basic Python Array\n\n# define a basic array\narr = [[1,2,3],\n       [4,5,6],\n       [7,8,9]]\n\nimport practicus\n\n# write to Excel. Since no colum names are provided, practicus will create new names\npracticus.export_data(arr, \"basic_array.xlsx\")\n\n# Now, with column names\npracticus.export_data(arr, \"basic_array_2.xlsx\", columns=['x1', 'x2', 'x3'])\n\n\n# Example 2, exporting NumPy Array\nfrom sklearn.datasets import load_boston\n\ndata_set = load_boston()\nnumpy_array = data_set.data\n\npracticus.export_data(numpy_array, \"boston_house_numpy.xlsx\", \n                  columns=data_set.feature_names)\n\n\n# Example 3, exporting Pandas Data Frame (recommended)\nimport pandas as pd\ndf = pd.DataFrame(data=data_set.data, columns=data_set.feature_names)\n\n# no need to pass columns since the pandas data frame already has them \npracticus.export_data(df, \"boston_house_pandas.xlsx\")\n\n\n# Example 4, reindexing \n\n# let's delete first 5 rows from the data frame \ndf = df.drop(df.index[[0,1,2,3,4]])\n\n# reindexing will start the index from 0 again\npracticus.export_data(df, \"boston_house_pandas_reindexed.xlsx\", reindex=True)\n</code></pre>"},{"location":"api-reference/#apply_changes","title":"apply_changes","text":"<pre><code>data_set2 = apply_changes(data_set, input_path, columns=None, inplace=False, reindex=False)\n</code></pre> <p>Applies detected or passed changes to the data_set. The way apply_changes() work depends on what kind of file is passed in input_path. If an Excel (.xlsx) file is passed, apply_changes() first executes detect_changes() to find out what kind of changes are made to the Excel (.xlsx) file, writes these changed to a .dp file and then applies these changes to the data_set. If input_path is a .dp file, detect_changes() is not called, and updates are applied directly from the .dp file.  </p> <p>data_set: Pandas DataFrame, NumPy Array or Python list. For large datasets, Pandas DataFrame will perform faster.</p> <p>input_path: This can be either an Excel (.xlsx) file with some changes in it, or a data prep (.dp) file that has changes recorded in it.   </p> <p>columns: Optional, default is None. We highly recommend providing column names if the data_set does not include them. If no column names found, Practicus AI will automatically add names for you, such as column_1, column_2 etc.  Being consistent in column names between export_data() and apply_changes() is important since the column names in the Excel file and the data_set we later try to apply changes on should match. </p> <p>inplace: Optional, default is False. If inplace=True, Practicus AI will apply updates directly on the data_set provided as an input, and will not return anything back. Please note that inplace editing only works  if a Pandas DataFrame is provided as an input. inplace editing might have some performance benefits for very large datasets. We discourage using inplace updates unless it is necessary, since it becomes harder to retry code. I.e. if the applied changes do not work as you expect, you can open the auto-generated .dp file, make changes and can re-run apply_changes(). This will not work with inplace=True.</p> <p>reindex: Optional, default is False. Only meaningful if a Pandas Data Frame is used. After several cycles of \"exporting and applying changes\", indexes might start jumping from value to value i.e. 0, 5, 14 due to filtering of values. Although it is perfectly fine for Practicus AI to work with sparse indexes, in some cases these kinds of indexing can confuse end users. When reindex=True, Practicus AI will reset the index so the returned data_set will have indexes reset, i.e. 0,1,2,3. </p> <p>Returns: The exact same type of data_set, with changes applied. If a Pandas DataFrame as passed as input value, this function return a Pandas DataFrame. Same for NumPy Array and Python Lists. If inplace=True, returns nothing.  Other than returning the data_set, apply_changes also writes a new .dp file with changes detected in it, if an Excel (.xlsx) file is passed in input_path. </p> <pre><code># Example 1\n\narr = [[1,2,3],\n       [4,5,6],\n       [7,8,9]]\n\nimport practicus\npracticus.export_data(arr, \"basic_array_2.xlsx\", columns=['x1', 'x2', 'x3'])\n\n# now open the Excel file and delete first column, x1\n</code></pre> <pre><code># running the  below will detect the change we made (drop column), \n# apply it to arr and return a new Array  \narr2 = practicus.apply_changes(arr, \"basic_array.xlsx\", columns=['x1', 'x2', 'x3'])\n# You should see something like the below  in output window.\n</code></pre> <pre><code>Detecting changes made in basic_array.xlsx\nSaved changes to data preparation file basic_array.dp in 0.21 seconds.\n\nApplying changes from basic_array.dp\nInput_Columns = [x1], [x2], [x3]\n Input columns match the DataFrame.\n\n# Columns deleted in Excel \nRunning: Drop_Column(Col[x1])\n Completed. \n\nAll steps completed.\n</code></pre> <pre><code># Example 2, inplace updates. (Pandas data frame is required for this to work)\nimport pandas as pd\ndf = pd.DataFrame(data=arr, columns=['x1', 'x2', 'x3'])\n\n# with inplace=True, apply_changes() doesn't return anything \npracticus.apply_changes(df, \"basic_array.xlsx\", inplace=True)\n# confirm x1 column is dropped\ndisplay(df)\n\n#Example 3, reindexing\n# now open basic_array.xlsx and put a filter on x2, uncheck 2 and leave 5 and 8 only\n# (to filter in Excel, you can click the drop down arrow next to the column name, x2)   \n# reindexing will start the index from 0 again\ndf = pd.DataFrame(data=arr, columns=['x1', 'x2', 'x3'])\ndf2 = practicus.apply_changes(df, \"basic_array.xlsx\", reindex=True)\ndisplay(df2)\n</code></pre> <pre><code>  x2  x3\n0  5   6\n1    8   9\n</code></pre>"},{"location":"api-reference/#detect_changes","title":"detect_changes","text":"<pre><code>detect_changes(input_path, output_path=None)\n</code></pre> <p>This function detects changes a user makes inside the Excel (.xlsx) file and writes the detected changes to a data prep (.dp) file. Once the data prep (.dp) file is created, you can review and make changes as needed and then finally run using apply_changes function.     </p> <p>input_path: Excel (.xlsx) file path with some changes in it. </p> <p>output_path: Optional. The path for the data prep (.dp) file that the detected changes will be written to. If no file name is passed, Practicus AI uses the same name of the input_path and replaces .xlsx with .dp. </p> <p>Returns: The path of the .dp file. If output_path is provided this function returns that name, if output_path was empty, returns the name of the filename that is generated.   </p> <pre><code># Example 1 \n\n# the below code detects all changes made in the Excel file, \n# and writes the result into basic_array.dp\npracticus.detect_changes(\"basic_array.xlsx\")\n\n# open basic_array.dp file, you will see the below\n</code></pre> <pre><code>Input_Columns = [x1], [x2], [x3]\n\n# Columns deleted in Excel \nDrop_Column(Col[x1])\n\n# Filtered rows in Excel \nRemove_Except(Col[x2] is in [5, 8])\n</code></pre> <pre><code># let's assume we don't want to filter anymore. \n# Place a # in front of Remove_Except(..) to comment it out, and save the .dp file\n\n# now let's apply changes to arr, but this time directly from .dp file\n# instead of .xlsx file.\narr2 = practicus.apply_changes(arr, \"basic_array.dp\", columns=['x1', 'x2', 'x3'])\n\ndisplay(arr2)\n# the result will be: [[2, 3], [5, 6], [8, 9]]\n# first column (x1) dropped, but teh value 2 is \"not\" filtered out , \n# since we commented the filter line out with # in the .dp file \n</code></pre>"},{"location":"api-reference/#export_model","title":"export_model","text":"<pre><code>export_model(model, output_path, columns=\"\", target_name=\"\", num_rows=1000)\n</code></pre> <p>Exports a model to an Excel (.xlsx) file, so that users can understand how the model works, and make predictions by entering new values. </p> <p>model: A Python model, pipeline, or the path of a .pmml file. If a model is passed, Practicus AI can read Linear Regression,  Logistic Regression, Decision Trees and Support Vector Machine models. If a pipeline is passed, Practicus AI can read several pre-processing steps like StandardScaler, MinMaxScaler etc., in addition to the model trained as part of the pipeline. Practicus AI can also read .pmml files, especially for models built outside the Python environment like R, KNIME etc.   </p> <p>output_path: The path of the Excel (.xlsx) file to export.</p> <p>columns: Optional, if a .pmml file is passed as model and has column names in it. Otherwise, required. The column names that will be used as input features for the model. For a basic linear model,  y = 2 * x1 + 3 * x2,  'x1' and 'x2' are the input feature names. These names can be used as column names in the Excel file.</p> <p>target_name: Optional, if a .pmml file is passed as model and has column names in it. Otherwise, required. The target column name that the model predicts for. For a basic linear model,  y = 2 * x1 + 3 * x2,  'y' would be the target name.</p> <p>num_rows: Optional, defaults to 1000 rows. Indicates the number of rows to use in the Excel (.xlsx) file to make predictions on. Max 1,048,576 rows are supported, but the performance will depend on model's complexity, and the user's computer speed. Please gradually increase and test number of rows before sending the .xlsx file to others.     </p> <p>Returns: The path of the .dp file. If output_path is provided this function returns that name, if output_path was empty, returns the name of the filename that is generated.   See some limitations below.</p> <pre><code># Example 1, model exporting\n# ... model training code here ... \nsome_model.fit(X, Y)\n\nimport practicus\npracticus.export_model(some_model, output_path=\"some_model.xlsx\",\n                   columns=['X1', 'X2'], target_name=\"Some Target\", num_rows=100)\n\n# Example 2, pipeline exporting\n# ... model and pipeline code here ...\nmy_pipeline = make_pipeline(\n    SomePreProcessing(),\n    SomeOtherPreProcessing(),  \n    SomeModelTraining())\n\npracticus.export_model(my_pipeline, output_path=\"some_model.xlsx\",\n                   columns=['Column 1', 'Column 2'], target_name=\"Target\", num_rows=150)\n\n\n# Example 3, pmml model exporting\n\n# let's assume we have a model trained in R, and saved in a .pmml file\n# let's also assume the .pmml file has column and target names in it\npracticus.export_model(\"some_R_model.pmml\", output_path=\"some_R_model.xlsx\", num_rows=150)\n\n# Please check the Samples' section for more.. \n</code></pre>"},{"location":"api-reference/#export_model-limitations","title":"export_model limitations","text":"<p>Practicus AI is new, and we are looking forward to hearing your feedback on adding new features. some of our current limitations are: </p> <ul> <li> <p>Exported models need to have fewer than 16,384 columns.  </p> </li> <li> <p>Maximum 1 million rows (1,048,576) are supported for both data preparation and model exporting. </p> </li> <li> <p>Model exporting currently work with Linear Regression, Logistic Regression, Decision Trees and Support Vector Machines. According to Kaggle forums, Google search trends and other forums, these are by far the most popular modeling techniques potentially covering 90+% of the predictive ML analytics use cases. Practicus AI currently doesn't target model exporting for cognitive use cases and deep learning frameworks, since the resulting Excel file would become very complicated making  model debugging and model explainability challenging.</p> </li> </ul>"},{"location":"aws-iam/","title":"AWS IAM policies","text":"<p>Create cloud IAM policies that apply to all of our users to create audit trails for data access and processing.</p>"},{"location":"aws-iam/#administrator-access","title":"Administrator Access","text":"<p>User can attach AdministratorAccess policy to their users via AWS IAM. Practicus can be used easily if the user with AdministratorAccess policy is added in the Settings/AWS Cloud User section of the application.</p> <p></p>"},{"location":"aws-iam/#minimum-privileged-aws-user","title":"Minimum Privileged AWS User","text":"<p>You can create a minimum privileged AWS User from the Settings / Create new cloud user section through the application. When the user is created, the policies to be used for Practicus on AWS are attached to the user.</p> <p></p> <p></p>"},{"location":"cloud/","title":"Cloud","text":"<p>A Cloud Worker is like your computer in the cloud. Various configurations of CPU, memory, storage, and networking capacity for your instances, known as instance types. There are many instance types with different features. You can choose different instance types according to your usage area. You can keep your data on Cloud Worker and work on big data quickly.</p>"},{"location":"cloud/#start-new-cloud-worker","title":"Start New Cloud Worker","text":"<p>You create a new Cloud Worker by choosing the type of instance you want to use, the EBS disk size,Name and the Cloud Worker Version.</p> <p></p> <p>You can stop, start, reset, reboot, replicate and terminate the created Cloud Worker.</p>"},{"location":"cloud/#terminal-cloud-worker","title":"Terminal Cloud Worker","text":"<p>Quickly SSH to the Cloud Worker using the terminal feature.</p> <p></p>"},{"location":"cloud/#cloud-worker-log","title":"Cloud Worker Log","text":"<p>It accesses the logs on the Cloud Worker, it can be saved.</p> <p></p>"},{"location":"cloud/#jupyter-notebook-mlflow-airflow","title":"Jupyter Notebook, MLFlow, Airflow","text":"<p>Provides Jupyter Notebook, MLFlow and Airflow connections with the selected Cloud Worker.</p> <p>Jupyter Notebook allows you to code and see the files in the Cloud Worker.</p> <p>MLFlow allows you to see the registered models and predict using these models.</p> <p>Airflow allows you to create and schedule a new DAG.</p>"},{"location":"code-export/","title":"Code Export","text":"<p>Create production ready data processing code with one-click, embed into Airflow or other orchestration engines.  Choose your preferred data processing engine including Pandas, DASK, RAPIDS (GPU), RAPIDS + DASK (multi-GPU) and SPARK. </p>"},{"location":"code-export/#export-code","title":"Export Code","text":"<p>You can export the operations performed when we do data preparation on the data to the code. If you wish, you can continue your operations over the exported code.</p> <p>You can choose Pandas, DASK, RAPIDS, multi-GPU, Spark as data platform, and you can choose Jupyter Notebook, Python Library and Airflow Library as Code Template.</p> <p>Note : You need a ready Cloud Worker for this operation.</p> <p></p> <p>When the process is complete, you can navigate through the files, continue with the exported code and review the code.</p> <p></p>"},{"location":"data-prep/","title":"Data Preparation use case","text":"<p>There are two main use cases for Practicus AI. Data Preparation and model sharing. </p> <p>Data preparation for ML consumes an estimated 80% to 90% of the time for a Data Scientist today. Practicus AI aims to help data preparation for ML by closing the gap between Excel and Python code. Both data scientists and other supporting personas like business analysts can take advantage of the below functionality, and work together to prepare the data for ML training. </p> <p>Basic data preparation use case</p> <p>1) Export a Pandas DataFrame, NumPy array or a Python list to Excel</p> <pre><code>import practicus\n# export data to an Excel file\npracticus.export_data(my_df, \"my_data.xlsx\")\n</code></pre> <p>2) Open the file in Excel, Google Sheets, LibreOffice or any other Spreadsheet platform to analyze and make changes as usual. </p> <p></p> <p>3) After you are finished updating your data in Excel, you can apply all changes made to create a new data set. </p> <pre><code># import back from Excel, detect all the changes made, and apply to the Data Frame  \nmy_df2 = practicus.apply_changes(my_df, \"my_data.xlsx\") \n\n# practicus auto-generates Python code for you, and applies the updates..\n\n# display the result, which will be the same as what you see in Excel\ndisplay(my_df2)\n</code></pre> <p></p> <p>4) (Optional) Practicus AI will automatically create a data prep (.dp) file containing all detected changes, before generating Python code. You can review this file, remove changes you don't like, or add new ones manually as you wish. Once done, you can apply the updates directly from the .dp file. </p> <p></p> <pre><code># apply changes, but this time directly from the .dp file that you reviewed / updated\nmy_df2 = practicus.apply_changes(my_df, \"my_data.dp\")\n</code></pre> <p>5) (Optional) Rinse and repeat... You can continue the above steps, also working with others in a collaborative environment, to keep generating new versions of Excel files and auto-generated data sets. The detected changes (.dp files) can be updated and archived as needed. Outside of Jupyter notebooks, you can also chain multiple .dp files to create complex data preparation / ML pipelines and later embed these data pipelines to a data engineering platform for production purposes.  Any production grade data integration platform that can run Python code will easily run Practicus AI detected changes at scale.   </p>"},{"location":"excel-predict/","title":"Predict with Excel","text":"<p>You can also use pure-Excel for prediction! (only at Practicus AI). Export and embed your AI models into Excel or Google  Sheets, attach to an email and allow your users to make predictions completely offline with no app / plugin / macro required. You can use the AutoML models, or export your own custom models. Currently supporting Linear Regression,  Logistic Regression, SVM and Decision Trees.</p>"},{"location":"excel-predict/#prediction","title":"Prediction","text":"<p>After the model is created with the build excel model option, you select the location where you will save the excel model from the dialog that appears.</p> <p></p> <p>You open the saved excel model and make predictions for the column you want. In this example, the target column was revenue and when we filled the temperature column with different values, the revenue column was filled with the predicted values.</p> <p></p>"},{"location":"excel-prep/","title":"Prepare with Excel","text":"<p>You can also use Excel for data preparation! (only at Practicus AI).  Share pure Excel or Google Sheets with others,  so they can analyze the data and make changes, and send it back to you. Practicus AI can then capture those  changes, so you can apply to any data locally or in the cloud, or export to Python code. Please watch the demo for more! </p>"},{"location":"excel-prep/#export-to-excel","title":"Export to Excel","text":"<p>With Export to Excel, you can save your data on which you have done preparation operations as excel anywhere on your local computer.</p> <p>Note : You need ready Cloud Worker for this operation.</p> <p></p>"},{"location":"excel-prep/#edit-in-excel","title":"Edit in Excel","text":"<p>You can delete columns, change their names, perform sort and filter operations on the exported excel, apply excel formulas and perform other data preparation operations. In this example, the CHAS column has been deleted and the name of the NOX column has been changed.</p> <p></p>"},{"location":"excel-prep/#import-from-excel","title":"Import from Excel","text":"<p>After saving your excel file on which you perform operations, you can import it to the application with Import from Excel.</p> <p></p> <p>The changes we made on the data with Excel were noticed. You can delete and edit them if you wish.</p> <p></p> <p>Thus, we have imported our data on Excel into the application with the changes we have made.</p>"},{"location":"explore/","title":"Explore","text":"<p>Up to 100+ times faster sampling. Random sample 1 billion+ rows down to 1 million in only a few seconds.  Switch between data engines if you need (Pandas, DASK, Spark, GPUs with RAPIDS).  Use S3 like a local drive on your laptop by simple copy/paste operations. Query Databases using SQL.</p>"},{"location":"explore/#local-files","title":"Local Files","text":"<p>You can browse local files in the Explore tab and preview the data you want to work with easily.</p> <p></p> <p>You can create a new file, copy and paste the files on Cloud Worker or S3</p>"},{"location":"explore/#cloud-worker-files","title":"Cloud Worker Files","text":"<p>Firstly you have to launch a new Cloud Worker from the Cloud Tab. Then you can navigate the Cloud Worker content. You can preview the file by clicking on it.</p> <p></p>"},{"location":"explore/#create-folder","title":"Create Folder","text":"<p>Let's create a new folder.</p> <p></p> <p></p> <p>A new folder has been created.</p> <p>You can also download and upload files on Cloud Worker and run the necessary scripts for databases.</p>"},{"location":"explore/#upload","title":"Upload","text":"<p>A directory is selected for the upload process.</p> <p></p> <p>You can select the file you want to upload from the file dialog.</p> <p></p> <p>After making your selection, you will be directed to the file transfer tab.</p> <p></p> <p>It starts the process with start transfer and you can close the tab when the process is finished.</p> <p></p> <p>You can see the uploaded file by navigating on Cloud Worker.</p> <p></p>"},{"location":"explore/#download","title":"Download","text":"<p>Select the files and start the download process.</p> <p></p> <p></p> <p>After completing the download via the file transfer tab, you can navigate to local files and find the file.</p> <p></p>"},{"location":"explore/#amazon-s3","title":"Amazon S3","text":"<p>After the required Cloud Config setup process is completed, you can load the buckets with Amazon S3, then select the bucket and navigate the files. Note : For this operation you need ready Cloud Worker.</p> <p></p> <p>You can also perform the features (copy, paste, upload, download, new folder) in the menu bar.</p>"},{"location":"explore/#relational-databases","title":"Relational Databases","text":"<p>You can connect to the database using relational databases, run SQL queries, and continue your operations on the data. Amazon Redshift, Snowflake, PostgreSQL, MySQL, SQLite, Amazon Athena, Hive(Hadoop), SQLServer, Oracle, ElasticSearch, AWSOpenSearch, OtherDB connections are supported. Amazon Athena, Hive(Hadoop), SQLServer, Oracle, ElasticSearch, AWSOpenSearch databases need driver installation.</p> <p>Note: You need a ready Cloud Worker to access Relational Databases.</p> <p></p>"},{"location":"faq/","title":"Faq","text":"<p>This section of the documentation is work in progress..</p>"},{"location":"feedback/","title":"Feedback","text":""},{"location":"feedback/#general-feedback","title":"General Feedback","text":"<p>We would love to hear your feedback!</p> <p>https://practicus.ai/feedback/</p>"},{"location":"feedback/#report-issue","title":"Report issue","text":"<p>Please use the below form to submit any issues you face. We will do our best to fix it as soon as possible and get back to you. </p> <p>https://practicus.ai/report-issue</p>"},{"location":"feedback/#request-new-feature","title":"Request new feature","text":"<p>We only build features that our users request. Please let us know what you need, and we will prioritize accordingly. </p> <p>https://practicus.ai/feature-request</p>"},{"location":"feedback/#other-questions","title":"Other Questions","text":"<p>If you have any questions regarding how to use Practicus AI, please submit your question on one of the popular forums  such as reddit  or stackoverflow. </p> <p>Since we will not get a notification from the public forum, please let us know the question's URL by using the below. We will get back to you as soon as possible.</p> <p>https://practicus.ai/questions</p>"},{"location":"get-started/","title":"Get started","text":"<p>This page has moved to getting-started</p>"},{"location":"getting-started/","title":"Getting Started with Practicus AI","text":"<p>The Practicus AI platform offers multiple tools for working with AI and data intelligence. Regardless of your role, experience level, or objectives, you can choose an entry point that fits your needs.</p>"},{"location":"getting-started/#personas-practicus-ai-tools-tasks","title":"Personas, Practicus AI Tools &amp; Tasks","text":"<p>The diagram below illustrates various user roles, the tools available to them, and the tasks they support:</p> <p></p>"},{"location":"getting-started/#finding-the-right-tool-for-the-job","title":"Finding the Right Tool for the Job","text":"<p>Your choice of tool depends on your goals, role, and technical skill level:</p> <p></p>"},{"location":"getting-started/#where-to-start","title":"Where to Start","text":"<p>Most users begin at Practicus AI Home, typically accessed at an address like <code>https://practicus.your-company.com</code>. If you do not have access, consult your system administrator. If you are not an existing enterprise user, you can experiment offline with the free Practicus AI Studio, or contact your IT team for enterprise installation options.</p> <p>Next, decide whether you prefer to work with code or not.</p>"},{"location":"getting-started/#if-you-code","title":"If You Code","text":"<ol> <li> <p>Create a Worker:    From Practicus AI Home, create one or more Workers\u2014isolated Kubernetes pods configured with the container image, CPU/RAM, GPU, and other resources you need.</p> </li> <li> <p>Use Jupyter Lab or VS Code:    After selecting a Worker, launch Jupyter Lab or VS Code. You can write code, train and deploy ML models, and interact with data. Sample jupyter notebooks are provided in each Worker's home directory for quick exploration.</p> </li> </ol>"},{"location":"getting-started/#if-you-do-not-code","title":"If You Do Not Code","text":"<ol> <li> <p>Access Practicus AI Studio or Workspaces:    From Practicus AI Home, open a browser-based Workspace or download AI Studio, for a no-code/low-code experience. If local installation isn\u2019t possible, online Workspaces already include Practicus AI Studio, office productivity tools, and other useful applications.</p> </li> <li> <p>Explore &amp; Build with No-Code Tools:    Use visual interfaces and guided workflows to analyze data, generate insights, and create AI solutions\u2014no programming required.</p> </li> </ol> <p>Previous: Welcome | Next: Tutorials</p>"},{"location":"join/","title":"Join","text":"<p>You can use Practicus AI to join data from very different data sources and even physical locations all around the world. This is not something most traditional databases or data warehouses can do. </p>"},{"location":"join/#how-does-it-work","title":"How does it work?","text":"<p>1) Load data normally. We will call this left worksheet </p> <p>2) (Optional) Make changes to the left worksheet as usual.</p> <p>3) Load data to join with. We will call this the right worksheet</p> <p>4) Join left worksheet to right using a single key column. If you need to use multiple columns to join, you can first concatenate them into a single column i.e. 123-abc-456 </p>"},{"location":"join/#changes-to-the-right-worksheet","title":"Changes to the right worksheet","text":"<p>If you need to make changes to the right worksheet right before the join, you need to export your data to some destination first. If you do not export, the original data will be used. </p> <p>A typical join with changes to the right worksheet could work like this: </p> <p>1) Load the left worksheet, i.e. from Redshift, make changes if you need to</p> <p>2) Load the right worksheet, i.e. from Snowflake</p> <p>3) Make your changes to the right worksheet as usual</p> <p>4) Export the right worksheet data, most likely to an intermediary destination i.e. to your data lake on S3</p> <p>5) Join left worksheet to the right, and the Cloud Worker will use the exported data to read it from the intermediary S3 location. The original source on Snowflake will not be used. </p> <p>If you skip the export step #4, the Cloud Worker would ignore your changes, and read original data directly from Snowflake. </p> <p>To learn more about why Practicus AI joins works like the above, please check the below</p> <p>Modern Data Pipelines </p> <p>section, where we explain atomicity, idempotency and big data scalability concepts of Practicus AI.</p>"},{"location":"join/#join-styles","title":"Join Styles","text":"<p>Practicus AI uses traditional join styles: </p> <p></p> <p>Note: Unless you have a very good reason to do so, please do not use Cartesian join since it will result in left worksheet x right worksheet number of rows. </p>"},{"location":"k8s-setup/","title":"Enterprise Cloud setup guide","text":""},{"location":"k8s-setup/#download-helper-files-recommended","title":"Download Helper files (Recommended)","text":"<p>Before you get started, please feel free to download the practicus_setup.zip file that includes sample configuration (.yaml) files for various cloud, on-prem, or local development environments, and helper scripts to accelerate your setup.    </p>"},{"location":"k8s-setup/#overview","title":"Overview","text":"<p>This document will guide you to install Practicus AI Enterprise Cloud Kubernetes backend. </p> <p>There are multiple backend options and Kubernetes is one of them. Please view the detailed comparison table to learn more. </p> <p>Practicus AI Kubernetes backend have some mandatory and optional components.</p>"},{"location":"k8s-setup/#mandatory-components","title":"Mandatory components","text":"<ul> <li>Kubernetes cluster: Cloud (e.g. AWS), on-prem (e.g. OpenShift) or Local (e.g. Docker Desktop)  </li> <li>Kubernetes namespace: Usually named prt-ns</li> <li>Management database: Low traffic database that holds users, connections, and other management components. Can be inside or outside the Kubernetes cluster.</li> <li>Console Deployment: Management console and APIs that the Practicus AI App or SDK communicates with.</li> <li>Istio Service Mesh: Secures, routes and load balances network traffic. You can think of it as an open source and modern NGINX Plus.   </li> </ul>"},{"location":"k8s-setup/#optional-components","title":"Optional components","text":"<ul> <li>Optional Services Deployment: Only required if you would like to enable OpenAI GPT and similar AI services.   </li> <li>Additional Kubernetes clusters: You can use multiple clusters in different geos to build a flexible data mesh architecture.</li> <li>Additional namespaces: You can also deploy a test environment, usually named prt-ns2 or more.  </li> </ul>"},{"location":"k8s-setup/#dynamic-components","title":"Dynamic components","text":"<ul> <li>Cloud worker pods: These are ephemeral (temporary) pods created and disposed by the management console service. E.g. When a user wants to process large data, perform AutoML etc., the management console creates worker pod(s) on the fly, and disposes after the user is done. The dynamic capacity offered to end users is governed by system admins.      </li> </ul>"},{"location":"k8s-setup/#big-picture","title":"Big Picture","text":""},{"location":"k8s-setup/#prerequisites","title":"Prerequisites","text":"<p>Before installing Practicus AI admin console, please make sure you complete the below prerequisite steps. </p> <p>Installation scripts assume you use macOS, Linux or WSL on Windows.  </p>"},{"location":"k8s-setup/#create-or-reuse-a-kubernetes-cluster","title":"Create or reuse a Kubernetes Cluster","text":"<p>Please create a new Kubernetes cluster if you do not already have one. For this document, we will use a new Kubernetes cluster inside Docker Desktop. Min 8GB RAM for Docker engine is recommended. Installation steps are similar for other Kubernetes environments, including cloud ones such as AWS EKS.</p> <p>View Docker Desktop memory settings</p> <p>View Docker Desktop Kubernetes setup guide</p>"},{"location":"k8s-setup/#install-kubectl","title":"Install kubectl","text":"<p>Install kubectl CLI tool to manage Kubernetes clusters </p> Install kubectl for macOS<pre><code>curl -LO \"https://dl.k8s.io/release/v1.27.7/bin/darwin/amd64/kubectl\"\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\nsudo chown root: /usr/local/bin/kubectl\n</code></pre>"},{"location":"k8s-setup/#install-helm","title":"Install Helm","text":"<p>Practicus AI installation is easiest using helm charts. </p> Install Helm<pre><code>curl -fsSL -o get_helm.sh \\\n  https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nrm get_helm.sh\n</code></pre>"},{"location":"k8s-setup/#verify-current-kubectl-context","title":"Verify current kubectl context","text":"<p>Make sure kubectl is pointing to the correct Kubernetes cluster</p> <pre><code>kubectl config current-context\n\n# Switch context to Docker Desktop if required \nkubectl config use-context docker-desktop\n</code></pre>"},{"location":"k8s-setup/#install-istio-service-mesh","title":"Install Istio service mesh","text":"<p>Practicus AI uses Istio to ingest, route traffic, secure and manage the modern data mesh microservices architecture.</p> <p>Istio can be installed on any Kubernetes cluster and designed to run side by side with any number of other production workloads. Istio will not interfere with a namespace unless you ask Istio to do so.</p> <p>Learn more about Istio </p> <p>Note: Some Kubernetes systems come with Istio 'forks' pre-installed, such as Red Hat OpenShift Service Mesh. Practicus AI is designed and tested to work with the original istio only. Istio is designed to be installed and run side-by-side with the forked projects, so you can safely install it on any Kubernetes cluster.  </p> <p>The below script downloads the latest istoctl version, e.g. 1.18. </p> <p>Please update the \"mv istio-... istio\" section below to a newer version if required.   </p> Install Istio<pre><code>cd ~ || exit\n\necho \"Downloading Istio\"\nrm -rf istio\ncurl -L https://istio.io/downloadIstio | sh -\nmv istio-1.20.3 istio || \\\n  echo \"*** Istio version is wrong in this script. \\\n        Please update to the version you just downloaded to your home dir ***\"\ncd ~/istio || exit\nexport PATH=$PWD/bin:$PATH\n\necho \"Analyzing Kubernetes for Istio compatibility\"\nistioctl x precheck \n\necho \"Install istio to your kubernetes cluster\"\nistioctl install --set profile=default -y\n\necho \"Recommended: Add istioctl to path\"\n# Add the below line to .zshrc or alike\n# export PATH=~/istio/bin:$PATH\n</code></pre>"},{"location":"k8s-setup/#preparing-a-kubernetes-namespace","title":"Preparing a Kubernetes namespace","text":"<p>Practicus AI Kubernetes backend is designed to run in a namespace and side-by-side with other production workloads. </p> <p>We strongly suggest you use namespaces, even for testing purposes.</p>"},{"location":"k8s-setup/#create-namespace","title":"Create namespace","text":"<p>You can use multiple namespaces for Practicus AI ,and we will use the name convention: prt-ns (e.g. for production), prt-ns2 (for testing) etc.</p> <p>In this document we will only use one namespace, prt-ns.  </p> <pre><code>echo \"Creating a Kubernetes namespace\"\nkubectl create namespace prt-ns\n</code></pre>"},{"location":"k8s-setup/#add-practicusai-helm-repository","title":"Add practicusai helm repository","text":"<p>Practicus AI helm repository will make installing Practicus AI console backend easier.</p> Add practicusai helm repo<pre><code>helm repo add practicusai https://practicusai.github.io/helm\nhelm repo update\necho \"Viewing charts in practicusai repo\"\nhelm search repo practicusai\n</code></pre>"},{"location":"k8s-setup/#create-or-reuse-postgresql-database","title":"Create or reuse PostgreSQL Database","text":"<p>Practicus AI management console uses PostgreSQL database to store some configuration info such user credentials, database connections and more.</p> <p>This is a management database with low transaction requirements even for production purposes.    </p>"},{"location":"k8s-setup/#creating-a-production-database","title":"Creating a production database","text":"<p>You can reuse an existing PostgreSQL Server or create a new one using one of the cloud vendors.</p> <p>Please make sure your kubernetes cluster will have access network access to the server.</p> <p>Once the PostgreSQL Server is ready, you can create a new database using a tool such as PgAdmin following the below steps:</p> <ul> <li>Login to PostgreSQL Server </li> <li>Crate a new database, E.g. console</li> <li>Create a new login, E.g. console_user and note its password</li> <li>Right-click the database (console) and go to properties &gt; Security &gt; Privileges &gt; hit + and add the login (e.g. console_user) as Grantee, \"All\" as Privileges.</li> <li>Expand the database items, go to Schemas &gt; public &gt; right click &gt; properties &gt; Security &gt; hit + and add the login (e.g. console_user) as Grantee, \"All\" as privileges. </li> </ul>"},{"location":"k8s-setup/#optional-creating-a-sample-test-database","title":"(Optional) Creating a sample test database","text":"<p>For testing or PoC purposes, you can create a sample PostgreSQL database in your Kubernetes cluster. </p> <p>Important: The sample database should not be used for production purposes. </p> <p><pre><code>echo \"Creating sample database\"\nhelm install practicus-sampledb practicusai/practicus-sampledb \\\n  --namespace prt-ns\n</code></pre> In order to connect to this database inside the kubernetes cluster you can use the below address: </p> <ul> <li>prt-svc-sampledb.prt-ns.svc.cluster.local</li> </ul> <p>Please note that if you installed the sample database with the above defaults, the rest of the installation will already have the sample database address and credentials set as the default for easier testing.    </p>"},{"location":"k8s-setup/#connecting-to-the-sample-database-from-your-laptop","title":"Connecting to the sample database from your laptop","text":"<p>If you ever need to connect the sample database from your laptop using a tool such as PgAdmin, you can open a temporary connection tunnel using kubectl. </p> <pre><code># Get sample db pod name \nSAMPLEDB_POD_NAME=$(kubectl -n prt-ns get pod -l \\\n  app=postgresdb -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Sample db pod name is: $SAMPLEDB_POD_NAME\"\n\necho \"Starting temporary connection tunnel\"\nkubectl -n prt-ns port-forward \"$SAMPLEDB_POD_NAME\" 5432:5432\n</code></pre>"},{"location":"k8s-setup/#deploying-management-console","title":"Deploying Management Console","text":""},{"location":"k8s-setup/#helm-chart-valuesyaml","title":"Helm chart values.yaml","text":"<p>Practicus AI helm chart's come with many default values that you can leave as-is, especially for local dev/test configurations. </p> <p>For all other settings, we suggest you to use values.yaml file</p> <pre><code>mkdir ~/practicus \nmkdir ~/practicus/helm\ncd ~/practicus/helm\ntouch values.yaml\n</code></pre> Sample values.yaml file contents for a local test environment<pre><code>migrate:\n  superUserEmail: \"your_email@your_company.com\"\n  superUserPassword: \"first super admin password\"\n\nenterpriseLicense:\n  email: \"your_email@your_company.com\"\n  key: \"__add_your_key_here__\"\n\ndatabase:\n  engine: POSTGRESQL\n  host: host.docker.internal\n  name: console\n  user: console\n  password: console\n\nadvanced:\n  debugMode: true\n  logLevel: DEBUG\n\nnotification:\n  api_auth_token: \"(optional) _your_email_notification_api_key_\"\n</code></pre> Sample values.yaml file contents for a production environment on AWS, GCE, Azure, OpenShift etc.<pre><code>main:\n  # Dns accessible by app\n  host: practicus.your_company.com\n  # try ssl: false to troubleshoot issues\n  ssl: true\n\nmigrate:\n  superUserEmail: \"your_email@your_company.com\"\n  superUserPassword: \"first super admin password\"\n\nenterpriseLicense:\n  email: \"your_email@your_company.com\"\n  key: \"__add_your_key_here__\"\n\ndatabase:\n  engine: POSTGRESQL\n  host: \"ip address or dns of db\"\n  name: \"db_name\"\n  user: \"db_user\"\n  password: \"db_password\"\n\njwt:\n  # API JWT token issuer, can be any value \n  issuer: iss.my_company.com\n\nnotification:\n  api_auth_token: \"(optional) _your_email_notification_api_key_\"\n</code></pre>"},{"location":"k8s-setup/#ingress-for-aws-eks","title":"Ingress for AWS EKS","text":"<p>This step is not required for a local test setup. </p> <p>For AWS, our helm charts automatically configure Application Load Balancer and SSL certificates. </p> <p>You can simply add the below to values.yaml file.</p> Ingress for AWS EKS<pre><code>aws:\n  albIngress: true\n  # AWS Certificate Manager (ACM) certificate ARN for your desired host address\n  certificateArn: \"arn:aws:acm:__aws_region__:__acct_id___:certificate/___cert_id___\"\n\nistio:\n  # In order to use ALB, Istio gateway host must be \"*\"\n  gatewayHosts:\n  - \"*\"\n</code></pre>"},{"location":"k8s-setup/#ingress-and-other-settings-for-various-kubernetes-systems","title":"Ingress and other settings for various Kubernetes systems","text":"<p>Please check the below documentation to configure Istio and Istio gateway depending on your Kubernetes infrastructure. </p> <ul> <li>Azure</li> <li>Google Cloud</li> <li>OpenShift</li> <li>Oracle Cloud</li> <li>IBM Cloud</li> <li>MicroK8s</li> </ul>"},{"location":"k8s-setup/#configuring-management-database","title":"Configuring management database","text":"<p>Since the management console will immediately try to connect to its database, it makes sense to prepare the database first. </p> <p>The below steps will create a temporary pod that will create or update the necessary tables and populate initial data.</p> <pre><code>cd ~/practicus/helm\n\n# Confirm you are using the correct Kubernetes environment\nkubectl config current-context\n\n# Step 1) Create a temporary pod that will create (or update) the database\n\nhelm install prt-migrate-console-db practicusai/practicus-migrate-console-db \\\n  --namespace prt-ns \\\n  --set advanced.imagePullPolicy=Always \\\n  --values ./values.yaml\n\n# Step 2) View the db migration pod status and logs. \n#   Run it multiple times if pulling the container takes some time.  \n\necho \"DB migration pod status\"\necho \"-----------------------\"\nkubectl get pod -n prt-ns | grep prt-pod-migrate-db\necho \"\"\necho \"Pod logs\"\necho \"--------\"\nkubectl logs --follow prt-pod-migrate-db -n prt-ns\n</code></pre> <p>Once the database migration is completed, you will see success log messages such as the below: </p> <pre><code>Running migrations:\n  Applying contenttypes.0001_initial... OK\n  Applying contenttypes.0002_remove_content_type_name... OK\n  Applying auth.0001_initial... OK\n  Applying auth.0002_alter_permission_name_max_length... OK\n  Applying auth.0003_alter_user_email_max_length... OK\n  Applying auth.0004_alter_user_username_opts... OK\n...\n</code></pre> <pre><code># Step 3) (Recommended) After you confirm that the tables are created,\n#   you can safely terminate the database migration pod using helm.\n#   If you do not, the pod will self-terminate after 10 minutes. \n\nhelm uninstall prt-migrate-console-db --namespace prt-ns \n</code></pre> <p>You can repeat the above 1-3 steps as many times as you need, and for each new version of the management console. </p> <p>If there are no updates to the database schema, the pod will not make any changes.   </p>"},{"location":"k8s-setup/#installing-management-console","title":"Installing management console","text":"<p>Practicus AI management console will be the central place for several administrative tasks. </p> Install management console<pre><code>cd ~/practicus/helm\nhelm repo update \n\nhelm install practicus-console practicusai/practicus-console \\\n  --namespace prt-ns \\\n  --values values.yaml\n</code></pre>"},{"location":"k8s-setup/#logging-in-to-management-console","title":"Logging in to management console","text":"<p>You should be able to log in to Practicus AI management console using http://local.practicus.io/console/admin or https://practicus.your_company.com/console/admin</p> <p>Note: local.practicus.io DNS entry points to localhost ip address (127.0.0.1) </p> <p>Your super admin username / password was defined at the top of your values.yaml file. (superUserEmail, superUserPassword)</p>"},{"location":"k8s-setup/#troubleshooting","title":"Troubleshooting","text":"<pre><code># Find the pod name(s)\nkubectl -n prt-ns get pod | grep prt-depl-console-\n\n# View status\nkubectl -n prt-ns describe pod prt-depl-console-...\n\n# View logs\nkubectl -n prt-ns logs --follow prt-depl-console-...\n\n# Analyze using the interactive terminal\nkubectl -n prt-ns exec -it prt-depl-console-... -- /bin/bash  \n</code></pre>"},{"location":"k8s-setup/#upgrading-management-console-to-a-newer-version","title":"Upgrading management console to a newer version","text":"<pre><code>cd ~/practicus/helm\nhelm repo update\n\nhelm upgrade practicus-console practicusai/practicus-console \\\n  --namespace prt-ns \\\n  --values values.yaml\n</code></pre>"},{"location":"k8s-setup/#uninstalling-management-console","title":"Uninstalling management console","text":"<pre><code>helm uninstall practicus-console --namespace=prt-ns\n</code></pre>"},{"location":"k8s-setup/#recommended-start-a-new-cloud-worker-using-practicus-ai-app","title":"(Recommended) Start a new Cloud Worker using Practicus AI App","text":"<ul> <li>Open Practicus AI App</li> <li>Go to settings &gt; click login</li> <li>Enter service address e.g. for local test http://local.practicus.io or https://practicus.your_company.com</li> <li>You can use your super admin user / password</li> <li>Click on either Explore or Cloud at the top bar</li> <li>For Explore: You should see a new \"Practicus AI Service\" (can be renamed later)</li> <li>Click on \"New Worker\", select a size (if on your laptop, select 1 or 2 GB RAM)</li> <li>For Cloud: Select the newly added region (upper right)</li> <li>Click \"Start New\", select a size (if on your laptop, select 1 or 2 GB RAM)</li> </ul> <p>This will start pulling the Cloud Worker image on first use, which can take a while since the Cloud Worker image is ~ 9GB in size. </p> <p>During this time the app will show the Cloud Worker (pod) status as pending. Once the pull is completed the app will notify you. Go to Explore tab, click on \"Worker-x Files\" (x is the counter) and view the local disk content of the pod. This verifies everything is deployed correctly. </p>"},{"location":"k8s-setup/#troubleshooting-cloud-workers","title":"Troubleshooting Cloud Workers","text":"<pre><code># Find the pod name(s)\nkubectl -n prt-ns get pod | grep prt-pod-wn-\n\n# View status\nkubectl -n prt-ns describe pod prt-pod-wn-...\n\n# View logs\nkubectl -n prt-ns logs --follow prt-pod-wn-...\n\n# Analyze using the interactive terminal\nkubectl -n prt-ns exec -it prt-pod-wn-... -- /bin/bash  \n</code></pre> <p>Tip: You can view the status of Cloud Workers for any user and terminate them if needed using the management console. Simply open http://local.practicus.io/console/admin &gt; scroll to Cloud Worker Admin &gt; click on Cloud Worker Consumption Logs. You will see all the active and terminated Cloud Workers. If you click on a log, you will see the Kubernetes pod conditions. </p>"},{"location":"k8s-setup/#openai-gpt-services-additional-settings","title":"OpenAI GPT services additional settings","text":"<p>Some optional services such as OpenAI GPT require additional setup.</p> <p>Sample setup:</p> <ul> <li>Open management console e.g. http://local.practicus.io/console/admin</li> <li>Go to \"Machine Learning Services\" &gt; \"API Configurations\" page</li> <li>Click \"Add API Configuration\"</li> <li>Select OpenAI GPT</li> <li>Enter your API key that you obtained from OpenAI E.g. \"sk-abc123...\" View your key</li> <li>In optional settings section add the below </li> </ul> <pre><code>OpenAI-Organization=your_openai_organization_id\nmodel=gpt-4\nmax_tokens=350\n</code></pre> <ul> <li>OpenAI-Organization You can find this id on OpanAI account page </li> <li>model You can choose between gpt-3.5-turbo, gpt-4 etc. View available models </li> <li>max_tokens This is a cost control measure preventing high OpenAI costs. The system logs tokens used by your users, so you can adjust this number later.</li> </ul>"},{"location":"k8s-setup/#management-console-settings","title":"Management console settings","text":"<p>There are several settings on the management console that you can easily change using the admin console page.</p> <p>These changes are stored in the management database, so we strongly suggest you to regularly back up your database.</p> <ul> <li>Groups: We strongly suggest you to create groups before granting rights. E.g.: power users, data scientists, data engineers, citizen data scientists. </li> <li>Users: You can create users and give fine-grained access to admin console elements. Staff users can log in to admin console. Most users should not need this level access, and only use Practicus AI App.</li> <li>Central Configuration: Please view \"Cluster Definitions\" to change your service name and location. E.g. to \"Practicus AI Service\" located in \"Seattle\". When end users login using the App, this is the information they will see while exploring data sources. This information is cached for future use, so the earlier you change the better. </li> <li>Cloud Worker Admin: It is crucial you visit every page on this section and adjust Cloud Worker (pod) capacity settings. You can adjust which group/user should have access to what kind of capacity.  </li> <li>Connection Admin: Users can only use analytical database connections that they add to the system AND the connections you make visible to certain groups / users. </li> <li>SaaS Admin: This section is only used if you activate self-service SaaS through a 3rd party payment gateway. We suggest only the super admin has access to it, and you make this section invisible to all other admin or staff users.</li> </ul>"},{"location":"k8s-setup/#advanced-settings","title":"Advanced settings","text":"<p>Practicus AI helm charts values.yaml files include many advanced settings and explanations as inline comments. Please navigate and alter these settings, upgrade your deployments and validate the state as you see fit.</p> <p>Please see below a sample values.yaml file where you can adjust replica count of a deployment:  <pre><code>...\ncapacity:\n  # Console API service replica\n  replicas: 1\n...\n</code></pre></p>"},{"location":"k8s-setup/#recommended-install-persistent-volume-support","title":"(Recommended) Install persistent volume support","text":"<p>Practicus AI Cloud Workers support 2 types of persistent volumes, personal and shared between users.</p>"},{"location":"k8s-setup/#personal-drives","title":"Personal drives","text":"<p>If enabled, every Cloud Worker gets a drive mounted under ~/my. With the personal drive, a user can persist their files under ~/my folder, so the files are not lost after a Cloud Worker is terminated. This can have some benefits, e.g. persisting jupyter notebooks on Cloud Workers.</p> <p>By default, the personal drive is shared between Cloud Workers using Kubernetes ReadWriteMany (RWX) mode.</p> <p>Please be aware that if you enable personal drives and force ReadWriteOnce (RWO), a user can only use one Cloud Worker at a time and this is not recommended.</p>"},{"location":"k8s-setup/#shared-drives-between-users","title":"Shared drives between users","text":"<p>If enabled, every Cloud Worker gets the shared folder(s) mounted in user home dir e.g. ~/shared/folder/..  </p> <p>You can control which group or individual user has access to which shared folder and the share name.</p>"},{"location":"k8s-setup/#kubernetes-storageclass","title":"Kubernetes StorageClass","text":"<p>In order to dynamically create persistent volumes and to avoid administrative burden, Practicus AI uses Kubernetes storage classes. Please prefer StorageClass read / write many mode. </p> <p>Please be aware that only some Kubernetes storage types, such as NFS, support read / write many mode. Common storage classes such as AWS EBS only allow one Kubernetes pod to mount a drive at a time (read / write once), which is not suitable for sharing use cases.</p> <p>If your storage class does not allow read / write many, you cannot implement shared drives between users. And for personal drives, implementing read / write once will cause users to be able to use one Cloud Worker at a time, which is not recommended.</p> <p>To summarize, please prefer to use NFS or similar style storage systems for Practicus AI persistent volumes.</p>"},{"location":"k8s-setup/#supported-nfs-implementations","title":"Supported NFS implementations","text":"<p>You can use any NFS system, inside or outside your Kubernetes cluster. You can also use NFS as a service, such as AWS EFS with Practicus AI. </p>"},{"location":"k8s-setup/#sample-nfs-server-for-your-local-computer","title":"Sample NFS server for your local computer","text":"<p>Please find below a simple implementation to install a NFS pod on your computer to test it with Practicus AI.</p> Sample NFS server configuration<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: nfs-service\nspec:\n  selector:\n    role: nfs\n  ports:\n    # Open the ports required by the NFS server\n    # Port 2049 for TCP\n    - name: tcp-2049\n      port: 2049\n      protocol: TCP\n    # Port 111 for UDP\n    - name: udp-111\n      port: 111\n      protocol: UDP\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: nfs-server-pod\n  labels:\n    role: nfs\nspec:\n  containers:\n    - name: nfs-server-container\n      image: cpuguy83/nfs-server\n      securityContext:\n        privileged: true\n      args:\n        - /exports\n</code></pre> <p>You can save the above to nfs-server.yaml and run</p> <pre><code>kubectl apply -f nfs-server.yaml\n\n# To delete \nkubectl delete -f nfs-server.yaml\n</code></pre> <p>After you create the NFS pod named nfs-server-pod, please run the below to get its IP address, e.g. 10.0.0.1. you will need this IP address in the below section.</p> <pre><code>kubectl get pod -o wide\n</code></pre> <p>Please note that after you restart your computer, the NFS server IP address might change, and in this case you would have to re-install (or upgrade) the below helm chart to update the IP address.  </p>"},{"location":"k8s-setup/#using-nfs-inside-kubernetes","title":"Using NFS inside Kubernetes","text":"<p>The below will create a provisioner pod and a storage class named prt-sc-primary. You can create as many provisioners and storage classes. These can point to the same or different NFS systems. </p> <pre><code>helm repo add nfs-subdir-external-provisioner \\\n  https://Kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\nhelm repo update\n\nexport NFS_DNS_NAME=\"add NFS server DNS or IP address here\"\n\nhelm install nfs-subdir-external-provisioner \\\n  nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n  --set nfs.server=\"$NFS_DNS_NAME\" \\\n  --set nfs.path=\"/\" \\\n  --set storageClass.accessModes=\"ReadWriteMany\" \\\n  --set storageClass.pathPattern=\"practicus\" \\\n  --set storageClass.onDelete=\"retain\" \\\n  --set storageClass.name=\"prt-sc-primary\"\n\n# To uninstall\nhelm uninstall nfs-subdir-external-provisioner\n</code></pre> <p>To learn more and customize the helm chart, please visit provisioner GitHub page.</p> <p>By using the above helm chart and granting user access, NFS server will have directories such as /practicus/users/john-acme.com that gets mounted to ~/my for a user with email john@acme.com. Only John will have access to this folder.</p> <p>You can also define several shared folders between users E.g. shared/finance which would map to /practicus/shared/finance on the NFS server and gets mounted to ~/shared/finance on Cloud Workers.  </p> <p>The above path structure is an example and be customized flexibly through the NFS system itself, StorageClass provisioner setup, or simply by using the Practicus AI Management Console.</p> <p>Please view Cloud Worker Admin section to customize user or group based persistent drives.       </p>"},{"location":"k8s-setup/#using-aws-efs","title":"Using AWS EFS","text":"<p>If you are using AWS EFS, you can use the above provisioner, or as an alternative, you can also use a CSI specific for AWS EKS and AWS EFS. Please view the AWS EFS CSI documentation to learn more. Tip: Even if you decide to use the generic NFS provisioner with AWS EKS, you can still review the CSI page to learn more about security group settings, availability zone optimization etc.</p>"},{"location":"k8s-setup/#optional-creating-sample-object-storage","title":"(Optional) Creating sample object storage","text":"<p>Although it is optional, using object storage systems such as Amazon S3 or compatible for Machine Learning is very common. If you are testing Practicus AI and do not have access to S3 or a compatible store, you can simply use MinIO. </p>"},{"location":"k8s-setup/#sample-object-storage-with-minio","title":"Sample Object Storage with MinIO","text":"<p>You can install MinIO inside your Kubernetes cluster. For demo purposes, we will use a simple Docker container. We will also avoid using the default MinIO S3 port 9000, in case you are also using Practicus AI standalone docker deployment (not K8s). This type of test deployment already uses port 9000.   </p> <pre><code>echo \"Creating sample object storage\"\nhelm install practicus-sampleobj practicusai/practicus-sampleobj \\\n  --namespace prt-ns \n</code></pre> <p>After the minio object storage is created, you can connect to minio management console to create buckets, access credentials etc. For these, you need to create a temporary connection tunnel to minio service.   </p> <pre><code>SAMPLEOBJ_POD_NAME=$(kubectl -n prt-ns get pod -l \\\n  app=minio -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Sample object store pod name is: $SAMPLEOBJ_POD_NAME\"\n\necho \"Starting temporary connection tunnel\"\nkubectl -n prt-ns port-forward \"$SAMPLEOBJ_POD_NAME\" 9090:9090\n</code></pre> <ul> <li>Login to MinIO Console using http://127.0.0.1:9090 and credentials:</li> <li>User: minioadmin password: minioadmin</li> <li>Click Buckets &gt; Create bucket and create testbucket</li> <li>Click Identity &gt; Users &gt; Create User &gt; select readwrite policy</li> <li>Click Access Keys &gt; Create &gt; Note your access and secret keys </li> <li>Click Object Browser &gt; testbucket &gt; upload a .csv file</li> </ul> <p>You should now see a .csv file in testbucket and created a user, access/secret keys. </p> <p></p> <p>View MinIO installation document</p> <p>To test MinIO or other S3 compatible storage with the Practicus AI app:</p> <ul> <li>Open App &gt; Explore tab &gt; Click on New Connection &gt; Amazon S3</li> <li>Enter your access / secret keys</li> <li>Enter sample object storage endpoint url http://prt-svc-sampleobj.prt-ns.svc.cluster.local</li> <li>Select the bucket you created: testbucket</li> </ul> <p>You can now connect to this object storage to upload/download objects using the Practicus AI app. You will not need to create a connection tunnel to the minio management console to test with the app. </p>"},{"location":"k8s-setup/#optional-install-kubernetes-dashboard","title":"(Optional) Install Kubernetes dashboard","text":"<p>You can visualize and troubleshoot Kubernetes clusters using the dashboard. Please follow the below steps to install and view Kubernetes dashboard on your local development environment. Production setup and viewing will require some additional changes, which are beyond the scope of this document.  </p> <pre><code>echo \"Installing Kubernetes Dashboard\"\nkubectl apply -f \\\n  https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n\necho \"Setting dashboard permissions\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: dashboard-admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dashboard-admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: dashboard-admin-user\n  namespace: kubernetes-dashboard\nEOF\n</code></pre> <p>After the dashboard is installed you can open it using the below commands. </p> <p>Please do not forget to run kubectl proxy in a separate terminal window first, so the web interface is accessible from your browser. </p> <pre><code>echo \"Generating a dashboard access token for 90 days and copying to clipboard\"\nkubectl -n kubernetes-dashboard create token dashboard-admin-user \\\n  --duration=2160h &gt; dashboard-token.txt\n\necho \"Generated token:\"\necho \"\"\ncat dashboard-token.txt\necho \"\"\necho \"\"\npbcopy &lt; dashboard-token.txt\nrm dashboard-token.txt\n\necho \"Opening dashboard at:\"\necho \"http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\"\n\necho \"\"\nopen \"http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\"\necho \"Paste the access token at login page.\"\n\necho \"No login page? Make sure you ran kubectl proxy first\"\n</code></pre> <p></p> <p>Please note that the above steps installed Practicus AI elements to prt-ns namespace. You will have to switch the namespace in the dashboard</p>"},{"location":"k8s-setup/#optional-using-a-private-image-registry","title":"(Optional) Using a private image registry","text":"<p>All Practicus AI container images are customizable, and you can use a private image registry to host your custom images.  In this case, your Kubernetes cluster needs to be able to access the private registry. The below is a step-by-step example for AWS ECR private registry. </p> <ul> <li>Create a practicus-private-test private registry repository </li> </ul> <p></p> <ul> <li>Create your Dockerfile.</li> </ul> <p>Important: We strongly recommend you to use virtual environments for your custom python packages and avoid updating global packages.  </p> <pre><code>FROM ghcr.io/practicusai/practicus:24.1.0\n\nRUN echo \"this is a private repo\" &gt; /home/ubuntu/private.txt\n\nRUN echo \"**** Creating Virtual Env ****\" &amp;&amp; \\\n    python3 -m venv /home/ubuntu/.venv/practicus_test --system-site-packages &amp;&amp; \\\n    echo \"**** Installing packages ****\" &amp;&amp; \\\n    /home/ubuntu/.venv/practicus_test/bin/python3 -m pip install some-package &amp;&amp; \\\n    echo \"**** Installing Jupyter Kernel ****\" &amp;&amp; \\\n    python3 -m ipykernel install --user --name practicus_test --display-name \"My virtual environment\"\n</code></pre> <ul> <li>Build and push the image to private repository</li> </ul> <pre><code>aws ecr get-login-password --region us-east-1 | docker login \\\n  --username AWS --password-stdin _your_account_id_.dkr.ecr.us-east-1.amazonaws.com\n\ndocker build -t practicus-private-test:24.1.0 .\n\ndocker tag practicus-private-test:24.1.0 \\\n  _your_account_id_.dkr.ecr.us-east-1.amazonaws.com/practicus-private-test:24.1.0\n\ndocker push _your_account_id_.dkr.ecr.us-east-1.amazonaws.com/practicus-private-test:24.1.0\n</code></pre> <p>After this step, you should see the image in the AWS ECR console. </p> <ul> <li>Create an access token for the repository, and add as a Kubernetes secret</li> </ul> <pre><code>TOKEN=`aws ecr get-login-password --region us-east-1 | cut -d' ' -f6`\nNAMESPACE=prt-ns\n\nkubectl create secret docker-registry practicus-private-test-secret \\\n  --docker-server=_your_account_id_.dkr.ecr.us-east-1.amazonaws.com/practicus-private-test \\\n  --docker-username=AWS \\\n  --docker-password=$TOKEN \\\n  -n $NAMESPACE\n</code></pre> <p>Note: Some private registry tokens have short lifespans. E.g. AWS ECR default is 12 hours. </p> <ul> <li>Open Practicus AI management console, create a new custom image and add the private registry secret.</li> </ul> <p></p> <ul> <li>The end users that you gave permission to the custom image will be able to launch workers from it.</li> </ul> <p></p> <p>Note: You can also define custom images for other workloads such as model hosting, workspaces, etc. </p>"},{"location":"k8s-setup/#optional-forwarding-dns-for-local-installations","title":"(Optional) Forwarding DNS for local installations","text":"<p>If you are deploying Practicus AI using a local DNS such as local.practicus.io (points to 127.0.0.1) traffic inside the Kubernetes cluster might work since each pod would search for the service locally. This might not be a problem for traffic outside Kubernetes since the local cluster such as Docker Desktop would be listening on 127.0.0.1</p> <p>To solve this problem, we can edit Kubernetes coreDNS setup, so that a local DNS queries would forward to the Istio load balancer. </p> <p>Tip: You can use Kubernetes dashboard UI for the below steps 1-3. </p> <p>1) Locate Istio ingress gateway cluster ip address. E.g. 10.106.28.249 by running the below.</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <p>2) Edit coredns configmap to add the ip address.</p> <pre><code>kubectl edit configmap coredns -n kube-system\n</code></pre> <p>Sample coredns configmap forwarding local.practicus.io to an istio ingress gateway ip address <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: coredns\n... \ndata:\n  Corefile: |\n    .:53 {\n        ...\n    }\n\n    local.practicus.io {\n      hosts {\n        10.106.28.249 local.practicus.io\n      }\n    }\n</code></pre></p> <p>3) Restart coredns for new changes to be active. </p> <pre><code>kubectl rollout restart deployment/coredns -n kube-system\n</code></pre> <p>4) Test. Start a Practicus AI worker, open a jupyter notebook and run the below code. You should get the istio gateway id address.</p> <pre><code>import socket\nprint(socket.gethostbyname(\"local.practicus.io\"))\n</code></pre>"},{"location":"k8s-setup/#troubleshooting-issues","title":"Troubleshooting issues","text":"<p>Please follow the below steps to troubleshoot some common issues with Kubernetes Dashboard, or equivalent kubectl commands if you do not use the dashboard.</p> <ul> <li>Did the prt-depl-console-... pod start? (Green) If not, view its details.</li> <li>If the pod started, but is not accessible using http://local.practicus.io/console/admin view the pod logs. Click on the pod name &gt; View Logs (upper right)</li> <li>If the logs do not show any errors, Istio sidecar proxy might not be running. Click on the pod name, scroll down to containers and verify there are 2 containers running, prt-cnt-console and istio-proxy.</li> <li>Analyze istio to see if there are any proxy issues detected istioctl analyze -n prt-ns</li> </ul>"},{"location":"k8s-setup/#no-enterprise-key","title":"No enterprise key?","text":"<p>This step is mandatory if you do not have your Enterprise license key. </p> <p>By installing Practicus AI app you will be able to test connectivity to your newly created Kubernetes cluster. You will also have access to your Enterprise license key.</p> <ul> <li>Install the app</li> <li>Go to settings &gt; container section</li> <li>Enter your email to activate your enterprise license </li> <li>View Practicus AI local docker setup guide if you need any help</li> </ul> <p>Once your enterprise license is activated, please open ~/.practicus/core.conf file on your computer, locate the license section, and copy license_key info.</p> <p>Sample license_key inside ~/.practicus/core.conf :  <pre><code>[license]\nemail = your_email@your_company.com\nlicense_key = abc12345-1234-1234-12ab-123456789012\nvalid_until = Wed, 01 Jan 2022 00:00:00 GMT\n</code></pre></p>"},{"location":"k8s-setup/#no-connectivity","title":"No Connectivity?","text":"<p>Most connectivity issues will be a result of mis-configured Istio.  To solve these issues we recommend you to create test namespace e.g. istio-test and install the sample Istio app using the below help page.</p> <p>https://istio.io/latest/docs/setup/getting-started/</p> <p>Once you pass this step, deploying Practicus AI will be very similar.    </p>"},{"location":"k8s-setup/#support","title":"Support","text":"<p>Need help? Please visit https://helpdesk.practicus.io/ to open a support ticket.</p> <p>Thank you!</p>"},{"location":"logging/","title":"Logging","text":"<p>You can log regular events or audit logs to additional log systems such as CloudWatch, Splunk, Sumo Logic etc. </p> <p>All you need to do is add the necessary handlers to core.conf file on your local computer (rare)  or on Cloud Worker (common). </p>"},{"location":"logging/#cloudwatch","title":"CloudWatch","text":"<p>For CloudWatch, watchtower Python library is already installed on Cloud Worker.  You can update core.conf like the below to start logging.  </p> <pre><code># 1- add CloudWatch handler key\n[handlers]\nkeys = ... ,  cwHandler\n\n# 2- Define handler, select level i.e. DEBUG, INFO and other parameters such as log format\n[handler_cwHandler]\nclass = watchtower.CloudWatchLogHandler\nlevel = DEBUG\nformatter = simpleFormatter\n\n# 3- Add the new handler to any logger  \n[logger_practicus]\nhandlers = ... , cwHandler\n</code></pre> <p>For more information you can check watchtower documentation </p>"},{"location":"logging/#other-log-systems","title":"Other log systems","text":"<p>For log systems other than CloudWatch, you need to install the necessary python logger libraries first. And then you can add the log handler the same way as to CloudWatch</p>"},{"location":"mlops/","title":"Introduction to MLOps","text":"<p>This section requires a Practicus AI Cloud Worker, S3 compatible object storage, and a Kubernetes Cluster. Please visit the introduction to Cloud Workers section of our tutorial to learn more.</p>"},{"location":"mlops/#what-is-mlops","title":"What is MLOps","text":"<ul> <li>MLOps, short for Machine Learning Operations, is a set of practices that automates the machine learning lifecycle from development to deployment to monitoring and maintenance. It bridges the gap between ML engineers and DevOps teams to create a culture of continuous improvement for ML products.</li> </ul>"},{"location":"mlops/#why-is-mlops-important","title":"Why is MLOps Important?","text":"<ul> <li>MLOps is a valuable tool for organizations that want to get more value from their ML investments. By automating and managing the ML lifecycle, MLOps can help organizations to deploy ML models to production faster, improve the quality and reliability of ML models, reduce the risk of ML failures, and increase the ROI of ML investments.</li> </ul>"},{"location":"mlops/#what-is-practicus-ais-mlops-approach","title":"What is Practicus AI's MLOps approach?","text":"<p>Practicus AI MLOps offer a way to deploy and manage AI models effectively. It does this by providing a unified user experience, open-source Cloud Native technology, different deployment methods, dynamic service mesh, fine-grained access control, global APIs, and the ability to modernize legacy systems.</p> <ol> <li>Unified user experience and federated governance: Practicus AI provides a single user interface for managing and consuming AI models, even if they are deployed in multiple locations using different cloud providers and data sources. This makes it easy for business users and developers to interact with your AI models, regardless of their technical expertise.</li> <li>Open-source Cloud Native technology: Practicus AI is built using open-source Cloud Native technology, so you can avoid vendor lock-in. To learn more about cloud native please visit Cloud Native Computing Foundation website</li> <li>Different deployment methods: Practicus AI offers a variety of deployment methods, so you can choose the one that best suits your needs. AutoML makes it easy to build and deploy models without writing any code. Jupyter Notebook allows you to experiment with models and deploy them to production with just a few clicks. And custom code gives you complete control over the deployment process.</li> <li>Dynamic service mesh: Practicus AI uses Kubernetes deployments and Istio technology to create a dynamic service mesh for your AI models. This makes it easy to scale your models up or down as needed, and to manage multiple model versions simultaneously.</li> <li>Fine-grained access control: Practicus AI provides fine-grained access control tokens that allow you to control who can deploy, consume, and manage your AI models. This helps you to protect your models from unauthorized access.</li> <li>Global APIs: Practicus AI allows you to enable global APIs that allow developers to use a single URL to automatically use the closest cloud region, edge location or on-premise deployment. This makes it easy to deploy and consume your AI models globally, with high availability and low latency.</li> <li>Modernize legacy or proprietary models: Practicus AI can be used to modernize legacy or proprietary AI systems by wrapping them with Practicus AI Open MLOps. This allows you to get the benefits of Practicus AI MLOps without having to make changes to your existing code.</li> </ol>"},{"location":"mlops/#deploying-ai-models-using-the-app","title":"Deploying AI Models using the app","text":"<p>There are multiple ways to deploy AI models. In this section we will learn how to deploy the models we create with Practicus AI AutoML. You can also use the SDK or admin web UI to deploy models.</p> <ul> <li>Open Practicus AI App.</li> <li>Click Start Exploring and see this screen.</li> </ul> <p></p> <ul> <li>In this step, tap on the dataset to load the dataset to build the model.</li> <li>Preview your dataset and make optional advanced adjustments, and Load dataset.</li> <li>Click on the model and choose the type of model according to your problem.(E.g. Classification, Clustering, Anomaly Detection...)</li> <li>Select Objective Columns and adjust Advanced settings.</li> </ul> <p></p> <ul> <li>Click OK.</li> <li>Wait for model setup and see the model dialog.</li> </ul> <p></p> <ul> <li>When you see the Model dialog, select Deploy Model (API)</li> </ul> <p></p> <ul> <li>Select the appropriate one from the Prefixes you created in the Admin Console. (We will examine creating them in the admin console tab)</li> <li>Click Select model prefix or model. (Hint: If you register your model on an existing model, it will register as version 2)</li> <li>See your model and specifications under this prefix.</li> </ul> <p></p> <ul> <li>Back to Dataset.</li> <li>Click predict and select the model you deploy.</li> <li>Click OK.</li> <li>You can see the new Predict columns created.</li> </ul> <p>Now that we have learned how to deploy the model we created, let's learn how to manage these operations from the Practicus AI Admin Console.</p>"},{"location":"mlops/#practicus-ai-admin-console-for-ai-model-hosting-administration","title":"Practicus AI Admin Console for AI Model Hosting Administration","text":"<ul> <li>Open Practicus AI Admin Console.</li> </ul>"},{"location":"mlops/#model-deployments","title":"Model Deployments","text":"<ul> <li>Open Practicus AI Admin Console and click AI Model Hosting.</li> </ul> <ul> <li>Click Model Deployments. (Hint: The reason why it is called Model Deployment is that the models you host are created as Deployment in Kubernetes. This has several advantages, and you can do all operations from the Admin Console without updating the YAML file.)</li> </ul> <ul> <li>In the area where you open Model Deployments, you can make configurations related to the model, set the resource to assign to the models and assign auto scaler.</li> </ul> <ul> <li>At the same time, there may be containers that you have prepared with extra packages in this area, you can assign them, and you can add extra libraries to these containers with pip install by writing a startup script.</li> </ul> <ul> <li> <p>You can assign these Deployments on a group and user basis so that you can easily manage the models. You can also authorize these users and groups to run custom code.</p> </li> <li> <p>After completing and saving the necessary operations, you can open Deployments from Kubernetes Dashboard and see the Deployment you created.</p> </li> </ul> <p></p>"},{"location":"mlops/#model-prefixes","title":"Model Prefixes","text":"<ul> <li>Click Model Prefixes.</li> </ul> <p>Prefixes also represent URLs. You can set OpenAPI documentation settings in this area. You can also upload your pre-built models to Practicus AI and manage production settings and super secrets from there.</p> <p></p> <ul> <li>You can set access authorizations on group and user basis to the prefixes you create from this area. You can also set who can run custom code in this prefix.</li> <li>You can also assign a token to this prefix and access the models with this token.</li> </ul> <p></p>"},{"location":"mlops/#models","title":"Models","text":"<ul> <li>The models you create may not work as they used to over time. You can create new models and save these models as new versions and assign these versions as staging and production.</li> <li>At the same time, you can easily switch between the models you have created and saved as staging and/or production.</li> <li>With Practicus AI's dynamic service mesh approach, you can easily route traffic between models. Let's take a look at how this works.</li> </ul> <ul> <li>Click Models.</li> <li>Select Owner.</li> <li>Select Model Prefix.</li> <li>Select Name for model.</li> </ul> <ul> <li>Perform Deployment assignment of models.</li> <li>You can mark Stage as Staging and/or Production.</li> <li>Perform traffic routing between these versions with Traffic Volume Weight%. </li> <li>If you select Cascade prefix access tokens, you can access this model with the access token you defined for the prefix.</li> </ul>"},{"location":"mlops/#model-versions","title":"Model Versions","text":"<ul> <li>Select Model Versions.</li> <li>Click Add Model Version.</li> <li>Choose your existing model.</li> <li>Enter Model Version.</li> <li>Assign Model Deployment.</li> <li>Set Stage status .</li> <li>Assign Traffic Volume Weight%.</li> <li>You can choose the model as a draft if you want. If you choose, this model will not be marked as latest.</li> <li>Click Save.</li> </ul>"},{"location":"mlops/#external-api-access-tokens","title":"External API Access Tokens","text":"<ul> <li>Click External API Access Tokens.</li> <li>Select Add External API Access Token.</li> <li>Select Owner.</li> <li>Set Expiry date. (Hint: For advanced use cases. Python style dictionary to add extra key value pairs to the JWT token.)</li> <li>Select Global id. (Hint: For advanced use cases.  To build highly available and accessible APIs distributed in different geos, you can define tokens with the same global id and use them across the globe. E.g. a mobile app can access a global API 'api.company.com', and the DNS can  route the traffic to 'us.api.company.com' or 'eu.api.company.com' depending on user location OR service availability.)</li> <li>Select Model Prefix External API Access Tokens or Add another Model Prefix External API Access Token.</li> <li>Select Model External API Access Tokens or Add another Model External API Access Token.</li> <li>Click Save.</li> </ul> <p>We have completed the MLOps operations that can be done from the Practicus AI Admin Console. </p> <p>Now you can access our MLOps video from the link below to try these operations and digest the information you have gained:</p> <p>Practicus AI Open MLOps</p>"},{"location":"mlops/#optional-model-documentation","title":"Optional: Model Documentation","text":"<ul> <li>Business users can easily explore individual systems with the interface and access data sources. Technical users can easily access the documentation of the models with Swagger, OpeAPI or Redoc. </li> </ul>"},{"location":"model-api/","title":"For developers","text":"<p>Welcome to Practicus AI Model API documentation. You can use the Practicus AI App or the SDK to build, distribute and deploy AI/ML models and then consume these models with the app or any other REST API compatible system.</p>"},{"location":"model-api/#model-prefixes","title":"Model prefixes","text":"<p>Practicus AI models are logically grouped with the model prefix concept using the https:// [some.address.com] / [some/model/prefix] / [model-name] / [optional-version] / format. E.g. https://practicus.company.com/models/marketing/churn/v6/ can be a model API address where models/marketing is the prefix, churn is the model, and v6 is the optional version.</p>"},{"location":"model-api/#model-documentation","title":"Model Documentation","text":"<p>Models under a prefix can be viewed by using Practicus AI App. If your admin enabled them, you can also view the documentation by visiting ../prefix/redoc/ or ../prefix/docs/ or E.g. https://practicus.company.com/models/marketing/redoc/</p>"},{"location":"model-api/#model-deployment-concept","title":"Model Deployment concept","text":"<p>Models are physically deployed to Kubernetes deployment(s). These have several characteristics including capacity, auto-scaling, additional security etc.</p>"},{"location":"model-api/#authentication","title":"Authentication","text":"<p>Consuming models inside the Practicus AI app does not require additional authentication. For external use, you will need an access token. An admin can create tokens at the model prefix or individual model level.</p>"},{"location":"model-api/#submitting-data-in-batches","title":"Submitting data in batches","text":"<p>Practicus AI app will automatically split large data into mini-batches to improve performance and avoid memory issues.  You are encouraged to do the same and submit data in a volume compatible with your model deployment capacity. E.g. If your model deployment pods have 1 GB RAM, it is advisable to submit data in 250MB or less batch size.    </p>"},{"location":"model-api/#compression","title":"Compression","text":"<p>Practicus AI app automatically compresses requests and responses for the model API. You can also compress using 'lz4', 'zlib', 'deflate', 'gzip' compression algorithms. All you need to do is to compress, and send the algorithm using 'content-encoding' http header, or by simply naming the attached file with the compression extension. E.g. my_data.lz4</p>"},{"location":"model-api/#model-metadata","title":"Model Metadata","text":"<p>You can learn more about any AI model by simply requesting it's meta data using ?get_meta=true query string. E.g. https://practicus.company.com/models/marketing/churn?get_meta=true</p>"},{"location":"model-api/#sample-python-code","title":"Sample Python code","text":"<pre><code>import requests\nheaders = {\n    'authorization': 'Bearer _your_access_token_',\n    'content-type': 'text/csv'\n}\nr = requests.post('https://practicus.company.com/models/marketing/churn/', headers=headers, data=some_csv_data)\nprint('Prediction result: ', r.text)\n</code></pre>"},{"location":"model-api/#getting-session-and-access-api-tokens","title":"Getting session and access API tokens","text":"<p>Please prefer a 'short-lived' session API access token where you can get one programmatically like the below example. If you do not use Practicus AI SDK, you can request a 'long-lived' API access token from a Practicus AI admin as well.  Please note that session tokens will offer increased security due to their short lifetime.  </p> <pre><code>import practicuscore as prt \ntoken = prt.models.get_session_token('https://practicus.company.com/models/marketing/churn/')\n</code></pre>"},{"location":"model-api/#compression-code","title":"Compression code","text":"<p>With or without streaming, you can submit the data after compressing with lz4', 'zlib', 'deflate', 'gzip' algorithms. If you compress the request, the response will also arrive using the same compression algorithm. </p> <pre><code>import lz4 \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv',\n    'content-encoding': 'lz4'\n}\ndata_csv = df.to_csv(index=False)\ncompressed = lz4.frame.compress(data_csv.encode())\nprint(f\"DataFrame compressed from {len(data_csv)} bytes to {len(compressed)} bytes\")\n\nr = requests.post(api_url, headers=headers, data=compressed)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\ndecompressed = lz4.frame.decompress(r.content)\nprint(f\"Result de-compressed from {len(r.content)} bytes to {len(decompressed)} bytes\")\n\npred_df = pd.read_csv(BytesIO(decompressed))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"model/","title":"Model","text":"<p>Build AI models on past data with one-click using Automated Machine Learning (AutoML) and then make predictions  on unseen data. If you are a data scientist, or working with one, export to Jupyter code and share your experimentation  details in a central database (MLflow). Your team will save significant amount of time by avoiding to start coding from  scratch and by collaborating better</p>"},{"location":"model/#create-a-model","title":"Create a Model","text":"<p>You need to have a Cloud Worker ready for the model creation process and start the model creation process with our data on the Cloud Worker.</p> <p></p> <p>We create a model of regression type by selecting the target column revenue using the ice cream data. You can also limit the model with the advanced options section and select build excel for model.</p> <p></p> <p>When the model creation process is finished, some options appear.</p> <p></p> <p>You can load the model in Excel, register it in MLflow, see the details of the experiment with MLflow, you can see the code with Jupyter Notebook.</p> <p></p> <p></p>"},{"location":"modern-data-pipelines/","title":"Modern Data Pipelines","text":"<p>Modern data pipelines often need to implement 2 key concepts: Atomicity and Idempotency to provide consistent and reliable solutions. In this section we will explain what these concepts are and how they are implemented in Practicus AI.    </p>"},{"location":"modern-data-pipelines/#atomicity","title":"Atomicity","text":"<p>Traditional ETL pipelines often processed data in small batches in order to fit in the limited memory of the available hardware. This sometimes created major issues and race conditions, such as a data pipeline running half-way through and failing, leaving the final destination having only half of the data.</p> <p>Sample Non-Atomic Pipeline: </p> <p>Sample Atomic Pipeline:</p> <p></p>"},{"location":"modern-data-pipelines/#idempotency","title":"Idempotency","text":"<p>Idempotence is the concept of allowing a task to run multiple times, producing the same result. For instance, you could press the on button of an idempotent machine multiple times, and it would not change the result.</p> <p>In the below pipeline, if a task had to run multiple times (often due to a failure) and gives the same result, we can say it implements idempotency principle. This wasn't the case in many traditional ETL pipelines.  </p> <p></p>"},{"location":"modern-data-pipelines/#practicus-ai-approach","title":"Practicus AI approach","text":"<p>The default Practicus AI data processing approach is to break down any data pipeline into atomic and idempotent tasks that include 4 key activities.</p>"},{"location":"modern-data-pipelines/#default-anatomy-of-a-practicus-ai-data-pipeline-task","title":"Default anatomy of a Practicus AI data pipeline task","text":"<p>1) Read data from any data source (mandatory, do it once)</p> <p>2) Apply data transformation steps (optional, can be done many times)</p> <p>3) Join to any data by reading it directly from its data source (optional, can do many times)</p> <p>4) Save final data to any destination (mandatory, do it once)</p>"},{"location":"modern-data-pipelines/#default-anatomy-of-a-practicus-ai-data-pipeline","title":"Default anatomy of a Practicus AI data pipeline","text":"<p>Practicus AI creates a modern DAG (directed acyclic graph) which defines the order, parallelism and dependency of as many atomic and idempotent tasks as you need. </p> <p>Let's take a look at the example below:</p> <p>1) Load table1, make some changes and export to table1_new. Keep this worksheet open in the app.</p> <p>2) Do the same for table_2</p> <p>3) Load table_3 and join to table_1 and table_2. Since data in these tables are exported, table_3 will use data from table1_new and table2_new to join.</p> <p>4) Load table4, do not make any changes</p> <p>5) In table3, join again, this time to table4. Since no changes are made and exported, the original data source of table4 will be used</p> <p>6) Export table3 data pipeline code to go to production. </p> <p></p> <p>Your exported Airflow DAG will look like the below:</p> <p></p> <ul> <li>The code that loads table1 and table2, processes the steps, and exports to table1_new and table2_new will work in parallel.</li> <li>If any of these tasks fail, it will retry (default 3 times)</li> <li>If, for some reason, these tasks run successfully again, the resulting tableX_new will not be different (idempotent) </li> <li>When both of these tasks are completed, table3 code will run: loads table3, executes all steps including joining to table1_new, table2_new and table4 in the requested order</li> <li>Once all of the steps and joins for table3 are completed, it will export to final_data (atomicity) </li> <li>Please note that in a traditional pipeline the table1 task could pass in-memory processed data to table3, eliminating the need to export to an intermediary location (table1_new) first. </li> <li>In comparison, Practicus AI generated pipelines require your changes to be exported first. This is due to scalability requirements of modern big data systems.     </li> <li>If you close a worksheet in the app before exporting the deployment code, the exported code will not include the task of that table in the Airflow DAG. </li> <li>I.e. In the above example, if you close table1 before exporting the deployment code, the DAG would become simpler table2 -&gt; table3.  In this case, we assume the task needed to create table1_new will be part of another data pipeline. The new DAG and code for table3 will simply expect table1_new is available and up to date.    </li> <li>Please note all of the above are simply convenience defaults. You can always make changes to your DAG, moving tasks back and forth after code is exported.</li> </ul>"},{"location":"predict/","title":"Predict with App","text":"<p>Predict the unknown with one-click. Search and use models built with any technology / platform. You can use model APIs hosted anywhere, or use models cold stored on your laptop, S3 or other location,  which will become live within seconds. With short startup time and ease of use, your organization\u2019s ML models will reach new users easier</p> <p>This section of the documentation is work in progress..</p>"},{"location":"predict/#prediction","title":"Prediction","text":"<p>You need to have a Cloud Worker ready for the prediction process and start the prediction process with our data on the Cloud Worker.</p> <p></p> <p>In the predict operation, where we will use the registered model, MLflow is selected as the model location(MLflow, Local Computer, S3).</p> <p></p> <p>You can search and select the ice cream model registered as Model URI.</p> <p></p> <p></p> <p>When the prediction is completed, the prediction column is created on the data.</p> <p></p>"},{"location":"prepare/","title":"Prepare with App","text":"<p>Process, clean and prepare your data without any coding. When clicking is not enough, use 200+ Excel compatible formulas.  Add custom Python code using the built-in editor for more complex requirements. Export the final clean data to a file,  data lake or database directly from the app. To build repeatable data pipelines, export to pure Python code and run anywhere you need.</p> <p>This section of the documentation is work in progress..</p>"},{"location":"prepare/#sort","title":"Sort","text":"<p>You can sort the columns you want in ascending or descending order. </p>"},{"location":"prepare/#filter","title":"Filter","text":"<p>You can filter any column using logical operators.</p> <p></p>"},{"location":"prepare/#rename","title":"Rename","text":"<p>You can change the name of the column you choose.</p> <p></p>"},{"location":"prepare/#formula","title":"Formula","text":"<p>You can run and test more than one formula on the column you want.</p> <p></p> <p>When you want to add the formula, the new column on the worksheet will be ready.</p> <p></p>"},{"location":"prepare/#code","title":"Code","text":"<p>In this section, you can run the code yourself and test the code you wrote on the worksheet. When you apply the code, the change on the worksheet is saved.</p> <p></p>"},{"location":"prepare/#other-features","title":"Other Features","text":"<p>In addition, you can easily perform Show/Hide, Move Left , Move Right, Delete , Split, Convert, Missing, Group By, Categorical, One Hot and Join operations on data and columns.</p>"},{"location":"sdk-start/","title":"Home","text":"<p>Basic data preparation use case</p> <p>1) Export a Pandas DataFrame, NumPy array or a Python list to Excel</p> <pre><code>import practicus\n# export data to an Excel file\npracticus.export_data(my_df, \"my_data.xlsx\")\n</code></pre> <p>2) Open the file in Excel, Google Sheets, LibreOffice or any other Spreadsheet platform to analyze and make changes as usual. </p> <p></p> <p>3) After you are finished updating your data in Excel, you can apply all changes made to create a new data set. </p> <pre><code># import back from Excel, detect all the changes made, and apply to the Data Frame  \nmy_df2 = practicus.apply_changes(my_df, \"my_data.xlsx\") \n\n# practicus auto-generates Python code for you, and applies the updates..\n\n# display the result, which will be the same as what you see in Excel\ndisplay(my_df2)\n</code></pre> <p></p> <p>4) (Optional) Practicus AI will automatically create a data prep (.dp) file containing all detected changes, before generating Python code. You can review this file, remove changes you don't like, or add new ones manually as you wish. Once done, you can apply the updates directly from the .dp file. </p> <p></p> <pre><code># apply changes, but this time directly from the .dp file that you reviewed / updated\nmy_df2 = practicus.apply_changes(my_df, \"my_data.dp\")\n</code></pre> <p>5) (Optional) Rinse and repeat... You can continue the above steps, also working with others in a collaborative environment, to keep generating new versions of Excel files and auto-generated data sets. The detected changes (.dp files) can be updated and archived as needed. Outside of Jupyter notebooks, you can also chain multiple .dp files to create complex data preparation / ML pipelines and later embed these data pipelines to a data engineering platform for production purposes.  Any production grade data integration platform that can run Python code will easily run Practicus AI detected changes at scale.   </p>"},{"location":"sdk-start/#aiml-model-sharing","title":"AI/ML Model Sharing","text":"<p>Beyond data preparation, Practicus AI can also be used to export ML models to Excel, which can be used for different purposes. Below you can find some use cases to export your models to Excel. </p> <ul> <li> <p>Practicus AI exported models can help with ML deployment and testing and increase your chances of getting them to production to be used by masses.   </p> </li> <li> <p>The exported models have zero dependency, meaning they only use core Excel functionality and do not depend on 3rd party libraries, products, services, REST APIs etc. You can attach the exported ML model to an email, and the recipient would be able to predict / infer offline without any issues. </p> </li> <li> <p>You can use the exported Excel file to debug your models, since the model representation will be in a very simple form. </p> </li> <li> <p>Model Explainability can be a key blocker for getting ML models to production. Often times, data analysts, business analysts and other business leaders will not allow moving an ML model to production, simply because they do not understand how the model works. Practicus AI exported models in Excel will be significantly easier to consume and understand.</p> </li> </ul> <p>Basic model sharing use case</p> <p>1) Build your ML model as usual. </p> <pre><code># sample Support Vector Machine model\n...\nmy_svm_model.fit(X, Y)\n</code></pre> <p>2) Export to Excel </p> <pre><code>import practicus    \npracticus.export_model(my_svm_model, output_path=\"iris_svm.xlsx\",\n      columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>3) Open the file in Excel, Google Sheets, LibreOffice or others to make predictions, analyze how the model works and make changes as well.</p> <p></p> <p></p> <p></p> <p>4) (Optional) You can use pre-processing pipelines as well. Necessary calculations prior to model prediction will also be exported to Excel as pre-processing steps.   </p> <pre><code># create a pipeline with StandardScaler and LogisticRegression\nmy_pipeline = make_pipeline(\n   StandardScaler(),\n   LogisticRegression())\n\n# train\nmy_pipeline.fit(X, y)\n\n# Export the pre-processing and model calculation to Excel\npracticus.export_model(my_pipeline, output_path=\"model_with_pre_processing.xlsx\",\n                   columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>5) (Optional) You can also export models with Practicus AI using PMML. If you are using R, KNIME, Alteryx or any other ML platform, you can export your models to a .pmml file first (optionally including pre-processing steps as well) and then use the .pmml file with Practicus AI in Python to export it to Excel. The final Excel file will not have any dependencies to your ML platform. </p> <pre><code>practicus.export_model(\"my_model_developed_on_R.pmml\", output_path=\"R_model.xlsx\",\n                   columns=['petal length', 'petal width'], target_name=\"Species\")\n</code></pre>"},{"location":"setup-guide/","title":"Setup Guide","text":"<p>Welcome! It should take a few minutes to set up everything and be ready to go!</p>"},{"location":"setup-guide/#overview","title":"Overview","text":"<p>You can see a simplified view of Practicus AI setup options below. </p> <ul> <li>Practicus AI App is Forever Free and include common analytics and data preparation features. </li> <li>Cloud Workers with Forever Free Tier are optional but highly recommended. They bring in more functionality such as AutoML. You can choose one or more Cloud Worker system. </li> </ul> <p></p>"},{"location":"setup-guide/#install-the-practicus-ai-app","title":"Install the Practicus AI App","text":"<p>Practicus AI App works on your computer and contains the core functionality of our platform. </p> <p>If you haven't already, please install Practicus AI App for Windows, macOS or Linux.  </p> <p>If you are a Python user and prefer to Install the Practicus AI App as a library, please check our Python Setup Guide section below. </p> <p>If you are a programmer and only interested in installing the lightweight Practicus AI SDK (5MB), please only install practicuscore library using the Python Setup Guide section below.</p>"},{"location":"setup-guide/#optional-choose-a-cloud-worker-system-backend","title":"Optional - Choose a Cloud Worker System (backend)","text":"<p>If you prefer the quickest option, you can use our Software as a Service (SaaS), which will be ready in less than a minute. Click here to start Practicus AI SaaS trial </p>"},{"location":"setup-guide/#what-is-a-cloud-worker","title":"What is a Cloud Worker?","text":"<p>Some Practicus AI features such as AutoML, making ** AI Predictions, Advanced Profiling and production deployment** capabilities require a larger setup, so we moved these from the app to a backend (server) system.  </p> <p>You have multiple Cloud Worker options to choose from, and you can find a quick summary on pros and cons of each option below.</p> <p>Please view the detailed comparison table for the pros and cons for each option. </p>"},{"location":"setup-guide/#software-as-a-service","title":"Software as a Service","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p>"},{"location":"setup-guide/#enterprise-cloud","title":"Enterprise Cloud","text":"<p>You cna think of the Enterprise Cloud as a private Practicus AI SaaS that can run on any cloud, or your private data enter. </p> <p>Configuration is exactly the same as Software as a Service. You only need to change your service address from https://service.practicus.io to your private address that your administrator provided, such as https://practicus.your_company.com</p> <p>Your password will be provided by your administrator. You can also click the Forgot Password button to reset your password. </p> <p></p>"},{"location":"setup-guide/#aws-marketplace","title":"AWS Marketplace","text":"<p>This section explains setting up AWS Cloud Workers. </p> <p></p>"},{"location":"setup-guide/#before-we-begin","title":"Before we begin","text":"<p>Some of our advanced features require cloud capacity. </p> <p>Instead of asking for access to your data, Practicus AI cloud runs 100% in your AWS account, fully isolated. This allows us to offer improved security and absolute privacy.</p> <p>You can activate Practicus AI on your AWS account in a few minutes, and use up to a certain cloud capacity free of charge.  Let's get started.</p>"},{"location":"setup-guide/#1-select-license","title":"1) Select license","text":"<p>Practicus AI cloud offers 3 different licenses. </p>"},{"location":"setup-guide/#free-license","title":"Free License","text":"<p>We offer a forever free cloud tier using 2 vCPUs and 1 GB RAM on AWS with t3.micro cloud instances.</p> <p>Please note that some AWS cloud regions charge roughly 1 cent / hour for t3.micro (see below). If you need to make sure everything your use is absolutely free, please make sure you pick an appropriate AWS cloud region. AWS also offer free tiers for other potential charges like S3 storage and network traffic. To keep everything free, you must experiment responsibly and make sure you do not go beyond these AWS limits. </p> <ul> <li>Please view AWS free tier details to learn more.</li> </ul>"},{"location":"setup-guide/#professional-license","title":"Professional license","text":"<p>When you need larger Practicus AI cloud capacity, you can simply pay-as-you-go (PAYG) hourly without sharing your credit card info, or making any commitments. AWS will charge for the Practicus AI cloud usage hourly, which will show as a separate Practicus AI line item in your monthly AWS cloud invoice. </p> <p>Practicus AI cloud works like electricity, you switch it on when you need it, and only pay for the total number of hours that the lights are on. Our Cloud Workers auto shut down after 90 minutes of inactivity by default, preventing unexpected costs. It works similar to this scenario: you leave your home and forget to turn the lights off. Your lights turn off automatically after 1.5 hours, since there is no motion detected in the house. </p> <p>Please visit practicus.ai/pricing for more info, and example pricing scenarios.   </p>"},{"location":"setup-guide/#enterprise-license","title":"Enterprise license","text":"<p>We offer a bring-your-own-license (BYOL) model with benefits such as unlimited use for multiple users, with a fixed fee. </p> <p>If you have an enterprise license, simply open Practicus AI app and go to settings (preferences in macOS), cloud tab, and enter your email to activate your license.</p> <p>Please feel free to contact us to learn more about our enterprise support and licensing.</p> <p>Use your email to activate the enterprise license in app settings: </p>"},{"location":"setup-guide/#2-ready-your-aws-cloud-account","title":"2) Ready your AWS cloud account","text":"<p>This is a one-time task for all users sharing the same AWS account. </p> <p>Please skip this step if you already have an AWS account. </p> <p>Create an Amazon Web Services (AWS) cloud account for free. </p> <p>Please make sure you have created an admin user as well. You can check our AWS account creation guide below for help.</p>"},{"location":"setup-guide/#3-enable-on-aws-marketplace","title":"3) Enable on AWS marketplace","text":"<p>This is a one-time task for all users sharing the same AWS Account. </p> <p>Learn more about AWS marketplace. </p> <p>We have multiple offerings on AWS marketplace. Please click on each in the below list to view the marketplace page explaining the offering, and then click Continue to Subscribe button to enable (see screenshot below). You need at least one AWS marketplace offering enabled to use Practicus AI cloud. </p> <p>Please note that it can take a few minutes for the offering to be active. Once your subscription is active, please do not create a new EC2 instance using AWS cloud console. The next step will take care of configuration.</p>"},{"location":"setup-guide/#free-professional-pay-as-you-go-payg","title":"Free / Professional pay-as-you-go (PAYG)","text":"<ul> <li>Practicus AI - Most common, offers free tier, will give you all the functionality.</li> <li>Practicus AI with GPUs - Accelerated computing for very large data or if you have limited time. Can be 500+ times faster for some operations. </li> </ul>"},{"location":"setup-guide/#enterprise-license-bring-your-own-license-byol","title":"Enterprise License bring-your-own-license (BYOL)","text":"<ul> <li>Practicus AI - Most common enterprise offer</li> <li>Practicus AI with GPUs - Accelerated computing, enterprise offer</li> </ul> Sample view of our AWS marketplace page <p>Please carefully review software, hardware and total cost / hour on our AWS marketplace page. Sample professional license below: </p> <p></p>"},{"location":"setup-guide/#4-activate-your-aws-user-in-the-app","title":"4) Activate your AWS user in the app","text":"<p>You should now have an AWS user Access key ID and Secret access key ready, and the AWS account for this user has at least one Practicus AI AWS marketplace offer enabled.   </p> <p>Simply open the Practicus AI app, go to settings (preferences in macOS), cloud tab, click the Activate your AWS user button, choose a default cloud region (you can change this later) and enter your cloud credentials:</p> <p></p> <p>Please note that your cloud credentials are not shared with 3rd parties, including Practicus AI. The app only uses the credentials to communicate with AWS cloud.</p> <p>Before you finalize the cloud settings, we will verify your configuration to check if everything is configured correctly.    </p> <p></p> Sample AWS marketplace verification result. You need at least one verified <p>Pro Tip: You can save the cloud configuration info to a file and share with others, so they can open this file with Practicus AI app and automatically configure the cloud setup. Please check the Setup for others section below to learn more.  </p>"},{"location":"setup-guide/#troubleshooting","title":"Troubleshooting","text":"<p>If you could not verify one of the AWS marketplace offers, please check the below as potential reasons:</p> <ul> <li>AWS marketplace activation can take a few minutes. Please make sure you stay on the AWS marketplace page, confirm the activation is completed and go back to the app settings to verify again.  </li> <li>If you use multiple AWS accounts, please make sure you  subscribe using the correct AWS account since it is very easy to mix them up. Simply log-off from your AWS account, click on one of the view buttons inside the app settings to view AWS marketplace page again, login using the correct user, click subscribe, wait for it to be completed, and finally go back to app settings and click verify again.</li> <li>In rare cases, a company admin can disable AWS marketplace usage. If this is the case, please contact your admin, or create a new AWS account. </li> </ul>"},{"location":"setup-guide/#local-container","title":"Local Container","text":"<p>This section explains setting up a local container Cloud Worker.</p>"},{"location":"setup-guide/#1-install-a-container-engine","title":"1) Install a container engine","text":"<p>In order to run a container on your computer you need to first install a container engine.</p> <p>Docker is the most popular option: Install Docker Desktop </p> <p>Although Docker Desktop is free, there has been some licensing changes in the recent years. </p> <p>Podman is a great Docker alternative: Install Podman Desktop</p> <p>Once the installation is completed, simply run Docker or Podman Desktop and confirm the container engine is running.</p> <p>Active Docker Desktop </p> <p></p> <p>Active Podman Desktop </p> <p></p>"},{"location":"setup-guide/#2-pull-download-practicus-ai-container-image","title":"2) Pull (download) Practicus AI container image","text":"<p>Additional Practicus AI software is bundled inside a container image. You need to pull this package on your computer before using it. </p> <ul> <li>Open Practicus AI App settings (preferences in macOS) dialog and navigate to the Container section.</li> <li>If you have a Practicus AI Enterprise license, enter your email to activate and unlock all features. If not, you can use the free tier. Please note that Professional pay-as-you-go license option is not available for local containers. Compare license options.   </li> <li>Choose a container engine, Docker or Podman, and confirm in the app the engine is running.   </li> <li>Click the Pull (download) Practicus AI container image button</li> </ul> <p></p> <ul> <li>A command prompt window (terminal in macOS) will open to start the pull. This one-time download task can take anywhere between 5 - 20 minutes depending on your internet speed. </li> </ul> <p></p> <ul> <li>Once the container pull is completed, go back to the app and click refresh to view active Practicus AI images on your computer. Confirm you successfully pulled the container image.</li> <li>Click New Cloud Worker button to open Cloud Workers tab.</li> <li>Click Save to close settings.</li> </ul> <p></p> <ul> <li>In the Cloud Workers tab, select local container as Cloud Region.</li> <li>Click Launch New button to start a Cloud Worker.</li> </ul> <p></p> <ul> <li>When navigating cloud data sources in the Explore tab, you can switch between local and Cloud Workers by using the drop-down list at the top right.</li> <li>Practicus AI app also attaches (mounts) container_shared folder, so you can easily copy files back and forth between your file system and the container. Simply open Windows Explorer (Finder in macOS), navigate to: your home folder / practicus / container_shared and copy files. Then navigate to Cloud Worker Files in Explore tab, and the files you copied will be visible under container_shared folder. Click Reload button at the top if you recently copied files.  </li> </ul> <p></p> <p>Issues? If you are facing any issues, please check the container troubleshooting section below.</p>"},{"location":"setup-guide/#references","title":"References","text":""},{"location":"setup-guide/#aws-account-creation","title":"AWS Account Creation","text":"<p>Practicus AI Cloud Workers can start and stop with a click using our app, and they run in your Amazon Web Services (AWS) cloud account in a fully isolated fashion. This way we can offer you 100% privacy. </p> <p>Please follow the below steps to create a free AWS account.</p> <ol> <li>Please click here to visit AWS home page and click Create an AWS account</li> <li>Follow the steps to finish account creation. Please check the AWS help page if you need assistance on creating an account. After this step you will have a root account.</li> <li>Login to AWS management console using your root account. </li> <li>Navigate to IAM (Identity and Access Management), click Users on the left menu and click Add users</li> <li>For User name enter admin, click Access key and Password check boxes (see below screenshot)</li> <li>In Set permissions section select Attach existing policies directly and pick AdministratorAccess (see below screenshot)</li> <li>In the last screen carefully save your Access Key ID, Secret access key and Password (see below screenshot)</li> <li>All done! You can continue with the next step, Enabling on AWS marketplace </li> </ol> <p>Notes:</p> <ul> <li>Although the admin AWS user will be sufficient for Practicus AI Cloud Workers to run, as a security best practice we recommend you to create a least privileged AWS user for day to day usage.  Practicus AI app cloud setup can create this user for you. If you rather prefer to create one manually please make sure the user has access for EC2 and S3 operations. </li> <li>If you plan on having multiple AWS users sharing the same AWS account, you can simply add new users to the appropriate practicus AWS IAM user group that our app creates for you. </li> <li>If you have a local AWS profile (i.e. to be used with AWS CLI) Practicus AI setup can use this directly. </li> </ul> AWS account creation screenshots <p></p> <p></p> <p></p>"},{"location":"setup-guide/#aws-marketplace-reference","title":"AWS Marketplace Reference","text":"<p>Similar to our Windows app being available on Microsoft app store, our cloud workers are available on AWS marketplace. This gives our users \"there's an app for that!\" type experience, but for AI in the cloud. </p> <p>Any time you need to do AI in the cloud, you can just click a button in the Practicus AI app, and we will create the cloud capacity of your choice. And also shut it down with a click when you no longer need it, saving on cost.,</p> <p>For our app to be able to start/stop cloud worker you need to enable (subscribe to) the AWS marketplace offering of your choice. </p> <p>If you use the free tier (t3.micro) with 2 vCPUs and 1 GB RAM, there won't be any software license charges. AWS also offers t3.micro free of charge for eligible customers and regions. For larger capacity, AWS will charge you per hour. i.e. you start a cloud worker, use it for 2 hours and never use it again. Your cloud bill will have a line item for the 2 hours you use. Larger capacity is more expensive and the larger the capacity the bigger discount you will get.  </p>"},{"location":"setup-guide/#setup-for-others","title":"Setup for others","text":"<p>You can save cloud setup information to a file and share with others, so they can simply open the file with Practicus AI app to complete the cloud setup. </p> <p>Practicus AI uses .prt files to save worksheet data and steps. Since .prt files directly open with Practicus AI app, we use the same file extension to configure the app as well.</p> <p>You can simply create a text file, give it a name such as cloud_setup.prt and add setup information like the below:</p> <p>Sample cloud_setup.prt file: <pre><code>[add_license]\nemail = user@mycompany.com\n\n[add_cloud_config]\ncloud_region = us-west-1\naccess_key = ...\nsecret_key = ...\n</code></pre></p> <p>[add_license] section can be used if you have an enterprise license. </p> <p>[add_cloud_config] section can be used to add an AWS cloud configuration. You can use the cloud_region key to set the default region (can be changed later), and it is optional.  access_key (Access Key ID) and secret_key (Secret access key) are mandatory, and can be obtained during AWS IAM user creation. </p>"},{"location":"setup-guide/#container-troubleshooting","title":"Container troubleshooting","text":"<p>If you face any issues using local containers on your computer, please follow the below steps.   </p> <p>View Status on Container Engine</p> <p>Open Docker Desktop or Podman Desktop, navigate to containers, and iew it's status. You can delete using these apps, which is the same thing as Terminate in Practicus AI, and create a new one using Launch New button inside Practicus AI App Cloud Workers tab.</p> <p>Starting a Container manually</p> <p>You can use the below command prompt (terminal) command to start a container from an image you downloaded.</p> <pre><code>docker run -p9000:9000 -p5500:5500 -p8585:8585 -p50000-50020:50000-50020 -d \\\n  --mount type=bind,src=$HOME/practicus/container_shared/,dst=/home/ubuntu/container_shared \\\n  --env MAX_PRACTICUS_WORKERS=21 \\\n  --env DEBUG_MODE=False \\\n  --name practicus ghcr.io/practicusai/practicus:22.11.0 \\\n  --env PRACTICUS_ENT_LIC_EMAIL='your-name@your-company.com' \\\n  --env PRACTICUS_ENT_LIC='abcd-1234-abcd-1234-abcd'\n</code></pre> <p>Notes:</p> <ul> <li>Please do not forget to create container_shared folder under your home directory / practicus folder first.   </li> <li>No Enterprise License? Please remove PRACTICUS_ENT_LIC_EMAIL and PRACTICUS_ENT_LIC lines.</li> <li>To find your Enterprise license key (PRACTICUS_ENT_LIC), please view your home directory / .practicus / core.conf file and search for text 'license_key'. Please note that .practicus is a hidden folder.</li> <li>Replace 'docker' with podman or another container engine if you are not using docker. </li> <li>Need less or more workers? change MAX_PRACTICUS_WORKERS and also the port range -p50000-50020:50000-50020. E.g. For 10 workers use  MAX_PRACTICUS_WORKERS=10 -p50000-50009:50000-50009</li> <li>If you use network proxies, add them in the below format <pre><code># ...\n  --env PRACTICUS_ENT_LIC='abcd-1234-abcd-1234-abcd' \\\n  --env HTTP_PROXY='http://192.168.0.1:9090' \\\n  --env HTTPS_PROXY='http://192.168.0.1:9090'  \n</code></pre></li> </ul> <p>Pulling a new container manually</p> <ul> <li>Please visit https://github.com/practicusai/deploy/pkgs/container/practicus and choose a version of Cloud Worker to pull. Use matching yy.mm versions between the App and the container. E.g. If your app is 22.11.2 you can use any 22.11.x Cloud Worker. Ideally use the latest e.g. 22.11.5 instead of 22.11.4. If you use Practicus AI App to pull, it already pulls the latest compatible image.</li> <li>Pull using the command. Prefer to use version tags instead of 'latest'</li> </ul> <pre><code>docker pull ghcr.io/practicusai/practicus:22.11.0\n</code></pre> <p>Hard reset</p> <p>Sometimes previously downloaded images can linger in the cache even if you deleted them. In order to hard reset and delete everything, you can use the below command.</p> <pre><code># Use with caution! This will delete all images and containers\ndcker image prune --force \n</code></pre>"},{"location":"setup-guide/#linux-installation","title":"Linux Installation","text":"<p>Since almost all Linux distros come with Python 3 installed, we do not offer a prepackaged installer. </p> <p>Please check the quick start guide below to see how you can install and run Practicus AI app on Linux. We have extensively tested on Ubuntu, if you encounter any issues please share with us using feedback section.</p>"},{"location":"setup-guide/#python-setup-guide","title":"Python Setup Guide","text":"<p>You can run pip install practicus (Windows/macOS/Linux: Python 3.10+) and then run practicus from the terminal. Or run, python -c \u201cimport practicus; practicus.run()\u201d  (python3 for macOS or Linux).</p> <p>Installing using pip will give you the exact same end-to-end GUI application experience. Similarly, if you download the packaged app you can still code freely when you want to. So, please select any method of installation as you prefer. </p> <p>As for any Python application, we strongly recommend you to use a virtual environment such as venv or conda. Please check the recommended QuickStart scripts on this page to create a virtual env, install Practicus AI and run with peace of mind and in one go.  </p> <p>For server environments or API only usage, you can pip install practicuscore to install the core library by itself without the GUI elements. (Windows/macOS/Linux: Python 3.10+) </p> <p>This is a small library with fewer requirements and no version enforcement for any of its dependencies. It\u2019s designed to run in existing virtual environments without overriding the version requirements of other libraries. Please check the documentation for more details. </p>"},{"location":"setup-guide/#windows-quickstart","title":"Windows QuickStart","text":"<pre><code>:: install \npython -m venv %UserProfile%\\practicus\\venv\n%UserProfile%\\practicus\\venv\\Scripts\\python -m pip install --upgrade practicus\n\n\n:: run\n%UserProfile%\\practicus\\venv\\Scripts\\activate\npracticus\n</code></pre>"},{"location":"setup-guide/#macos-quickstart","title":"macOS QuickStart","text":"<pre><code># install\npython3 -m venv ~/practicus/venv \n~/practicus/venv/bin/python -m pip install --upgrade practicus\n\n\n# run\nsource ~/practicus/venv/bin/activate\npracticus\n</code></pre>"},{"location":"setup-guide/#linux-quickstart","title":"Linux QuickStart","text":"<pre><code># install\npython3 -m venv ~/practicus/venv \n~/practicus/venv/bin/python -m pip install --upgrade practicus\n\n\n# run\nsource ~/practicus/venv/bin/activate\npracticus\n\n# Note: if you get qt.qpa.plugin error please run\nsudo apt-get install '^libxcb.*-dev' libx11-xcb-dev libglu1-mesa-dev libxrender-dev libxi-dev libxkbcommon-dev libxkbcommon-x11-dev\n</code></pre>"},{"location":"setup-guide/#starter-app","title":"Starter app","text":"<p>Instead of running Practicus AI from the command prompt (terminal) you can create shortcuts and run with double click. The below tips assume you installed using the QuickStart tips above.</p> <p>Windows: Navigate to ** %UserProfile%\\practicus\\venv\\Scripts\\ ** folder and locate practicus.exe, which is essentially a starter for practicus Python library. You can right-click and select pin to start. You can also create a shortcut to this .exe and change its name to Practicus AI and its icon by downloading our icon practicus.ico. </p> <p>macOS: You can download Practicus AI Starter app which is a tiny (100KB) app that starts the Python virtual env in ** ~/practicus/venv ** and then starts Practicus AI GUI from practicus Python library. To keep the app in dock please drag &amp; drop the .app file on the dock itself. Right-clicking and choosing \u201ckeep in dock\u201d will not create a shortcut.</p>"},{"location":"share-models/","title":"Sharing AI Models use case","text":"<p>Beyond data preparation, Practicus AI can also be used to export ML models to Excel, which can be used for different purposes. Below you can find some use cases to export your models to Excel. </p> <ul> <li> <p>Practicus AI exported models can help with ML deployment and testing and increase your chances of getting them to production to be used by masses.   </p> </li> <li> <p>The exported models have zero dependency, meaning they only use core Excel functionality and do not depend on 3rd party libraries, products, services, REST APIs etc. You can attach the exported ML model to an email, and the recipient would be able to predict / infer offline without any issues. </p> </li> <li> <p>You can use the exported Excel file to debug your models, since the model representation will be in a very simple form. </p> </li> <li> <p>Model Explainability can be a key blocker for getting ML models to production. Often times, data analysts, business analysts and other business leaders will not allow moving an ML model to production, simply because they do not understand how the model works. Practicus AI exported models in Excel will be significantly easier to consume and understand.</p> </li> </ul> <p>Basic model sharing use case</p> <p>1) Build your ML model as usual. </p> <pre><code># sample Support Vector Machine model\n...\nmy_svm_model.fit(X, Y)\n</code></pre> <p>2) Export to Excel </p> <pre><code>import practicus    \npracticus.export_model(my_svm_model, output_path=\"iris_svm.xlsx\",\n      columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>3) Open the file in Excel, Google Sheets, LibreOffice or others to make predictions, analyze how the model works and make changes as well.</p> <p></p> <p></p> <p></p> <p>4) (Optional) You can use pre-processing pipelines as well. Necessary calculations prior to model prediction will also be exported to Excel as pre-processing steps.   </p> <pre><code># create a pipeline with StandardScaler and LogisticRegression\nmy_pipeline = make_pipeline(\n   StandardScaler(),\n   LogisticRegression())\n\n# train\nmy_pipeline.fit(X, y)\n\n# Export the pre-processing and model calculation to Excel\npracticus.export_model(my_pipeline, output_path=\"model_with_pre_processing.xlsx\",\n                   columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>5) (Optional) You can also export models with Practicus AI using PMML. If you are using R, KNIME, Alteryx or any other ML platform, you can export your models to a .pmml file first (optionally including pre-processing steps as well) and then use the .pmml file with Practicus AI in Python to export it to Excel. The final Excel file will not have any dependencies to your ML platform. </p> <pre><code>practicus.export_model(\"my_model_developed_on_R.pmml\", output_path=\"R_model.xlsx\",\n                   columns=['petal length', 'petal width'], target_name=\"Species\")\n</code></pre>"},{"location":"trial-ent-saas/","title":"Enterprise License and limited SaaS Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your Enterprise license will unlock all advanced features.   </p>"},{"location":"trial-ent-saas/#activating-your-enterprise-license","title":"Activating your Enterprise License","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is not included in your trial. </li> <li>Please click here to start Practicus AI SaaS trial </li> </ul> <p>Enterprise Kubernetes</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p>"},{"location":"trial-ent-saas/#optional-using-your-limited-saas-account","title":"Optional - Using your limited SaaS account","text":"<p>Since we are allocating our online resources to production workloads, your SaaS account is limited to online features such as GPT, but not Cloud Workers. </p> <p>For features such as AutoML, please use Cloud Workers on local containers or AWS marketplace.</p>"},{"location":"trial-ent-saas/#sample-data-exploration-view-utilizing-cost-effective-options","title":"Sample Data Exploration view, utilizing cost-effective options","text":""},{"location":"trial-ent-saas/#using-gpt-and-other-online-only-features-with-your-limited-saas","title":"Using GPT and other \"online only\" features with your limited SaaS","text":"<p>When you use the App + local container or AWS marketplace option, the App intelligently decides when an \"online only\" feature such as GPT is required to use the SaaS. </p> <p>By combining local container or AWS marketplace with the limited SaaS, you will be able to trial all features with no limitations, and in a cost-effective way. </p>"},{"location":"trial-ent-saas/#unlimited-saas-option","title":"Unlimited SaaS option","text":"<p>If you prefer to use our SaaS unlimited, please click here to start Practicus AI SaaS trial </p>"},{"location":"trial-ent-saas/#logging-in-to-your-limited-saas-account","title":"Logging in to your limited SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-ent/","title":"Enterprise License Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your Enterprise license will unlock all advanced features.   </p>"},{"location":"trial-ent/#activating-your-enterprise-license","title":"Activating your Enterprise License","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is not included in your trial. </li> <li>Please click here to start Practicus AI SaaS trial </li> </ul> <p>Enterprise Kubernetes</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-saas-ent/","title":"SaaS and Enterprise License Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Both your SaaS account and the enterprise license will unlock all advanced features.   </p>"},{"location":"trial-saas-ent/#logging-in-to-your-saas-account","title":"Logging in to your SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p>"},{"location":"trial-saas-ent/#optional-activating-your-enterprise-license","title":"Optional - Activating your Enterprise License","text":"<p>If you prefer to use local containers on your laptop offline, or your personal AWS account (GPUs available) instead of Practicus AI SaaS, please activate your enterprise license.</p> <p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is included in your trial. </li> </ul> <p>Enterprise Cloud</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-saas/","title":"Software as a Service (SaaS) Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your SaaS account will unlock all advanced features.   </p>"},{"location":"trial-saas/#logging-in-to-your-saas-account","title":"Logging in to your SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Practicus AI offers two primary types of tutorials\u2014one designed for technical users who prefer to code and another suited for those who don\u2019t code, using AI Studio\u2019s more intuitive, no-code/low-code capabilities.</p>"},{"location":"tutorials/#technical-tutorial-if-you-code","title":"Technical Tutorial (if you code)","text":"<p>You can start by reading the technical tutorial right here in the documentation\u2014simply open the Technical Tutorial</p>"},{"location":"tutorials/#for-a-hands-on-coding-experience-recommended","title":"For a hands-on coding experience (recommended)","text":"<p>If you\u2019d like a hands-on experience, follow these steps to run the tutorial notebooks in your Practicus AI environment:</p> <ol> <li> <p>Create a Worker:    From Practicus AI Home, create a Worker.</p> </li> <li> <p>Open Jupyter Lab or VS Code:    Open Jupyter Lab or VS Code on the worker you just created.</p> </li> <li> <p>View the <code>samples/notebooks</code> Directory:    Here you\u2019ll find the tutorial notebooks that match exactly to what you read in the technical tutorial.</p> </li> <li> <p>View the SDK Docs:    Check out the SDK documentation for additional reference and guidance.</p> </li> </ol>"},{"location":"tutorials/#ai-studio-tutorial-if-you-do-not-code","title":"AI Studio Tutorial (if you do not code)","text":"<p>If coding isn\u2019t your focus, the AI Studio tutorial is ideal:</p> <ol> <li> <p>Access Practicus AI Studio or Workspaces:    From Practicus AI Home, open a browser-based Workspace or download AI Studio. </p> </li> <li> <p>Follow the tutorial steps    Open AI Studio tutorial and start following the tutorial steps.</p> </li> </ol>"},{"location":"tutorials/#operations-tutorial","title":"Operations Tutorial","text":"<p>If your primary focus is MLOps, DataOps, SecOps, or DevOps, we recommend that you first skim the Technical Tutorial to gain a solid understanding of the Practicus AI platform fundamentals. Once you\u2019re familiar with the basics, proceed to the Operations tutorial.</p> <p>Previous: Tutorials | Next: Choose one of the below.</p> <ul> <li>Technical Tutorial (if you code)</li> <li>AI Studio tutorial (if you do not code)</li> <li>Operations tutorial </li> </ul>"},{"location":"ai-studio-tutorial/Experiment-Service/","title":"Introduction to Advanced Model, Experiment, and Artifact Management in Practicus AI with MLFlow","text":"<p>The MLFlow integration of the Practicus AI platform provides a solution that significantly simplifies and optimizes the management process of your machine learning projects. </p> <p>This guide explains how to use model creation, management of experiments and other features MLFlow offers.</p>"},{"location":"ai-studio-tutorial/Experiment-Service/#access-to-mlflow-interface","title":"Access to MLFlow Interface:","text":"<p>Within Practicus AI, to access MLFlow:</p> <ul> <li>Open Explore tab</li> <li>Select the MLFlow service defined for you in the working region</li> <li>You can see the models and experiments saved in MLFlow here</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/Experiment-Service/#saving-a-created-model-in-mlflow","title":"Saving a created model in MLFLow","text":"<ul> <li>Click on Explore tab</li> <li>Make sure your Worker is already selected upper left</li> <li>Click Worker Files to load content </li> <li>Expand samples folder and click on ice_cream.csv</li> <li>Click Load </li> <li>Click on the Model button</li> <li>Click Advance</li> <li>Choose Log Exp. Service as MLFlow Primary</li> </ul> <ul> <li>Click OK</li> <li>When the model is created, a plugin will arrive and set the incoming plugin like this</li> </ul> <ul> <li>Click OK</li> </ul>"},{"location":"ai-studio-tutorial/Experiment-Service/#models-experiments-and-artifacts","title":"Models, Experiments and Artifacts","text":"<ul> <li>Open the opened MLFLow service in the browser from the tab above</li> <li>Find the session you created and open the session</li> <li>Here you can see the prt format file, the json containing the model's metadata and the pickle</li> <li>Click Parameters</li> </ul> <ul> <li>Back to Session</li> <li>Find the session you created and click on the '+' sign under table</li> </ul> <ul> <li>Select the first model under session here</li> </ul> <ul> <li>Click on Metrics and see the error metrics saved in MLFlow:</li> </ul> <ul> <li>Scroll to the bottom of the page and access the model artifacts</li> </ul> <p>Scroll to the bottom of the page and access the model artifacts</p> <p></p>"},{"location":"ai-studio-tutorial/Experiment-Service/#sending-an-experiment-from-notebook-to-mlflow","title":"Sending an Experiment from Notebook to MLFlow","text":"<ul> <li>Back to Notebook opened after the model</li> </ul> <ul> <li>Run step by step and create exp in step 3</li> </ul> <ul> <li>Update setup params and run the cell</li> </ul> <ul> <li>Now you can setup experiments</li> </ul> <ul> <li> <p>Run the rest of the steps</p> </li> <li> <p>Save setup </p> </li> </ul> <p></p> <ul> <li>Open MLFlow in the browser</li> </ul> <p></p> <ul> <li>Click on the new Experiment and open it</li> <li>See the changes here</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/Workflow/","title":"Introduction to Workflow Automation","text":"<p>The Workflow tab is a feature in the Practicus AI platform that allows users to automate their workflow. </p> <p>This feature offers a wide range of functionality from code generation to execution of steps on the desired engine.</p> <p>In the scenario we will consider, we will make simple preview steps in the interface and examine their transformation into workflows</p> <p>Hint: From this tab we will only cover the use of Workflow Automation. For other options, you can check the Airflow tab.</p>"},{"location":"ai-studio-tutorial/Workflow/#1-access-to-workflow-tab","title":"1. Access to Workflow Tab","text":"<ul> <li>Open Explore tab</li> <li>Select Cloud Worker Files and open the file below </li> <li>Explore &gt; samples &gt; select and open any csv file</li> <li>You will see 3 different options for Deploy.</li> </ul>"},{"location":"ai-studio-tutorial/Workflow/#2-workflow-creation","title":"2. Workflow Creation","text":"<ul> <li>Open Explore tab</li> <li>Select Cloud Worker Files and open the file below </li> <li>Explore &gt; samples &gt; insurance.csv</li> <li>Click filter and set charges &lt; Less than 40000 and add then click OK</li> <li>Then select sex and click one hot from the prepare tab.</li> <li>Then select sex and click delete from the prepare tab.</li> <li>Click Steps and see following steps:</li> </ul> <ul> <li>In a moment we will translate these steps into workflows</li> <li>Click Deploy and Select Workflow Automation</li> </ul> <p>Hint: This step allows the created workflow to be automated and run on the specified engine. The \"Deploy\" option turns your workflow into a practical application and allows it to run automatically with the schedule and settings you specify.</p> <p>You can set Deployment name, Schedule interval, Cloud Region, and Workflow Service</p> <ul> <li> <p>Click Advanced and select Data processing engine as SPARK </p> </li> <li> <p>Click OK</p> </li> </ul> <p></p>"},{"location":"ai-studio-tutorial/Workflow/#3-generated-codes","title":"3. Generated codes","text":"<p>After deploying the workflow, the generated scripts are opened. In fact, these codes generated by Practicus IA are located in the user's git repository. </p> <p></p> <p></p> <ul> <li>Here you can see that 2 .py files are generated. </li> <li>One of them contains the Airflow DAG and the other one contains our preprocess steps. </li> <li>After generating the code, users can play with the code and redeploy it.</li> </ul>"},{"location":"ai-studio-tutorial/Workflow/#4-airflow-ui-on-app-and-browser","title":"4. Airflow UI on App and Browser","text":"<ul> <li>Open Explore and choose Workflow Service</li> <li>See Airflow UI on app </li> <li>Then click your DAG </li> <li>Then click browser </li> <li>Then click your DAG and trigger</li> <li>Firstly you need to set configuration  </li> <li>Then trigger your DAG and see status your dag</li> </ul>"},{"location":"ai-studio-tutorial/chatgpt/","title":"Analyzing and preparing data with GPT","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"ai-studio-tutorial/chatgpt/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; insurance.csv</li> </ul>"},{"location":"ai-studio-tutorial/chatgpt/#using-gpt-to-generate-code","title":"Using GPT to generate code","text":"<p>To generate code, you should provide GPT with a prompt that describes the code you want to generate. The prompt should be written in English and include the desired functionality of the code snippet. ChatGPT will then generate a code snippet that matches the description.</p> <p></p> <ul> <li>When you select GPT, you will see the below dialog.</li> <li>Explain to the GPT what you want to do.</li> </ul> <p></p> <ul> <li>If you want to see the code generated by GPT, you can click Advanced. In addition, you can set other options.</li> </ul> <p></p> <ul> <li>Click Apply GPT and see the final dataset.</li> </ul> <p>Let's create a slightly more advanced query with GPT.</p> <ul> <li>Click Test GPT then click Advanced</li> </ul> <p></p> <ul> <li>Please make specific comments in advanced queries so that GPT generates the code properly.</li> </ul> <p></p> <ul> <li>Click Apply GPT and see the final dataset with new variables.</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/chatgpt/#optionalhow-can-i-generate-the-optimal-gpt-query","title":"(Optional)How can I generate the optimal GPT query?","text":"<p>Prompt engineering is the process of crafting effective input prompts to elicit the desired output from large language models (LLMs). LLMs are trained on massive datasets of text and code, and can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way. However, they need to be prompted correctly in order to produce the desired output.</p> <p>Here is a brief overview of the steps involved in prompt engineering:</p> <p>1) Identify the desired output.</p> <p>2) Craft a prompt. </p> <ul> <li>The prompt should be written in plain language that is easy to understand. It should also be as specific as possible, so that the LLMs knows what you want it to generate.</li> </ul> <p>3) Test the prompt.</p> <ul> <li>Once you have crafted a prompt, you need to test it to make sure that it works.</li> </ul> <p>4) Refine the prompt. </p> <ul> <li>If the LLMs does not generate the desired output, you need to refine the prompt. This may involve making the prompt more specific, or providing more examples.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/code/","title":"Extensibility with Python Coding","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/collaboration/","title":"Collaboration","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/data-analysis-intro/","title":"Introduction to Data Analysis","text":"<p>This section only requires Practicus AI app and can work offline.</p>"},{"location":"ai-studio-tutorial/data-analysis-intro/#working-with-local-files","title":"Working with local files","text":"<ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on Local Files</li> <li>Navigate to the samples directory and open ice_cream.csv : </li> <li>home &gt; practicus &gt; samples &gt; data &gt; ice_cream.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p>This simple dataset shows how much revenue an ice cream shop generates, based on the outside temperature (Celsius) </p>"},{"location":"ai-studio-tutorial/data-analysis-intro/#visualizing-data","title":"Visualizing Data","text":"<ul> <li>Click on Analyze and select Graph</li> </ul> <ul> <li>Select Temperature for X axis and Revenue for Y, click ok</li> </ul> <ul> <li>You will see a new tab opens up and a graph is plotted</li> </ul> <ul> <li>Move your mouse over the blue line, and you will see the coordinates changing at the top right</li> <li>Click on zoom, move your mouse to any spot, left-click and hold to draw a rectangle. You will zoom into that area</li> <li>Click on Pan, left-click and hold a spot and move around</li> </ul> <p>Now let's use a more exciting data set:</p> <ul> <li>Go back to the Explore tab and load the file below:</li> <li>home &gt; practicus &gt; samples &gt; data &gt; insurance.csv</li> </ul> <p></p> <p>This dataset is about the insurance charges of U.S. individuals based on demographics such as, age, sex, bmi .. </p> <ul> <li>Click on the charges column name to select it </li> <li>You see a mini-histogram on the upper right with basic quartile information</li> <li>Min, 25%, 50% (median), Avg (Average / mean), 75%, and Max</li> <li>Move your mouse over a distribution (shown as blue lines) on the histogram, and a small pop-up will show you the data range, how many samples are there, and the total percent of the samples in that distribution. </li> </ul> <p></p> <p>Now let's open a larger histogram for a closer look:</p> <ul> <li>Click on Analyze &gt; Graph</li> <li>Select Histogram for style </li> <li>Select charges, click ok</li> </ul> <p></p> <p>You will see the below histogram </p> <p></p>"},{"location":"ai-studio-tutorial/data-analysis-intro/#visualizing-outliers","title":"Visualizing Outliers","text":"<p>Now let's analyze to see the outliers in our data set.</p> <ul> <li>Click Analyze &gt; Graph</li> <li>Select boxplot style, charges and click ok</li> </ul> <p></p> <p>You will see the boxplot graph visualizing outliers. </p> <p></p> <p>The above tells us that some individuals pay significantly more insurance  charges compared to the rest. E.g. $60,000 which is more than 5x the median (50%). </p> <p>Please note: Since Q1 - 1.5 x IQR is -$10,768, overall sample minimum $1,121 is used as boxplot min. This is common in skewed data.</p> <p>Sometimes outliers are due to data errors, and we will see how to remove these in the next section. And sometimes we still remove them even if they are correct to improve AI model quality. We will also discuss this later.</p>"},{"location":"ai-studio-tutorial/data-analysis-intro/#group-by-to-summarize-data","title":"Group by to Summarize Data","text":"<p>Since our insurance data also has demographic information such as region, we can summarize (aggregate) based on how we wish to break down our data. </p> <ul> <li>Select Analyze &gt; Group By</li> </ul> <p></p> <ul> <li>Select region and then sex for the Group by section</li> <li>Select charges - Mean (Average), charges - Median (50%), charges - Std. Dev (Standard Deviation), age - Mean(Average), bmi - Variance for the summarize section</li> <li>Click ok </li> </ul> <p></p> <p>You will see the selected charges summaries for region and sex break-down. There is no limit, you can break down for as many columns as you need.</p> <p></p> <p>Now let's create a more advanced multi-layer graph:</p> <ul> <li>Select Analyze &gt; Graph</li> <li>Click on Advanced Options</li> <li>Select region for X, charges_mean for Y</li> <li>Click Add Layer</li> <li>Select region for X, charges_median for Y</li> <li>Click Add Layer again</li> <li>Click ok</li> </ul> <p></p> <p>You will see the mean and median for different U.S. regions. </p> <p></p> <p>Let's say we want to email this graph to someone:</p> <ul> <li>Click on Save</li> <li>Select a file name. e.g. insurance.png</li> </ul> <p></p> <p>You will get a graphics file saved on your computer.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/data-prep-intro/","title":"Introduction to Data Preparation","text":"<p>This section only requires Practicus AI app and can work offline.</p> <p>Let's start by loading boston.csv. We will ignore the meaning of this dataset since we will only use it to manipulate values.</p> <p></p> <p>Click on Prepare button to view some common data preparation actions</p> <p></p> <p></p>"},{"location":"ai-studio-tutorial/data-prep-intro/#sorting-data","title":"Sorting Data","text":"<ul> <li>Click on CRIM column</li> <li>Hit Ctrl (Cmd for macOS) + down arrow to sort ascending</li> <li>Hit Ctrl (Cmd for macOS) + up arrow to sort descending</li> </ul> <p>You can also open the advanced sort menu by clicking Prepare &gt; Sort</p>"},{"location":"ai-studio-tutorial/data-prep-intro/#filtering-data","title":"Filtering Data","text":"<p>There are several ways to filter data: </p> <ul> <li>Click on RM column to view the mini-histogram</li> <li>Click on the left most distribution to view filter menu</li> <li>Select Filter to keep &gt;= 4.605</li> </ul> <p></p> <ul> <li>Click on a cell value in INDUS column, e.g. 6.2</li> <li>Select Filter to keep &lt;</li> </ul> <p>This will remove all INDUS values greater than 6.2</p> <p></p>"},{"location":"ai-studio-tutorial/data-prep-intro/#updating-wrong-values","title":"Updating Wrong Values","text":"<ul> <li>Click on any cell with 12.5 in ZN column  </li> <li>Select Change Values</li> </ul> <ul> <li>Enter 100, click ok</li> </ul> <p>You will see that ALL cells with the value 12.5 in ZN column will be updated to 100</p> <p></p> <p>Please note that Practicus AI does not allow you to update the value of an individual cell only. All updates need to be rule-based. The reason for this is to be able to create production data pipelines. E.g. what you design can be used on fresh data every night, automatically. Individual cell updates do not work for this scenario.</p>"},{"location":"ai-studio-tutorial/data-prep-intro/#formulas","title":"Formulas","text":"<p>Practicus AI supports 200+ Excel compatible functions to write formulas. If you can use formulas in Excel, you can in Practicus AI.</p> <ul> <li>Click on the Formula button to open up the formula designer</li> </ul> <p></p> <ul> <li>Select ISODD function under Math section to find odd numbers  </li> <li>You will be asked to choose a column, select RAD column as Number </li> <li>Click Add</li> </ul> <p>You can use the designer to build the formula or type by hand. You can also create them in Excel and copy / paste. Unlike Excel, you do not use cell references e.g. A5, D8, but column names directly like the below:</p> <p></p> <ul> <li>Leave the column name as suggested: ISODD_RAD</li> <li>Click ok to run the formula </li> <li>You will see a new column named ISODD_RAD added to the dataset</li> <li>Click on ISODD_RAD column to select and hit Ctrl (Cmd in macOS) + left arrow key to move the column to left. Keep doing it until it is next to RAD column</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/data-prep-intro/#advanced-filter","title":"Advanced filter","text":"<ul> <li>Click on INDUS column name to select and then Prepare &gt; Filter</li> <li>Advanced filter designer will open and INDUS column already selected </li> <li>Select &lt;= Less Than and Equal as criteria </li> <li>Click on ... and choose 6.91 select ok </li> <li>Click on Add</li> <li>Now, select our newly created column ISODD_RAD instead of INDUS</li> <li>Leave Is True and click Add</li> </ul> <p>You will see a filter statement is getting created like the below. You can use brackets and keywords such as (, ), and, or, to build the filter you need.  </p> <p></p> <p>After applying your filter you will see the new data set.</p> <ul> <li>Click on INDUS column</li> <li>Hit Ctrl (or Cmd) + up arrow to sort descending</li> <li>Hit Ctrl (or Cmd) + right arrow to move the column next to RAD</li> </ul> <p>You can view the columns we are working on listed together, like the below </p> <p></p>"},{"location":"ai-studio-tutorial/data-prep-intro/#viewing-and-updating-steps","title":"Viewing and updating Steps","text":"<p>You can make mistakes while manipulating data, and it is possible to fix these without starting from scratch.</p> <ul> <li>Click on Steps button</li> </ul> <p></p> <p>You will view the current steps so far</p> <ul> <li>Select the filter step</li> <li>Hit Edit step button</li> <li>Hint: Practicus AI detects the types of variables and does type conversion automatically. You can see this in steps 2 and 3.</li> </ul> <p></p> <ul> <li>Change the filter value from 6.91 to 5</li> <li>Click ok</li> </ul> <p></p> <p>You will see the updated step in green</p> <ul> <li>Click ok to make the change</li> </ul> <p></p> <p>You will see that INDUS column is now less than 5</p> <p></p> <p>Please note that updating steps will reset the column order, such as moving columns left / right or hiding them.</p> <p>Instead of opening the Steps dialog you can quickly undo / redo as well:</p> <ul> <li>Hit Ctrl (or Cmd) + Z to undo a step. </li> <li>Do it a few more times, you will see data is updated automatically </li> <li>Now, hit Ctrl (or Cmd) + Y few times to redo the steps you undid</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/data-prep/","title":"Data Preparation","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/data-profiling/","title":"Data Profiling","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Profiling your data is an extremely powerful way to get a good understanding of your data distribution and correlations.</p> <ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; insurance.csv</li> <li>Select Analyze &gt; Profile </li> </ul> <p></p> <p>After a few minutes, you will see the data profile completed.</p> <p>You will see several statistical calculations about this data. </p> <p></p> <p>When you click Alerts, you will see various alerts, such as high correlation.</p> <p></p> <p>Click Variables on the up-right tab, and Select Variables as Age</p> <p></p> <p>When you click Correlations on the up-right tab, you will see a correlation heatmap.</p> <ul> <li>If you want to see the correlation matrix, you can click Table.</li> </ul> <p></p> <p>In this example, all correlations are positive, so they are displayed with blue. Negative correlations are displayed in red. E.g. If we had a column about how fast someone can run, age column would probably have a negative correlation. I.e. if someone is younger, they would run faster.</p>"},{"location":"ai-studio-tutorial/data-profiling/#profiling-for-difference","title":"Profiling for difference","text":"<p>You can make changes to your data and then create a profile to compare with the original.</p> <ul> <li>Go back to the insurance dataset </li> <li>Under smoker column click on a cell that has the no value </li> <li>Select Filter to keep cell value</li> <li>Select Analyze &gt; Profile again</li> <li>You will be asked if you would like to compare with the original data set, select yes</li> </ul> <p></p> <p>You will now see all the statistics for the original and current data and can compare them side-by-side. </p> <p></p> <p>Click Variables on the up-right tab, and Select Variables as Age</p> <ul> <li>See the descriptive statistics of the comparison of the old version of the age variable with the new version. </li> <li>In addition, you can click More Details to see statistics, histograms, etc.</li> <li></li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/deploy/","title":"Deployment to Production","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/develop_ml_models/","title":"Develop Machine Learning Models with Jupyter Notebooks","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>This section will provide information on how technical and non-technical users can easily intervene in the code and improve the machine learning models they build.</p> <p>You have set up the model and everything is fine. You can either complete the model at this point and save it, or you can easily intervene in the generated Jupyter Notebook code to improve the model. </p> <ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; titanic.csv</li> <li>Select Model &gt; Predictive Objective </li> <li>Choose Objective Column as Survived and Technique should be Classifications</li> <li>Click OK</li> <li>After the model build is completed you will see a dialog</li> <li>Select Open Jupyter Notebook to experiment further</li> </ul>"},{"location":"ai-studio-tutorial/develop_ml_models/#scale-and-transform","title":"Scale and Transform","text":"<p>In the Scale and Transform section, you can rescale the values of numeric columns in the dataset without distorting differences in the ranges of values or losing information.</p> <p></p> <p>Parameters for Normalization in shortly:</p> <ul> <li>You must first set normalize to True</li> <li>You can choose z-score, minmax, maxabs, or robust methods for normalize</li> </ul> <p>Parameters for Feature Transform in shortly: </p> <ul> <li>You must first set normalize to True</li> <li>You can choose yeo-johnson or quantile methods.</li> <li>For the Target Transform you can choose yeo-johnson or quantile methods</li> </ul> <p></p> <p>If you want to have deeper knowledge for Scale and Transform, you can review this link</p>"},{"location":"ai-studio-tutorial/develop_ml_models/#feature-engineering","title":"Feature Engineering","text":"<p>In the Feature Engineering section, you can automatically try new variables and have them improve the accuracy of the model. You can also apply rare encoding to data with low frequency</p> <p></p> <p>Parameters for Polynomial Features in shortly:</p> <ul> <li>You must first set polynomial_features to True</li> <li>polynomial_degree should be int</li> </ul> <p>Parameters for Bin Numeric Features in shortly:</p> <ul> <li>bin_numeric_features should be list</li> </ul> <p>Parameters for Combine Rare Levels in shortly:</p> <ul> <li>rare_to_value: float, default=None</li> <li>rare_value: default='rare'</li> </ul> <p></p> <p>If you want to have deeper knowledge for Feature Engineering, you can review this link</p>"},{"location":"ai-studio-tutorial/develop_ml_models/#feature-selection","title":"Feature Selection","text":"<p>In the Feature Selection section, you can set which variables to include in the model and exclude some variables based on the relationships between the variables.</p> <p></p> <p>Parameters for Remove Multicollinearity in shortly:</p> <ul> <li>You must first set remove_multicollinearity to True</li> <li>multicollinearity_threshold should be float</li> </ul> <p>Parameters for Principal Component Analysis in shortly:</p> <ul> <li>You must first set pca to True</li> <li>pca_method should be linear, kernel, or incremental</li> <li>pca_components should be None, int, float, mle</li> <li>Hint: Minka\u2019s MLE is used to guess the dimension (ony for pca_method='linear')</li> </ul> <p>Parameters for Ignore Low Variance in shortly:</p> <ul> <li>low_variance_threshold should be float or None.</li> </ul> <p>If you want to have deeper knowledge for Feature Selection, you can review this link</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/end-advanced/","title":"End of Advanced Topics","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/end-basic/","title":"End of Basic Tutorial","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/engines/","title":"Data Processing Engines","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/excel-prep/","title":"Excel Prep","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/explore/","title":"Exploring Cloud Data Sources","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Once you have a Practicus AI Cloud Worker running and ready, you can access and explore different data sources on the cloud. </p>"},{"location":"ai-studio-tutorial/explore/#cloud-worker-files","title":"Cloud Worker Files","text":"<p>Every Cloud Worker has some local storage that you can use to quickly upload/download to use cloud-only features. </p> <ul> <li>Click on Explore</li> <li>Make sure your Cloud Worker is already selected upper left</li> <li>Click Cloud Worker Files to load content </li> <li>Expand samples folder and click on boston.csv</li> <li>Click Load </li> </ul> <p></p> <p>Since this data is loaded using a Cloud Worker, a copy of it is already on the Cloud Worker. </p> <p>Congrats, now you can use all the Practicus AI features, including advanced AI ones such as AutoML.</p> <p></p> <p>For curious minds: When you make changes to your data using the app, you will see the results instantly on the app, and in parallel the changes are applied on the cloud. This avoids wait time.</p>"},{"location":"ai-studio-tutorial/explore/#uploading-files","title":"Uploading Files","text":"<p>There are multiple ways to quickly upload your data to the cloud, so you can take advantage of cloud-only Practicus AI features or use external services such as Jupyter. </p> <ul> <li>Click on Explore </li> <li>Select Cloud Worker Files</li> <li>Click on New Folder</li> <li>Enter a name, such as my_data</li> <li>Select your newly created folder, my_data </li> <li>Click on Upload button</li> </ul> <p></p> <ul> <li>Select one of our local data samples</li> <li>Home &gt; practicus &gt; samples &gt; data &gt; boston.csv</li> <li>A file Transfer tab will open, click Start Transfer</li> <li>Close after done </li> <li>Go back to Explore tab, click Reload </li> </ul> <p>You will see that your data is now uploaded to the local disk of the Cloud Worker</p> <p></p> <p>Tips:</p> <ul> <li>You can upload / download entire directories </li> <li>Your data is compressed during upload / download, saving on bandwidth and time</li> <li>You can also use Copy / Paste between data sources to quickly move data around</li> </ul>"},{"location":"ai-studio-tutorial/explore/#quick-upload","title":"Quick Upload","text":"<p>Sometimes you will not know in advance if you are going to use a cloud-only feature or not, and start working offline. </p> <p>Practicus AI offers a quick way for you to start offline and continue in the cloud later:</p> <ul> <li>Open Explore tab</li> <li>Click on Local Files and load the below file </li> <li>Home &gt; practicus &gt; samples &gt; data &gt; boston.csv </li> <li>Select CRIM column, hit delete or backspace to delete the column</li> <li>Click on Model button (AutoML) </li> </ul> <p>You will see a note telling this is a cloud-only feature and asks if you would like to upload your data to the cloud.</p> <ul> <li>Select Yes</li> </ul> <p></p> <ul> <li>Select a Cloud Worker, such as the one you created in the previous section</li> <li>Click Ok</li> </ul> <p></p> <p>Your data will be uploaded AND all your steps (such as deleting CRIM column) will be applied. </p> <ul> <li>Click Model button again to verify it now works</li> <li>Click cancel, we will visit AutoML later</li> </ul>"},{"location":"ai-studio-tutorial/explore/#working-with-s3","title":"Working with S3","text":"<p>S3 is a very common location for Big Data and AI/ML workloads. Your AWS account might already have access to some S3 buckets (locations)  </p> <ul> <li>Open Explore tab</li> <li>Click on S3</li> <li>Select Load Buckets under Bucket</li> <li>In as moment, you will see all the S3 buckets you have access to under Bucket</li> </ul> <p></p> <p>Notes: </p> <ul> <li>If you need to use a bucket under another AWS account, you can select I will enter my credentials under Cloud Config to manually enter security credentials</li> <li>If you would like to learn how to create a bucket please visit the AWS guide</li> <li>If you would like to experiment using public S3 buckets, please check this registry</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/gpu/","title":"GPU Acceleration","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/model/","title":"Modeling with AutoML","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <ul> <li>You can use Practicus AI for both supervised and unsupervised learning.</li> <li>Hint: In supervised learning, your dataset is labeled, and you know what you want to predict.  In unsupervised learning, your dataset is unlabeled, and you don't know what to do. But with Practicus AI, you can switch from unsupervised to supervised learning.</li> </ul>"},{"location":"ai-studio-tutorial/model/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; insurance.csv and Load </li> <li>Click Model button</li> </ul> <ul> <li>View the below optional sections, and then click ok to build the model</li> </ul>"},{"location":"ai-studio-tutorial/model/#optional-building-excel-model","title":"(Optional) Building Excel Model","text":"<p>By default, models you build can be consumed using Practicus AI app or any other AI system. If you would like to build an Excel model, please do the following. Please note that this feature is not available for the free cloud tier.</p> <ul> <li>In the Model dialog click on Advanced Options</li> <li>Select Build Excel model and enter for how many rows, such as 1500</li> </ul>"},{"location":"ai-studio-tutorial/model/#optional-explaining-models","title":"(Optional) Explaining Models","text":"<p>If you would like to build graphics that explain how your model works, please do the following. Please note that the free tier cloud capacity can take a long time to build these visualizations.  </p> <ul> <li>In the Model dialog click on Advanced Options</li> <li> <p>Select Explain</p> </li> <li> <p>Click ok to start building the model</p> </li> <li> <p>Hint: The Select Top % features by importance setting only includes the most important variables in the model. If two variables are highly correlated, then the model can already predict the target variable with one variable, so the other variable is not included.</p> </li> </ul> <p>If you choose the optional sections, model dialog should look like the below:</p> <p></p> <p>You should see a progress bar at the bottom building the model. </p> <p></p> <p>For a fresh Cloud Worker with regular size (2 CPUs) the first time you build this model it should take 5-10 minutes to be completed. Subsequent model runs will take less time. Larger Cloud Workers with more capacity build models faster and more accurately.  </p> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select all the options</li> <li>Click ok</li> </ul> <p></p> <p>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</p> <p></p> <p>If you requested to build an Excel model, you will be asked if you want to download. </p> <p></p> <p>We will make predictions in the next section using these models, or models that other built. </p>"},{"location":"ai-studio-tutorial/model/#optional-reviewing-model-experiment-details","title":"(Optional) Reviewing Model Experiment Details","text":"<p>If you chose to explain how the model works: </p> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Navigate to relevant graphics, for instance Feature Importance</li> </ul> <p></p> <p>The above tells us that an individual not being a smoker (smoker = 0), their bmi, and age are the most defining features to predict the insurance charge they will pay.</p> <p>Note: You can always come back to this screen later by opening Cloud Workers tab, clicking on MLflow button and finding the experiment you are interested with. </p>"},{"location":"ai-studio-tutorial/model/#optional-downloading-model-experiment-files","title":"(Optional) Downloading model experiment files","text":"<p>You can always download model related files, including Excel models, Python binary models, Jupyter notebooks, model build detailed logs, and other artifacts by going back to Explore tab and visiting Home &gt; models</p> <p>You can then select the model experiment you are interested, and click download  </p> <p></p>"},{"location":"ai-studio-tutorial/model/#optional-saving-models-to-a-central-database","title":"(Optional) Saving Models to a central database","text":"<p>In the above steps, the model we built are stored on a Cloud Worker and will disappear if we delete the Cloud Worker without downloading the model first. This is usually ok for an ad-hoc experimentation. A better alternative can be to configure a central MLflow database, so your models are visible to others, and vice versa, you will be easily find theirs. We will visit this topic later.    </p>"},{"location":"ai-studio-tutorial/model/#modelling-with-time-series","title":"Modelling with Time Series","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; airline.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select Explain </li> </ul> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select Predict on Current data and entry Forecast Horizon. It sets how many periods ahead you want to forecast in the forecast horizon.</li> <li>Click ok</li> </ul> <p></p> <p>Select Visualize after predicting and see below graph.</p> <ul> <li>The orange color in the graph is the 12-month forecast result.</li> </ul> <p></p> <ul> <li>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/model/#modelling-with-anomaly-detection","title":"Modelling with Anomaly Detection","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; unusual.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select all the options</li> <li>Click ok</li> </ul> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select all the options</li> </ul> <p></p> <ul> <li>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</li> </ul> <p></p> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Click t-SNE(3d) Dimension Plot</li> </ul> <p>You can hover over this 3D visualization and analyze the anomaly results.</p> <p></p>"},{"location":"ai-studio-tutorial/model/#modelling-with-segmentationclustering","title":"Modelling with Segmentation(Clustering)","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; customer.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select all the options</li> <li>Click ok</li> </ul> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Click Elbow Plot</li> <li>See the optimal number of clusters</li> <li>Hint: The elbow method is a heuristic used to determine the optimum number of clusters in a dataset. </li> </ul> <ul> <li>Click Distribution Plot</li> <li>You can hover over this Bar chart</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/next-steps/","title":"Next Steps","text":"<p>Congrats on completing Practicus AI Studio tutorial!</p> <p>There are still many topics that we haven't covered in our tutorial, and we will be adding sections about these in the near future. So please be on the lookout.</p> <p>Please feel free to experiment with these features yourself, since many of them are very intuitive to use.</p> <p>We have several videos where you can learn about Practicus AI. Please view them on the Demo Videos section.</p> <p>Some topics we haven't covered are:</p>"},{"location":"ai-studio-tutorial/next-steps/#advanced-data-preparation","title":"Advanced Data Preparation","text":"<ul> <li>Joins: You can join any data, from different data sources. E.g. You can join data on S3 to data on a relational database. Please view the join help page in our documentation to learn more. </li> <li>Handling missing values</li> <li>Machine learning related tasks</li> </ul>"},{"location":"ai-studio-tutorial/next-steps/#excel-prep","title":"Excel Prep","text":"<p>If you work with someone who doesn't use Practicus AI, you can share any data over Excel / Google Sheets with them, so they can make changes and send back to you. You can then detect their changes and apply as steps to any data.</p> <p>Please view Excel prep help page to learn more</p>"},{"location":"ai-studio-tutorial/next-steps/#python-coding","title":"Python Coding","text":"<p>You can use the built-in Python editor in Practicus AI app to extend for any functionality. This feature doesn't require the cloud and the app comes with all the relevant libraries pre-installed such as pandas, and requests for API calls.</p>"},{"location":"ai-studio-tutorial/next-steps/#deployment-to-production","title":"Deployment to production","text":"<p>Once you finish preparing your data, building models, making predictions, and other tasks, you can automate what you have designed, so it can run automatically on a defined schedule such as every night or every week.</p> <p>Please view the following pages to learn more:</p> <ul> <li>Code Export (Deploy)</li> <li>Airflow Integration</li> <li>Modern Data Pipelines</li> </ul>"},{"location":"ai-studio-tutorial/next-steps/#observability","title":"Observability","text":"<p>Please view logging help page to learn more.</p>"},{"location":"ai-studio-tutorial/next-steps/#gpu-acceleration","title":"GPU Acceleration","text":"<p>You can choose to use GPU powered Cloud Workers by simply selecting Accelerated Computing family while launching a Cloud Worker. </p>"},{"location":"ai-studio-tutorial/next-steps/#data-processing-engines","title":"Data Processing Engines","text":"<p>If you are a technical user, you can choose an engine of your choice, including pandas, DASK, RAPIDS, RAPIDS+DASK and SPARK. Simply click the Advanced button before loading data. </p>"},{"location":"ai-studio-tutorial/next-steps/#advanced-sampling","title":"Advanced Sampling","text":"<p>If you work with very large data, you can load a different sample size on the Cloud Worker. E.g. original data source can be 1 billion rows, you can read 200 million on the cloud and 1 million on the app. Simply click the Advanced button before loading data.      </p> <p>This concludes our tutorial. Thank you for reading!</p> <p>&lt; Previous</p>"},{"location":"ai-studio-tutorial/observability/","title":"Observability","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/plot/","title":"Introduction to Data Visualization","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Plotting datasets visually aids in data exploration, revealing patterns, and relationships. Thus, it is very important for decision-making, storytelling and insight generation. For that Practicus AI give you Plot service which plots the data in worker as well as in app.</p> <p>Let's have a look of Plot basics by loading salary.csv.  We will ignore the meaning of this dataset since we will only use it to explain basics of plot.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open salary.csv :</li> <li>samples &gt; salary.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>After loading the data set, click on Plot button to start ploting service.</p> <p></p>"},{"location":"ai-studio-tutorial/plot/#basics-of-plot","title":"Basics of Plot","text":"<p>The first thing we will see at Plot tab is going to be Data Source, from this menu we can select the data sheet which we want to visualize.</p> <ul> <li>Click Data Source drop down menu</li> <li>select salary</li> </ul> <p>After choosing the data sheet then we could chose graphic from Graphic drop down menu which we want use while working on visualizing.</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> </ul> <p></p> <p>After choosing the graphic style we want to work with we will see the options listed down below:</p> <ul> <li>Sampling: This option refers to a subset of data set selected from a larger dataset to represent its characteristics. Smaller samples in large data sets can be plotted more quickly, enhancing the efficiency of exploratory data analysis.</li> <li>X Coordinate: This option refers to the horizontal axis of the plot, representing the column(s) of the data set. Within Bar and H Bar graphic styles axis could get string columns as well as numerical columns.</li> <li>Y Cooridnate: This option refers to the vertical axis of the plot, also representing column(s).</li> <li>Color: This option refers to color which will be the filling of shapes within selected graphic style.</li> <li>Size: This option refers to size of the shapes within selected graphic style,  with the exception of the Bar and H Bar graphic styles, where size refers to the spacing between bars.</li> </ul> <p>Let's have a quick look to these options with a simple examle.</p> <ul> <li>Click to  X Coordinate drop down menu and select YearsExperience</li> <li>Click to  Y Coordinate drop down menu and select Salary</li> <li>Click to Add Layer</li> </ul> <p></p> <p>In the end we will have the plot down below:</p> <p></p>"},{"location":"ai-studio-tutorial/plot/#advanced-techniques-in-plot","title":"Advanced Techniques in Plot","text":"<p>In this section we will try to have look to more advance techniques we can use in Plot such as adding multiple layer of visualizing, dynamic size and color, transparency, tooltip and usage of Geo Map graphic style.</p>"},{"location":"ai-studio-tutorial/plot/#dynamic-size-color","title":"Dynamic Size &amp; Color","text":"<p>One of the most illustrative datasets for demonstrating dynamic size and color options is the Titanic dataset. Let's load it into one of our worker environments.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open titanic.csv :</li> <li>samples &gt; titanic.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>The Titanic dataset is a popular dataset used in machine learning and data analysis. It contains information about passengers aboard the RMS Titanic, including whether they survived or not. Within this data set we will use Circle graphic style from Plot and columns of pclass, fare, age and survived. Let's describe what these columns means for better understanding.</p> <ul> <li>Pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).</li> <li>Fare: Passenger fare.</li> <li>Age: Passenger's age in years.</li> <li>Survived: Indicates whether the passenger survived or not (0 = No, 1 = Yes).</li> </ul> <p>Let's start our plotting journey,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select titanic</li> <li>Click Graphic drop down menu</li> <li>Select Circle</li> <li>Click to Advanced</li> </ul> <p></p> <p>After open up advanced section you will see the options of Dynamic Size and Dynamic Color. Dynamic size and color in a circle plot refer to adjusting the size and color of circles based on additional variables, beyond the x and y coordinates. Let's have look with the example of titanic data set.</p> <ul> <li>Click to  X Coordinate drop down menu and select age</li> <li>Click to  Y Coordinate drop down menu and select fare</li> <li>Click to Dynamic size drop down menu and select pclass</li> <li>Click to Dynamic color drop down menu and select survive</li> <li>Click to Add Layer</li> </ul> <p>The plot down below should be showed up:</p> <p></p> <p>Hence, we can deduce from the analysis that passengers with smaller data points (indicating lower values of \"Pclass\") paid higher fares and had a better chance of survival. Moreover, it's evident that passengers with lower ages (on the X-axis) had a higher likelihood of surviving.</p>"},{"location":"ai-studio-tutorial/plot/#analyze-over-multiple-layer","title":"Analyze over multiple layer","text":"<p>One of the most illustrative datasets for demonstrating multiple layer anlyze is the Iris dataset. Let's load it into one of our worker environments.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open iris.csv :</li> <li>samples &gt; iris.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>The Iris dataset is a popular dataset in machine learning and statistics, often used for classification tasks. It consists of 150 samples of iris flowers, each belonging to one of three species: Setosa, Versicolor, or Virginica. Within this data set we will use both Bar and Circle graphic style from Plot. The dataset comprises four features, each representing measurements of the length and width of both the petals and sepals of flowers.</p> <p>Before start let's use label encode and group by on the species and for better visualisation:</p> <ul> <li>Click Snippets</li> <li>Click Advanced</li> <li>Locate and select Label encoder</li> <li>Select species from Text columns drop down menu</li> <li>Click +</li> <li>Click OK</li> </ul> <p></p> <ul> <li>Click Prepare</li> <li>Click Group By</li> <li>Select species from Group by drop down menu</li> <li>Select sepal_length and Mean (Average) from Summarize drop down menus</li> <li>Select sepal_windth and Mean (Average) from Summarize drop down menus</li> <li>Select petal_length and Mean (Average) from Summarize drop down menus</li> <li>Select petal_windth and Mean (Average) from Summarize drop down menus</li> <li>Click OK</li> </ul> <p></p> <p>In the end we should have the table down below:</p> <p></p> <p>Let's start plotting for multiple layer analyze,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select iris</li> </ul> <p>For first layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Bars</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select sepal_length_mean</li> <li>Click to Advanced</li> <li>Select greenish color from Color drop down menu</li> <li>Enter a value of 50 for the Transparency % input</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For second layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select sepal_windth_mean</li> <li>Click to Advanced</li> <li>Select a darker greenish color from Color drop down menu</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For third layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Bars</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select petal_length_mean</li> <li>Click to Advanced</li> <li>Select blueish color from Color drop down menu</li> <li>\u00canter a value of 50 for the Transparency % input</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For fourth layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select petal_windth_mean</li> <li>Click to Advanced</li> <li>Select a darker blueish color from Color drop down menu</li> <li>Click to Add Layer</li> </ul> <p></p> <p>In the end we sould have a plot like down below:</p> <p></p> <p>As we hover over the bars and lines, data point values will be displayed. Additionally, on the right side of plot, there are options available for zooming in, zooming out, and saving the plot.</p> <p>Observing this multi-layer graph, it becomes evident that both sepal length and petal length play a crucial role in distinguishing between classes. Similarly, the same differentiation can be observed for petal width.</p>"},{"location":"ai-studio-tutorial/plot/#geo-map-tutorial","title":"Geo-map Tutorial","text":"<p>To use the Geo-map feature of Plot, the initial requirement is to define the Google Maps API either through the admin console or within the application itself. If you don't know how to retrieve a Google Maps API key you can check Google's documentetion.</p> <p>Defining a Google Maps API over admin console:</p> <ul> <li>Open Admin Console of Practicus AI</li> <li>Expand (Click) Definitions from left menu</li> <li>Click Cluster Definitions</li> <li>Click GOOGLE_MAPS_API_KEY from table</li> </ul> <p></p> <ul> <li>Enter your key to Value input</li> <li>(Optional) Enter a description to Description input</li> <li>Click Save</li> </ul> <p></p> <p>Defining a Google Maps API within application:</p> <ul> <li>Click Settings frop top menu</li> <li>Click Other tab from opened window</li> <li>Enter your Google Maps API to Personal API Key at down below</li> <li>Click Save</li> </ul> <p></p> <p>After assigning the Google Map API we could have a look to Geo-Map by using car_insurance dataset. This dataset contains information about the insurance company's past customers who have purchased health insurance.  The objective is to use this dataset to train a predictive model that can determine whether these past customers would also be interested in purchasing vehicle insurance from the same company.</p> <p>The features can be listed as:</p> <p>id: A unique identifier assigned to each customer. Gender: The gender of the customer. Age: The age of the customer. Driving_License: Indicates whether the customer possesses a driving license. Region_Code: A distinct code assigned to denote the region of the customer. Previously_Insured: Indicates whether the customer already holds vehicle insurance. Vehicle_Age: The age of the vehicle. Vehicle_Damage: Indicates whether the customer's vehicle has been damaged in the past. Annual_Premium: The yearly premium amount that the customer is required to pay. Policy_Sales_Channel: An anonymized code representing the outreach channel used to contact the customer, including different agents, mail, phone, and in-person visits. Vintage: The duration, in days, for which the customer has been associated with the company. Response: Indicates customer interest. 1 indicates interest, while 0 signifies no interest.</p> <p>Let's load the dataset to our worker:</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open airports.csv :</li> <li>samples &gt; car_insurance.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>Before start let's group the data on the Regeion_Code column for better visualisation:</p> <ul> <li>Click Prepare</li> <li>Click Group By</li> <li>Select Regeion_Code from Group by drop down menu</li> <li>Select Response and sum from Summarize drop down menus</li> <li>Select Previously_Insured and sum from Summarize drop down menus</li> <li>Select Lat and max from Summarize drop down menus</li> <li>Select Lon and max from Summarize drop down menus</li> <li>Click OK</li> </ul> <p></p> <p>Let's start our plotting journey,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select car_insurance</li> <li>Click Graphic drop down menu</li> <li>Select Geo Map</li> </ul> <p>After selecting the \"Geo Map\" graphic style, we observe four distinct options that set it apart from other graphic styles:</p> <ul> <li>Latitude: Indicates distance north or south of the Equator.</li> <li>Longitude: Specifies distance east or west of the Prime Meridian.</li> <li>Map Type: Indicates Google Maps styles.</li> <li>Zoom: Provides an approximation of the number of miles/kilometers that fit into the area represented by the plot.</li> </ul> <p>Let's try to visualize the relation between Response and Previously_Insured on Google Maps by plotting data from the car_insurance dataset:</p> <ul> <li>Select Lon_max from Longitude drop down menu</li> <li>Select Lat_max from Latitude drop down menu</li> <li>Enter 1500 to Zoom input</li> <li>Click Advanced</li> <li>Select Response_sum from Dynamic Size drop down menu</li> <li>Select Previously_Insured_sum from Dynamic Color drop down menu</li> <li>Click Add Layer</li> </ul> <p></p> <p>Let's say we want to email this plot to someone:</p> <ul> <li>Click on Save from menu at right side</li> <li>Select a file name. e.g. us_flight.png</li> </ul> <p></p> <p>You will get a graphic file saved on your computer.</p>"},{"location":"ai-studio-tutorial/predict/","title":"Making Predictions","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"ai-studio-tutorial/predict/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; insurance.csv and Load </li> <li>Click Predict button</li> </ul>"},{"location":"ai-studio-tutorial/predict/#ai-model-locations","title":"AI Model locations","text":"<p>The AI model you or some other person built can be found on a variety of locations or databases. </p> <p>Practicus AI currently support predicting models in the below locations:</p> <ul> <li>MLflow model database</li> <li>Cloud Worker</li> <li>Your computer</li> <li>S3</li> <li>An API endpoint (for programmers)</li> </ul> <p>In this tutorial we will show MLflow usage. </p>"},{"location":"ai-studio-tutorial/predict/#predicting-using-mlflow-database","title":"Predicting using MLflow database","text":"<ul> <li>To locate the model we just built click on the ... to search for it</li> </ul> <ul> <li> <p>In the search dialog that opens, type insurance and hit enter</p> </li> <li> <p>You will find the AI model you built in the previous section of this tutorial.</p> </li> </ul> <p>Cannot find model? It is probably because you are not using a central MLflow database yet, and you built the AI model using a different Cloud Worker. Please check the using a central MLflow database section below to learn more. For now, please read insurance.csv using the same Cloud Worker that you built the AI model with.  </p> <p></p> <ul> <li>Select the model and click ok</li> </ul> <p>You will see the model location (URI) on screen. Although you can click ok to make the  predictions now, we strongly suggest you load the model details (metadata) first. </p> <ul> <li>Click Load to load model details</li> </ul> <p></p> <ul> <li>If you prefer, make the dialog larger</li> <li>Rename predicted column name to charges predicted since we already have a column named charges. If you don't, predicted values will overwrite the original ones.</li> <li>Confirm the features (columns) requested by the model match the ones that we provide</li> </ul> <p>Note: Sometimes the feature names will not match. E.g. AI model could need a feature named children, and our dataset could have a feature named child. Practicus AI uses a simple AI algorithm to match the features names, but it is possible there is a mistake. It is always a good practice to confirm the matching is correct. </p> <ul> <li>Click ok to predict </li> </ul> <p></p> <p>Your predictions will be made.</p> <p></p>"},{"location":"ai-studio-tutorial/predict/#analyzing-model-error","title":"Analyzing model error","text":"<p>Your AI model will almost always have a margin of error. We can estimate the error we expect this model will make. </p> <ul> <li>Click on Formula button </li> <li>Add the below formula, and name the new column Model Error</li> </ul> <p><pre><code>ABS(col[charges] - col[charges predicted])\n</code></pre> We used ABS() function to calculate absolute value E.g. |-1| = 1 so error is always a positive value. </p> <ul> <li>Click ok </li> </ul> <p></p> <p>You will see the errors our model makes. </p> <ul> <li>Click on Model Error column</li> <li>Move your mouse over the mini-histogram</li> </ul> <p>You will see that in ~75% of the cases, we expect that our model will make an error less than ~$2200</p> <p></p> <ul> <li>Move your mouse to the right on the mini-histogram to see in what % of the cases our model makes not so good predictions</li> <li>Click on one of the bad prediction bars on the mini-histogram, and select Filter to Keep &gt;= ...</li> </ul> <p></p> <p>You will see all the cases where our model did not do a good job.</p> <ul> <li>Select Model Errors column </li> <li>Hit Ctrl (or Cmd) + Up Arrow to sort descending</li> </ul> <p>You will now see the worst predictions located at the top. When you select a column, bottom right hand of the screen shows some basic statistics in addition to the mini-histogram. You will see that in 127 cases our model had some room for improvement.</p> <p></p> <p>This might be a good time to analyze the individuals of the not so good predictions, and see if there is anything we can do to get our data in a better shape. Often, better data will lead to better models.</p> <p>You can use the same analytics tasks to visualize and profile this data to see if there is a pattern you can detect. In some cases, your data will have errors, and fixing these will improve the model. In some other cases, you will find out that you are simply missing key features. E.g. for the insurance example, existing health problems could very well be a key indicator for the charges, and we miss this data. </p> <p>Sadly, there is no standard way to improve AI models. Sometimes a technical machine learning coder can greatly improve a model by tweaking parameters. And in some other cases a domain experts' professional experience to optimize the data will be the answer. For some tough problems, you will most likely need both of these personas do their magic, and collaborate effectively. </p>"},{"location":"ai-studio-tutorial/predict/#optional-understanding-ground-truth","title":"(Optional) Understanding Ground Truth","text":"<p>In most real life scenarios, you will make predictions for unknown situations based on what we already know, the ground truth. </p> <p>Then, \"life will happen\", and you will observe the real outcome. </p> <p>For instance, you can make predictions on which customer you \"might\" lose. This is called customer churn prediction AI use case. 6 months later, you will see that (unfortunately) you really lost some customers. And you will have more up-to-date ground truth. </p> <p>With Practicus AI, you can make regular predictions, for instance weekly or nightly, by deploying your models to production. We will discuss how to do this later. </p> <p>Practicus AI allows you to store your predictions in a database, so you can compare them later with the ground truth life brings, This will give you the real errors your models make, in comparison to the estimated errors we discussed in the previous section.   </p>"},{"location":"ai-studio-tutorial/predict/#optional-understanding-model-drift","title":"(Optional) Understanding Model Drift","text":"<p>If you choose to store your prediction results regularly, and then compare these to the ground truth, AND you do this on a regular basis you will have a good feeling of how your models perform. In some cases, a model that was doing great a while back will start to perform not so good. This is called the model drift.</p> <p>With Practicus AI, you can implement systems to automatically detect which of models start drifting. We will discuss this topic later. </p>"},{"location":"ai-studio-tutorial/predict/#optional-using-a-central-mlflow-database","title":"(Optional) Using a central MLflow Database","text":"<p>In this tutorial we used a local MLflow database on a Cloud Worker, which will disappear when you terminate it. This will be perfectly fine if you are just experimenting. For other cases, you can copy your models from the Cloud Worker manually. Or even better, you can store them in a central database at first place. </p> <p>To store models in a central database and share with others, do the following:  - Create or reuse a cloud database, such as MySQL or Postgresql  - Create an S3 bucket to store model artifacts. These are binary files, which can be very large, so it is not a good idea to store them in a relational database  - Open Practicus AI app, go to settings &gt; MLflow - Enter the connection string of the database - Enter S3 bucket</p> <p>Sample MLflow setup </p> <p></p> <p>When you have a central MLflow database configured, your Cloud Workers will automatically switch to using this database, instead of the local ones. </p> <p>With a central model database you can:</p> <ul> <li>Consume other users models to make predictions</li> <li>View other users experiment results </li> <li>View model explanation visuals </li> <li>Predict using models that are not built with Practicus AI </li> <li>Share Practicus AI models with other systems, so they can make predictions without Practicus AI </li> </ul> <p>Central model databases can greatly democratize how AI models are built, and how they are consumed. </p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/snippets/","title":"Snippets","text":""},{"location":"ai-studio-tutorial/snippets/#what-is-snippets","title":"What is Snippets?","text":"<p>Snippets are blocks of code written in Python, prepared for technical and non-technical users. These snippets are designed to perform specific and frequently encountered programming tasks so that users can manage their data processing faster and more efficiently. Snippets are especially ideal for tasks such as data manipulation because these code blocks eliminate the need for users to rewrite code every time.</p>"},{"location":"ai-studio-tutorial/snippets/#functionality-and-benefits-of-snippets","title":"Functionality and Benefits of Snippets","text":"<ul> <li>Snippets provides standardized and optimized code solutions, allowing users to quickly apply frequently needed operations on data sets with a few clicks. This saves time and minimizes coding errors.</li> </ul>"},{"location":"ai-studio-tutorial/snippets/#advantages-of-using-snippets","title":"Advantages of Using Snippets","text":"<ul> <li> <p>Time Efficiency: Saves time by reducing the need to write code for repetitive tasks.</p> </li> <li> <p>Accessibility: Snippets are open source. Users can share the snippets they create with their teammates or all Practicus AI users.</p> </li> <li> <p>Standardization: Data processing operations are applied in a consistent and repeatable way for each user, ensuring continuity across projects.</p> </li> </ul>"},{"location":"ai-studio-tutorial/snippets/#using-existing-snippets","title":"Using Existing Snippets","text":"<ul> <li> <p>Hint: By default, snippets are located in users' hidden .practicus folder. In this tutorial you will see more than one snippet folder. This is because I have also put the snippets in the other Practicus folder. The advantage is that you can create and test new snippets there and easily customize existing snippets. </p> </li> <li> <p>Open Explore tab </p> </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; airline.csv</li> <li>Click Prepare</li> <li>Select period column and click convert then choose Date Time</li> <li>Click OK</li> <li>Then select Snippets</li> <li>Open the Date time folder and choose Year</li> <li>Then choose Datetime column as period</li> <li>Then set the result name to Year and click on the test, then see the preview.</li> </ul> <p></p> <ul> <li>Then Click OK and see the new variable created</li> </ul> <p></p>"},{"location":"ai-studio-tutorial/snippets/#using-advanced-snippets","title":"Using Advanced Snippets","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; insurance.csv</li> <li>Select Snippets</li> <li>Open the Advanced folder and select Normalize</li> <li>Then choose Number columns as age, bmi, and charges</li> <li>Then set Normalize option as Z-Score Normalization</li> </ul> <ul> <li>Then Click OK and see the new variable created with columnname + _normalized</li> </ul>"},{"location":"ai-studio-tutorial/snippets/#advanced-section-for-technical-users","title":"Advanced Section for Technical Users","text":""},{"location":"ai-studio-tutorial/snippets/#snippets-for-spark-engine","title":"Snippets for SPARK Engine","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; insurance.csv</li> <li>Click customize sampling at the bottom left </li> <li>Click Advanced and set Data Engine as SPARK </li> </ul> <ul> <li>Then click OK and Load</li> <li>Select Snippets</li> <li>Open the Statistical folder and select Corr</li> <li>Then choose Number columns as age, bmi, and charges</li> <li>Then set Corr method option as Pearson (other options kendall and spearman)</li> </ul> <ul> <li>Then Click OK and see that the correlation table is formed </li> </ul> <ul> <li>Hint: Not all snippets work on SPARK Engine. To see which engines the snippets work in, you can look at the Snippets Code at the bottom of the screen when you select a snippet.</li> </ul>"},{"location":"ai-studio-tutorial/snippets/#creating-new-snippets","title":"Creating New Snippets","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Open any csv file</li> <li>Select Snippets and click New at the bottom</li> <li>Then, you will see a template for generating snippets, and in this template you will have access to a detailed explanation of how to create snippets.</li> </ul>"},{"location":"ai-studio-tutorial/sql/","title":"Working with SQL","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"ai-studio-tutorial/sql/#using-sql-with-a-sql-database","title":"Using SQL with a SQL database","text":"<p>This is quite straightforward, if the database you connect to support SQL, you can simply start by authoring a SQL statement. </p> <p>Practicus AI Cloud Workers come with a simple music artists database.</p> <ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select SQLite on the left menu </li> <li>Click Run Query</li> </ul> <p>You will see the result of a sample SQL, feel free to experiment and rerun the SQL</p> <p></p> <p>As you will see later in this tutorial, Practicus AI also allows you to run SQL on the result of your previous SQL, as many times as you need. On a SQL database you would need to use temporary tables to do so, which is a relatively advanced topic.</p>"},{"location":"ai-studio-tutorial/sql/#using-sql-on-any-data","title":"Using SQL on any data","text":"<p>Practicus AI allows you to use SQL without a SQL database. Let's demonstrate using an Excel file. You can do the same on other non-SQL data, such as S3. </p> <ul> <li>Open Explore tab</li> <li>Select Local Files</li> <li>Load Home &gt; practicus &gt; samples &gt; data &gt; worker_aws.xlsx</li> </ul> <p></p> <ul> <li>Click on SQL Query button</li> </ul> <p></p> <ul> <li>After click on the Query tab, the dialog will open. Select Yes.</li> </ul> <p></p> <p>Since SQL is an advanced feature, it will require a Cloud Worker to run. You will be asked if you would like to quickly upload to a Cloud Worker. Click Yes, select a Cloud Worker, and now your Excel file will be on the cloud. </p> <p>Click on SQL Query button again. This time the SQL query editor will be displayed.</p> <ul> <li>Type the below SQL </li> </ul> <pre><code>SELECT \"name\", \"total_price\", \"bang_for_buck\" \nFROM \"node_types\" \nWHERE \"total_price\" &lt; 5\nORDER BY \"bang_for_buck\" DESC\n</code></pre> <p></p> <p>Tip: double-clicking on a column name adds its name to the query editor, so you can write SQL faster. If you select some text before double-clicking, your selected text is replaced with the column name.</p> <ul> <li>Click on Test SQL button</li> </ul> <p>Note: Your first SQL on a particular Cloud Worker (cold run) will take a little longer to run. Subsequent SQL queries will run instantly. </p> <ul> <li>(Optional) Experiment further with the SQL, click</li> <li>When ready, click Apply SQL button</li> </ul> <p>You will get the result of the SQL back in the worksheet.  </p>"},{"location":"ai-studio-tutorial/sql/#optional-visualize-sql-result","title":"(Optional) Visualize SQL result","text":"<ul> <li>Click Analyze &gt; Graph</li> <li>Select bang_for_buck for X and total_price for Y</li> <li>Click ok</li> </ul> <p>You will see the estimated total cost (Practicus AI license cost + Cloud infrastructure cost), and how much cloud capacity value you would expect to get (bang for buck) visualized. </p> <p></p> <p>Note: If you have Practicus AI Enterprise license, your software is already paid for. So this graph would not make any sense.  This is only meaningful for the professional pay-as-you-go license type.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ai-studio-tutorial/start/","title":"Practicus AI Studio Tutorial","text":"<p>Welcome to Practicus AI Studio hands-on tutorial. </p>"},{"location":"ai-studio-tutorial/start/#topics-covered-in-this-tutorial","title":"Topics covered in this tutorial","text":"<p>This tutorial will help you get started with the below topics using Practicus AI</p> <ul> <li>Explore and Analyze Big Data</li> <li>Build AI models using AutoML </li> <li>Make predictions using AI models </li> <li>Data preparation to manipulate data interactively </li> </ul>"},{"location":"ai-studio-tutorial/start/#how-can-this-tutorial-help-you","title":"How can this tutorial help you?","text":""},{"location":"ai-studio-tutorial/start/#non-technical-users","title":"Non-technical Users","text":"<p>You will learn to be self-sufficient for most AI and Big Data tasks. If things get complicated, you will also learn to effectively collaborate with technical-users.</p>"},{"location":"ai-studio-tutorial/start/#semi-technical-users","title":"Semi-technical Users","text":"<p>Can you write SQL or Excel formulas? In addition to the above, you will also learn to extend the functionality quite a bit.</p>"},{"location":"ai-studio-tutorial/start/#data-scientists","title":"Data Scientists","text":"<p>If you are ok to click for some tasks, this tutorial will teach you to accelerate plenty, especially for data prep.  If you are a die-hard coder, you can use Practicus AI to get support from others, especially domain experts, for data discovery, preparation, quality, and model consumption. </p>"},{"location":"ai-studio-tutorial/start/#data-ml-engineers","title":"Data / ML Engineers","text":"<p>You can use Practicus AI as a modern and interactive ETL tool. You will also get clean production ready code for all the tasks (Data + AutoML + Predictions) that others perform, technical or not.</p>"},{"location":"ai-studio-tutorial/start/#big-picture-summary","title":"Big Picture Summary","text":"<p>The below is a summary of what this tutorial will cover and for whom. Please check our documentation for other topics. </p> <p></p>"},{"location":"ai-studio-tutorial/start/#before-we-begin","title":"Before we begin","text":"<p>If you are viewing this tutorial inside the Practicus AI app, you can right-click the tutorial tab and open it inside your browser as well. This can be useful if you would like to view the tutorial side-by-side with the app.  </p> <p></p> <p>Let's get started! </p> <p>Next &gt;</p>"},{"location":"ai-studio-tutorial/worker-node-intro/","title":"Introduction to Practicus AI Cloud Workers","text":"<p>Some advanced Practicus AI features require to use software in addition to Practicus AI APP or the developer SDK. In this section we will learn how to use Practicus AI Cloud Workers.</p>"},{"location":"ai-studio-tutorial/worker-node-intro/#what-is-a-cloud-worker","title":"What is a Cloud Worker?","text":"<p>Some Practicus AI features such as AutoML, making ** AI Predictions, Advanced Profiling and production deployment** capabilities require a larger setup, so we moved these from the app to a backend (server) system.  </p> <p>You have multiple Cloud Worker options to choose from. You can run them in the cloud or on your computer. Please view help on choosing a Cloud Worker to learn more.  </p>"},{"location":"ai-studio-tutorial/worker-node-intro/#setup-cloud-worker","title":"Setup Cloud Worker","text":"<p>Please check the Setup Guide to learn how to configure Practicus AI Cloud Workers. </p> <p>You can use the free cloud tier for this tutorial, or use containers on your computer as well.</p>"},{"location":"ai-studio-tutorial/worker-node-intro/#launching-a-new-cloud-worker","title":"Launching a new Cloud Worker","text":"<ul> <li>Click on the Cloud button to open the Cloud Workers tab</li> <li>Make sure the selected local for your computer, or the optimal AWS Cloud Region. The closest region geographically will usually give you the best internet network performance</li> <li>Click Start New </li> </ul> <ul> <li>Pick the Cloud Worker Size of your Cloud Worker</li> <li>Click ok to Start new Cloud Worker </li> </ul> <p>The default size will be enough for most tasks. You can also choose the free cloud tier.</p> <p></p> <p>In a few seconds you will see your Cloud Worker is launching, and in 1-2 minutes you will get a message saying your Cloud Worker is ready.</p> <p></p>"},{"location":"ai-studio-tutorial/worker-node-intro/#stopping-a-cloud-worker","title":"Stopping a Cloud Worker","text":"<p>If you use local container Cloud Workers you have less to worry about stopping them.  </p>"},{"location":"ai-studio-tutorial/worker-node-intro/#cloud-workers","title":"Cloud Workers","text":"<p>Similar to electricity, water, or other utilities, your cloud vendor (AWS) will charge you a fee for the hours your Cloud Worker is running. Although Practicus AI Cloud Workers automatically shut down after 90 minutes, it would be a practical approach to shut down your Cloud Workers manually when you are done for the day.</p> <p>For this, you can simply select a clod node and click on the Stop button. The next day, you can select the stopped Cloud Worker, click Start and continue where you are left.</p> <p>Tip: It is usually not a good idea to frequently stop / start instances. Please prefer to stop if your break is at least a few hours for optimal cost and wait time.</p>"},{"location":"ai-studio-tutorial/worker-node-intro/#terminating-a-cloud-worker","title":"Terminating a Cloud Worker","text":"<p>Practicus AI Cloud Workers are designed to be disposable, also called ephemeral. You can choose a Cloud Worker and click Terminate to simply delete everything related to it.</p> <p>Please be careful that if you choose to store data on the local disk of your Cloud Worker, this will also get lost after termination. In this case, you can prefer to copy your data manually, or simply click the Replicate button before terminating a Cloud Worker. </p>"},{"location":"ai-studio-tutorial/worker-node-intro/#optional-using-jupyter-lab","title":"(Optional) Using Jupyter Lab","text":"<p>For technical users.</p> <p>Every Cloud Worker comes with some external services preconfigured, such as Jupyter Lab, Mlflow, Airflow.  </p> <ul> <li>Select a Cloud Worker that is running and ready</li> <li>Click on Jupyter button</li> </ul> <p>This will start the Jupyter Lab service and view inside the app. You can also right-click tab name and select Open in browser to view the notebook on your default browser.</p> <p></p> <p>Notes: </p> <ul> <li>If you shut down the app, the secure connection tunnel to the Cloud Worker notebook service will be lost even if the Cloud Worker continues to run.</li> <li>There are two separate Conda kernels configured for your notebook server. Big data one will have common libraries and data engines, such as DASK, RAPIDS (if you have GPUs) and Spark installed. The ML one, as the name suggests, will have ML related libraries such as scikit-learn, Xgboost, Pycaret ..</li> </ul>"},{"location":"ai-studio-tutorial/worker-node-intro/#optional-using-the-terminal","title":"(Optional) Using the Terminal","text":"<p>For technical users.</p> <p>You can choose a Cloud Worker and click the Terminal button to instantly open up the terminal. You have sudo (super-user) access and this can be a very powerful and flexible way to customize your Cloud Worker.</p> <p></p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/add-on-services/","title":"Add-On Services","text":""},{"location":"ops-tutorial/add-on-services/#overview","title":"Overview","text":"<p>This document outlines how to manage and configure Add-On Services within the Practicus AI platform. It includes steps for adding new services, defining service types, and assigning connections for database, object storage, and Git repositories.</p>"},{"location":"ops-tutorial/add-on-services/#add-add-on-service","title":"Add Add-On Service","text":"<p>To add a new Add-On Service, follow these steps:</p> <ul> <li> <p>Navigate to Add-On Services &gt; Add Add-On Service.</p> </li> <li> <p>Fill in the required fields:</p> </li> <li>Key: Enter a unique key for the service.</li> <li>Name: Provide a user-friendly name for the service.</li> <li>Add-On Service Type: Select the type of service from the dropdown menu.</li> <li> <p>URL: Enter the primary URL of the service.</p> </li> <li> <p></p> </li> <li> <p>Optional fields:</p> </li> <li>Enterprise SSO App: If applicable, select an SSO app to authenticate users with Practicus AI login credentials.</li> <li>Database Connection: Choose a database connection if the service requires database access.</li> <li>Object Storage Connection: Select an object storage connection for data storage.</li> <li>Git Repo Connection: Specify a Git repository connection if the service integrates with a repository.</li> </ul> <p></p> <ul> <li>Click Save, or choose Save and add another to configure additional services.</li> </ul>"},{"location":"ops-tutorial/add-on-services/#add-add-on-service-type","title":"Add Add-On Service Type","text":"<p>To define a new service type, follow these steps:</p> <ul> <li> <p>Navigate to Add-On Services &gt; Add-On Service Types &gt; Add Add-On Service Type.</p> </li> <li> <p>Fill in the fields:</p> </li> <li>Key: Enter a unique identifier for the service type.</li> <li>Name: Provide a descriptive name for the service type.</li> </ul> <p></p> <ul> <li>Click Save, or choose Save and add another to configure additional service types.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/app-deployment/","title":"Application (App) Deployment","text":""},{"location":"ops-tutorial/app-deployment/#deployment-settings","title":"Deployment Settings","text":""},{"location":"ops-tutorial/app-deployment/#overview","title":"Overview","text":"<p>Deployment settings in the Practicus AI platform define configurations for deploying applications. These settings include worker types, object stores, scaling parameters, and observability options.</p>"},{"location":"ops-tutorial/app-deployment/#adding-a-deployment-setting","title":"Adding a Deployment Setting","text":"<p>To add a new deployment setting, click the Add Deployment Setting button. The following fields are required:</p> <ul> <li>Key: Internal name for the deployment setting.</li> <li>Name: User-friendly name.</li> <li>App Object Store: The object store where app files are located.</li> <li>Worker Type: Specifies the worker size (e.g., <code>2X-Small (1.0GB)</code>).</li> <li>Default Replica: Number of initial replicas for deployment.</li> <li>Auto Scaled: Enables or disables dynamic scaling.</li> <li>Min Replica and Max Replica: Define scaling boundaries.</li> <li>Enable Observability: Toggles observability settings for metrics collection.</li> <li>Log Level: Defines the verbosity of logs (e.g., <code>Default</code>).</li> </ul> <p> </p> <p>Click Save to finalize the configuration.</p>"},{"location":"ops-tutorial/app-deployment/#app-prefixes","title":"App Prefixes","text":""},{"location":"ops-tutorial/app-deployment/#overview_1","title":"Overview","text":"<p>App prefixes group applications under a shared path and configuration. These prefixes simplify app management and API accessibility.</p>"},{"location":"ops-tutorial/app-deployment/#adding-an-app-prefix","title":"Adding an App Prefix","text":"<p>To add a new app prefix, click the Add App Prefix button. Configure the following fields:</p> <ul> <li>Key: Unique identifier for the prefix.</li> <li>Prefix: Path under which all apps are grouped (e.g., <code>apps/finance</code>).</li> <li>Visible Name: Optional user-friendly name for the prefix.</li> <li>Description: Brief description of the apps under the prefix.</li> <li>Icon: Optional Font Awesome icon for visual identification.</li> <li>Sort Order: Determines display order (higher values appear first).</li> </ul> <p></p> <p>Click Save to complete.</p>"},{"location":"ops-tutorial/app-deployment/#application-versions","title":"Application Versions","text":""},{"location":"ops-tutorial/app-deployment/#overview_2","title":"Overview","text":"<p>Application versions allow users to manage multiple iterations of the same app, ensuring flexibility and version control.</p>"},{"location":"ops-tutorial/app-deployment/#managing-versions","title":"Managing Versions","text":"<p>The App Versions page lists all application versions, including their deployment configurations. You can edit or remove versions as needed.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/create-user-group/","title":"Managing Users and Groups","text":"<p>This section requires access to the Practicus AI Admin Console. Please make sure you have the necessary permissions to manage users and groups.</p>"},{"location":"ops-tutorial/create-user-group/#creating-a-user-create-user","title":"Creating a User (Create User)","text":""},{"location":"ops-tutorial/create-user-group/#adding-a-new-user","title":"Adding a New User","text":"<ul> <li>Open the Users tab under Users &amp; Groups in the left-hand navigation menu.  </li> <li>Click the + (Add User) button in the top-right corner.  </li> </ul> <ul> <li>Fill out the following details in the form:  </li> </ul> <ul> <li>Click Save to create the user.</li> </ul>"},{"location":"ops-tutorial/create-user-group/#assigning-permissions-optional","title":"Assigning Permissions (Optional)","text":"<p>After creating the user, you may need to assign roles or permissions:</p> <ul> <li>Locate the newly created user in the Users list and click on their name.  </li> <li> <p>Use the Permissions section to assign groups or specific roles: </p> </li> <li> <p>Click Save after making changes.</p> </li> </ul>"},{"location":"ops-tutorial/create-user-group/#tips-for-managing-users","title":"Tips for Managing Users","text":"<ul> <li>Group Assignments: Assign users to groups for role-based access control.  </li> <li>Permission Updates: Regularly review permissions to ensure compliance with organizational policies.</li> </ul> <p>By following these steps, you can effectively manage user creation and permissions in the Practicus AI Admin Console.</p>"},{"location":"ops-tutorial/create-user-group/#adding-a-group-create-group","title":"Adding a Group (Create Group)","text":""},{"location":"ops-tutorial/create-user-group/#adding-a-new-group","title":"Adding a New Group","text":"<ul> <li> <p>Open the Groups tab under Users &amp; Groups in the left-hand navigation menu.  </p> </li> <li> <p>Click the + (Add Group) button in the top-right corner.  </p> </li> <li> <p>In the Add Group form, provide the following details:  </p> </li> <li>Name: Enter a unique name for the group (e.g., \"Default\", \"Developers\").  </li> <li> <p>Permissions: Use the selector to assign available permissions to the group:  </p> <ul> <li>Select desired permissions from the Available permissions list.  </li> <li>Use the arrow button to move selected permissions to the Chosen permissions list. </li> </ul> </li> <li> <p>Click Save to create the group.</p> </li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/custom-images/","title":"Custom Container Images","text":"<p>This section provides guidance on managing and inserting container images into the Practicus AI platform. These images can be configured for various services like Cloud Workers, App Hosting, Model Hosting, and Workspaces.</p>"},{"location":"ops-tutorial/custom-images/#viewing-existing-container-images","title":"Viewing Existing Container Images","text":"<ul> <li>Navigate to Infrastructure &gt; Container Images in the left-hand navigation menu.  </li> <li>The list displays available container images with the following details:</li> <li>Image: The name of the container image.</li> <li>Service Type: Specifies where the image is used (e.g., Cloud Worker, App Hosting, Model Hosting).</li> <li>Version: The version of the image.</li> <li>Minimum and Recommended Versions: Ensures compatibility with the Practicus AI platform.</li> <li>Image Pull Policy: Specifies how the image is pulled (e.g., Always, Namespace default).    </li> </ul>"},{"location":"ops-tutorial/custom-images/#adding-a-new-container-image","title":"Adding a New Container Image","text":"<ul> <li>Click the + (Add Container Image) button in the top-right corner.</li> <li>Fill out the required fields in the Add Container Image form:</li> <li>Image: Provide the full image address (e.g., <code>registry.practicus.io/practicus/llm-cpu</code>).</li> <li>Service Type: Select the applicable service type (e.g., Cloud Worker, App Hosting).</li> <li>Version: Specify the version of the container image.</li> <li>Name and Description: Provide a short name and optional description for easy identification. </li> </ul>"},{"location":"ops-tutorial/custom-images/#additional-configuration","title":"Additional Configuration","text":"<ul> <li>Configure the following optional fields:</li> <li>Minimum and Recommended Versions: Define version constraints for compatibility.</li> <li>Image Pull Policy: Choose the policy for pulling the container image (e.g., Always).</li> <li> <p>Private Registry Details: Provide credentials and secrets if the image is stored in a private registry:</p> <ul> <li>Secret Name: Kubernetes secret for private registry access.</li> <li>Username and Password: Credentials for authentication.</li> </ul> </li> <li> <p>(Optional) Add a Startup Script to be executed when the container starts. </p> </li> <li> <p>Click Save to finalize the image insertion.</p> </li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/enterprise-sso/","title":"Enterprise SSO","text":"<p>This section describes how to manage applications and configure Single Sign-On (SSO) settings in the Enterprise SSO module.</p>"},{"location":"ops-tutorial/enterprise-sso/#managing-applications","title":"Managing Applications","text":""},{"location":"ops-tutorial/enterprise-sso/#viewing-applications","title":"Viewing Applications","text":"<p>To view the list of existing applications configured for Enterprise SSO:</p> <ul> <li>Navigate to Enterprise SSO in the sidebar menu.</li> <li>Select Applications.</li> <li>A table of applications is displayed with the following columns:</li> <li>Id: Unique identifier of the application.</li> <li>Name: Application name.</li> <li>User: Associated user or administrator for the application.</li> <li>Client Type: Indicates whether the client is Confidential or Public.</li> <li>Authorization Grant Type: Type of OAuth 2.0 grant used by the application (e.g., Authorization Code).    </li> </ul>"},{"location":"ops-tutorial/enterprise-sso/#adding-a-new-application","title":"Adding a New Application","text":"<p>To add a new application:</p> <ul> <li>Click the + button in the top-right corner.</li> <li>Fill in the following fields:</li> <li>Client Id: Automatically generated unique identifier for the client.</li> <li>User: Search and assign a user to the application (optional).</li> <li>Redirect URIs: Specify allowed URIs for redirecting after successful login.</li> <li>Post Logout Redirect URIs: Specify allowed URIs for redirecting after logout.</li> <li> <p>Client Type: Choose between:</p> <ul> <li>Confidential</li> <li>Public </li> </ul> </li> <li> <p>Authorization Grant Type: Select the appropriate grant type:</p> <ul> <li>Authorization Code</li> <li>Implicit</li> <li>Resource Owner Password-Based</li> <li>Client Credentials</li> <li>OpenID Connect Hybrid</li> </ul> </li> <li>Client Secret: Automatically generated secret for confidential clients. Ensure to copy this if it is a new secret.</li> <li>Name: Provide a descriptive name for the application.</li> <li>Skip Authorization: Optionally enable this to bypass authorization prompts.</li> <li>Algorithm: Specify OIDC support algorithm if applicable.</li> <li> <p>Allowed Origins: Add origins allowed to access the application.    </p> </li> <li> <p>Click Save to finalize the application creation or choose other saving options:</p> </li> <li>Save and add another: Save and immediately start configuring a new application.</li> <li>Save and continue editing: Save but remain on the same configuration screen.</li> </ul> <p>&lt; Previous</p>"},{"location":"ops-tutorial/git-config/","title":"Select System Git Repository","text":"<p>This document describes the settings and configuration for managing system Git repositories within Practicus AI.</p>"},{"location":"ops-tutorial/git-config/#overview","title":"Overview","text":"<p>The System Git Repositories section allows you to manage Git repositories for system-level operations such as Airflow DAGs or other shared resources.</p>"},{"location":"ops-tutorial/git-config/#system-git-repository-list","title":"System Git Repository List","text":"<p>The list displays all existing Git repositories, including details such as:</p> <p></p>"},{"location":"ops-tutorial/git-config/#adding-a-new-system-git-repository","title":"Adding a New System Git Repository","text":"<p>To add a new Git repository, click the Add System Git Repository button and fill in the following fields:</p> <ul> <li>Key: (Required) Unique name for the repository.</li> <li>Http(s) URL: URL for the repository (e.g., <code>https://github.com/your_account/repo_name.git</code>).</li> <li>Username: Optional Git username for authentication.</li> <li>Token: Optional password or token for authentication.</li> <li>SSH URL: Optional SSH address for the repository (e.g., <code>git@github.com:your_account/repo_name.git</code>).</li> <li>Branch: (Required) Specify the branch name (e.g., <code>main</code>).</li> <li>Committer Email: Optional email to override the default email used in commits.</li> </ul> <p></p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/model-deployment/","title":"Model Deployment Settings","text":""},{"location":"ops-tutorial/model-deployment/#overview","title":"Overview","text":"<p>This document provides details for configuring model deployment settings within the Practicus AI platform. It focuses on essential fields and configurations required to create, modify, and manage model deployments efficiently.</p>"},{"location":"ops-tutorial/model-deployment/#adding-a-model-deployment","title":"Adding a Model Deployment","text":""},{"location":"ops-tutorial/model-deployment/#key-fields","title":"Key Fields","text":"<ul> <li>Key (Required): Unique identifier for the deployment.</li> <li>Name (Required): Human-readable name for the deployment.</li> <li>Model Object Store (Required): The storage system containing the model files.</li> <li>Worker Type (Required): Defines the capacity (e.g., <code>Small</code>, <code>Large</code>) of the worker used for the deployment.</li> <li>Default Replica (Required): Specifies the default number of pods to run.</li> <li>Auto Scaled: Enables dynamic scaling of pod counts based on workload.</li> <li>Min Replica: Minimum number of pods when auto-scaling is enabled.</li> <li>Max Replica: Maximum number of pods when auto-scaling is enabled.</li> <li>Enable Observability: Activates metrics collection for external systems like Prometheus.</li> <li>Log Level: Sets the granularity of logs (e.g., <code>DEBUG</code>, <code>INFO</code>).</li> </ul>"},{"location":"ops-tutorial/model-deployment/#advanced-options","title":"Advanced Options","text":"<ul> <li>Node Selector: Assigns deployments to specific Kubernetes nodes using labels.</li> <li>Custom Image: Allows selecting or defining a custom container image.</li> <li>Startup Script: Shell commands executed before starting the API endpoint.</li> <li>Traffic Log Object Store: Specifies where request data and prediction logs are stored.</li> <li>Deployment Group Accesses: Defines groups with access permissions.</li> <li>Deployment User Accesses: Specifies individual user access rights.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#steps-to-add-a-deployment","title":"Steps to Add a Deployment","text":"<ul> <li>Navigate to ML Model Hosting &gt; Model Deployments.</li> <li>Click Add Model Deployment.</li> <li>Fill in the required fields under the \"Add Model Deployment\" form.</li> <li>Specify advanced configurations, if necessary.</li> <li>Save the deployment using one of the options:</li> <li>Save and add another: Save and open a new form.</li> <li>Save and continue editing: Save and remain on the current form.</li> <li>Save: Save and return to the main list.     </li> </ul>"},{"location":"ops-tutorial/model-deployment/#managing-deployment-settings","title":"Managing Deployment Settings","text":""},{"location":"ops-tutorial/model-deployment/#viewing-deployment-settings","title":"Viewing Deployment Settings","text":"<ul> <li>Navigate to the list of deployments in ML Model Hosting &gt; Model Deployments.</li> <li>Select a deployment to view or modify its settings.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#key-information-displayed","title":"Key Information Displayed","text":"<ul> <li>Model Object Store</li> <li>Worker Type</li> <li>Default Replica Count</li> <li>Observability Settings</li> <li>Traffic Log Object Store</li> </ul>"},{"location":"ops-tutorial/model-deployment/#modifying-deployments","title":"Modifying Deployments","text":"<ul> <li>Select the deployment to modify from the list.</li> <li>Update necessary fields.</li> <li>Save the changes using the appropriate option.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#observability-settings","title":"Observability Settings","text":""},{"location":"ops-tutorial/model-deployment/#core-metrics","title":"Core Metrics","text":"<ul> <li>Enables tracking of essential metrics like request count and total prediction time.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#model-drift-detection","title":"Model Drift Detection","text":"<ul> <li>Activates comparisons between predicted results and ground truth to detect model drift.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#logging","title":"Logging","text":"<ul> <li>Prediction Percentiles: Logs percentiles of prediction results for analysis.</li> <li>Custom Metrics: Allows defining custom metrics in Python.</li> <li>Traffic Logging: Logs request and prediction data in a compatible format.</li> </ul>"},{"location":"ops-tutorial/model-deployment/#advanced-logging-options","title":"Advanced Logging Options","text":"<ul> <li>Log Batch Rows: Sets the number of rows per log batch.</li> <li>Log Batch Minutes: Defines time intervals for flushing logs.  </li> </ul>"},{"location":"ops-tutorial/model-deployment/#model-deployment-best-practices","title":"Model Deployment Best Practices","text":"<ul> <li>Use auto-scaling for deployments with variable workloads.</li> <li>Enable observability for monitoring model performance and drift.</li> <li>Leverage custom images for deployments requiring specialized environments.</li> <li>Use traffic log object store for centralized logging and analysis.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/object-storage/","title":"Object Storage","text":"<p>This document outlines the configuration process for adding and managing system object storages within Practicus AI.</p>"},{"location":"ops-tutorial/object-storage/#overview","title":"Overview","text":"<p>The System Object Storages section enables you to integrate external object storage services (e.g., AWS S3) for managing and accessing data objects, such as model files and other resources.</p>"},{"location":"ops-tutorial/object-storage/#adding-a-new-object-storage","title":"Adding a New Object Storage","text":"<p>To add a new object storage, click the Add System Object Storage button and fill in the following fields:</p> <ul> <li>Key: (Required) A unique name for the object storage used for administrative purposes.</li> <li>Endpoint URL: (Required) The URL of the object storage service. For AWS S3, use the format <code>https://s3.{aws_region}.amazonaws.com</code> where <code>{aws_region}</code> is the region (e.g., <code>us-east-1</code>).</li> <li>Bucket Name: (Required) The name of the object storage bucket where data will be stored. Models will be stored under the path <code>[endpoint_url]/[bucket_name]/models/[model_host_key]/[model_key]/[version]/[model_files]</code>.</li> <li>Prefix: (Optional) A prefix path to use before accessing objects. For example, if the prefix is <code>some/prefix</code>, objects will be accessed using <code>https://[endpoint]/[bucket]/some/prefix/object.data</code>.</li> <li>Access Key ID: (Required) The access key for the object storage service.</li> <li>Secret Access Key: (Required) The secret key for the object storage service. </li> </ul>"},{"location":"ops-tutorial/object-storage/#actions","title":"Actions","text":"<ul> <li>Save: Saves the new object storage configuration.</li> <li>Save and Add Another: Saves the configuration and opens a new form to add another object storage.</li> <li>Save and Continue Editing: Saves the configuration but keeps the form open for further adjustments.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/resources-management/","title":"Resource Management","text":"<p>This section focuses on managing worker sizes, consumption logs, group limits, and user limits to optimize resource utilization in the Practicus AI platform.</p>"},{"location":"ops-tutorial/resources-management/#worker-sizes","title":"Worker Sizes","text":"<p>The Worker Sizes section allows you to configure and view the available compute resources for tasks. You can define worker sizes with specific configurations, such as CPU cores, memory, GPU availability, and security contexts.</p>"},{"location":"ops-tutorial/resources-management/#adding-a-worker-size","title":"Adding a Worker Size","text":"<ul> <li>Navigate to Infrastructure &gt; Worker Sizes in the left-hand navigation menu.  </li> <li>Click the + (Add Worker Size) button in the top-right corner.  </li> <li>Fill out the required fields in the form, such as:</li> <li>Name: Unique identifier for the worker size.</li> <li>CPU cores and memory: Specify resource allocation.</li> <li>GPU and VRAM: Assign GPU resources if applicable.</li> <li>Click Save to add the worker size. </li> </ul>"},{"location":"ops-tutorial/resources-management/#consumption-logs","title":"Consumption Logs","text":"<p>The Consumption Logs section provides an overview of resource usage, including detailed logs of worker activities. - Fields Available:   - User: Indicates the user utilizing the resource.   - Instance ID: Unique ID of the running worker.   - Size: Worker size (e.g., Small, Medium).   - CPU cores, memory, GPUs: Resource allocation.   - Stop Time and Duration: Logs the completion time and total runtime.  </p> <p>Use this section to monitor resource consumption and analyze trends.</p> <p></p>"},{"location":"ops-tutorial/resources-management/#consumption-summary","title":"Consumption Summary","text":"<p>The Consumption Summary provides an aggregated view of resource usage by users over specific periods (daily, weekly, monthly). - Key Metrics:   - Memory and VRAM usage: Tracks active and total consumption for CPU and GPU resources.   - User-based usage: Detailed summary per user.</p> <p>This feature helps identify heavy resource consumers and optimize allocations.</p>"},{"location":"ops-tutorial/resources-management/#group-limits","title":"Group Limits","text":"<p>The Group Limits section allows you to set resource usage caps for groups. - Steps to Add Group Limits:   - Navigate to Infrastructure &gt; Group Limits.   - Click the + (Add Group Limit) button.   - Specify the following fields:      - Group: Select the group to configure.      - Memory GB active: Define the active memory capacity for the group.      - Daily, weekly, and monthly limits: Set resource caps.      - VRAM limits: Allocate GPU-specific resources if needed.   - Save the changes to apply the limits. </p>"},{"location":"ops-tutorial/resources-management/#user-limits","title":"User Limits","text":"<p>The User Limits section allows you to define individual resource usage caps per user. - Steps to Add User Limits:   - Navigate to Infrastructure &gt; User Limits.   - Click the + (Add User Limit) button.   - Configure the following:      - User: Select the target user.      - Memory and VRAM limits: Define active and total usage thresholds.      - Daily, weekly, and monthly limits: Set specific caps for resources.   - Save the changes to enforce the user limit.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"ops-tutorial/start/","title":"Practicus AI Operations Tutorial","text":"<p>In this demo we will focus on the operational side of Practicus AI. This tutorial will guide you through how to manage, monitor, and maintain the platform in production at scale. Whether you're running Practicus AI on a public cloud Kubernetes environment such as AWS EKS, Azure AKS, Google GKE, or on-premises solutions like Red Hat OpenShift or Rancher, understanding these operational best practices ensures a stable, scalable system.</p>"},{"location":"ops-tutorial/start/#overview","title":"Overview","text":"<ul> <li> <p>Cloud-Native &amp; On-Prem Flexibility   Practicus AI is fully cloud-native and can be deployed across various Kubernetes-based environments\u2014from major cloud providers to on-premises clusters. </p> </li> <li> <p>Observability &amp; Monitoring   Ability to track logs, metrics, events, and errors across your Practicus AI deployments. By leveraging add-on services like Grafana, you can create real-time dashboards and alerts to ensure continuous uptime and optimal performance.</p> </li> <li> <p>Workflow &amp; Scheduling   Airflow integration provides a robust solution for scheduling and automating complex data pipelines. In an enterprise setting, these workflows often involve cross-team or cross-department coordination\u2014this tutorial shows you how to manage and monitor such tasks seamlessly.</p> </li> <li> <p>Security &amp; Compliance   As part of day-2 operations, you\u2019ll need to ensure that your deployments adhere to security best practices. This includes understanding Kubernetes namespace isolation, role-based access control (RBAC), and any compliance measures your organization must meet.</p> </li> </ul> <p>Next &gt;</p>"},{"location":"ops-tutorial/storage/","title":"Storage (My/Shared Folders)","text":"<p>This section outlines how to manage personal and shared storage configurations in the Practicus AI platform. You can configure storage for individual users or groups, ensuring proper allocation and permissions.</p>"},{"location":"ops-tutorial/storage/#group-storage","title":"Group Storage","text":""},{"location":"ops-tutorial/storage/#viewing-group-storage","title":"Viewing Group Storage","text":"<ul> <li>Navigate to Infrastructure &gt; Group Storage in the left-hand navigation menu.</li> <li>View existing group storage configurations:</li> <li>Storage Class Name: Defines the type of storage being used.</li> <li>Access Mode: Specifies the level of access (e.g., Read Write Many).</li> <li>Storage Prefix: Indicates the directory path assigned to the group (e.g., <code>users/</code>).</li> </ul>"},{"location":"ops-tutorial/storage/#adding-group-storage","title":"Adding Group Storage","text":"<ul> <li>Click + (Add Personal Storage) in the top-right corner.</li> <li>Configure the following fields:</li> <li>Group: Select the group for which the storage is being configured.</li> <li>Storage Class Name: Define the Kubernetes storage class.</li> <li>Access Mode: Choose access level (e.g., Read Write Many).</li> <li>Storage Prefix: Specify the path prefix for the storage.</li> <li>Priority: Set the priority for the group (higher values take precedence).</li> <li>Size (MB): Optionally set a size limit for the storage.</li> <li>Click Save to apply changes. </li> </ul>"},{"location":"ops-tutorial/storage/#group-shared-storage","title":"Group Shared Storage","text":""},{"location":"ops-tutorial/storage/#viewing-group-shared-storage","title":"Viewing Group Shared Storage","text":"<ul> <li>Navigate to Infrastructure &gt; Group Shared Storage in the left-hand navigation menu.</li> <li>View shared storage configurations for groups:</li> <li>Group: Indicates the group using the shared storage.</li> <li>Service Type: Shows whether storage is for Cloud Worker or Workspace.</li> <li>Storage Class Name: Defines the storage class.</li> <li>Share Name: Path of the shared directory (e.g., <code>shared/partner</code>).</li> <li>Access Mode: Specifies access permissions (e.g., Read Write Many).</li> </ul>"},{"location":"ops-tutorial/storage/#adding-shared-storage-for-groups","title":"Adding Shared Storage for Groups","text":"<ul> <li>Click + (Add Shared Storage) in the top-right corner.</li> <li>Fill in the following details:</li> <li>Group: Select the group to configure shared storage for.</li> <li>Service Type: Choose the service type (e.g., Cloud Worker).</li> <li>Storage Class Name: Specify the Kubernetes storage class.</li> <li>Share Name: Define the directory name for shared storage.</li> <li>Access Mode: Set the access level (e.g., Read Write Many).</li> <li>Size (MB): Optionally limit the size of the shared storage.</li> <li>Click Save to finalize the configuration. </li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"technical-tutorial/distributed-computing/introduction/","title":"Distributed Computing with Practicus AI","text":"<p>Distributed computing enables you to leverage multiple machines or compute nodes to process data, train models, or execute tasks in parallel, achieving scalability, efficiency, and speed. This paradigm is crucial for modern workloads such as large-scale data processing, distributed training of machine learning models, and real-time analytics.</p> <p>Practicus AI simplifies distributed computing by offering a flexible adapter-based system. This approach abstracts the complexities of managing distributed jobs while remaining extensible to suit a wide range of use cases.</p>"},{"location":"technical-tutorial/distributed-computing/introduction/#adapter-system","title":"Adapter System","text":"<p>Practicus AI's adapter system acts as a bridge between the platform and various distributed frameworks. It ensures seamless integration and management of distributed jobs, such as:</p> <ul> <li>Data Processing Jobs:  </li> <li>Apache Spark: Process large-scale datasets using distributed Spark jobs, enabling parallel computation across multiple nodes.  </li> <li> <p>Dask: Execute lightweight, distributed computations for real-time analytics or complex workflows.  </p> </li> <li> <p>Distributed Training for Machine Learning:  </p> </li> <li>DeepSpeed: Efficiently fine-tune large language models (LLMs) using distributed GPU resources.  </li> <li>FairScale: Scale your training workflows with advanced memory optimization and model parallelism.  </li> <li> <p>Horovod: Run distributed training jobs across heterogeneous clusters.  </p> </li> <li> <p>Extensibility for Custom Needs:   Practicus AI allows customers to build and integrate custom adapters to extend its capabilities. Whether you\u2019re using a proprietary framework or need to incorporate specialized tools, the adapter system provides the flexibility to tailor distributed computing to your organization\u2019s unique requirements.</p> </li> </ul>"},{"location":"technical-tutorial/distributed-computing/introduction/#key-benefits","title":"Key Benefits","text":"<p>Practicus AI\u2019s distributed computing platform provides the following advantages:</p> <ol> <li> <p>Unified Interface: Manage distributed jobs across various frameworks using a consistent SDK. Whether it\u2019s a Spark job or a DeepSpeed training task, the process remains intuitive and unified.</p> </li> <li> <p>Scalability: Automatically scale compute resources based on workload demands, enabling efficient processing of massive datasets or training of complex machine learning models.</p> </li> <li> <p>Isolation and Control: Each distributed job runs within isolated Kubernetes pods, ensuring secure and reliable execution.</p> </li> <li> <p>Extensibility: The adapter-based system supports existing frameworks and can be extended to accommodate custom tools or workflows.</p> </li> </ol>"},{"location":"technical-tutorial/distributed-computing/introduction/#use-cases","title":"Use Cases","text":"<ul> <li>Data Pipelines: Create distributed pipelines to process petabytes of data with Spark or Dask, optimizing performance through parallelism and resource allocation.</li> <li>Model Training: Fine-tune large models like GPT or BERT using DeepSpeed, FairScale, or custom frameworks, ensuring efficient memory utilization and scaling across GPU clusters.</li> <li>Custom Workflows: Build proprietary adapters for niche frameworks or processes, integrating them seamlessly with Practicus AI\u2019s distributed system</li> </ul> <p>Previous: Sample Vector Db | Next: Spark &gt; Interactive &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/","title":"Executing batch jobs in Dask Cluster","text":"<p>In this example we will: - Create a Dask cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"dask\" under your \"~/my\" folder</li> <li>And copy job.py under this folder</li> </ul> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/dask\"\n\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.dask,\n    job_dir = job_dir,\n    py_file = \"job.py\",\n    worker_count = 2,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    distributed_config=distributed_config,\n    log_level=\"DEBUG\",\n)\n\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n)\n</code></pre> <pre><code># You can view the logs during or after the job is completed\n# To view coordinator (master) set rank = 0\nrank = 0\n# To view other workers set rank = 1,2, ..\n\nprt.distributed.view_log(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n    rank=rank\n)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/#wrapping-up","title":"Wrapping up","text":"<ul> <li>Once the job is completed, you can view the results in <code>~/my/dask/result.csv/</code></li> <li>Please note that result.csv is a folder that can contain <code>parts of the processed file</code> by each worker (Dask executors)</li> <li>Also note that you do not need to terminate the cluster since it has a 'py_file' to execute, which defaults <code>terminate_on_completion</code> parameter to True.</li> <li>You can change terminate_on_completion to False to keep the cluster running after the job is completed to troubleshoot issues.</li> <li>You can view other <code>prt.DistJobConfig</code> properties to customize the cluster</li> </ul>"},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/dask/batch-job/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt \nimport dask.dataframe as dd\n\n# Let's get a Dask session\nprint(\"Getting Dask session\")\ndask = prt.distributed.get_client()\n\nprint(\"Reading diamond data\")\ndf = dd.read_csv('/home/ubuntu/samples/diamond.csv')  \n\nprint(\"Calculating\")\ndf[\"New Price\"] = df[\"Price\"] * 0.8\n\nprint(\"Since Dask is a lazy execution engine,\")\nprint(\" actual calculations will happen when you call compute() or save.\")\n\nprint(\"Saving\")\ndf.to_csv('/home/ubuntu/my/dask/result.csv')\n\n# Note: the save location must be accessible by all workers\n# A good place to save for distributed processing is object storage\n</code></pre> <p>Previous: Use Cluster | Next: Distributed Training &gt; XGBoost</p>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/","title":"Distributed XGBoost with Dask for Scalable Machine Learning","text":"<p>This example showcases the use of Dask for distributed computing with XGBoost, enabling efficient training on large datasets. We cover:</p> <ul> <li>Training an XGBoost model on a Dask cluster.</li> <li>Saving the trained model to disk.</li> <li>Loading the saved model and making predictions on new data.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's start with creating an interactive Dask cluster \n# Note: you can also run this as a batch job.\n# To learn more, please view the batch section of this guide.\n\nif prt.distributed.running_on_a_cluster():\n    print(\"You are already running this code on a distributed cluster. No need to create a new one..\")\nelse:\n    print(\"Starting a new distributed Dask cluster.\")\n    distributed_config = prt.DistJobConfig(\n        job_type = prt.DistJobType.dask,\n        worker_count = 2,\n    )\n    worker_config = prt.WorkerConfig(\n        worker_size=\"X-Small\",\n        distributed_config=distributed_config,\n    )\n    coordinator_worker = prt.create_worker(\n        worker_config=worker_config,\n    )\n\n    # Let's login to the cluster coordinator\n    notebook_url = coordinator_worker.open_notebook()\n\n    print(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#execute-on-dask-cluster","title":"Execute on Dask Cluster","text":"<ul> <li>If you just created a new cluster, please open the new browser tab to login to the Distributed Dask coordinator, and continue with the below steps..</li> <li>If you are already on the cluster, you can continue with the below steps..</li> </ul>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#optional-viewing-training-details-on-the-dask-dashboard","title":"(Optional) Viewing training details on the Dask dashboard","text":"<p>If you would like to view training details, please login the Dask dashboard with the below.</p> <pre><code>import practicuscore as prt\n\ndashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code>import practicuscore as prt \n\n# Let's get a Dask session\nclient = prt.distributed.get_client()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#training-with-xgboost-on-dask-cluster","title":"Training with XGBoost on Dask Cluster","text":"<pre><code>import xgboost as xgb\nimport dask.array as da\nfrom dask_ml.model_selection import train_test_split\nimport numpy as np\nimport dask.distributed\n\n# Generate some random data (replace with your actual data)\nX = da.random.random((1000, 10), chunks=(100, 10))\ny = da.random.randint(0, 2, size=1000, chunks=100)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Convert Dask arrays to DaskDMatrix (XGBoost-compatible format)\ndtrain = xgb.dask.DaskDMatrix(client, X_train, y_train)\ndtest = xgb.dask.DaskDMatrix(client, X_test, y_test)\n\n# Set XGBoost parameters\nparams = {\n    'objective': 'binary:logistic',\n    'max_depth': 6,\n    'eta': 0.3,\n    'tree_method': 'hist'\n}\n\n# Train the model\nbst = xgb.dask.train(client, params, dtrain, num_boost_round=100)\nprint(\"Model trained successfully\")\n\n# Optionally, evaluate the model on test data\npreds = xgb.dask.predict(client, bst, dtest)\nprint(\"Predictions made successfully\")\n\nmodel_path = \"model.ubj\"\nbst['booster'].save_model(model_path)\nprint(f\"Model saved to {model_path}\")\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#deploying-model-as-an-api","title":"Deploying model as an API","text":"<p>Note: If you would like to deploy the XGBoost model as an API, please visit the modeling basics section.</p>"},{"location":"technical-tutorial/distributed-computing/dask/distributed-training/xgboost/#load-and-train-with-the-xgboost-model","title":"Load and train with the XGBoost model","text":"<pre><code>print(\"Loading Model and Predicting\")\n\n# Load the saved model\nloaded_bst = xgb.Booster()\nloaded_bst.load_model(model_path)\nprint(\"Model loaded successfully\")\n\n# Generate a *new* random dataset (important: different from training/testing)\nX_new = da.random.random((500, 10), chunks=(100, 10))  # New data!\nX_new_computed = client.compute(X_new).result() # Important to compute before creating DMatrix\ndnew = xgb.DMatrix(X_new_computed)\n\n# Make predictions using the loaded model\nnew_preds = loaded_bst.predict(dnew)\nprint(\"New predictions made successfully\")\n\n# Print some predictions (convert to NumPy array for easier printing)\nprint(\"First 10 New Predictions:\")\nprint(new_preds[:10])\n</code></pre> <pre><code># Cleanup\ntry:\n    # if code is running where you started the cluster\n    coordinator_worker.terminate()\nexcept:\n    # Or else, let's terminate self, which will also terminate the cluster.\n    prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Batch Job | Next: DeepSpeed &gt; Basics &gt; Intro To DeepSpeed</p>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/start-cluster/","title":"Starting an interactive Dask Cluster","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Dask cluster, and execute simple Dask operations.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.dask,\n    worker_count = 2,\n)\n\n# Let's define worker features of the cluster \nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker \n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Dask cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next example in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator \n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Batch Job | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/use-cluster/","title":"Using the interactive Dask Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Dask cluster we created, and execute simple Dask operations.</li> <li>Please run this example on the <code>Dask Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt \n\n# Let's get a Dask session\nclient = prt.distributed.get_client()\n</code></pre> <pre><code># And execute some code\nimport dask.array as da\n\nprint(\"Starting calculation.\")\n\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\nresult = (x + x.T).mean(axis=0).compute()\n\nprint(\"Completed calculation. Results:\", result)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/use-cluster/#dask-dashboard","title":"Dask Dashboard","text":"<p>Practicus AI Dask offers an interactive dashboard where you can view execution details. Let's open the dashboard.</p> <pre><code>dashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code># Let's execute the same code\nimport dask.array as da\n\nprint(\"Starting calculation.\")\n\nx = da.random.random((10000, 10000), chunks=(1000, 1000))\nresult = (x + x.T).mean(axis=0).compute()\n\nprint(\"Completed calculation. Results:\", result)\n</code></pre> <p>Now you should see in real-time the execution details in a view similar to the below.</p> <p></p>"},{"location":"technical-tutorial/distributed-computing/dask/interactive/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Batch Job &gt; Batch Job</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/","title":"Distributed DeepSpeed Training","text":"<p>This example demonstrates the process of setting up distributed workers for distributed training using Practicus AI DeepSpeed.</p> <p>Focuses: - Configuring and launching distributed workers using Practicus AI. - Monitoring and logging distributed job performance and resource usage. - Terminating the distributed job after completion.</p> <p>The train.py script and ds_config.json configuration files are used to define the model fine-tuning process.</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#importing-libraries-and-configuring-distributed-job","title":"Importing Libraries and Configuring Distributed Job","text":"<p>This step imports the required libraries, including <code>practicuscore</code>, and sets up the configurations for a distributed job. The key elements are: - <code>job_dir</code>: Directory containing DeepSpeed configuration files and the training script. - <code>DistJobConfig</code>: Defines distributed job parameters such as worker count and termination policy. - <code>WorkerConfig</code>: Specifies worker parameters, including the Docker image, worker size, and startup script.</p> <p>The configuration prepares a coordinator worker that initializes a distributed job.</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"deepspeed\" under your \"~/my\" folder.</li> <li>And copy <code>train.py</code> and <code>ds_config.json</code> under this folder.</li> </ul> <pre><code>import practicuscore as prt\n\n# DeepSpeed job directory must have default files ds_config.json and train.py (can be renamed)\njob_dir = \"~/my/deepspeed\"\nworker_count = 2\n\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.deepspeed,\n    job_dir = job_dir,\n    worker_count = worker_count,\n    terminate_on_completion=False,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"ghcr.io/practicusai/practicus-gpu-deepspeed\",\n    worker_size=\"L-GPU\",\n    log_level=\"DEBUG\",\n    distributed_config=distributed_config\n)\n\ncoordinator_worker = prt.create_worker(worker_config)\n\njob_id = coordinator_worker.job_id\n\nassert job_id, \"Could not create distributed job\"\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#monitoring-distributed-job","title":"Monitoring Distributed Job","text":"<p>The <code>live_view</code> and <code>view_log</code> utilities from the Practicus SDK are used to monitor the progress of the distributed job. This provides details such as: - Job ID, start time, worker states, and GPU utilization. - Resource allocation for each worker in the distributed cluster.</p> <p>It helps in tracking the real-time status of the distributed job.</p> <pre><code># Live resource allocation\nprt.distributed.live_view(job_dir, job_id)\n</code></pre> <pre><code># For master logs, you can check Rank-0 logs:\nprt.distributed.view_log(job_dir, job_id, rank=0)\n</code></pre> <pre><code># For pair logs, you must specify pair IDs, e.g. 1:\nprt.distributed.view_log(job_dir, job_id, rank=1)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#terminating-distributed-job-cluster","title":"Terminating Distributed Job Cluster","text":"<p>The distributed job cluster and all associated workers are terminated using the <code>terminate</code> method of the coordinator worker.</p> <pre><code>coordinator_worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#ds_configjson","title":"ds_config.json","text":"<pre><code>{\n    \"train_batch_size\": 4,\n    \"gradient_accumulation_steps\": 2,\n    \"optimizer\": {\n        \"type\": \"Adam\",\n        \"params\": {\n            \"lr\": 0.001\n        }\n    },\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 1\n    }\n}\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/basics/intro-to-deepspeed/#trainpy","title":"train.py","text":"<pre><code>import torch\nimport torch.nn as nn\nimport deepspeed\nimport torch.distributed as dist\nimport os\nimport json\n\n\n# Define a dummy large neural network\nclass LargeModel(nn.Module):\n    def __init__(self):\n        super(LargeModel, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(8192, 4096), nn.ReLU(),\n            nn.Linear(4096, 2048), nn.ReLU(),\n            nn.Linear(2048, 1024), nn.ReLU(),\n            nn.Linear(1024, 512), nn.ReLU(),\n            nn.Linear(512, 256), nn.ReLU(),\n            nn.Linear(256, 128), nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n\n# Function to check FP16\ndef is_fp16_enabled(config_path=\"ds_config.json\"):\n    with open(config_path) as f:\n        config = json.load(f)\n    return config.get(\"fp16\", {}).get(\"enabled\", False)\n\n\n# Main training function\ndef train():\n    # Distributed setup\n    dist.init_process_group(\n        backend=\"nccl\",\n        init_method=f\"tcp://{os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}\",\n        rank=int(os.environ['RANK']),\n        world_size=int(os.environ['WORLD_SIZE']),\n    )\n\n    device = torch.device(f\"cuda:0\")\n    model = LargeModel().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # Initialize DeepSpeed\n    model, optimizer, _, _ = deepspeed.initialize(\n        model=model,\n        optimizer=optimizer,\n        config=\"ds_config.json\"\n    )\n\n    # Training loop\n    for epoch in range(5):\n        optimizer.zero_grad()\n\n        # Creation of dummy train data\n        data = torch.randn(32768, 8192).to(device)\n        target = torch.randn(32768, 1).to(device)\n\n        # Convert data and target to FP16 if enabled\n        if is_fp16_enabled(\"ds_config.json\"):\n            data, target = data.half(), target.half()\n\n        # Log memory usage and loss\n        loss = nn.MSELoss()(model(data), target)\n        model.backward(loss)\n        model.step()\n\n        print(f\"[GPU {os.environ['RANK']}] Epoch {epoch}, Loss: {loss.item()}, \"\n              f\"Allocated VRAM: {torch.cuda.memory_allocated(device) / 1e9:.2f} GB\")\n\n    # Clean cache\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    train()\n</code></pre> <p>Previous: XGBoost | Next: LLM Fine Tuning &gt; Llms With DeepSpeed</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/","title":"Distributed LLM Fine-Tuning with DeepSpeed","text":"<p>Fine-tuning large language models (LLMs) often requires distributed computing to efficiently handle the computational demands of large datasets and model sizes. Practicus AI, combined with the DeepSpeed library, provides a streamlined platform for distributed training and fine-tuning of LLMs. This notebook demonstrates the end-to-end process of dataset preparation, worker configuration, distributed job execution, and model testing using Practicus AI.</p> <p>Key highlights include: - Preparing datasets and downloading pre-trained models. - Setting up distributed workers using DeepSpeed. - Monitoring and logging distributed jobs. - Testing the fine-tuned model against its base version.</p> <p>With Practicus AI's flexible distributed computing framework, you can efficiently manage resource-intensive tasks like LLM fine-tuning, ensuring scalability, idempotency, and atomicity across the entire workflow.</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#dataset-preparation-and-model-download","title":"Dataset Preparation and Model Download","text":"<p>The first step in fine-tuning involves preparing the dataset and downloading the pre-trained model. These steps include:</p> <ol> <li>Loading and saving the fine-tuning dataset.</li> <li>Authenticating with Hugging Face to securely access model repositories.</li> <li>Downloading a pre-trained LLM (e.g., LLaMA-3B-Instruct) from the Hugging Face Hub for fine-tuning.</li> </ol> <pre><code>import pandas as pd\n\nurl = 'https://raw.githubusercontent.com/practicusai/sample-data/refs/heads/main/customer_support/Customer_Support_Dataset.csv'\ndf = pd.read_csv(url, index_col=0)\n\ndf.head()\n</code></pre> <pre><code>df.to_csv('Customer_Support_Dataset.csv')\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#hugging-face-authentication","title":"Hugging Face Authentication","text":"<p>To download models from Hugging Face, you need to authenticate using your API token. This ensures secure access to both public and private repositories.</p> <pre><code>from huggingface_hub.hf_api import HfFolder\ntry:\n    HfFolder.save_token('...')  # Replace with your Hugging Face API token\n\nexcept:\n    print('Hugging face token is wrong.')\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#download-pre-trained-model","title":"Download Pre-Trained Model","text":"<p>The pre-trained LLM is downloaded to a local directory for fine-tuning. Replace <code>local_dir</code> and <code>REPO_ID</code> with the desired directory and model ID, respectively.</p> <pre><code>from huggingface_hub import snapshot_download\n\ntry:\n    local_dir = \"...\"  # Example: /home/ubuntu/my/llm_fine_tune/llama-3B-instruct\n    REPO_ID = \"...\"  # Example: meta-llama/Llama-3.2-3B-Instruct\n\n    snapshot_download(repo_id=REPO_ID, local_dir=local_dir)\n\nexcept:\n    print(\"Snapshot didn't found.\")\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#building-workers-for-distributed-llm-fine-tuning","title":"Building Workers for Distributed LLM Fine-Tuning","text":"<p>This section demonstrates how to configure and launch distributed workers for fine-tuning an LLM using DeepSpeed. The main focus areas are: - Configuring distributed job parameters (e.g., worker count, job directory). - Setting up workers with appropriate Docker images and resource configurations. - Initializing the distributed cluster for fine-tuning.</p>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a folder <code>~/my/deepspeed/llm_fine_tune</code>.</li> <li>Copy <code>train.py</code> and <code>ds_config.json</code> into this directory.</li> </ul> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/deepspeed/llm_fine_tune\"\nworker_count = 2\n\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.deepspeed,\n    job_dir=job_dir,\n    worker_count=worker_count,\n    terminate_on_completion=False,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"ghcr.io/practicusai/practicus-gpu-deepspeed\",\n    worker_size=\"L-GPU\",\n    log_level=\"DEBUG\",\n    distributed_config=distributed_config,\n    startup_script='pip install accelerate trl peft datasets'\n)\n\ncoordinator_worker = prt.create_worker(worker_config)\njob_id = coordinator_worker.job_id\nassert job_id, \"Could not create distributed job\"\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#monitoring-distributed-job","title":"Monitoring Distributed Job","text":"<p>Use the <code>live_view</code> and <code>view_log</code> utilities to monitor the job's progress. These tools provide real-time insights into worker status, GPU utilization, and job logs.</p> <pre><code>prt.distributed.live_view(job_dir, job_id)\n</code></pre> <pre><code>prt.distributed.view_log(job_dir, job_id, rank=0)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#terminating-distributed-job-cluster","title":"Terminating Distributed Job Cluster","text":"<p>Once the job completes, terminate the distributed cluster and release resources.</p> <pre><code>coordinator_worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#testing-the-fine-tuned-model","title":"Testing the Fine-Tuned Model","text":"<p>This section demonstrates how to load and test a fine-tuned LLM. The steps include: - Installing required libraries. - Loading the fine-tuned model and tokenizer. - Comparing the fine-tuned model\u2019s outputs with those from the base model.</p> <pre><code>! pip install peft accelerate\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#comparing-fine-tuned-model-and-base-model","title":"Comparing Fine-Tuned Model and Base Model","text":"<p>The fine-tuned model and tokenizer are loaded from the <code>output_dir</code>. The <code>device_map='auto'</code> parameter ensures efficient resource allocation, distributing the model across available GPUs. A conversation-like input is passed to both models, and their outputs are compared.</p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load the fine-tuned model\nmodel_name_or_path = './output_dir'  # Path to the saved fine-tuned model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, device_map='auto')\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='auto')\n</code></pre> <pre><code># Define chat messages\nmessages = [{\"role\": \"user\", \"content\": \"want assistance to cancel purchase 554\"}]\n\n# Apply chat template\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Tokenize input text\ninputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(\"cuda\")\n\n# Generate model response\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\n# Decode model output\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract response content\nprint(text.split(\"assistant\")[1])\n</code></pre> <pre><code># Load the base model\nmodel_name_or_path = './llama-3B-instruct'  # Path to the saved fine-tuned model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, device_map='auto')\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='auto')\n</code></pre> <pre><code># Define chat messages\nmessages = [{\"role\": \"user\", \"content\": \"want assistance to cancel purchase 554\"}]\n\n# Apply chat template\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Tokenize input text\ninputs = tokenizer(prompt, return_tensors='pt', truncation=True).to(\"cuda\")\n\n# Generate model response\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\n# Decode model output\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract response content\nprint(text.split(\"assistant\")[1])\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#ds_configjson","title":"ds_config.json","text":"<pre><code>{\n    \"train_batch_size\": 16,\n    \"gradient_accumulation_steps\": 1,\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"reduce_bucket_size\": 4194304,\n        \"stage3_prefetch_bucket_size\": 3774873,\n        \"stage3_param_persistence_threshold\": 20480,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/deepspeed/llm-fine-tuning/llms-with-deepspeed/#trainpy","title":"train.py","text":"<pre><code>import os\nimport pandas as pd\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport torch\nfrom transformers import (\n    set_seed,\n    TrainingArguments,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n)\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\nfrom datasets import Dataset\n\n\nBASE_DIR = \"/home/ubuntu/my/deepspeed/llm_fine_tune\"\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: str = field(\n        default=os.path.join(BASE_DIR, 'llama-3B-instruct'),  # Set default model path here\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    lora_alpha: Optional[int] = field(default=16)\n    lora_dropout: Optional[float] = field(default=0.1)\n    lora_r: Optional[int] = field(default=64)\n    lora_target_modules: Optional[str] = field(\n        default=\"q_proj,k_proj,v_proj,o_proj,down_proj,up_proj,gate_proj\",\n        metadata={\"help\": \"Comma-separated list of target modules to apply LoRA layers to.\"},\n    )\n    use_flash_attn: Optional[bool] = field(\n        default=False, metadata={\"help\": \"Enables Flash attention for training.\"}\n    )\n\n\n@dataclass\nclass DataTrainingArguments:\n    dataset_path: str = field(\n        default=os.path.join(BASE_DIR, \"Customer_Support_Dataset.csv\"),\n        metadata={\"help\": \"Path to the CSV file containing training data.\"}\n    )\n    input_field: str = field(\n        default=\"instruction\",\n        metadata={\"help\": \"Field name for user input in the dataset.\"}\n    )\n    target_field: str = field(\n        default=\"response\",\n        metadata={\"help\": \"Field name for target responses in the dataset.\"}\n    )\n    max_seq_length: int = field(\n        default=180,\n        metadata={\"help\": \"Maximum sequence length for tokenization.\"}\n    )\n\n\ndef create_and_prepare_model(args, data_args):\n    # LoRA configuration\n    peft_config = LoraConfig(\n        lora_alpha=args.lora_alpha,\n        lora_dropout=args.lora_dropout,\n        r=args.lora_r,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=args.lora_target_modules.split(\",\")\n    )\n\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    # Load model\n    model = AutoModelForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        attn_implementation=\"flash_attention_2\" if args.use_flash_attn else \"eager\",\n    )\n    model.resize_token_embeddings(len(tokenizer))\n\n    return model, peft_config, tokenizer\n\n\ndef preprocess_data(df, tokenizer, data_args):\n    def tokenize_function(row):\n        inputs = tokenizer(\n            row[data_args.input_field],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=data_args.max_seq_length\n        )\n        targets = tokenizer(\n            row[data_args.target_field],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=data_args.max_seq_length\n        )\n        return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"],\n                \"labels\": targets[\"input_ids\"]}\n\n    # Adding instruction to training dataset\n    instruction = \"\"\"You are a top-rated customer service agent named John. \n    Be polite to customers and answer all their questions.\"\"\"\n\n    df[data_args.input_field] = df[data_args.input_field].apply(lambda x: instruction + str(x))\n\n    # Convert pandas DataFrame to Hugging Face Dataset\n    dataset = Dataset.from_pandas(df)\n\n    # Apply tokenization\n    tokenized_dataset = dataset.map(tokenize_function, batched=False)\n\n    return tokenized_dataset\n\n\ndef main():\n    # Define training arguments internally\n    training_args = TrainingArguments(\n        output_dir=\"output_dir\",  # Directory where the model and logs will be saved\n        overwrite_output_dir=True,\n        do_train=True,\n        per_device_train_batch_size=8,\n        num_train_epochs=3,\n        logging_dir=\"logs\",\n        logging_strategy=\"steps\",\n        logging_steps=500,\n        save_strategy=\"steps\",\n        save_steps=500,\n        save_total_limit=2,\n        evaluation_strategy=\"steps\",\n        eval_steps=500,\n        load_best_model_at_end=True,\n        seed=42,\n        deepspeed=\"ds_config.json\",  # Path to your DeepSpeed config file\n        fp16=True,\n    )\n\n    # Model and data arguments\n    model_args = ModelArguments()\n    data_args = DataTrainingArguments()\n\n    # Set seed for reproducibility\n    set_seed(training_args.seed)\n\n    # Load dataset\n    train_dataset = pd.read_csv(data_args.dataset_path)\n\n    # Taking sample of the dataset\n    train_dataset = train_dataset.sample(frac=1.0, random_state=65).reset_index(drop=True)\n    train_dataset = train_dataset.iloc[:10]\n\n    # Prepare model and tokenizer\n    model, peft_config, tokenizer = create_and_prepare_model(model_args, data_args)\n\n    # Tokenize data\n    tokenized_data = preprocess_data(train_dataset, tokenizer, data_args)\n\n    # Trainer setup\n    trainer = SFTTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=training_args,\n        train_dataset=tokenized_data,\n        eval_dataset=tokenized_data,\n        peft_config=peft_config,\n        max_seq_length=512\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Save the final model\n    trainer.save_model()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Intro To DeepSpeed | Next: Ray &gt; Interactive &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/","title":"Executing batch jobs in Ray Cluster","text":"<p>In this example we will: - Create a Ray cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"ray\" under your \"~/my\" folder</li> <li>And copy job.py under this folder</li> </ul> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/ray\"\n\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.ray,\n    job_dir = job_dir,\n    py_file = \"job.py\",\n    worker_count = 2,\n)\n\nworker_config = prt.WorkerConfig(\n    # Please note that Ray requires a specific worker image\n    worker_image=\"practicus-ray\",\n    worker_size=\"Medium\",\n    distributed_config=distributed_config,\n    log_level=\"DEBUG\",\n)\n\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n)\n</code></pre> <pre><code># You can view the logs during or after the job is completed\n# To view coordinator (master) set rank = 0\nrank = 0\n# To view other workers set rank = 1,2, ..\n\nprt.distributed.view_log(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n    rank=rank\n)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/#wrapping-up","title":"Wrapping up","text":"<ul> <li>Once the job is completed, you can view the results in <code>~/my/ray/result.csv/</code></li> <li>Please note that result.csv is a folder that can contain <code>parts of the processed file</code> by each worker (Ray executors)</li> <li>Also note that you do not need to terminate the cluster since it has a 'py_file' to execute, which defaults <code>terminate_on_completion</code> parameter to True.</li> <li>You can change terminate_on_completion to False to keep the cluster running after the job is completed to troubleshoot issues.</li> <li>You can view other <code>prt.DistJobConfig</code> properties to customize the cluster</li> </ul>"},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/ray/batch-job/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt\n\nray = prt.distributed.get_client()\n\n\n@ray.remote\ndef square(x):\n    return x * x\n\n\ndef calculate():\n    numbers = [i for i in range(10)]\n    futures = [square.remote(i) for i in numbers]\n    results = ray.get(futures)\n    print(\"Distributed square results of\", numbers, \"is\", results)\n\n\nif __name__ == \"__main__\":\n    calculate()\n    ray.shutdown()\n</code></pre> <p>Previous: Use Cluster | Next: Modin &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/start-cluster/","title":"Starting an interactive Ray Cluster","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Ray cluster, and execute simple Ray operations.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.ray,\n    worker_count = 2,\n)\n\n# Let's define worker features of the cluster \nworker_config = prt.WorkerConfig(\n    # Please note that Ray requires a specific worker image\n    worker_image=\"practicus-ray\",\n    worker_size=\"Medium\",\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker \n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Ray cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next example in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator \n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Llms With DeepSpeed | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/use-cluster/","title":"Using the interactive Ray Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Ray cluster we created, and execute simple Ray operations.</li> <li>Please run this example on the <code>Ray Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt \n\n# Let's get a Ray session.\n# this is similar to running `import ray` and then `ray.init()`\nray = prt.distributed.get_client()\n</code></pre> <pre><code>@ray.remote\ndef square(x):\n    return x * x\n\ndef calculate():\n    numbers = [i for i in range(10)]\n    futures = [square.remote(i) for i in numbers]\n    results = ray.get(futures)\n    print(\"Distributed square results of\", numbers, \"is\", results)\n\ncalculate()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/use-cluster/#ray-dashboard","title":"Ray Dashboard","text":"<p>Practicus AI Ray offers an interactive dashboard where you can view execution details. Let's open the dashboard.</p> <pre><code>dashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code>@ray.remote\ndef square(x):\n    return x * x\n\ndef calculate():\n    numbers = [i for i in range(10)]\n    futures = [square.remote(i) for i in numbers]\n    results = ray.get(futures)\n    print(\"Distributed square results of\", numbers, \"is\", results)\n\ncalculate()\n</code></pre> <p>Now you should see in real-time the execution details in a view similar to the below. You can click the Job tab for useful information.</p> <p></p> <pre><code># Let's close the session\nray.shutdown()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/interactive/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Batch Job &gt; Batch Job</p>"},{"location":"technical-tutorial/distributed-computing/ray/modin/start-cluster/","title":"Distributed Data processing with Modin and Ray","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Ray cluster, and execute simple modin + Ray operations.</li> <li>Although the example is interactive, you can apply the same for batch jobs as well.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/ray/modin/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.ray,\n    worker_count = 2,\n)\n\n# Let's define worker features of the cluster \nworker_config = prt.WorkerConfig(\n    # Please note that Ray requires a specific worker image\n    worker_image=\"practicus-ray\",\n    worker_size=\"Medium\",\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker \n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Ray cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/modin/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next example in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator \n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Batch Job | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/modin/use-cluster/","title":"Using the interactive Ray Cluster for Modin","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Ray cluster we created, and execute modin + Ray operations.</li> <li>Please run this example on the <code>Ray Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt \n\n# Let's get a Ray session.\n# this is similar to running `import ray` and then `ray.init()`\nray = prt.distributed.get_client()\n</code></pre> <pre><code># Modin aims to be a drop-in replacement for pandas\n# import pandas as pd\nimport modin.pandas as pd\n\ndf = pd.read_csv(\"/home/ubuntu/samples/airline.csv\")\n\nprint(\"DataFrame type is:\", type(df))\n\ndf[\"passengers\"] = df[\"passengers\"] * 2\n\ndf\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/modin/use-cluster/#ray-dashboard","title":"Ray Dashboard","text":"<p>Practicus AI Ray offers an interactive dashboard where you can view execution details. Let's open the dashboard.</p> <pre><code>dashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code>df[\"passengers\"] = df[\"passengers\"] * 2\n\ndf\n</code></pre> <pre><code># Let's close the session\nray.shutdown()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/modin/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Vllm &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/start-cluster/","title":"Distributed vLLM with Ray","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Ray cluster, and execute simple vLLM + Ray operations.</li> <li>Although the example is interactive, you can apply the same for batch jobs as well.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.ray,\n    worker_count = 2,\n)\n\n# Let's define worker features of the cluster \nworker_config = prt.WorkerConfig(\n    # Please note that this example requires GPUs\n    # Please note that Ray requires a specific worker image\n    worker_image=\"practicus-gpu-ray\",\n    worker_size=\"Medium-GPU\",\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker \n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Ray cluster,\n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next example in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator \n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Use Cluster | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/use-cluster/","title":"Using the interactive Ray Cluster for vLLM","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Ray cluster we created, and execute vLLM + Ray operations.</li> <li>Please run this example on the <code>Ray Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt \n\n# Let's get a Ray session.\n# this is similar to running `import ray` and then `ray.init()`\nray = prt.distributed.get_client()\n</code></pre> <pre><code>from vllm import LLM, SamplingParams\nprompts = [\n    \"Mexico is famous for \",\n    \"The largest country in the world is \",\n]\n\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\nllm = LLM(model=\"facebook/opt-125m\")\nresponses = llm.generate(prompts, sampling_params)\n\nfor response in responses:\n    print(response.outputs[0].text)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/use-cluster/#ray-dashboard","title":"Ray Dashboard","text":"<p>Practicus AI Ray offers an interactive dashboard where you can view execution details. Let's open the dashboard.</p> <pre><code>dashboard_url = prt.distributed.open_dashboard()\n\nprint(\"Page did not open? You can open this url manually:\", dashboard_url)\n</code></pre> <pre><code># Let's close the session\nray.shutdown()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/ray/vllm/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Unified DevOps &gt; Introduction</p>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/","title":"Batch Job","text":""},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#starting-a-batch-job-on-auto-scaled-spark-cluster","title":"Starting a Batch Job on auto-scaled Spark Cluster","text":"<p>This example demonstrates how to set up and run an auto-scaled batch job in Practicus AI. Instead of launching a fixed-size environment, we will create a batch job with the ability to automatically scale its compute resources based on demand.</p>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#important-note-on-worker-container-image","title":"Important note on worker container image","text":"<ul> <li>Unlike standard Spark cluster, auto-scaled Spark cluster executors have a separate type of container image <code>ghcr.io/practicusai/practicus-spark</code></li> <li>This means packages accessible to coordinator worker might not be accessible to the executors.</li> <li>To install packages please install to both the coordinator and the executor images and create custom container images.</li> <li>While creating the spark client, you can then pass arguments to specify which executor image to use.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#important-note-on-privileged-access","title":"Important note on privileged access","text":"<ul> <li>For auto-scaled Spark to work, <code>you will need additional privileges</code> on the Kubernetes cluster.</li> <li>Please ask your admin to grant you access to worker size definitions with privileged access before you continue with this example.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#finding-an-auto-scaled-privileged-worker-size","title":"Finding an Auto-Scaled (Privileged) Worker Size","text":"<p>Let's identify a worker size that supports auto-scaling and includes the required privileged capabilities for running batch jobs.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\n</code></pre> <pre><code>auto_dist_worker_size = None\n</code></pre> <p>If you don't know your auto-distributed (privileged) workers you can check them out by using the SDK like down below:</p> <pre><code>worker_size_list = region.worker_size_list\ndisplay(worker_size_list.to_pandas()) # Check auto_distributed col.\n</code></pre> <pre><code>assert auto_dist_worker_size, \"Please select an auto-distributed (privileged) worker sizes.\"\n</code></pre> <pre><code># Configure distributed job settings\n# This example uses Spark with auto-scaling capabilities.\ndistributed_config = prt.DistJobConfig(\n    job_type=prt.DistJobType.spark,\n    auto_distributed=True,   # Enables automatic scaling of the Spark cluster\n    initial_count=1,         # Start with 1 executor plus the coordinator (2 workers total)\n    max_count=4              # Allow the cluster to scale up to 4 additional executors if needed\n)\n\n# Define the worker configuration\n# Ensure that the chosen worker size includes privileged access\n# to support auto-scaling.\nworker_config = prt.WorkerConfig(\n    worker_size=auto_dist_worker_size,\n    distributed_config=distributed_config,\n)\n\n# Note: We are not creating a coordinator worker interactively here\n# since this setup is intended for batch tasks rather than interactive sessions.\n# Example: coordinator_worker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Running the batch job:\n# - Starts a worker\n# - Submits 'job.py' to run on the cluster\n# - 'job.py' creates a Spark session and triggers cluster creation \n#   with multiple executors as defined above.\n# - Monitors execution and prints progress\n# - On completion, terminates the Spark session and executors\nworker, success = prt.run_task(\n    file_name=\"job.py\",\n    worker_config=worker_config,\n    terminate_on_completion=False  # Leave the cluster running until we decide to terminate\n)\n</code></pre> <pre><code>if success:\n    print(\"Job is successful, terminating cluster.\")\n    worker.terminate()\nelse:\n    print(\"Job failed, opening notebook on coordinator to analyze.\")\n    worker.open_notebook()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/batch/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt\n\nprint(\"Requesting a Spark session...\")\nspark = prt.distributed.get_client()\n\n# Create a sample DataFrame\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\nprint(\"Creating DataFrame...\")\ndf = spark.createDataFrame(data, columns)\n\nprint(\"Applying filter: Age &gt; 30\")\ndf_filtered = df.filter(df.Age &gt; 30)\n\nprint(\"Filtered results:\")\ndf_filtered.show()\n\n# Note:\n# Auto-scaled Spark executors are different from standard Practicus AI workers.\n# They use a specialized container image and do not have direct access to\n# `~/my` or `~/shared` directories.\n# For saving results, consider using a data lake or object storage.\n</code></pre> <p>Previous: Use Cluster | Next: Dask &gt; Interactive &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/","title":"Starting an auto-scaled Spark Cluster","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Spark auto-scaled cluster, and execute simple Spark operations. </li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/#important-note-on-worker-container-image","title":"Important note on worker container image","text":"<ul> <li>Unlike standard Spark cluster, auto-scaled Spark cluster executors have a separate type of container image ghcr.io/practicusai/practicus-spark</li> <li>This means packages accessible to coordinator worker might not be accessible to the executors.</li> <li>To install packages please install to both the coordinator and the executor images and create custom container images.</li> <li>While creating the spark client, you can then pass arguments to specify which executor image to use.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/#important-note-on-privileged-access","title":"Important note on privileged access","text":"<ul> <li>For auto-scaled Spark to work, <code>you will need additional privileges</code> on the Kubernetes cluster.</li> <li>Please ask your admin to grant you access to worker size definitions with privileged access before you continue with this example.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/#finding-an-auto-scaled-privileged-worker-size","title":"Finding an Auto-Scaled (Privileged) Worker Size","text":"<p>Let's identify a worker size that supports auto-scaling and includes the required privileged capabilities for running batch jobs.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\n</code></pre> <pre><code>auto_dist_worker_size = None\n</code></pre> <p>If you don't know your auto-distributed (privileged) workers you can check them out by using the SDK like down below:</p> <pre><code>worker_size_list = region.worker_size_list\ndisplay(worker_size_list.to_pandas()) # Check auto_distributed col.\n</code></pre> <pre><code>assert auto_dist_worker_size, \"Please select an auto-distributed (privileged) worker sizes.\"\n</code></pre> <pre><code># Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.spark,\n    # ** The below changes the default cluster behavior **\n    auto_distributed=True,\n    # Set the initial size. \n    # These are 'additional` executors to coordinator, \n    # E.g. the below will create a cluster of 2 workers.\n    initial_count=1,\n    # Optional: set a maximum to auto-scale to, if needed.\n    # E.g. with the below, the cluster can scale up to 5 workers\n    max_count=4,\n)\n\n# Let's define worker features of the cluster \nworker_config = prt.WorkerConfig(\n    # Please make sure to use a worker size with\n    #   privileged access.\n    worker_size=auto_dist_worker_size,\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker:\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n# - The above will NOT create the executors instantly.\n# - You will only create one worker.\n# - Additional executors will be created when needed.\n</code></pre> <pre><code># Since this is an interactive Spark cluster, \n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next notebook in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator \n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Batch Job | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/use-cluster/","title":"Using the interactive Spark Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Spark cluster we created, and execute simple Spark operations.</li> <li>Please run this example on the <code>Spark Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt \n\n# Let's get a Spark session\nspark = prt.distributed.get_client()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/use-cluster/#behind-the-scenes","title":"Behind the scenes","text":"<ul> <li>After the above code, new Spark executors will start running.</li> <li>This is specific to auto-scaled Spark only and not the dfault behavior.</li> </ul> <pre><code># And execute some code\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Perform a transformation\ndf_filtered = df.filter(df.Age &gt; 30)\n\n# Show results\ndf_filtered.show()\n</code></pre> <pre><code># Explicitly delete spark session\nprt.engines.delete_spark_session()\n# Unlike the standard Spark cluster, the below won't work for auto-scaled.\n# spark.stop()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/auto-scaled/interactive/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <pre><code>coordinator_worker.terminate()\n</code></pre> <ul> <li>Or, terminate \"self\" and children workers with the below:</li> </ul> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Batch &gt; Batch Job</p>"},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/","title":"Executing batch jobs in Spark Cluster","text":"<p>In this example we will: - Create a Spark cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"spark\" under your \"~/my\" folder</li> <li>And copy job.py under this folder</li> </ul> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/spark\"\n\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.spark,\n    job_dir = job_dir,\n    py_file = \"job.py\",\n    worker_count = 2,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    distributed_config=distributed_config,\n    log_level=\"DEBUG\",\n)\n\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n)\n</code></pre> <pre><code># You can view the logs during or after the job is completed\n# To view coordinator (master) set rank = 0\nrank = 0\n# To view other workers set rank = 1,2, ..\n\nprt.distributed.view_log(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n    rank=rank\n)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/#wrapping-up","title":"Wrapping up","text":"<ul> <li>Once the job is completed, you can view the results in <code>~/my/spark/result.csv/</code></li> <li>Please note that result.csv is a folder that contains <code>parts of the processed file</code> by each worker (Spark executors)</li> <li>Also note that you do not need to terminate the cluster since it has a 'py_file' to execute, which defaults <code>terminate_on_completion</code> parameter to True.</li> <li>You can change terminate_on_completion to False to keep the cluster running after the job is completed to troubleshoot issues.</li> <li>You can view other <code>prt.DistJobConfig</code> properties to customize the cluster</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/distributed-computing/spark/batch-job/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt \n\n# Let's get a Spark session\nprint(\"Getting Spark session\")\nspark = prt.distributed.get_client()\n\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\nprint(\"Creating DataFrame\")\ndf = spark.createDataFrame(data, columns)\n\nprint(\"Calculating\")\ndf_filtered = df.filter(df.Age &gt; 30)\n\nprint(\"Writing to csv\")\ndf_filtered.write.csv(\"/home/ubuntu/my/spark/result.csv\", header=True, mode=\"overwrite\")\n</code></pre> <p>Previous: Use Cluster | Next: Auto Scaled &gt; Interactive &gt; Start Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/start-cluster/","title":"Starting an interactive Spark Cluster","text":"<ul> <li>This example demonstrates how to create, and connect to a Practicus AI Spark cluster, and execute simple Spark operations.</li> </ul>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/start-cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accessible by multiple workers, such as Practicus AI <code>~/my</code> or <code>~/shared</code> folders.</li> <li>If you do not have access to ~/my or ~/shared folders, please check the auto-scaled examples which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.spark,\n    worker_count = 2,\n)\n\n# Let's define worker features of the cluster \nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker \n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Spark cluster, \n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/start-cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next notebook in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator \n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Introduction | Next: Use Cluster</p>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/use-cluster/","title":"Using the interactive Spark Cluster Client","text":"<ul> <li>This example demonstrates how to connect to the Practicus AI Spark cluster we created, and execute simple Spark operations.</li> <li>Please run this example on the <code>Spark Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt \n\n# Let's get a Spark session\nspark = prt.distributed.get_client()\n</code></pre> <pre><code># And execute some code\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Perform a transformation\ndf_filtered = df.filter(df.Age &gt; 30)\n\n# Show results\ndf_filtered.show()\n</code></pre> <pre><code># Let's end the session\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/distributed-computing/spark/interactive/use-cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Batch Job &gt; Batch Job</p>"},{"location":"technical-tutorial/extras/data-analysis/eda/analyze/","title":"EDA Sample","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.style as plt_styl\n\nimport warnings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 30)\npd.set_option('display.width', 150)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\nwarnings.simplefilter(action = \"ignore\")\n</code></pre> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n\nproc = worker.load(data_set_conn, engine='AUTO') \n\ndf = proc.get_df_copy()\ndisplay(df)\n</code></pre> <pre><code>df.info()\n</code></pre> <pre><code>def grab_col_names(dataframe, cat_th=10, car_th=25, show_date=False):\n    date_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"datetime64[ns]\"]\n    cat_cols = dataframe.select_dtypes([\"object\", \"category\"]).columns.tolist()\n    num_but_cat = [col for col in dataframe.select_dtypes([\"float\", \"integer\"]).columns if dataframe[col].nunique() &lt; cat_th]\n    cat_but_car = [col for col in dataframe.select_dtypes([\"object\", \"category\"]).columns if dataframe[col].nunique() &gt; car_th]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    num_cols = dataframe.select_dtypes([\"float\", \"integer\"]).columns\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'date_cols: {len(date_cols)}')\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n\n    if show_date == True:\n        return date_cols, cat_cols, cat_but_car, num_cols, num_but_cat\n    else:\n        return cat_cols, cat_but_car, num_cols, num_but_cat\n</code></pre> <pre><code>grab_col_names(df)\n</code></pre> <pre><code>cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df)\n</code></pre> <pre><code>df.head()\n</code></pre> <pre><code>df[(df[\"region\"]==3)]\n</code></pre> <pre><code>print(cat_cols)\n</code></pre> <pre><code>def cat_analyzer(dataframe, variable, target = None):\n    print(variable)\n    if target == None:\n        print(pd.DataFrame({\n            \"COUNT\": dataframe[variable].value_counts(),\n            \"RATIO\": dataframe[variable].value_counts() / len(dataframe)}), end=\"\\n\\n\\n\")\n    else:\n        temp = dataframe[dataframe[target].isnull() == False]\n        print(pd.DataFrame({\n            \"COUNT\":dataframe[variable].value_counts(),\n            \"RATIO\":dataframe[variable].value_counts() / len(dataframe),\n            \"TARGET_COUNT\":dataframe.groupby(variable)[target].count(),\n            \"TARGET_MEAN\":temp.groupby(variable)[target].mean(),\n            \"TARGET_MEDIAN\":temp.groupby(variable)[target].median(),\n            \"TARGET_STD\":temp.groupby(variable)[target].std()}), end=\"\\n\\n\\n\")\n</code></pre> <pre><code>cat_analyzer(df, 'region') \n</code></pre> <pre><code>df[num_cols].hist(figsize = (25,20), bins=15);\n</code></pre> <pre><code>df[num_cols].describe([0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.95, 0.99]).T.drop(['count'], axis=1)\n</code></pre> <pre><code>def outliers_threshold(dataframe, column):\n    q1 = dataframe[column].quantile(0.05)\n    q3 = dataframe[column].quantile(0.95)\n    inter_quartile_range = q3 - q1\n    low = q1 - 1.5 * inter_quartile_range\n    up = q3 + 1.5 * inter_quartile_range\n    return low, up\n\ndef grab_outlier(dataframe, column, index=False):\n    low, up = outliers_threshold(dataframe, column)\n    if dataframe[(dataframe[column] &lt; low) |\n                 (dataframe[column] &gt; up)].shape[0] &lt; 10:\n        print(dataframe[(dataframe[column] &lt; low) | (dataframe[column] &gt; up)][[column]])\n    else:\n        print(dataframe[(dataframe[column] &lt; low) |\n                 (dataframe[column] &gt; up)][[column]])\n    if index:\n        outlier_index = dataframe[(dataframe[column] &lt; low) |\n                                  (dataframe[column] &gt; up)].index.tolist()\n        return outlier_index\n\ndef replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outliers_threshold(dataframe, col_name)\n    if low_limit &gt; 0:\n        dataframe.loc[(dataframe[col_name] &lt; low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] &gt; up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] &gt; up_limit), col_name] = up_limit\n</code></pre> <pre><code>df[df['age'] &lt;= 64]['age'].plot(kind='box')\n</code></pre> <pre><code>for col in num_cols:\n        print('********************************************************************* {} *****************************************************************************'.format(col.upper()))\n        grab_outlier(df, col, True)\n        replace_with_thresholds(df, col)\n        print('****************************************************************************************************************************************************************', end='\\n\\n\\n\\n\\n')\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>cat_cols\n</code></pre> <pre><code>df.head()\n</code></pre> <pre><code>plt.figure(figsize=(30,20))\ncorr_matrix = df.select_dtypes(include=['int64', 'int32', 'float64']).corr()\nsns.heatmap(corr_matrix, annot=True, cmap='Reds')\nplt.title('Correlation Heatmap')\n</code></pre> <pre><code>def cat_summary(dataframe, x_col, plot=False, rotation=45):\n    display(pd.DataFrame({x_col: dataframe[x_col].value_counts(),\n                          \"Ratio\": 100 * dataframe[x_col].value_counts() / len(dataframe)}))\n\n    if plot:\n        count = dataframe.groupby(x_col).size().sum()\n        dataframe_grouped = dataframe.groupby(x_col).size().reset_index(name='counts').sort_values('counts', ascending=False)\n        num_bars = len(dataframe_grouped[x_col].unique())\n        colors = plt.cm.Set3(np.linspace(0, 1, num_bars))\n        fig, ax = plt.subplots(figsize=(8, 5))\n\n        x_pos = range(len(dataframe_grouped[x_col]))\n\n        ax.bar(x_pos, dataframe_grouped['counts'], color=colors)\n        ax.set_xlabel(x_col)\n        ax.set_ylabel('Count')\n        ax.set_title(f'Distribution by {x_col}')\n\n        ax.set_xticks(x_pos)\n        ax.set_xticklabels(dataframe_grouped[x_col], rotation=rotation)\n\n        for i, value in enumerate(dataframe_grouped['counts']):\n            ax.annotate('{:.1%}'.format(value / count), (i, value), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\n        plt.show()\n</code></pre> <pre><code>for col in cat_cols:\n    cat_summary(df, col, plot=True)\n</code></pre> <pre><code>proc.kill()\n</code></pre> <p>Previous: Multiple Layers | Next: Data Processing &gt; Pre Process Data &gt; Preprocess</p>"},{"location":"technical-tutorial/extras/data-analysis/plot/introduction/","title":"Introduction to plotting with Bokeh","text":"<p>In this example we are going to give you a brief tutorial on how to use color and size feature of glyphs (a.k.a graphs and plots) dynamically by assigning features of dataset to color and size parameters.</p> <ul> <li>How to create figure</li> <li>How to create and edit circle plots </li> <li>How to add dynamic explanations over glyphs</li> </ul>"},{"location":"technical-tutorial/extras/data-analysis/plot/introduction/#before-you-begin","title":"Before you begin","text":"<ul> <li>At the time of this writing, Bokeh only support <code>Jupyter Lab</code> and not <code>VS Code</code></li> <li>View issue details</li> <li>Please make sure you run this example in Jupyter Lab, or verify on Bokeh repo that the issue is resolved.</li> </ul> <pre><code>from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import LinearColorMapper, ColumnDataSource, ColorBar, HoverTool\nfrom bokeh.transform import transform\nimport practicuscore as prt\n</code></pre> <p>Here's a breakdown of each bokeh function we've imported:</p> <ol> <li>bokeh.plotting:<ul> <li>figure: This module provides a high-level interface for creating Bokeh plots. It includes functions for creating and customizing figures, such as setting titles, axis labels, plot size, and other visual properties.</li> <li>show: This function displays a Bokeh plot in the current environment, such as a browser or a Jupyter notebook.</li> <li>output_notebook: This function configures Bokeh to display plots directly within Jupyter notebooks.</li> </ul> </li> <li>bokeh.models:<ul> <li>ColumnDataSource: This module contains a collection of classes and functions representing various components of a Bokeh plot, such as glyphs (shapes representing data points), axes, grids, annotations, and tools. ColumnDataSource is a fundamental data structure in Bokeh that holds the data to be plotted and allows for efficient updating and sharing of data between different plot elements.</li> <li>HoverTool: This module provides a tool for adding interactive hover tooltips to Bokeh plots. It allows users to display additional information about data points when the mouse cursor hovers over them.</li> <li>LinearColorMapper: A mapper that maps numerical data to colors in a linear manner. It's often used to color glyphs based on a continuous range of data values.</li> <li>ColorBar: A color bar that provides a visual representation of the mapping between data values and colors, often used with LinearColorMapper</li> </ul> </li> <li>bokeh.transform:<ul> <li>transform: This function is used to apply a transformation to the data in a column of a ColumnDataSource. It takes two arguments, <ul> <li>column_name: The name of the column in the ColumnDataSource to transform.</li> <li>transform_expression: A JavaScript expression defining the transformation to apply to the data. This expression can involve mathematical operations, functions, or other JavaScript constructs.</li> </ul> </li> </ul> </li> </ol> <pre><code>worker = prt.get_local_worker()\n</code></pre> <p>One of the most illustrative datasets for demonstrating dynamic size and color is the Titanic dataset.</p> <p>The Titanic dataset is a popular dataset used in machine learning and data analysis. It contains information about passengers aboard the RMS Titanic, including whether they survived or not. Within this data set we will use columns of pclass, fare, age and survived. Let's describe what these columns means for better understanding.</p> <ul> <li>Pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).</li> <li>Fare: Passenger fare.</li> <li>Age: Passenger's age in years.</li> <li>Survived: Indicates whether the passenger survived or not (0 = No, 1 = Yes).</li> </ul> <p>Let's load it into one of our worker environments.</p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"sample_size\": 1180,\n    \"file_path\": \"/home/ubuntu/samples/titanic.csv\"\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine='AUTO') \n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>To display Bokeh plots inline in a classic Jupyter notebook, use the output_notebook() function from bokeh.io.</p> <pre><code>output_notebook()\n</code></pre> <p>We need to create a data structure that holds the data to be plotted to use Bokeh more efficiently. For this we will use ColumnDataSource() function of Bokeh.</p> <pre><code>source = ColumnDataSource(df)\n</code></pre> <p>We need to create a mapper of colors to use color feature as dynamically. We will use LinearColorMapper() built-in function of Bokeh to create our mapper.</p> <pre><code>color_mapper = LinearColorMapper( \n    palette='Sunset11', \n    low=df['survived'].min(), \n    high=df['survived'].max()\n)\n</code></pre> <p>Here's a breakdown of the parameters:</p> <ul> <li>palette: This parameter specifies the color palette to use for mapping the data values. In this case, it's set to 'Sunset11', which is refers to a predefined color palette named 'Sunset11' of Bokeh. This palette consists of 11 distinct colors arranged in a gradient from light to dark or from one color to another. You can look at Bokeh's documentation to see more options.</li> <li>low: This parameter sets the lowest data value in the range of values to be mapped to colors. It's typically set to the minimum value of the data being mapped. In this case, it's set to df['survived'].min(), indicating that the lowest value in the 'survived' column of the DataFrame (df) will be mapped to the lowest color in the palette.</li> <li>high: This parameter sets the highest data value in the range of values to be mapped to colors. It's typically set to the maximum value of the data being mapped. Here, it's set to df['survived'].max(), indicating that the highest value in the 'survived' column of the DataFrame (df) will be mapped to the highest color in the palette.</li> </ul> <p>Let's create our figure to do some visualisation.</p> <pre><code>p = figure(title=\"Analysis Over Survivors\", x_axis_label='age', y_axis_label='fare', width=600, height=400)\n</code></pre> <p>Here's an explanation of each parameter in the figure function call:</p> <ul> <li>title: Sets the title of the plot. In this case, it's set to \"Analysis Over Survivors\". The title is displayed at the top of the plot.</li> <li>x_axis_label: Specifies the label for the x-axis. It provides information about the data represented on the x-axis. In this case, it's set to 'age', indicating that the x-axis represents age of travellers.</li> <li>y_axis_label: Specifies the label for the y-axis. Similar to x_axis_label, it provides information about the data represented on the y-axis. Here, it's set to 'fare', indicating that the y-axis represents paid fares of travellers.</li> <li>width: Sets the width of the plot in pixels. In this case, it's set to 400 pixels, determining the horizontal size of the plot.</li> <li>height: Sets the height of the plot in pixels. Here, it's set to 300 pixels, determining the vertical size of the plot.</li> </ul> <p>Let's continue with our first plotting of circle (circle plotting)!</p> <pre><code>circle = p.circle(x='age', y='fare', radius='pclass', color=transform('survived', color_mapper), alpha=0.5, source=source)\nshow(p)\n</code></pre> <p>Here's an explanation of each parameter:</p> <ul> <li>p: This is the figure object where the circles will be added. It seems like p is previously defined as a figure with certain settings like title, axis labels, etc.</li> <li>circle: This variable stores the result of the p.circle function call, representing the circles added to the plot.</li> <li>x: This parameter specifies the x-coordinates of the circles. It's mapped to the 'age' column in the data source (source), indicating the position of each circle along the x-axis.</li> <li>y: This parameter specifies the y-coordinates of the circles. It's mapped to the 'fare' column in the data source (source), indicating the position of each circle along the y-axis.</li> <li>radius: This parameter specifies the radius of the circles. It's mapped to the 'pclass' column in the data source (source), indicating the radius of each circle.</li> <li>color: This parameter specifies the color of the circles. Here, it's set to transform('survived', color_mapper).</li> <li>transform('survived', color_mapper): This function applies the color mapping defined by the LinearColorMapper object (color_mapper) to the 'survived' column in the data source (source). The color of each circle will be determined by the value in the 'survived' column, mapped to colors based on the color_mapper.</li> <li>alpha: This parameter sets the transparency of the circles. It's set to 0.5, making the circles partially transparent.</li> <li>source: This parameter specifies the data source from which the circles will pull their data. It's set to source, which is likely a ColumnDataSource object containing the data needed to plot the  circles.</li> </ul> <p>At this point we should add a bar which describes the meaning of colors. We could do this by using ColorBar() feature of Bokeh.</p> <pre><code>color_bar = ColorBar(color_mapper=color_mapper, padding=3,\n                         ticker=p.xaxis.ticker, formatter=p.xaxis.formatter)\n\np.add_layout(color_bar, 'right')\nshow(p)\n</code></pre> <p>Here's an explanation of each parameter:</p> <ul> <li>color_mapper: This parameter specifies the LinearColorMapper object (color_mapper) that defines the mapping between data values and colors. The color bar will use this mapper to display the range of colors corresponding to the range of data values.</li> <li>padding: This parameter sets the padding (in pixels) between the color bar and other elements of the plot. It's set to 3 pixels in this case, providing some space around the color bar.</li> <li>ticker: This parameter specifies the ticker to use for labeling the color bar axis. It's set to p.xaxis.ticker, which likely means that the same ticker used for the x-axis of the plot (p) will be used for the color bar axis.</li> <li>formatter: This parameter specifies the formatter to use for formatting the tick labels on the color bar axis. It's set to p.xaxis.formatter, meaning that the same formatter used for the x-axis of the plot (p) will be used for the color bar axis.</li> <li>p.add_layout: This method adds a layout element to the plot (p). Here, we're adding the color bar to the plot.</li> <li>color_bar: This is the ColorBar object that we created earlier, representing the color bar to be added to the plot.</li> <li>'right': This parameter specifies the location where the color bar will be added relative to the plot. Here, it's set to 'right', indicating that the color bar will be placed to the right of the plot.</li> </ul> <p>We still missing something, it would be a cool feature if we could see the values of data points. Actually, we could use HoverTool() bokeh to do that!</p> <pre><code>tips = [\n    ('Fare', '@fare'),\n    ('Age', '@age'),\n    ('Survived', '@survived'),\n    ('Pclass', '@pclass')\n]\n\np.add_tools(HoverTool(tooltips=tips)) \nshow(p)\n</code></pre> <p>Right now, when we hover over plots we could see the values of data points.</p> <p>Let's break down the code we have used:</p> <ul> <li>tips: This is a list of tuples, where each tuple contains two elements. The first element of each tuple represents the label for the tooltip, and the second element represents the data field from the data source (source) to be displayed in the tooltip. For example, Fare is the label for the tooltip, and @fare instructs Bokeh to display the value of the fare column from the data source (source) when hovering over a data point</li> <li>p.add_tools: This method adds tools to the plot (p). Here, we're adding the HoverTool to enable hover tooltips.</li> <li>HoverTool: This is a tool provided by Bokeh for adding hover functionality to plots. It displays additional information when the mouse cursor hovers over a data point.</li> <li>tooltips=tips: This parameter of the HoverTool constructor specifies the tooltips to be displayed when hovering over data points. We pass the tips list, which contains the tooltip labels and data fields.</li> </ul> <p>That was the end. You can always checkout our other notebooks about plotting or documentation of bokeh to see more!</p> <pre><code>proc.kill()\n</code></pre> <p>Previous: Mail E-Assistant | Next: Multiple Layers</p>"},{"location":"technical-tutorial/extras/data-analysis/plot/multiple-layers/","title":"Plotting with Multiple Layers","text":"<p>In this example we are going to give you a brief tutorial of how to use multiple graphics on one figure by using bokeh.</p> <p>We are going to cover these topics: - How to create and edit a figure - How to process multiple glyphs (graphs) over a figure - How to add dynamic explanations over glyphs - How to create Bar and Line plot</p>"},{"location":"technical-tutorial/extras/data-analysis/plot/multiple-layers/#before-you-begin","title":"Before you begin","text":"<p>At the time of this writing, Bokeh only support <code>Jupyter Lab</code> and not <code>VS Code</code> View issue details Please make sure you run this example in Jupyter Lab, or verify on Bokeh repo that the issue is resolved.</p> <p>Let's begin by importing our libraries</p> <pre><code>from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import ColumnDataSource, HoverTool\nimport practicuscore as prt\n</code></pre> <p>Here's a breakdown of each bokeh function we've imported:</p> <ol> <li>bokeh.plotting<ul> <li>figure: Creates a new Bokeh plot with customizable options such as plot size, title, axis labels, etc.</li> <li>show: Displays a Bokeh plot in the current environment, such as a browser or a Jupyter notebook.</li> <li>output_notebook: Configures Bokeh to display plots directly in Jupyter notebooks.</li> </ul> </li> <li>bokeh.models<ul> <li>ColumnDataSource: A data structure that holds the data to be plotted and facilitates efficient updating and sharing of data between different plot elements.</li> <li>HoverTool: A tool that provides interactive hover tooltips, displaying additional information about data points when the mouse cursor hovers over them</li> </ul> </li> </ol> <pre><code>worker = prt.get_local_worker()\n</code></pre> <p>One of the most illustrative datasets for demonstrating multiple layer analyze is the Iris dataset.</p> <p>The Iris dataset is a popular dataset in machine learning and statistics, often used for classification tasks. It consists of 150 samples of iris flowers, each belonging to one of three species: Setosa, Versicolor, or Virginica. Within this data set we will use both Bar and Circle graphic style from Plot. The dataset comprises four features, each representing measurements of the length and width of both the petals and sepals of flowers.</p> <p>Let's load it into one of our worker environments.</p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"sample_size\": 150,\n    \"file_path\": \"/home/ubuntu/samples/iris.csv\"\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine='AUTO') \n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Before starting data analyzing with bokeh, we need to do some preprocess on Iris dataset</p> <pre><code>means = df.groupby('species').mean().reset_index()\n</code></pre> <pre><code>from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\nmeans['species_encoded'] = label_encoder.fit_transform(means['species'])\n</code></pre> <pre><code>means\n</code></pre> <p>To display Bokeh plots inline in a classic Jupyter notebook, use the output_notebook() function from bokeh.io.</p> <pre><code>output_notebook()\n</code></pre> <p>We need to create a data structure that holds the data to be plotted to use Bokeh more efficiently. For this we will use ColumnDataSource() function of Bokeh.</p> <pre><code>source = ColumnDataSource(means)\n</code></pre> <p>Let's create our figure to do some visualisation.</p> <pre><code>p = figure(title=\"Analysis Over Species\", x_axis_label='Species', y_axis_label='Features', width=400, height=300)\n</code></pre> <p>Here's an explanation of each parameter in the figure function call:</p> <ul> <li>title: Sets the title of the plot. In this case, it's set to \"Analysis Over Species\". The title is displayed at the top of the plot.</li> <li>x_axis_label: Specifies the label for the x-axis. It provides information about the data represented on the x-axis. In this case, it's set to 'Species', indicating that the x-axis represents different species.</li> <li>y_axis_label: Specifies the label for the y-axis. Similar to x_axis_label, it provides information about the data represented on the y-axis. Here, it's set to 'Features', indicating that the y-axis represents various features.</li> <li>width: Sets the width of the plot in pixels. In this case, it's set to 400 pixels, determining the horizontal size of the plot.</li> <li>height: Sets the height of the plot in pixels. Here, it's set to 300 pixels, determining the vertical size of the plot.</li> </ul> <p>Let's continue with our first plotting of vbar (vertical bar plotting)!</p> <pre><code>first_layer = p.vbar(x = 'species_encoded', top = 'sepal_length', width=0.9, line_color='green', fill_color='lime', fill_alpha=0.5, legend_label=\"Sepal Length\", source=source)\nshow(p)\n</code></pre> <p>Here's an explanation of each vbar parameters:</p> <ul> <li>p: This is the figure object where the plot will be added.</li> <li>vbar: This is the glyph function used to create vertical bar glyphs (rectangles) on the plot.</li> <li>x: This parameter specifies the x-coordinates of the bars.</li> <li>top: This parameter specifies the top edge of each bar.</li> <li>width: This parameter determines the width of the bars. Here, it's set to 0.9, indicating that the bars will have a width of 0.9 units along the x-axis.</li> <li>line_color: This parameter sets the color of the outline of the bars. It's set to 'green', giving the bars a green outline.</li> <li>fill_color: This parameter sets the fill color of the bars. It's set to 'lime', giving the bars a lime green color.</li> <li>fill_alpha: This parameter sets the transparency of the fill color. It's set to 0.5, making the bars partially transparent.</li> <li>legend_label: This parameter sets the label for the legend entry corresponding to this glyph. It's set to \"Sepal Length\", which will be displayed in the legend.</li> <li>source: This parameter specifies the data source from which the glyph will pull its data. It's set to source, which is defined previously by using ColumnDataSource().</li> </ul> <p>Overall, this line of code creates a vertical bar plot of sepal lengths for different species, with customization for appearance and legend labeling, and it adds this plot as a layer to the existing figure p. You can check out bokeh documentation of colors for more coloring options.</p> <p>Let's add another vbar, a second layer, which will show case petal_length.</p> <pre><code>second_layer = p.vbar(x = 'species_encoded', top = 'petal_length', width=0.9, line_color='blue', fill_color='lightskyblue', fill_alpha=0.5, legend_label=\"Petal Length\", source=source)\nshow(p)\n</code></pre> <p>After adding our second layer it started to be too crowded for such a small figure frame, let's expand it and we could also use some more explanatory labels for axis.</p> <pre><code>p.width = 800\np.height = 600\np.xaxis.axis_label = 'Flower Species'\np.yaxis.axis_label = 'Flower Features'\n\nshow(p)\n</code></pre> <p>Let's add some plots about our flowers width, but if we add more bars it will make the figure too confusing to read. Therefore, let's add line plots to visualize width features of our flowers.</p> <pre><code>third_layer = p.line(x='species_encoded', y='sepal_width', line_width=4, line_color='darkolivegreen', legend_label=\"Sepal Width\", source=source)\nshow(p)\n</code></pre> <p>Here's an explanation of each line parameters:</p> <ul> <li>p: This is the figure object where the line plot will be added.</li> <li>line: This is the glyph function used to create line glyphs on the plot.</li> <li>x: This parameter specifies the x-coordinates of the line.</li> <li>y: This parameter specifies the y-coordinates of the line.</li> <li>line_width: This parameter determines the width of the line. It's set to 4, indicating that the line will be drawn with a width of 4 units.</li> <li>line_color: This parameter sets the color of the line. It's set to 'darkolivegreen', giving the line a dark olive green color.</li> <li>legend_label: This parameter sets the label for the legend entry corresponding to this glyph. It's set to \"Sepal Width\", which will be displayed in the legend.</li> <li>source: This parameter specifies the data source from which the glyph will pull its data.  It's set to source, which is defined previously by using ColumnDataSource().</li> </ul> <p>Overall, this line of code creates a line plot of sepal widths for different species, with customization for appearance and legend labeling, and it adds this plot as a layer to the existing figure p.</p> <p>Let's add our fourth and last plot!</p> <pre><code>fourth_layer = p.line(x='species_encoded', y='petal_width', line_width=4, line_color='darkblue', legend_label=\"Petal Width\", source=source)\nshow(p)\n</code></pre> <p>After adding all layer we still missing something, it would be a cool feature if we could see the values of data points. Actually, we could use HoverTool() bokeh to do that!</p> <pre><code>tips = [\n    ('Sepal Length', '@sepal_length'),\n    ('Petal Length', '@petal_length'),\n    ('Sepal Width', '@sepal_width'),\n    ('Petal Width', '@petal_width')\n]\n\np.add_tools(HoverTool(tooltips=tips)) \nshow(p)\n</code></pre> <p>Right now, when we hover over plots we could see the values of data points.</p> <p>Let's break down the code we have used:</p> <ul> <li>tips: This is a list of tuples, where each tuple contains two elements. The first element of each tuple represents the label for the tooltip, and the second element represents the data field from the data source (source) to be displayed in the tooltip. For example, Sepal Length is the label for the tooltip, and @sepal_length instructs Bokeh to display the value of the sepal_length column from the data source (source) when hovering over a data point</li> <li>p.add_tools: This method adds tools to the plot (p). Here, we're adding the HoverTool to enable hover tooltips.</li> <li>HoverTool: This is a tool provided by Bokeh for adding hover functionality to plots. It displays additional information when the mouse cursor hovers over a data point.</li> <li>tooltips=tips: This parameter of the HoverTool constructor specifies the tooltips to be displayed when hovering over data points. We pass the tips list, which contains the tooltip labels and data fields.</li> </ul> <p>Last but not least, we could remove layers by using their visible parameter:</p> <pre><code>first_layer.visible = False\nshow(p)\n</code></pre> <pre><code>first_layer.visible = True\nshow(p)\n</code></pre> <p>That was the end. You can always checkout our other notebooks about plotting or documentation of bokeh to see more!</p> <pre><code>proc.kill()\n</code></pre> <p>Previous: Introduction | Next: Eda &gt; Analyze</p>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/","title":"Data Processing","text":""},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#scenario-pre-process-steps-by-using-sdk","title":"Scenario: Pre-process steps by using SDK","text":"<p>In this example, we'll showcase how to apply pre-process steps by using our SDK, practicuscore.</p> <ol> <li> <p>Loading the \"income\" dataset</p> </li> <li> <p>Profiling the dataset</p> </li> <li> <p>Applying pre-process steps:</p> <ul> <li>Suppressing outliers by using snippets</li> <li>Applying one hot encoding</li> <li>Applying label encoding</li> <li>Re-naming columns</li> <li>Deleting columns</li> <li>Applying standardization by using snippets</li> </ul> </li> </ol>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#step-1-loading-the-dataset","title":"Step-1: Loading the Dataset","text":"<pre><code>dataset_conn ={\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/income.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(dataset_conn)\n\nproc.show_head()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#step-2-profiling-the-dataset","title":"Step-2: Profiling the dataset","text":"<p>\"ydata_profiling\" Python library is a powerful tool for data analysts and data scientists to analyze data sets quickly and effectively. </p> <p>The ProfileReport method of this library performs a thorough inspection of a data frame and generates a detailed profile report. This report provides comprehensive summaries of the dataset's overall statistics, missing values, distributions, correlations and other important information. With the report, users can quickly identify potential problems and patterns in the data set, which greatly speeds up and simplifies the data cleaning and pre-processing phases.</p> <pre><code>from ydata_profiling import ProfileReport\n</code></pre> <pre><code>df_raw = proc.get_df_copy()\n</code></pre> <pre><code>ProfileReport(df_raw)\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#step-3-pre-process","title":"Step-3: Pre-process","text":""},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#31-handling-with-missing-values","title":"3.1: Handling with missing values","text":"<p>The 'handle_missing' method of the SDK can be utilized to fill or drop missing values on target columns.</p> <ul> <li>technique: The method which used in handling missing value. It could take the values down below:<ul> <li>'delete': drops the rows with missing values</li> <li>'custom': filling the missing values with a custom value</li> <li>'minimum': filling the missing values with minunmum value of column</li> <li>'maximum': filling the missing values with maximum value of column</li> <li>'average': filling the missing values with average value of column</li> </ul> </li> <li>column_list: List of targeted columns (columns with missing values)</li> <li>custom_value: The value which will be used in filling columns, if not using 'custom' method leave it to be 'None'</li> </ul> <pre><code>proc.handle_missing(technique='minimum', column_list=['workclass'], custom_value='None')\nproc.handle_missing(technique='custom', column_list=['native-country'], custom_value='unknown')\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#32-suppressing-of-outliers-by-using-snippets","title":"3.2: Suppressing of outliers by using snippets","text":"<p>Snippets are built-in python functions prepared by Practicus AI but, also you can build your own snippets for your company (for more information please visit https://docs.practicus.ai/tutorial)</p> <p>To utilize snippets effectively, ensure that you create and open a folder named 'snippets' within your working directory. Then, place the snippet files into this designated folder.</p> <p>Every snippets has parameters which are optional or mandatory to run. You can checkout the parameters within the snippet code.</p> <p>E.g. the paramaters within 'suppress_outliers' can be listed as: - outlier_float_col_list: list[str] | None (List of numeric columns to check for outliers. If left empty, applies to all numeric columns.), - q1_percentile: float = 0.25 (Custom percentile for Q1, takes 0.25 as default value), - q3_percentile: float = 0.75 (Custom percentile for Q3, takes 0.75 as default value), - result_col_suffix: str | None = \"no_outlier\" (suffix for the new column where the suppressed data will be stored, takes \"no_outlier\" as default), - result_col_prefix: str | None = None (Prefix for the new column where the suppressed data will be stored.),</p> <pre><code>proc.run_snippet( \n    'suppress_outliers', \n    outlier_float_col_list=[\"capital-gain\", \"capital-loss\"],\n    q1_percentile = 0.05,\n    q3_percentile = 0.95\n)\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#33-one-hot-encoding","title":"3.3: One-hot Encoding","text":"<p>The 'one_hot' method of the SDK can be utilized to apply one-hot encoding to the selected column.</p> <ul> <li>column_name: The name of the current column to be one-hot encoded.</li> <li>column_prefix: A prefix to use for the new one-hot encoded columns.</li> </ul> <pre><code>cat_col_list = [\n    'workclass', \n    'education', \n    'marital-status', \n    'occupation', \n    'relationship', \n    'native-country'\n]\n</code></pre> <pre><code>for col in cat_col_list:\n    proc.one_hot(column_name=col, column_prefix=col)\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#34-label-encoding","title":"3.4: Label Encoding","text":"<p>The 'categorical_map' method of the SDK can be utilized to apply label encoding to the selected column.</p> <ul> <li>column_name: The name of the current column to be one-hot encoded.</li> <li>column_prefix: A prefix to use for the new one-hot encoded columns.</li> </ul> <pre><code>proc.categorical_map(column_name='sex', column_suffix='cat')\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#35-re-naming-columns","title":"3.5: Re-naming Columns","text":"<p>The 'rename_column' method of the SDK can be utilized to rename columns.</p> <pre><code>proc.rename_column('hours-per-week', 'hours_per_week')\n</code></pre> <pre><code>proc.rename_column('capital-loss', 'capital_loss')\n</code></pre> <pre><code>proc.rename_column('capital-gain', 'capital_gain')\n</code></pre> <pre><code>proc.rename_column('education-num', 'education_num')\n</code></pre> <pre><code>proc.rename_column('income &gt;50K', 'income_50K') \n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#36-deleting-columns","title":"3.6: Deleting Columns","text":"<p>The 'delete_columns' method of the SDK can be utilized to delete columns.</p> <pre><code>proc.delete_columns(['sex', \n                     'workclass', \n                     'education', \n                     'marital-status', \n                     'occupation', \n                     'relationship', \n                     'native-country'])\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#37-standardization-of-numerical-columns","title":"3.7: Standardization of numerical columns","text":"<p>The 'normalize.py' snippet of the SDK can be utilized to apply standardization to numeric columns.</p> <pre><code>proc.run_snippet( \n    'normalize', \n    numeric_col_list=['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'],    \n    normalization_option=\"Min-Max Normalization\",  \n    result=None\n) \n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#38-logging-the-pre-process","title":"3.8: Logging the pre-process","text":"<p>The 'wait_until_done' and 'show_logs' methods of the SDK can be utilized to check and log the pre-process steps.</p> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#39-exporting-the-data-set-into-pandas-dataframe","title":"3.9: Exporting the data set into pandas dataframe","text":"<p>The 'get_df_copy' methods of the SDK can be utilized to export the dataset as pandas dataframe to continue working with dataset on different aspect of Data Science.</p> <pre><code>df_processed = proc.get_df_copy()\n</code></pre> <pre><code>df_processed.head()\n</code></pre> <pre><code>ProfileReport(df_processed)\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#snippetsimpute_missing_knnpy","title":"snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#snippetsnormalizepy","title":"snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/pre-process-data/preprocess/#snippetssuppress_outlierspy","title":"snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre> <p>Previous: Analyze | Next: Process Data &gt; Insurance</p>"},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/","title":"Insurance With Remote Worker","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/#insurance-sample-sdk-usage-with-remote-worker","title":"Insurance Sample SDK Usage with Remote Worker","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/#scenario-process-on-the-notebook","title":"Scenario: Process on the Notebook","text":"<ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Create json for worker and connection.(Json generation code coming soon)</p> </li> <li>The Worker json file must contain the following configurations:</li> <li> <p>Service url, worker_size, worker_image, email, and refresh_token</p> </li> <li> <p>The Connection json file must contain the following configurations:</p> </li> <li> <p>Connection_type, ws_uuid, ws_name, and file_path</p> </li> <li> <p>Encoding categorical variables</p> </li> <li> <p>Delete the originals of the columns you encoded</p> </li> <li> <p>Run the process and kill processing when finished</p> </li> </ol> <p>Create a new worker with practicuscore method of \"create_worker\" and use this new worker for your operations</p> <pre><code>worker_conf = {\n    \"worker_size\": \"Medium\",\n    \"worker_image\": \"practicus\",\n    #\"service_url\": \"\",\n    #\"email\": \"\",\n    #\"refresh_token\": \"**entry_your_token**\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.create_worker(worker_conf)\n</code></pre> <ul> <li>To access the dataset you need to work with connection configuration dictionary</li> <li>Also, you can choose a different Engine than Advance in the Deploy phase</li> </ul> <pre><code># configuration of connection\n\ndata_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine='AUTO') \n</code></pre> <p>Data prep with Practicus ai SDK</p> <pre><code>proc.categorical_map(column_name='sex', column_suffix='category') \n</code></pre> <pre><code>proc.categorical_map(column_name='smoker', column_suffix='category') \n</code></pre> <pre><code>proc.categorical_map(column_name='region', column_suffix='category') \n</code></pre> <pre><code>proc.delete_columns(['region', 'smoker', 'sex']) \n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Finish the process</p> <pre><code>proc.kill()\n</code></pre> <pre><code>worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/#you-can-also-prepare-this-code-directly-in-pipeline","title":"You can also prepare this code directly in pipeline:","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance-with-remote-worker/#if-you-make-the-process-functional-and-do-it-with-with-you-dont-need-to-kill-the-worker-when-the-process-is-finished-the-worker-is-automatically-killed-when-this-process-is-finished","title":"If you make the process functional and do it with with, you don't need to kill the worker when the process is finished. The worker is automatically killed when this process is finished","text":"<pre><code>with prt.create_worker(worker_conf) as worker: \n    with worker.load(data_set_conn) as proc:\n        proc.categorical_map(column_name='sex', column_suffix='category'), \n        proc.categorical_map(column_name='smoker', column_suffix='category'),\n        proc.categorical_map(column_name='region', column_suffix='category'),\n        proc.delete_columns(['region', 'smoker', 'sex']) \n        proc.wait_until_done(timeout_min=600)\n        proc.show_logs()\n</code></pre> <p>Previous: Insurance | Next: Spark Custom Config</p>"},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/","title":"Insurance","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/#insurance-sample-sdk-usage-with-local-worker","title":"Insurance Sample SDK Usage with Local Worker","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/#scenario-process-on-the-interface-deploy-and-access-the-process","title":"Scenario: Process on the interface, deploy and access the process","text":"<ol> <li> <p>Open insurance.csv</p> </li> <li> <p>Encoding categorical variables</p> </li> <li> <p>Delete the originals of the columns you encoded</p> </li> <li> <p>Navigating the Deploy button, choosing Jupyter Notebook Option:</p> </li> <li> <p>Clicking Advanced and selecting view code and include security token options:</p> </li> <li> <p>Seeing your connection and worker json created and your code ready:</p> </li> </ol> <p>Connect to your active worker with get local worker</p> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n</code></pre> <ul> <li>To access the dataset you need to work with connection configuration dictionary</li> <li>Also, you can choose a different Engine than Advance in the Deploy phase</li> </ul> <pre><code># configuration of connection\n\ndata_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"ws_uuid\": \"d9b92183-8832-4fd1-8187-ac741ff6aab0\",\n    \"ws_name\": \"insurance\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn) \n</code></pre> <p>Data preparation with Practicus AI SDK</p> <pre><code>proc.categorical_map(column_name='sex', column_suffix='category') \n</code></pre> <pre><code>proc.categorical_map(column_name='smoker', column_suffix='category') \n</code></pre> <pre><code>proc.categorical_map(column_name='region', column_suffix='category') \n</code></pre> <pre><code>proc.delete_columns(['region', 'smoker', 'sex']) \n</code></pre> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Finish the process</p> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/#you-can-also-prepare-this-code-directly-in-pipeline","title":"You can also prepare this code directly in pipeline:","text":""},{"location":"technical-tutorial/extras/data-processing/process-data/insurance/#if-you-make-the-process-functional-and-do-it-with-with-you-dont-need-to-kill-the-worker-when-the-process-is-finished-the-worker-is-automatically-killed-when-this-process-is-finished","title":"If you make the process functional and do it with with, you don't need to kill the worker when the process is finished. The worker is automatically killed when this process is finished","text":"<pre><code># Running the below will terminate the worker \n# with prt.get_local_worker() as worker:\n\n# E.g. the below would run the task, and then kill the process first, and then the worker.\n# with prt.get_local_worker() as worker:\n#     with worker.load(data_set_conn) as proc:\n#         proc.categorical_map(column_name='sex', column_suffix='category'), \n#         proc.categorical_map(column_name='smoker', column_suffix='category'),\n#         proc.categorical_map(column_name='region', column_suffix='category'),\n#         proc.delete_columns(['region', 'smoker', 'sex']) \n#         proc.wait_until_done()\n#         proc.show_logs()\n</code></pre> <p>Previous: Preprocess | Next: Insurance With Remote Worker</p>"},{"location":"technical-tutorial/extras/data-processing/process-data/spark-custom-config/","title":"Spark Custom Config","text":"<pre><code># Defining Parameters\ns3_access = None\ns3_secret = None\ns3_bucket_uri = None # E.g \"s3a://sample-bucket/boston.csv\"\n</code></pre> <pre><code>assert s3_access, \"Please enter an access key\"\nassert s3_secret, \"Please enter an secret key\"\nassert s3_bucket_uri, \"Please enter s3 bucket uri\"\n</code></pre> <pre><code># For AWS S3 \ns3_endpoint = \"s3.amazonaws.com\"\n# For others, e.g. Minio\n# s3_endpoint = \"http://prt-svc-sampleobj.prt-ns.svc.cluster.local\"\n</code></pre> <pre><code>extra_spark_conf = {\n    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n    \"spark.hadoop.fs.s3a.access.key\" : s3_access,\n    \"spark.hadoop.fs.s3a.secret.key\" : s3_secret,\n    \"spark.hadoop.fs.s3a.endpoint\": s3_endpoint\n}\n\nimport practicuscore as prt \n\nspark = prt.engines.get_spark_session(extra_spark_conf=extra_spark_conf)\n\ndf = spark.read.csv(s3_bucket_uri)\ndf.head()\n</code></pre> <p>Previous: Insurance With Remote Worker | Next: Spark Object Storage</p>"},{"location":"technical-tutorial/extras/data-processing/process-data/spark-object-storage/","title":"Spark Object Storage","text":"<pre><code># Defining Parameters\naws_region = None\naws_access_key_id = None\naws_secret_access_key= None\nendpoint_url = None # example \"http://prt-svc-sampleobj.prt-ns.svc.cluster.local\",\ns3_bucket_uri = None # E.g \"s3a://sample-bucket/boston.csv\"\n</code></pre> <pre><code>assert aws_region, \"Please enter a aws_region\"\nassert aws_access_key_id, \"Please enter a aws_access_key_id\"\nassert aws_secret_access_key, \"Please enter a aws_secret_access_key\"\nassert endpoint_url, \"Please enter a endpoint_url\"\nassert s3_bucket_uri, \"Please enter s3 bucket uri\"\n</code></pre> <pre><code># For AWS S3\nconnection = {\n    \"connection_type\": \"S3\",\n    \"aws_region\": aws_region,\n    \"aws_access_key_id\": aws_access_key_id,\n    \"aws_secret_access_key\":aws_secret_access_key,\n    # Optional\n    # \"aws_session_token\", \"...\"\n}\n</code></pre> <pre><code># For others, e.g. Minio\nconnection = {\n    \"connection_type\": \"S3\",\n    \"endpoint_url\": endpoint_url, \n    \"aws_access_key_id\": aws_access_key_id,\n    \"aws_secret_access_key\": aws_secret_access_key,\n}\n</code></pre> <pre><code>import practicuscore as prt \n\n# Create a Spark session\nspark = prt.engines.get_spark_session(connection)\n\n# If you are using distributed Spark, you should now have the Spark cluster up &amp; running. \n</code></pre> <pre><code>df = spark.read.csv(s3_bucket_uri)\ndf.head()\n</code></pre> <pre><code># Optional: delete Spark Session \nprt.engines.delete_spark_session(spark)\n\n# If you are using distributed Spark, you should now have the Spark cluster terminated.\n# You can also terminate your worker, which will automatically terminate the child Spark Cluster. \n</code></pre> <p>Previous: Spark Custom Config</p>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/","title":"LangChain LLM Model: RAG Model for Banking","text":"<p>This notebook demonstrates the development of a Retrieval-Augmented Generation (RAG) model for banking-related queries.  The RAG model retrieves relevant contextual data and generates meaningful answers using a language model.  We utilize LangChain, Transformers, and other related libraries to build the model.</p> <pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None # Example url -&gt; 'company.practicus.com'\nembedding_model_path = None\nmodel_name = None\nmodel_prefix = None\n\nvector_store = None\n\nif vector_store == 'MilvusDB':\n    milvus_uri = None # Milvus connection url, E.g. 'company.practicus.milvus.com'\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>assert host, \"Please enter your host url\" \nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert model_name, \"Please enter your embedding model_name.\"\n\n# You can use one of ChromaDB or MilvusDB as vector store\nassert model_prefix, \"Please enter your embedding model_prefix.\"\n\nassert vector_store in ['ChromaDB', 'MilvusDB'], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"\nif vector_store == 'MilvusDB':\n    assert 'milvus_uri', \"Please enter your milvus connection uri\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#preparing-data","title":"Preparing Data","text":"<pre><code>import os\nimport requests\n\n# Create the GitHub API URL\nurl = 'https://api.github.com/repos/practicusai/sample-data/contents/FAQ_Sample?ref=main'\n\n# Call the API\nresponse = requests.get(url)\nif response.status_code == 200:\n    files = response.json()  # Get response in JSON format\n\n    for file in files:\n        file_url = file['download_url']\n        file_name = file['name']\n\n        # Download files\n        file_response = requests.get(file_url)\n        if file_response.status_code == 200:\n            with open(file_name, 'wb') as f:\n                f.write(file_response.content)\n            print(f\"'{file_name}' successfully downloaded.\")\n        else:\n            print(f\"'{file_name}' file failed to download.\")\nelse:\n    print(f\"Failed to retrieve data from API, HTTP status: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#libraries-installation","title":"Libraries Installation","text":"<p>Ensure the necessary libraries are installed using the following commands:</p> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb pypdf\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#importing-required-libraries","title":"Importing Required Libraries","text":"<p>The notebook imports libraries such as Transformers for text processing, LangChain for building components like text splitters and vector stores, and additional utilities for embedding generation and document processing.</p> <pre><code>from transformers import pipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#define-llm-api-function","title":"Define LLM API Function","text":"<p>A function is defined to interact with the ChatPracticus API, sending input prompts and receiving language model-generated responses.  The API URL and token are required to authenticate and access the service.</p> <pre><code>def call_llm_api(inputs, api_url, api_token):\n    # We need to give input to 'generate_response'. This function will use our 'api_token' and 'endpoint_url' and return the response.\n\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n    # response = chat.invoke(\"What is Capital of France?\")  # This also works\n\n    return(response.content)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#load-and-split-pdf-files","title":"Load and Split PDF Files","text":"<p>PDF documents are loaded and split into manageable text chunks using LangChain's <code>CharacterTextSplitter</code>.  This enables the processing of large documents for retrieval tasks.</p> <pre><code>def load_and_split_pdfs(pdf_files, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load all pdf files and split with using the 'seperator'.\n\n    :param pdf_files: A list of paths to the PDF files to be processed.\n    :param chunk_size: The maximum number of characters in each text chunk. \n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = CharacterTextSplitter( # langchain method used to separate documents, there are different methods as well\n        separator=\"  \\n \\n \\n \\n\", # Defines the separator used to split the text.\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False)\n\n    for pdf_file in pdf_files:\n        loader = PyPDFLoader(pdf_file) # pdf loader compatible with langchain\n        documents = loader.load_and_split()\n        split_docs = text_splitter.split_documents(documents)\n        all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#define-pdf-files-and-process-them","title":"Define PDF Files and Process Them","text":"<p>A list of PDF files is specified, and the <code>load_and_split_pdfs</code> function processes these files into text chunks for downstream retrieval and analysis.</p> <pre><code>pdf_list = ['faq1.pdf', 'faq2.pdf', 'faq3.pdf', 'faq4.pdf', 'faq5.pdf']\n\ntext_chunks = load_and_split_pdfs(pdf_list)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#create-chroma-vector-store","title":"Create Chroma Vector Store","text":"<p>The function creates a Chroma vector store, which encodes the text chunks into embeddings using a pre-trained model.  The vector store allows similarity-based document retrieval.</p> <pre><code>if vector_store == 'ChromaDB':\n    # Generate embeddings and create ChromaDB vector store\n    def create_chroma_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings( # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path, # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={'device': 'cpu'}, # Configuration for running model on cpu.\n            encode_kwargs={'normalize_embeddings': False})\n\n        # This method creates a vector store from the provided documents (chunks) and embeddings.\n        vectorstore_pdf = Chroma.from_documents(collection_name=\"langchain_example\", \n                                                documents=chunks, \n                                                embedding=embeddings) \n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to\n        # the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_pdf = vectorstore_pdf.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) \n        return retriever_pdf\n\n    retriever_pdf = create_chroma_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#optional-create-milvus-db","title":"(OPTIONAL) Create Milvus DB","text":"<p>This function creates a vector store in Milvus by generating embeddings for text chunks using a HuggingFace pre-trained model. It connects to the Milvus database, stores the embeddings in a specified collection, and ensures any old collections with the same name are dropped. The resulting vector store is converted into a retriever for similarity-based searches, retrieving the top 2 relevant documents for a query.</p> <pre><code>from langchain_milvus import Milvus\nfrom pymilvus import connections\n\nif vector_store == 'MilvusDB':\n    def create_milvus_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings( # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path, # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={'device': 'cpu'}, # Configuration for running model on cpu.\n            encode_kwargs={'normalize_embeddings': False})\n\n        connections.connect(\"default\", host=milvus_uri, port=\"19530\") # Connection to milvus db\n\n        vectorstore = Milvus.from_documents(\n            documents=chunks,\n            embedding=embeddings,\n            collection_name=\"langchain_example\", # Name for created vector table\n            connection_args={\n                \"uri\": f\"https://{milvus_uri}:19530\" # Connection configuration to milvus db\n            },\n            drop_old=True,  # Drop the old Milvus collection if it exists\n        ) \n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents\n        # to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_pdf = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) \n        return retriever_pdf\n\n    retriever_pdf = create_milvus_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#format-document-chunks","title":"Format Document Chunks","text":"<p>A utility function combines document chunks into a single string format.  This formatted text is passed to the language model for querying.</p> <pre><code>def format_docs(docs):\n     # Retrieves the content of each document in the `docs` list and joins the content of all documents into a single string, with each document's content separated by two newline characters.\n     return \"\\n\\n\".join(doc.page_content for doc in docs) \n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#query-pdf-using-llm","title":"Query PDF Using LLM","text":"<p>This function combines document retrieval, prompt construction, and LLM inference to answer a given query.  The RAG model retrieves context from the vector store, formats it, and queries the LLM.</p> <pre><code># Query the PDF using the API-based LLM\ndef query_pdf(retriever, question, api_url, api_token):\n    \"\"\"\n    this function is used for returning response by using all of the chains we defined above\n\n    :param retriever : An instance of a retriever used to fetch relevant documents.\n    :param question : The question to be asked about the PDF content.\n    \"\"\"\n\n    prompt_template = PromptTemplate( # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=( # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        )\n    )\n\n    docs = retriever.get_relevant_documents(question) # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs) # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question) # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split('Answer:')[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#practicus-integration","title":"Practicus Integration","text":"<p>Imports the Practicus library, which is used for managing API configurations and token generation for LLM queries.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/advanced-langchain/lang-chain-llm-model/#execute-a-sample-query","title":"Execute a Sample Query","text":"<p>Demonstrates querying the RAG model with a sample question about banking services.  The system retrieves relevant context and generates an answer using the integrated LLM.</p> <pre><code># Example query\nanswer = query_pdf(retriever = retriever_pdf, \n                   question=\"My transaction was interrupted at an ATM, where should I apply?\", \n                   api_url = api_url,\n                   api_token = token)\nprint(answer)\n</code></pre> <p>Previous: API Triggers For Airflow | Next: LLM Apps &gt; API LLM Apphost &gt; Build</p>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/","title":"Cv Assistant","text":"<pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None # E.g. 'company.practicus.com'\nembedding_model_path = None\nmodel_name = None\nmodel_prefix = None\n\nvector_store = None # ChromaDB or MilvusDB\n\nif vector_store == 'MilvusDB':\n    milvus_uri = None # E.g. 'company.practicus.milvus.com'\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>assert host, \"Please enter your host url\" \nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert model_name, \"Please enter your embedding model_name.\"\nassert model_prefix, \"Please enter your embedding model_prefix.\"\n\n# You can use one of ChromaDB or MilvusDB as vector store\nassert vector_store in ['ChromaDB', 'MilvusDB'], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"\n\nif vector_store == 'MilvusDB':\n    assert 'milvus_uri', \"Please enter your milvus connection uri\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#firstly-we-need-install-transformers-and-torch","title":"Firstly we need install transformers and torch","text":"<p>Run at terminal:</p> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#pip-install-transformers-sentence-transformers-langchain-langchain-community-chromadb","title":"pip install transformers sentence-transformers langchain langchain-community chromadb","text":"<ul> <li> <p>Transformers: It allows you to easily use Transformer-based models (such as BERT, GPT, etc.).-</p> </li> <li> <p>Sentence-Transformers: It produces vector representations of sentences using Transformer models.</p> </li> <li> <p>LangChain: It is used to manage more complex workflows with language models. </p> </li> <li> <p>Langchain-Community: Contains additional modules and components developed by the community for the LangChain library.</p> </li> <li> <p>ChromaDB: Used as a vector database. It is optimized for embeddings and similarity searches.</p> </li> <li> <p>PyPDF: A library used to process PDF files with Python. </p> </li> </ul>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#pip-install-torch-index-url-httpsdownloadpytorchorgwhlcpu","title":"pip install torch --index-url https://download.pytorch.org/whl/cpu","text":"<p>This command is used to install the PyTorch library with CPU support.</p> <p>Details:</p> <ul> <li>Torch (PyTorch): A library used for developing machine learning and deep learning models. It offers features such as tensor computations, automatic differentiation, and advanced modeling.</li> <li>--index-url https://download.pytorch.org/whl/cpu: This parameter uses a specific index URL to download the CPU version of PyTorch. If you do not want to install a GPU-specific version, this URL is used.</li> </ul> <pre><code># Prepare data\n\nimport os\nimport requests\n\nrepo_owner = \"practicusai\"\nrepo_name = \"sample-data\"\nfile_path = \"hr_assistant\"\nbranch = \"main\"\n\n\nurl = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{file_path}?ref={branch}\"\n\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    files = response.json()  \n\n    for file in files:\n        file_url = file['download_url'] \n        file_name = file['name']  \n\n        file_response = requests.get(file_url)\n        if file_response.status_code == 200:\n\n            with open(file_name, 'wb') as f:\n                f.write(file_response.content)\n            print(f\"'{file_name}' successfully downloaded.\")\n        else:\n            print(f\"'{file_name}' not successfully downloaded.\")\nelse:\n    print(f\"HTTP status: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#import-libraries","title":"Import Libraries","text":"<pre><code>from langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#define-llm-api-function-and-call-chatpracticus-in-this-function","title":"Define llm api function and call ChatPracticus in this function","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#function-call_llm_api","title":"Function: <code>call_llm_api</code>","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#description","title":"Description","text":"<p>This function interacts with the ChatPracticus API to invoke a response from a language model using the provided inputs, API URL, and API token. The function is designed to send data to the API and retrieve the response content.</p>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#parameters","title":"Parameters","text":"<ul> <li> <p><code>inputs</code>:   The input query or data to be sent to the API for processing. This is typically a string or JSON object depending on the API's requirements.</p> </li> <li> <p><code>api_url</code>:   The endpoint URL of the ChatPracticus API. This is the location where the API call will be directed.</p> </li> <li> <p><code>api_token</code>:   The authentication token for accessing the ChatPracticus API. This ensures secure communication and proper authorization.</p> </li> <li> <p>The function initializes a <code>ChatPracticus</code> object with the specified API URL and token. The <code>model_id</code> parameter is currently unused or ignored but can be included for future model-specific configurations.</p> </li> <li> <p>The <code>invoke</code> method of the <code>ChatPracticus</code> object is called with the given input. This sends the query to the API and retrieves the response.</p> </li> <li> <p>The function returns the <code>content</code> attribute of the response, which contains the text generated by the language model.</p> </li> </ul> <pre><code>def call_llm_api(inputs, api_url, api_token):\n    # We need to give input to 'generate_response'. This function will use our 'api_token' and 'endpoint_url' and return the response.\n\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n    :params api_token: Token of our model.\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n\n    return(response.content)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#get-all-resumes-and-use-seperator-for-split-questions","title":"Get all resumes and use seperator for split questions","text":"<ol> <li><code>df = pd.read_csv(\"HR.csv\")</code>:  </li> <li>Reads the CSV file <code>HR.csv</code> into a pandas DataFrame called <code>df</code>.  </li> <li> <p>The DataFrame should contain a column named <code>Resume_str</code>.</p> </li> <li> <p><code>merged_resumes = ''</code>:  </p> </li> <li> <p>Initializes an empty string <code>merged_resumes</code>, which will store the concatenated resume data.</p> </li> <li> <p>For Loop:</p> </li> <li>Iterates over each resume string in the <code>Resume_str</code> column of the DataFrame.  </li> <li>Appends each resume to the <code>merged_resumes</code> string, preceded by the delimiter <code>//m-n-m//</code>.</li> </ol>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#final-output","title":"Final Output","text":"<ul> <li>The variable <code>merged_resumes</code> contains all resumes concatenated into a single string, with <code>//m-n-m//</code> acting as a separator between each resume.</li> </ul> <pre><code>df = pd.read_csv(\"HR.csv\")\nmerged_resumes = ''\nfor resume in df['Resume_str']:\n    merged_resumes = merged_resumes + '//m-n-m//' + resume\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#description_1","title":"Description","text":"<p>This function processes a concatenated string of resumes, splits them into individual documents based on a delimiter, and further divides these documents into smaller chunks for analysis. The function utilizes a <code>CharacterTextSplitter</code> to handle the chunking process.</p> <ol> <li>Split the Resumes:</li> <li> <p>The input <code>merged_resumes</code> is split into individual resume strings using the <code>//m-n-m//</code> delimiter.</p> </li> <li> <p>Create Document Objects:</p> </li> <li> <p>Each resume is transformed into a <code>Document</code> object. Empty or whitespace-only resumes are excluded.</p> </li> <li> <p>Initialize the Text Splitter:</p> </li> <li> <p>A <code>CharacterTextSplitter</code> is set up with the following configuration:</p> <ul> <li><code>separator=\"//m-n-m//\"</code>: The delimiter used for splitting.</li> <li><code>chunk_size</code>: Controls the maximum size of each text chunk.</li> <li><code>chunk_overlap</code>: Adds overlapping text between chunks for better context retention.</li> </ul> </li> <li> <p>Split Documents:</p> </li> <li> <p>The documents are further divided into smaller chunks using the <code>CharacterTextSplitter</code>.</p> </li> <li> <p>Aggregate Results:</p> </li> <li>The chunks are appended to the <code>all_docs</code> list, which is returned as the final output.</li> </ol>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#returns","title":"Returns","text":"<ul> <li><code>all_docs</code>:   A list of smaller text chunks, each represented as a document object, ready for further processing.</li> </ul> <pre><code>def load_and_split_resumes(merged_resumes, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load and split email strings into chunks.\n\n    :param merged_resumes: A single string containing all resumes contents, separated by '//m-n-m//'.\n    :param chunk_size: The maximum number of characters in each text chunk. \n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = CharacterTextSplitter(\n        separator=\"//m-n-m//\",  # Defines the separator used to split the text.\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False\n    )\n\n    # Split resumes\n    resumes = merged_resumes.split('//m-n-m//')\n\n    # Transform to Document\n    documents = [Document(page_content=resume.strip()) for resume in resumes if resume.strip()]\n\n    # Split docs\n    split_docs = text_splitter.split_documents(documents) \n    all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre> <ol> <li>Function Call:</li> <li> <p><code>load_and_split_resumes(merged_resumes)</code>:</p> <ul> <li>The <code>merged_resumes</code> string, containing all resumes concatenated and separated by <code>//m-n-m//</code>, is passed to the <code>load_and_split_resumes</code> function.</li> <li>This function:</li> <li>Splits the resumes into individual documents.</li> <li>Further chunks each document into smaller pieces based on the defined <code>chunk_size</code> and <code>chunk_overlap</code> parameters.</li> </ul> </li> <li> <p>Output:</p> </li> <li>The result of the function is assigned to the variable <code>text_chunks</code>.</li> <li><code>text_chunks</code> is a list of chunked text segments, ready for downstream processing or analysis.</li> </ol> <pre><code># Create our chunks\ntext_chunks = load_and_split_resumes(merged_resumes)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#create-the-vector-store-and-model-path-is-given","title":"Create the vector store and model path is given","text":"<ol> <li>Condition Check:</li> <li> <p><code>if vector_store == 'ChromaDB':</code></p> <ul> <li>Ensures that the code block is executed only if <code>ChromaDB</code> is selected as the vector store.</li> </ul> </li> <li> <p><code>create_chroma_vector_store</code> Function:</p> </li> <li> <p>This function generates embeddings for the provided text chunks and creates a Chroma vector store.</p> </li> <li> <p>Assign Retriever:</p> </li> <li><code>retriever_resumes = create_chroma_vector_store(text_chunks, embedding_model_path)</code>:<ul> <li>Calls the <code>create_chroma_vector_store</code> function with <code>text_chunks</code> and the embeddings model path to generate the retriever.</li> </ul> </li> </ol> <pre><code>if vector_store == 'ChromaDB':\n    # Generate embeddings and create vector store\n    def create_chroma_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings( # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path, # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={'device': 'cpu'}, # Configuration for running model on cpu.\n            encode_kwargs={'normalize_embeddings': False})\n\n        db_name = str(random.random())\n        vectorstore_resumes = Chroma.from_documents(collection_name=db_name, documents=chunks, embedding=embeddings) # This method creates a vector store from the provided documents (chunks) and embeddings.\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_resumes = vectorstore_resumes.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) \n        return retriever_resumes\n\n    retriever_resumes = create_chroma_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#optional-milvus-vector-db","title":"(OPTIONAL) Milvus Vector DB","text":"<ol> <li>Condition Check:</li> <li> <p><code>if vector_store == 'MilvusDB':</code></p> <ul> <li>Ensures that the Milvus-based vector store logic is executed only when <code>MilvusDB</code> is selected.</li> </ul> </li> <li> <p><code>create_milvus_vector_store</code> Function:</p> </li> <li> <p>This function generates embeddings for the provided text chunks and stores them in a Milvus vector database.</p> </li> <li> <p>Inputs:</p> <ul> <li><code>chunks</code>: The text chunks to be embedded and stored.</li> <li><code>embeddings_model_path</code>: Path to the pre-trained embeddings model.</li> </ul> </li> <li> <p>Steps:</p> <ul> <li>Generate Embeddings:</li> <li><code>HuggingFaceEmbeddings</code> generates embeddings for the text chunks.</li> <li> <p>Parameters:</p> <ul> <li><code>model_name</code>: Path to the pre-trained embeddings model.</li> <li><code>model_kwargs</code>: Specifies the device to run the model (<code>'cpu'</code> in this case).</li> <li><code>encode_kwargs</code>: Optional configuration for encoding (e.g., <code>normalize_embeddings</code>).</li> </ul> </li> <li> <p>Connect to Milvus:</p> </li> <li><code>connections.connect</code> establishes a connection to the Milvus server.</li> <li> <p>Parameters:</p> <ul> <li><code>host</code>: The URI of the Milvus server.</li> <li><code>port</code>: Default port for Milvus, <code>19530</code>.</li> </ul> </li> <li> <p>Create Vector Store:</p> </li> <li><code>Milvus.from_documents</code> creates a vector store with:<ul> <li><code>documents</code>: The input text chunks.</li> <li><code>embedding</code>: The generated embeddings.</li> <li>`collection_</li> </ul> </li> </ul> </li> </ol> <pre><code>from langchain_milvus import Milvus\nfrom pymilvus import connections\n\nif vector_store == 'MilvusDB':\n    def create_milvus_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings( # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path, # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={'device': 'cpu'}, # Configuration for running model on cpu.\n            encode_kwargs={'normalize_embeddings': False})\n\n        connections.connect(\"default\", host=milvus_uri, port=\"19530\")\n\n        vectorstore = Milvus.from_documents(\n            documents=chunks,\n            embedding=embeddings,\n            collection_name=\"langchain_example\",\n            connection_args={\n                \"uri\": f\"https://{milvus_uri}:19530\"\n            },\n            drop_old=True,  # Drop the old Milvus collection if it exists\n        ) \n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_resumes = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) \n        return retriever_resumes\n\n    retriever_resumes = create_milvus_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#define-format_docs-for-join-all-chunks","title":"Define format_docs for join all chunks","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#description_2","title":"Description","text":"<p>This function processes a list of document objects, extracts their <code>page_content</code> attributes, and concatenates all the document contents into a single string. Each document's content is separated by two newline characters (<code>\\n\\n</code>) to enhance readability.</p> <ol> <li>Extract Document Content:</li> <li> <p>Iterates over the list <code>docs</code> and retrieves the <code>page_content</code> attribute from each document object.</p> </li> <li> <p>Join Content:</p> </li> <li> <p>Combines all extracted content into a single string, with each document's content separated by two newline characters (<code>\\n\\n</code>).</p> </li> <li> <p>Return Result:</p> </li> <li>The resulting string is returned for use in other parts of the application.</li> </ol> <pre><code>def format_docs(docs):\n     # Retrieves the content of each document in the `docs` list and joins the content of all documents into a single string, with each document's content separated by two newline characters.\n     return \"\\n\\n\".join(doc.page_content for doc in docs) \n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#all-chains-merged-into-each-other-at-this-function","title":"All chains merged into each other at this function","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#description_3","title":"Description","text":"<p>This function retrieves relevant documents for a given question using a retriever, formats a prompt with the documents and question, and queries a language model API to generate an answer.</p> <ol> <li>Define Prompt Template:</li> <li>A <code>PromptTemplate</code> object is created to format the input for the language model.</li> <li> <p>Template Details:</p> <ul> <li>Includes retrieved context (relevant documents).</li> <li>Contains the question.</li> <li>Instructs the model to respond with \"I don't know\" if the answer is unavailable.</li> </ul> </li> <li> <p>Retrieve Relevant Documents:</p> </li> <li> <p>Calls <code>retriever.get_relevant_documents(question)</code> to fetch documents most relevant to the provided question.</p> </li> <li> <p>Format the Retrieved Documents:</p> </li> <li> <p>The retrieved documents are passed to <code>format_docs</code> to be concatenated into a single string, separated by two newline characters.</p> </li> <li> <p>Construct the Prompt:</p> </li> <li> <p>The <code>PromptTemplate</code> is used to format the prompt with the retrieved context and the question.</p> </li> <li> <p>Query the API:</p> </li> <li> <p>Calls the <code>call_llm_api</code> function with the formatted prompt, <code>api_url</code>, and <code>api_token</code> to query the language model.</p> </li> <li> <p>Extract and Return the Answer:</p> </li> <li>Extracts the answer from the API response by splitting the output on the keyword <code>Answer:</code> and removing extra whitespace.</li> </ol> <pre><code>def query_resume(retriever, question, api_url, api_token):\n\n\n    prompt_template = PromptTemplate( # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=( # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        )\n    )\n\n    docs = retriever.get_relevant_documents(question) # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs) # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question) # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split('Answer:')[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#chat-examples","title":"Chat Examples","text":""},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#code-explanation","title":"Code Explanation","text":"<p>This code snippet imports the <code>practicuscore</code> module and retrieves the current region using the <code>get_region</code> function.</p>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#workflow","title":"Workflow","text":"<ol> <li>Import the Module:</li> <li> <p><code>import practicuscore as prt</code>:</p> <ul> <li>The <code>practicuscore</code> library is imported and aliased as <code>prt</code> for convenience.</li> <li>This module likely provides utility functions or methods for interacting with Practicus-related resources.</li> </ul> </li> <li> <p>Get the Region:</p> </li> <li><code>region = prt.get_region()</code>:<ul> <li>Calls the <code>get_region</code> function from the <code>practicuscore</code> module.</li> <li>This function retrieves information about the current region, which may be used for region-specific configurations or operations.</li> </ul> </li> </ol> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <ol> <li>Construct the API URL:</li> <li> <p><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"</code>:</p> <ul> <li>Uses string formatting to dynamically create the API URL based on:</li> <li><code>host</code>: The base URL or hostname of the server.</li> <li><code>model_prefix</code>: The selected model prefix.</li> <li><code>model_name</code>: The selected model name.</li> <li>The resulting URL specifies the endpoint for accessing the model.</li> </ul> </li> <li> <p>Generate Session Token:</p> </li> <li><code>token = prt.models.get_session_token(api_url=api_url)</code>:<ul> <li>Calls the <code>get_session_token</code> function from the <code>practicuscore.models</code> module.</li> <li>The <code>api_url</code> parameter specifies the endpoint for which the token is generated.</li> <li>The session token ensures secure and authorized communication with the API.</li> </ul> </li> </ol> <pre><code>#Create api url and token\napi_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/cv-assistant/cv-assistant/#create-query-and-print-answer","title":"Create query and print answer","text":"<pre><code># Example query\nanswer = query_resume(retriever = retriever_resumes, question=\"What are the leadership qualities of an HR Director?\", api_url = api_url,api_token = token)\n# Get Answer\nprint(answer)\n</code></pre> <p>Previous: Milvus Chain | Next: Deploying LLM &gt; Introduction</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/","title":"LLM Deployment Tutorial","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#welcome-to-the-llm-deployment-tutorial-this-guide-is-designed-to-provide-you-with-a-comprehensive-understanding-of-deploying-large-language-models-llms-and-creating-api-endpoints-tailored-to-various-use-cases-the-tutorial-is-divided-into-three-sections-each-addressing-a-specific-deployment-scenario-by-following-these-steps-you-will-learn-both-foundational-and-advanced-deployment-techniques-including-integration-with-practicusais-powerful-sdk-for-enhanced-functionality","title":"Welcome to the LLM Deployment Tutorial. This guide is designed to provide you with a comprehensive understanding of deploying Large Language Models (LLMs) and creating API endpoints tailored to various use cases. The tutorial is divided into three sections, each addressing a specific deployment scenario. By following these steps, you will learn both foundational and advanced deployment techniques, including integration with PracticusAI's powerful SDK for enhanced functionality.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#preparation","title":"Preparation","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#before-deploying-an-llm-it-is-essential-to-prepare-the-model-and-its-environment-this-section-outlines-the-steps-for-downloading-an-open-source-llm-from-hugging-face-and-uploading-it-to-an-object-storage-system-this-preparation-is-critical-as-the-model-host-service-will-download-the-llm-from-object-storage-when-running-for-the-first-time","title":"Before deploying an LLM, it is essential to prepare the model and its environment. This section outlines the steps for downloading an open-source LLM from Hugging Face and uploading it to an object storage system. This preparation is critical, as the model host service will download the LLM from object storage when running for the first time.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#basic-llm-deployment","title":"Basic LLM Deployment","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#in-this-section-we-will-explore-how-to-deploy-an-open-source-llm-and-create-an-api-endpoint-using-only-the-transformers-library-this-straightforward-approach-is-ideal-for-scenarios-where-a-standalone-llm-api-is-needed-without-the-additional-complexity-of-pipeline-integrations-or-dependencies-on-the-practicusai-sdk-by-the-end-of-this-section-you-will-have","title":"In this section, we will explore how to deploy an open-source LLM and create an API endpoint using only the Transformers library. This straightforward approach is ideal for scenarios where a standalone LLM API is needed without the additional complexity of pipeline integrations or dependencies on the PracticusAI SDK. By the end of this section, you will have:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-a-deployed-open-source-llm","title":"- A deployed open-source LLM.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-a-functional-api-endpoint-to-interact-with-the-model","title":"- A functional API endpoint to interact with the model.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-insights-into-managing-a-basic-deployment-workflow","title":"- Insights into managing a basic deployment workflow.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#while-this-setup-is-not-designed-for-langchain-compatibility-it-serves-as-a-foundational-building-block-for-standalone-applications","title":"While this setup is not designed for LangChain compatibility, it serves as a foundational building block for standalone applications.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#langchain-suitable-llm-deployment","title":"LangChain-Suitable LLM Deployment","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#this-section-focuses-on-deploying-an-llm-and-creating-an-api-endpoint-that-integrates-seamlessly-with-langchain-pipelines-leveraging-the-practicusai-sdk-this-method-provides-advanced-functionality-and-ensures-compatibility-with-langchain-operations-key-sdk-methods-you-will-learn-include","title":"This section focuses on deploying an LLM and creating an API endpoint that integrates seamlessly with LangChain pipelines. Leveraging the PracticusAI SDK, this method provides advanced functionality and ensures compatibility with LangChain operations. Key SDK methods you will learn include:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-prtlangmessage-structuring-communication-flows-with-the-hosted-llm","title":"- PrtLangMessage: Structuring communication flows with the hosted LLM.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-prtlangrequest-sending-structured-messages-to-the-llm-and-requesting-a-response","title":"- PrtLangRequest: Sending structured messages to the LLM and requesting a response.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-prtlangresponse-parsing-and-interpreting-the-responses-from-the-llm","title":"- PrtLangResponse: Parsing and interpreting the responses from the LLM.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-chatpracticus-a-comprehensive-method-that-simplifies-the-entire-interaction-process-for-langchain-integration","title":"- ChatPracticus: A comprehensive method that simplifies the entire interaction process for LangChain integration.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#by-the-end-of-this-section-you-will-have","title":"By the end of this section, you will have:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-a-fully-deployed-llm-api-endpoint","title":"- A fully deployed LLM API endpoint.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-compatibility-with-langchain-pipelines","title":"- Compatibility with LangChain pipelines.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-practical-knowledge-of-using-the-practicusai-sdk-to-enhance-deployment","title":"- Practical knowledge of using the PracticusAI SDK to enhance deployment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#combined-usage","title":"Combined Usage","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#in-the-final-section-we-bring-together-the-methods-covered-in-the-previous-sections-this-comprehensive-tutorial-demonstrates-how-to-combine-the-basic-llm-deployment-and-langchain-suitable-llm-deployment-approaches-to-create-versatile-api-endpoints-whether-you-need-standalone-functionality-or-seamless-integration-with-langchain-this-section-will-equip-you-with-the-skills-to","title":"In the final section, we bring together the methods covered in the previous sections. This comprehensive tutorial demonstrates how to combine the Basic LLM Deployment and LangChain-Suitable LLM Deployment approaches to create versatile API endpoints. Whether you need standalone functionality or seamless integration with LangChain, this section will equip you with the skills to:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-transition-between-basic-and-advanced-deployments","title":"- Transition between basic and advanced deployments.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-combine-deployment-strategies-for-maximum-flexibility","title":"- Combine deployment strategies for maximum flexibility.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#-optimize-workflows-for-diverse-application-requirements","title":"- Optimize workflows for diverse application requirements.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Introduction/#by-completing-this-tutorial-you-will-gain-a-deep-understanding-of-llm-deployment-techniques-and-how-to-leverage-practicusai-sdk-for-enhanced-functionality-and-integration","title":"By completing this tutorial, you will gain a deep understanding of LLM deployment techniques and how to leverage PracticusAI SDK for enhanced functionality and integration.","text":"<p>Previous: Cv Assistant | Next: Preparation &gt; Model Download</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/","title":"Model Download","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>file_path = None  # e.g. /home/ubuntu/shared/LLM-Models/llama-1B-instruct\nREPO_ID = None  # e.g. meta-llama/Llama-3.2-1B-Instruct\nhf_token = None # for details checkout step 1 \n</code></pre> <pre><code>assert file_path, \"Please enter a file_path\"\nassert REPO_ID, \"Please enter a REPO_ID\"\nassert hf_token, \"Please enter a hf_token\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#model-download","title":"Model Download","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#the-first-step-in-llm-deployment-involves-downloading-the-pre-trained-model-these-steps-include","title":"The first step in llm-deployment involves downloading the pre-trained model. These steps include:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#1-authenticating-with-huggingface-hub-to-securely-access-model-repositories","title":"1. Authenticating with HuggingFace Hub to securely access model repositories.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#2-getting-access-to-an-open-source-llm-model-from-hugging-face","title":"2. Getting access to an open source llm model from Hugging Face.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#3-downloading-the-pre-trained-open-source-llm-eg-llama-3b-instruct-llama-1b-instruct-from-the-hugging-face-hub-for-deployment","title":"3. Downloading the pre-trained open source LLM (e.g., LLaMA-3B-Instruct, LLaMA-1B-Instruct) from the Hugging Face Hub for deployment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#step-1-authenticating-with-hugging-face","title":"Step-1: Authenticating with Hugging Face","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#to-download-models-from-the-hugging-face-hub-you-must-authenticate-using-your-personal-api-token-this-step-ensures-secure-authorized-access-to-both-public-and-private-model-repositories","title":"To download models from the Hugging Face Hub, you must authenticate using your personal API token. This step ensures secure, authorized access to both public and private model repositories.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#obtain-your-api-token","title":"Obtain Your API Token","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#follow-the-instructions-in-the-hugging-face-documentation-to-generate-or-retrieve-your-api-token-keep-this-token-confidential-and-do-not-share-it-publicly","title":"Follow the instructions in the Hugging Face documentation to generate or retrieve your API token. Keep this token confidential and do not share it publicly.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#authenticate-your-environment","title":"Authenticate Your Environment","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#after-acquiring-your-token-run-the-following-code-to-authenticate-this-will-enable-seamless-access-to-the-hugging-face-hub-and-allow-you-to-download-and-work-with-models-directly-in-your-environment","title":"After acquiring your token, run the following code to authenticate. This will enable seamless access to the Hugging Face Hub and allow you to download and work with models directly in your environment.","text":"<pre><code>from huggingface_hub.hf_api import HfFolder\n\nHfFolder.save_token(hf_token)  # Replace with your Hugging Face API token\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#step-2-getting-access-to-an-open-source-llm-model","title":"Step-2: Getting access to an open source llm model","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#second-you-need-to-obtain-access-to-download-the-model-visit-metas-llama-huggingface-page-and-request-access","title":"Second, you need to obtain access to download the model. Visit Meta's LLaMA HuggingFace page and request access.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#httpshuggingfacecometa-llamallama-32-1b-instruct","title":"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#once-approved-you-will-receive-an-e-mail-with-conformation-link-resembling-the-following-format","title":"Once approved, you will receive an e-mail with conformation link, resembling the following format:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#httpshuggingfacecoemail_confirmationbbfx","title":"https://huggingface.co/email_confirmation/bBFX...","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#step-3-install-dependencies-and-download-the-model","title":"Step 3: Install Dependencies and Download the Model","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#to-download-a-model-from-hugging-face-hub-we-can-use-the-snapshot_download-method-like-down-below","title":"To download a model from Hugging Face Hub we can use the snapshot_download method like down below,","text":"<pre><code>from huggingface_hub import snapshot_download\n\n\nsnapshot_download(repo_id=REPO_ID, local_dir=file_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#and-to-consume-the-model-we-should-install-the-necessary-libraries-to-host-service-image","title":"And to consume the model, we should install the necessary libraries to host service image,","text":"<pre><code>pip install transformers sentence-transformers langchain langchain-community chromadb pypdf\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#if-youre-unsure-how-to-install-the-necessary-libraries-to-host-the-service-we-recommend-referring-to-our-comprehensive-documentation-on-installing-libraries-within-the-practicus-ai-environment-it-provides-clear-step-by-step-instructions-to-ensure-a-smooth-setup-process","title":"If you're unsure how to install the necessary libraries to host the service, we recommend referring to our comprehensive documentation on installing libraries within the Practicus AI environment. It provides clear, step-by-step instructions to ensure a smooth setup process.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#it-is-also-necessary-to-install-the-torch-according-to-the-system-you-can-check-the-torch-installation-document","title":"It is also necessary to install the torch according to the system, you can check the Torch Installation Document.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#step-4-upload-cache-to-object-storage","title":"Step 4: Upload Cache to Object Storage","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Model-Download/#using-the-upload_downloadipynb-notebook-upload-the-downloaded-large-language-model-directory-to-your-object-storage-this-action-facilitates-easy-access-to-the-model-and-its-dependencies","title":"Using the upload_download.ipynb notebook, upload the downloaded large language model directory to your object storage. This action facilitates easy access to the model and its dependencies.","text":"<p>Previous: Introduction | Next: Upload LLM</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/","title":"Upload to Object Storage","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#in-this-guide-well-walk-through-the-process-of-uploading-and-downloading-model-related-files-to-and-from-an-object-storage-solution-using-aws-s3-as-an-example-this-functionality-is-essential-for-deploying-and-managing-models-in-the-practicus-ai-environment-allowing-you-to-efficiently-handle-model-files-configurations-and-other-necessary-assets-we-will-use-the-boto3-library-for-interacting-with-aws-services-specifically-s3-for-object-storage","title":"In this guide, we'll walk through the process of uploading and downloading model-related files to and from an object storage solution using AWS S3 as an example. This functionality is essential for deploying and managing models in the Practicus AI environment, allowing you to efficiently handle model files, configurations, and other necessary assets. We will use the boto3 library for interacting with AWS services, specifically S3 for object storage.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>_aws_access_key_id = None\n_aws_secret_access_key = None\n_bucket = None # The name of your target bucket, e.g. \"my-data-bucket\"\n_prefix = None # Your prefix within bucket e.g. cache/llama-1b-instruct/\n\n_folder_path = None # The local path containing files to upload.\n                    # e.g. \"/home/ubuntu/shared/LLM-Models/llama-1B-instruct\"\n\n_aws_session_token = None  # (Optional) AWS session token for temporary credentials\n_aws_region = None         # (Optional) Your AWS region. If unknown, you may leave it as None.\n_endpoint_url = None       # (Optional) Endpoint URL for S3-compatible services (e.g., MinIO API URL)\n\n_prefix = None  # (Optional) Prefix for organizing objects within the bucket. \n                # Use None or \"\" for root-level placement, or specify something \n                # like \"folder\" or \"folder/subfolder\" for nested directories.\n\n_source_path_to_cut = None  # (Optional) A prefix within the local folder path \n                            # that you want to remove from the uploaded object keys.\n                            # Leave as None to default to the entire folder path.\n</code></pre> <pre><code># Ensure that essential parameters are provided\nassert _aws_access_key_id and _aws_secret_access_key and _bucket and _prefix\n\n# Ensure the folder path is provided\nassert _folder_path\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#step-1-import-required-libraries","title":"Step 1: Import Required Libraries","text":"<pre><code>import os\nimport boto3\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#the-os-module-is-used-for-operating-system-dependent-functionality-like-reading-or-writing-files-whereas-boto3-is-the-amazon-web-services-aws-sdk-for-python-allowing-you-to-interact-with-aws-services-including-s3","title":"The os module is used for operating system-dependent functionality like reading or writing files, whereas boto3 is the Amazon Web Services (AWS) SDK for Python, allowing you to interact with AWS services including S3.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#step-2-uploading-files-to-object-storage","title":"Step 2: Uploading Files to Object Storage","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#define-a-function-upload_files-that-recursively-uploads-all-files-from-a-specified-folder-to-your-s3-bucket-this-function-is-particularly-useful-for-batch-uploading-model-files-and-associated-configurations","title":"Define a function upload_files that recursively uploads all files from a specified folder to your S3 bucket. This function is particularly useful for batch uploading model files and associated configurations.","text":"<pre><code>def upload_files(folder_path, bucket_name, prefix, s3_client):\n    for subdir, dirs, files in os.walk(folder_path):\n        for file in files:\n            try:\n                # Create the full local path of the file\n                full_path = os.path.join(subdir, file)\n\n                # Create the relative path (relative to the folder_path)\n                relative_path = os.path.relpath(full_path, folder_path)\n\n                # Use the relative path as the prefix for the S3 object key\n                s3_key = relative_path.replace(\"\\\\\", \"/\")  # Ensure compatibility with S3 (Unix-style paths)\n                s3_key = prefix+s3_key\n\n                # Upload the file\n                s3_client.upload_file(full_path, bucket_name, s3_key)\n\n                print(f\"Successfully uploaded {relative_path} to {s3_key}\")\n            except Exception as ex:\n                print(f\"Failed to upload {relative_path} to {s3_key}\\n{ex}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#step-3-configure-s3-client-and-execute-functions","title":"Step 3: Configure S3 Client and Execute Functions","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#before-executing-the-upload-and-download-functions-configure-your-s3-client-with-your-aws-credentials-ensure-your-aws-access-key-id-and-aws-secret-access-key-are-securely-stored-and-not-hard-coded-or-exposed-in-your-scripts","title":"Before executing the upload and download functions, configure your S3 client with your AWS credentials. Ensure your AWS Access Key ID and AWS Secret Access Key are securely stored and not hard-coded or exposed in your scripts.","text":"<pre><code>s3_client = boto3.client(\n    's3',\n    aws_access_key_id=_aws_access_key_id,\n    aws_secret_access_key=_aws_secret_access_key\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#now-call-the-upload_files-function-to-upload-your-model-directory-to-s3","title":"Now, call the upload_files function to upload your model directory to S3.","text":"<pre><code>upload_files(_folder_path, _bucket, _prefix, s3_client)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#optional-use-our-sdk-to-upload-files-to-an-s3-bucket","title":"(OPTIONAL) Use Our SDK to Upload Files to an S3 Bucket","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/Preparation/Upload-LLM/#you-can-also-conveniently-upload-files-to-an-s3-bucket-using-our-sdk-which-provides-seamless-integration-and-simplifies-the-process","title":"You can also conveniently upload files to an S3 bucket using our SDK, which provides seamless integration and simplifies the process.","text":"<pre><code>import practicuscore as prt\n\n\n_upload_conf = prt.connections.UploadS3Conf(\n    bucket=_bucket,\n    prefix=_prefix,\n    folder_path=_folder_path,\n    source_path_to_cut=_source_path_to_cut,\n    aws_access_key_id=_aws_access_key_id,\n    aws_secret_access_key=_aws_secret_access_key\n)\n\nprt.connections.upload_to_s3(_upload_conf)\n</code></pre> <p>Previous: Model Download | Next: Basic Deployment &gt; Model</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/","title":"Consume LLM API","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#this-tutorial-demonstrates-how-to-interact-with-a-practicusai-llm-deployment-for-making-predictions-using-simple-api-requests","title":"This tutorial demonstrates how to interact with a PracticusAI LLM deployment for making predictions using simple api requests.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#the-workflow-illustrates-obtaining-a-session-token-invoking-the-llm-api-endpoint-and-processing-responses-in-parallel","title":"The workflow illustrates obtaining a session token, invoking the LLM API endpoint, and processing responses in parallel.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>api_url = None # e.g. 'https://company.practicus.com/llm-models/llama-1b-basic-test/'\n</code></pre> <pre><code>assert api_url, \"Please enter your model api url.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#use-the-practicusai-sdk-to-generate-a-session-token-ensuring-secure-access-to-the-llm-api","title":"Use the PracticusAI SDK to generate a session token, ensuring secure access to the LLM API.","text":"<pre><code>import practicuscore as prt\n\n\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#send-a-get-request-with-the-session-token-to-check-if-the-model-and-its-api-are-active-and-ready-for-use","title":"Send a GET request with the session token to check if the model and its API are active and ready for use.","text":"<pre><code>from requests import get\n\nheaders = {'authorization': f'Bearer {token}'}\nr = get(api_url + '?get_meta=true', headers=headers)\n\nprint('Model details: ', r.text)\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#interacting-with-the-llm-api-to-retrieve-a-response-measuring-performance-and-analyzing-the-results","title":"Interacting with the LLM API to retrieve a response, measuring performance, and analyzing the results","text":"<pre><code>from requests import get\nimport json\n\n# Provide a user prompt to the LLM API and retrieve the generated response.\ndata = {\n    #'system_context': '',\n    'user_prompt': \"Who is Einstein?\"\n    }\nr = get(api_url, headers=headers, json=data)\n\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n\n# Print API response for generated prediction\nprint('Prediction result:')\ntry:\n    parsed = json.loads(r.text)\n    print(json.dumps(parsed, indent=1))\nexcept:\n    print(r.text)\n\n# Examine response headers for debugging or additional metadata about the request.\nprint(\"Headers: \", r.headers)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/consume-parallel/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\" # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model to generate a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()   \n    return {\n        'answer': f'Time:{total_time}\\nanswer:{text}'\n    }\n</code></pre> <p>Previous: Deploy | Next: LangChain Deployment &gt; Model</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/","title":"Deploy","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#deploying-a-model-with-practicus-ai-involves-a-sequence-of-steps-designed-to-securely-and-efficiently-transition-a-model-from-development-to-a-production-ready-state-heres-a-step-by-step-explanation-aimed-at-providing-clarity-and-guidance","title":"Deploying a model with Practicus AI involves a sequence of steps designed to securely and efficiently transition a model from development to a production-ready state. Here's a step-by-step explanation, aimed at providing clarity and guidance:","text":"<pre><code>import practicuscore as prt\nregion = prt.get_region() # The region where the deployments are stored\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#defining-parameters","title":"Defining parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#this-section-defines-key-parameters-for-the-notebook-parameters-control-the-behavior-of-the-code-making-it-easy-to-customize-without-altering-the-logic-by-centralizing-parameters-at-the-start-we-ensure-better-readability-maintainability-and-adaptability-for-different-use-cases","title":"This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.","text":"<pre><code>_deployment_key = None # e.g. \"llm-depl\"\n_prefix = None # e.g. \"llm-models\"\n_model_name = None # e.g. \"llama-1b-basic-test\"\n</code></pre> <pre><code>assert _deployment_key and _prefix and _model_name, \"Please enter your deployment parameters.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code># Let's list our model deployments and select one of them.\nmy_model_deployments = region.model_deployment_list\ndisplay(my_model_deployments.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#deploying-the-model","title":"Deploying the Model","text":"<pre><code>prt.models.deploy(\n    deployment_key=_deployment_key,\n    prefix=_prefix, \n    model_name=_model_name, \n    model_dir=None # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#model-deployment-a-call-to-deploy-initiates-the-deployment-process-it-requires-the-host-url-the-obtained-auth_token-and-other-previously-defined-parameters","title":"Model Deployment: A call to deploy() initiates the deployment process. It requires the host URL, the obtained auth_token, and other previously defined parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#feedback-upon-successful-deployment-youll-receive-a-confirmation-if-authentication-fails-or-other-issues-arise-youll-be-prompted-with-an-error-message-to-help-diagnose-and-resolve-the-issue","title":"Feedback: Upon successful deployment, you'll receive a confirmation. If authentication fails or other issues arise, you'll be prompted with an error message to help diagnose and resolve the issue.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#this-process-encapsulates-a-secure-and-structured-approach-to-model-deployment-in-practicus-ai-leveraging-the-datapipeline-for-effective-model-management-by-following-these-steps-you-ensure-that-your-model-is-deployed-to-the-right-environment-with-the-appropriate-configurations-ready-for-inference-at-scale-this-systematic-approach-not-only-simplifies-the-deployment-process-but-also-emphasizes-security-and-organization-critical-factors-for-successful-ai-project-implementations","title":"This process encapsulates a secure and structured approach to model deployment in Practicus AI, leveraging the DataPipeline for effective model management. By following these steps, you ensure that your model is deployed to the right environment with the appropriate configurations, ready for inference at scale. This systematic approach not only simplifies the deployment process but also emphasizes security and organization, critical factors for successful AI project implementations.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/deploy/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\" # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model to generate a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()   \n    return {\n        'answer': f'Time:{total_time}\\nanswer:{text}'\n    }\n</code></pre> <p>Previous: Model Json | Next: Consume Parallel</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/","title":"Model.json","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#the-provided-modeljson-snippet-exemplifies-how-configuration-files-are-used-to-specify-operational-parameters-for-deploying-and-running-large-language-models-llms-within-an-ecosystem-like-practicus-ai-this-json-configuration-plays-a-critical-role-in-streamlining-the-deployment-process-enhancing-model-management-and-ensuring-the-model-operates-efficiently-within-its-environment-heres-an-explanation-of-why-this-modeljson-content-is-significant","title":"The provided model.json snippet exemplifies how configuration files are used to specify operational parameters for deploying and running Large Language Models (LLMs) within an ecosystem like Practicus AI. This JSON configuration plays a critical role in streamlining the deployment process, enhancing model management and ensuring the model operates efficiently within its environment. Here's an explanation of why this model.json content is significant:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#specifying-resource-locations","title":"Specifying Resource Locations","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#download_files_from-cachellama-1b-instruct","title":"\"download_files_from\": \"cache/llama-1b-instruct/\":","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#this-key-value-pair-indicates-the-directory-or-path-from-which-the-necessary-model-files-should-be-downloaded-in-the-context-of-deploying-an-llm-these-files-could-include-the-model-weights-tokenizer-files-and-any-other-dependencies-required-for-the-model-to-run-this-parameter-ensures-that-the-deployment-system-knows-where-to-fetch-the-models-components-which-is-crucial-for-initializing-the-model-in-the-target-environment","title":"This key-value pair indicates the directory or path from which the necessary model files should be downloaded. In the context of deploying an LLM, these files could include the model weights, tokenizer files, and any other dependencies required for the model to run. This parameter ensures that the deployment system knows where to fetch the model's components, which is crucial for initializing the model in the target environment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#customizable-download-target","title":"Customizable Download Target","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#_comment-you-can-also-define-download_files_to-otherwise-varpracticuscache-is-used-this-comment-within-the-json-highlights-an-optional-parameter-that-could-be-specified-in-a-similar-json-configuration-file-if-the-download_files_to-parameter-is-provided-it-would-dictate-the-destination-directory-on-the-local-system-where-the-downloaded-files-should-be-stored-in-the-absence-of-this-parameter-a-default-location-varpracticuscache-is-used-this-flexibility-allows-for-adaptability-to-different-deployment-environments-and-configurations-ensuring-that-the-files-are-stored-in-a-location-that-is-accessible-and-appropriate-for-the-models-operation","title":"\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\": This comment within the JSON highlights an optional parameter that could be specified in a similar JSON configuration file. If the download_files_to parameter is provided, it would dictate the destination directory on the local system where the downloaded files should be stored. In the absence of this parameter, a default location (/var/practicus/cache) is used. This flexibility allows for adaptability to different deployment environments and configurations, ensuring that the files are stored in a location that is accessible and appropriate for the model's operation.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#modeljson_1","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model-json/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\" # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model to generate a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()   \n    return {\n        'answer': f'Time:{total_time}\\nanswer:{text}'\n    }\n</code></pre> <p>Previous: Model | Next: Deploy</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/","title":"Preparation of Model File","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#the-modelpy-file-is-a-critical-component-when-deploying-a-large-language-model-llm-in-environments-like-practicus-ai-it-encapsulates-the-logic-for-initializing-the-model-making-predictions-and-cleaning-up-resources-below-we-will-provide-a-detailed-explanation-of-a-modelpy-script-designed-for-deploying-an-llm-ensuring-no-step-is-overlooked","title":"The model.py file is a critical component when deploying a Large Language Model (LLM) in environments like Practicus AI. It encapsulates the logic for initializing the model, making predictions, and cleaning up resources. Below, we will provide a detailed explanation of a model.py script designed for deploying an LLM, ensuring no step is overlooked.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#import-statements","title":"Import Statements","text":"<pre><code>import sys \nfrom datetime import datetime\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#global-variables","title":"Global Variables","text":"<pre><code>generator = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#generator-holds-the-model-instance-initialized-as-none-and-later-assigned-the-llm-object","title":"generator: Holds the model instance. Initialized as None and later assigned the LLM object.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#initialization-function","title":"Initialization Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#the-init-function-attempts-to-import-the-llama-library-and-build-the-model-with-specified-parameters","title":"The <code>init</code> function  attempts to import the LLaMA library and build the model with specified parameters.","text":"<pre><code>async def init(model_meta=None, *args, **kwargs):\n    global generator\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\" # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#cleanup-function","title":"Cleanup Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#this-function-is-designed-to-free-up-resources-once-theyre-no-longer-needed-setting-generator-back-to-none-and-clearing-the-gpu-memory-cache-to-prevent-memory-leaks-crucial-for-maintaining-performance","title":"This function is designed to free up resources once they're no longer needed, setting generator back to None and clearing the GPU memory cache to prevent memory leaks, crucial for maintaining performance.","text":"<pre><code>async def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n    global generator\n    generator = None\n    from torch import cuda\n    cuda.empty_cache()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#prediction-wrapper-function","title":"Prediction Wrapper Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#the-predict-function-processes-user-input-and-generates-responses-using-the-llm-key-steps-include","title":"The <code>predict</code> function processes user input and generates responses using the LLM. Key steps include:","text":"<pre><code>async def predict(payload_dict: dict, **kwargs):\n\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model for generating a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()   \n    return {\n        'answer': f'Time:{total_time}\\answer:{text}'\n    }\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#this-modelpy-script-outlines-a-robust-framework-for-deploying-and-interacting-with-a-llm-in-a-scalable-asynchronous-manner-it-highlights-essential-practices-like-dynamic-library-loading-concurrent-processing-with-threads-resource-management-and-detailed-logging-for-performance-monitoring-this-setup-is-adaptable-to-various-models-and-can-be-tailored-to-fit-specific-requirements-of-different-llm-deployments","title":"This model.py script outlines a robust framework for deploying and interacting with a LLM in a scalable, asynchronous manner. It highlights essential practices like dynamic library loading, concurrent processing with threads, resource management, and detailed logging for performance monitoring. This setup is adaptable to various models and can be tailored to fit specific requirements of different LLM deployments.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/basic-deployment/model/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\" # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    # Recording the start time to measure execution duration.\n    start = datetime.now()\n\n    # Extracting given prompt from the http request\n    sentence = payload_dict[\"user_prompt\"]\n\n    # Passing the prompt to the `generator`, loaded llm model to generate a response.\n    res = generator([sentence])\n    text = res[0]\n\n    # Returning a structured response containing the generated text and execution time.\n    total_time = (datetime.now() - start).total_seconds()   \n    return {\n        'answer': f'Time:{total_time}\\nanswer:{text}'\n    }\n</code></pre> <p>Previous: Upload LLM | Next: Model Json</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/","title":"Consume LLM API With ChatPracticus","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#this-tutorial-demonstrates-how-to-interact-with-a-practicusai-llm-deployment-for-making-predictions-using-the-practicusai-sdk-the-methods-used-include-chatpracticus-for-invoking-the-model-endpoint-and-practicuscore-for-managing-api-tokens","title":"This tutorial demonstrates how to interact with a PracticusAI LLM deployment for making predictions using the PracticusAI SDK. The methods used include <code>ChatPracticus</code> for invoking the model endpoint and <code>practicuscore</code> for managing API tokens.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#the-workflow-illustrates-obtaining-a-session-token-invoking-the-llm-api-endpoint-and-processing-responses-in-parallel","title":"The workflow illustrates obtaining a session token, invoking the LLM API endpoint, and processing responses in parallel.","text":"<pre><code>from langchain_practicus import ChatPracticus\nimport practicuscore as prt\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>api_url = None # Model API e.g. \"https://company.practicus.com/llm-models/llama-3b-chain-test/\"\n</code></pre> <pre><code>assert api_url, \"Please enter your model api url.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#the-test_langchain_practicus-function-is-defined-to-interact-with-the-practicusai-model-endpoint-it-uses-the-chatpracticus-object-to-invoke-the-api-with-the-provided-url-token-and-input-data-the-response-is-printed-in-two-formats-a-raw-dictionary-and-its-content","title":"The <code>test_langchain_practicus</code> function is defined to interact with the PracticusAI model endpoint. It uses the <code>ChatPracticus</code> object to invoke the API with the provided URL, token, and input data. The response is printed in two formats: a raw dictionary and its content.","text":"<pre><code>def test_langchain_practicus(api_url, token, inputs):\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n\n    print(\"\\n\\nReceived response:\\n\", dict(response))\n    print(\"\\n\\nReceived Content:\\n\", response.content)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#we-retrieve-an-api-session-token-using-practicusai-sdk-this-token-is-required-to-authenticate-and-interact-with-the-practicusai-deployment","title":"We retrieve an API session token using PracticusAI SDK. This token is required to authenticate and interact with the PracticusAI deployment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#the-method-below-creates-a-token-that-is-valid-for-4-hours-longer-tokens-can-be-retrieved-from-the-admin-console","title":"The method below creates a token that is valid for 4 hours, longer tokens can be retrieved from the admin console.","text":"<pre><code>token = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#we-invoke-the-test_langchain_practicus-function-with-the-api-url-session-token-and-an-example-query-what-is-the-capital-of-england-the-function-sends-the-query-to-the-practicusai-endpoint-and-prints-the-received-response","title":"We invoke the <code>test_langchain_practicus</code> function with the API URL, session token, and an example query, <code>'What is the capital of England?'</code>. The function sends the query to the PracticusAI endpoint and prints the received response.","text":"<pre><code>test_langchain_practicus(api_url, token, ['What is the capital of England?'])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#consume-llm-api-with-basic-http-requests","title":"Consume LLM API With basic HTTP requests","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#use-the-practicusai-sdk-to-generate-a-session-token-ensuring-secure-access-to-the-llm-api","title":"Use the PracticusAI SDK to generate a session token, ensuring secure access to the LLM API.","text":"<pre><code>import practicuscore as prt\n\n# We will be using using the SDK to get a session token.\napi_url = None # Model API e.g. \"https://company.practicus.com/llm-models/llama-3b-chain-test/\"\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#send-a-get-request-with-the-session-token-to-check-if-the-model-and-its-api-are-active-and-ready-for-use","title":"Send a GET request with the session token to check if the model and its API are active and ready for use.","text":"<pre><code>from requests import get\n\nheaders = {'authorization': f'Bearer {token}'}\nr = get(api_url + '?get_meta=true', headers=headers)\n\nprint('Model details: ', r.text)\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#interacting-with-the-llm-api-to-retrieve-a-response-measuring-performance-and-analyzing-the-results","title":"Interacting with the LLM API to retrieve a response, measuring performance, and analyzing the results","text":"<pre><code>from requests import get\nimport json\n\n# Provide a user prompt to the LLM API and retrieve the generated response.\ndata = {\n    #'system_context': '',\n    'user_prompt': \"Who is Nikola Tesla?\"\n    }\nr = get(api_url, headers=headers, json=data)\n\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n\n# Print API response for generated prediction\nprint('Prediction result:')\ntry:\n    parsed = json.loads(r.text)\n    print(json.dumps(parsed, indent=1))\nexcept:\n    print(r.text)\n\n# Examine response headers for debugging or additional metadata about the request.\nprint(\"Headers: \", r.headers)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/consume-parallel/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()   \n        return {\n            'answer': f'Time:{total_time}\\nanswer:{text}'\n        }\n\n    # For langchain applications:\n    else: \n\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item['content'] for item in payload['messages']])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0]['generated_text']\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload['lang_model'],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre> <p>Previous: Deploy | Next: Email E Assistant &gt; Mail E-Assistant</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/","title":"Deploy","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#deploying-a-model-with-practicus-ai-involves-a-sequence-of-steps-designed-to-securely-and-efficiently-transition-a-model-from-development-to-a-production-ready-state-heres-a-step-by-step-explanation-aimed-at-providing-clarity-and-guidance","title":"Deploying a model with Practicus AI involves a sequence of steps designed to securely and efficiently transition a model from development to a production-ready state. Here's a step-by-step explanation, aimed at providing clarity and guidance:","text":"<pre><code>import practicuscore as prt\nregion = prt.get_region() # The region where the deployments are stored\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#defining-parameters","title":"Defining parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#this-section-defines-key-parameters-for-the-notebook-parameters-control-the-behavior-of-the-code-making-it-easy-to-customize-without-altering-the-logic-by-centralizing-parameters-at-the-start-we-ensure-better-readability-maintainability-and-adaptability-for-different-use-cases","title":"This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.","text":"<pre><code>_deployment_key = None # e.g. \"llm-depl\"\n_prefix = None # e.g. \"llm-models\"\n_model_name = None # e.g. \"llama-1b-combined-test\"\n</code></pre> <pre><code>assert _deployment_key and _prefix and _model_name, \"Please enter your deployment parameters.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code># Let's list our model deployments and select one of them.\nmy_model_deployments = region.model_deployment_list\ndisplay(my_model_deployments.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#deploying-the-model","title":"Deploying the Model","text":"<pre><code>prt.models.deploy(\n    deployment_key=_deployment_key,\n    prefix=_prefix, \n    model_name=_model_name, \n    model_dir=None # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#model-deployment-a-call-to-deploy-initiates-the-deployment-process-it-requires-the-host-url-the-obtained-auth_token-and-other-previously-defined-parameters","title":"Model Deployment: A call to deploy() initiates the deployment process. It requires the host URL, the obtained auth_token, and other previously defined parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#feedback-upon-successful-deployment-youll-receive-a-confirmation-if-authentication-fails-or-other-issues-arise-youll-be-prompted-with-an-error-message-to-help-diagnose-and-resolve-the-issue","title":"Feedback: Upon successful deployment, you'll receive a confirmation. If authentication fails or other issues arise, you'll be prompted with an error message to help diagnose and resolve the issue.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#this-process-encapsulates-a-secure-and-structured-approach-to-model-deployment-in-practicus-ai-leveraging-the-datapipeline-for-effective-model-management-by-following-these-steps-you-ensure-that-your-model-is-deployed-to-the-right-environment-with-the-appropriate-configurations-ready-for-inference-at-scale-this-systematic-approach-not-only-simplifies-the-deployment-process-but-also-emphasizes-security-and-organization-critical-factors-for-successful-ai-project-implementations","title":"This process encapsulates a secure and structured approach to model deployment in Practicus AI, leveraging the DataPipeline for effective model management. By following these steps, you ensure that your model is deployed to the right environment with the appropriate configurations, ready for inference at scale. This systematic approach not only simplifies the deployment process but also emphasizes security and organization, critical factors for successful AI project implementations.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/deploy/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()   \n        return {\n            'answer': f'Time:{total_time}\\nanswer:{text}'\n        }\n\n    # For langchain applications:\n    else: \n\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item['content'] for item in payload['messages']])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0]['generated_text']\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload['lang_model'],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre> <p>Previous: Model Json | Next: Consume Parallel</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/","title":"Model.json","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#the-provided-modeljson-snippet-exemplifies-how-configuration-files-are-used-to-specify-operational-parameters-for-deploying-and-running-large-language-models-llms-within-an-ecosystem-like-practicus-ai-this-json-configuration-plays-a-critical-role-in-streamlining-the-deployment-process-enhancing-model-management-and-ensuring-the-model-operates-efficiently-within-its-environment-heres-an-explanation-of-why-this-modeljson-content-is-significant","title":"The provided model.json snippet exemplifies how configuration files are used to specify operational parameters for deploying and running Large Language Models (LLMs) within an ecosystem like Practicus AI. This JSON configuration plays a critical role in streamlining the deployment process, enhancing model management, and ensuring the model operates efficiently within its environment. Here's an explanation of why this model.json content is significant:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#specifying-resource-locations","title":"Specifying Resource Locations","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#download_files_from-cachellama-1b-instruct","title":"\"download_files_from\": \"cache/llama-1b-instruct/\":","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#this-key-value-pair-indicates-the-directory-or-path-from-which-the-necessary-model-files-should-be-downloaded-in-the-context-of-deploying-an-llm-these-files-could-include-the-model-weights-tokenizer-files-and-any-other-dependencies-required-for-the-model-to-run-this-parameter-ensures-that-the-deployment-system-knows-where-to-fetch-the-models-components-which-is-crucial-for-initializing-the-model-in-the-target-environment","title":"This key-value pair indicates the directory or path from which the necessary model files should be downloaded. In the context of deploying an LLM, these files could include the model weights, tokenizer files, and any other dependencies required for the model to run. This parameter ensures that the deployment system knows where to fetch the model's components, which is crucial for initializing the model in the target environment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#customizable-download-target","title":"Customizable Download Target","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#_comment-you-can-also-define-download_files_to-otherwise-varpracticuscache-is-used-this-comment-within-the-json-highlights-an-optional-parameter-that-could-be-specified-in-a-similar-json-configuration-file-if-the-download_files_to-parameter-is-provided-it-would-dictate-the-destination-directory-on-the-local-system-where-the-downloaded-files-should-be-stored-in-the-absence-of-this-parameter-a-default-location-varpracticuscache-is-used-this-flexibility-allows-for-adaptability-to-different-deployment-environments-and-configurations-ensuring-that-the-files-are-stored-in-a-location-that-is-accessible-and-appropriate-for-the-models-operation","title":"\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\": This comment within the JSON highlights an optional parameter that could be specified in a similar JSON configuration file. If the download_files_to parameter is provided, it would dictate the destination directory on the local system where the downloaded files should be stored. In the absence of this parameter, a default location (/var/practicus/cache) is used. This flexibility allows for adaptability to different deployment environments and configurations, ensuring that the files are stored in a location that is accessible and appropriate for the model's operation.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#modeljson_1","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model-json/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()   \n        return {\n            'answer': f'Time:{total_time}\\nanswer:{text}'\n        }\n\n    # For langchain applications:\n    else: \n\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item['content'] for item in payload['messages']])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0]['generated_text']\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload['lang_model'],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre> <p>Previous: Model | Next: Deploy</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/","title":"Preparation of Model File","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#this-section-provides-a-detailed-explanation-of-the-code-used-to-deploy-a-model-catering-to-both-the-langchain-compatible-large-language-model-llm-api-endpoint-via-the-practicusai-sdk-and-standard-llm-deployments-for-text-in-text-out-tasks-the-modelpy-script-serves-as-the-core-of-this-implementation-managing-model-initialization-payload-processing-and-response-generation-below-we-offer-a-comprehensive-breakdown-of-each-segment","title":"This section provides a detailed explanation of the code used to deploy a model, catering to both the LangChain-compatible Large Language Model (LLM) API endpoint via the PracticusAI SDK and standard LLM deployments for text-in, text-out tasks. The model.py script serves as the core of this implementation, managing model initialization, payload processing, and response generation. Below, we offer a comprehensive breakdown of each segment:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#import-statements","title":"Import Statements","text":"<pre><code>import sys \nfrom datetime import datetime\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#global-variables","title":"Global Variables","text":"<pre><code>generator = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#generator-holds-the-model-instance-initialized-as-none-and-later-assigned-the-llm-object","title":"generator: Holds the model instance. Initialized as None and later assigned the LLM object.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#sys-used-for-interacting-with-the-interpreter-including-adding-paths-for-python-to-search-for-modules","title":"sys: Used for interacting with the interpreter, including adding paths for Python to search for modules.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#datetime-facilitates-recording-timestamps-useful-for-performance-monitoring","title":"datetime: Facilitates recording timestamps, useful for performance monitoring.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#initialization-function","title":"Initialization Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#the-init-function-attempts-to-import-the-llama-library-and-build-the-model-with-specified-parameters","title":"The <code>init</code> function  attempts to import the LLaMA library and build the model with specified parameters.","text":"<pre><code>async def init(model_meta=None, *args, **kwargs):\n    global generator\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\" # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#cleanup-function","title":"Cleanup Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#this-function-is-designed-to-free-up-resources-once-theyre-no-longer-needed-setting-generator-back-to-none-and-clearing-the-gpu-memory-cache-to-prevent-memory-leaks-crucial-for-maintaining-performance","title":"This function is designed to free up resources once they're no longer needed, setting generator back to None and clearing the GPU memory cache to prevent memory leaks, crucial for maintaining performance.","text":"<pre><code>async def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n    global generator\n    generator = None\n    from torch import cuda\n    cuda.empty_cache()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#prediction-wrapper-function","title":"Prediction Wrapper Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#the-predict-function-processes-user-input-and-generates-responses-using-the-llm-key-steps-include","title":"The <code>predict</code> function processes user input and generates responses using the LLM. Key steps include:","text":"<pre><code>async def predict(payload_dict: dict, **kwargs):\n\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()   \n        return {\n            'answer': f'Time:{total_time}\\answer:{text}'\n        }\n\n    # For langchaing applications:\n    else: \n\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item['content'] for item in payload['messages']])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0]['generated_text']\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload['lang_model'],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#this-modelpy-script-outlines-a-robust-framework-for-deploying-and-interacting-with-a-llm-in-a-scalable-asynchronous-manner-it-highlights-essential-practices-like-dynamic-library-loading-concurrent-processing-with-threads-resource-management-and-detailed-logging-for-performance-monitoring-this-setup-is-adaptable-to-various-models-and-can-be-tailored-to-fit-specific-requirements-of-different-llm-deployments","title":"This model.py script outlines a robust framework for deploying and interacting with a LLM in a scalable, asynchronous manner. It highlights essential practices like dynamic library loading, concurrent processing with threads, resource management, and detailed logging for performance monitoring. This setup is adaptable to various models and can be tailored to fit specific requirements of different LLM deployments.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/combined-method/model/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    # For basic text-in, text-out task:\n    if \"user_prompt\" in payload_dict:\n\n        # Recording the start time to measure execution duration.\n        start = datetime.now()\n\n        # Extracting given prompt from the http request\n        sentence = payload_dict[\"user_prompt\"]\n\n        # Passing the prompt to the `generator`, loaded llm model to generate a response.\n        res = generator([sentence])\n        text = res[0]\n\n        # Returning a structured response containing the generated text and execution time.\n        total_time = (datetime.now() - start).total_seconds()   \n        return {\n            'answer': f'Time:{total_time}\\nanswer:{text}'\n        }\n\n    # For langchain applications:\n    else: \n\n        from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n        # The payload dictionary is validated against PrtLangRequest.\n        practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n        # Converts the validated request object to a dictionary.\n        data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n        payload = json.loads(data_js)\n\n        # Joins the content field from all messages in the payload to form the prompt string.\n        prompt = \" \".join([item['content'] for item in payload['messages']])\n\n        # Generate a response from the model\n        response = generator(prompt)\n        answer = response[0]['generated_text']\n\n        # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n        resp = PrtLangResponse(\n            content=answer,\n            lang_model=payload['lang_model'],\n            input_tokens=0,\n            output_tokens=0,\n            total_tokens=0,\n            # additional_kwargs={\n            #     \"some_additional_info\": \"test 123\",\n            # },\n        )\n\n        return resp\n</code></pre> <p>Previous: Consume Parallel | Next: Model Json</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/","title":"Consume LLM API","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#this-tutorial-demonstrates-how-to-interact-with-a-practicusai-llm-deployment-for-making-predictions-using-the-practicusai-sdk-the-methods-used-include-chatpracticus-for-invoking-the-model-endpoint-and-practicuscore-for-managing-api-tokens","title":"This tutorial demonstrates how to interact with a PracticusAI LLM deployment for making predictions using the PracticusAI SDK. The methods used include <code>ChatPracticus</code> for invoking the model endpoint and <code>practicuscore</code> for managing API tokens.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#the-workflow-illustrates-obtaining-a-session-token-invoking-the-llm-api-endpoint-and-processing-responses-in-parallel","title":"The workflow illustrates obtaining a session token, invoking the LLM API endpoint, and processing responses in parallel.","text":"<pre><code>from langchain_practicus import ChatPracticus\nimport practicuscore as prt\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>api_url = None # E.g. \"https://company.practicus.com/llm-models/llama-3b-chain-test/\"\n</code></pre> <pre><code>assert api_url, \"Please enter your model api url.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#the-test_langchain_practicus-function-is-defined-to-interact-with-the-practicusai-model-endpoint-it-uses-the-chatpracticus-object-to-invoke-the-api-with-the-provided-url-token-and-input-data-the-response-is-printed-in-two-formats-a-raw-dictionary-and-its-content","title":"The <code>test_langchain_practicus</code> function is defined to interact with the PracticusAI model endpoint. It uses the <code>ChatPracticus</code> object to invoke the API with the provided URL, token, and input data. The response is printed in two formats: a raw dictionary and its content.","text":"<pre><code>def test_langchain_practicus(api_url, token, inputs):\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n\n    print(\"\\n\\nReceived response:\\n\", dict(response))\n    print(\"\\n\\nReceived Content:\\n\", response.content)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#we-retrieve-an-api-session-token-using-practicusai-sdk-this-token-is-required-to-authenticate-and-interact-with-the-practicusai-deployment","title":"We retrieve an API session token using PracticusAI SDK. This token is required to authenticate and interact with the PracticusAI deployment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#the-method-below-creates-a-token-that-is-valid-for-4-hours-longer-tokens-can-be-retrieved-from-the-admin-console","title":"The method below creates a token that is valid for 4 hours, longer tokens can be retrieved from the admin console.","text":"<pre><code>token = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#we-invoke-the-test_langchain_practicus-function-with-the-api-url-session-token-and-an-example-query-what-is-the-capital-of-england-the-function-sends-the-query-to-the-practicusai-endpoint-and-prints-the-received-response","title":"We invoke the <code>test_langchain_practicus</code> function with the API URL, session token, and an example query, <code>'What is the capital of England?'</code>. The function sends the query to the PracticusAI endpoint and prints the received response.","text":"<pre><code>test_langchain_practicus(api_url, token, ['What is the capital of England?'])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/consume-parallel/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item['content'] for item in payload['messages']])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0]['generated_text']\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload['lang_model'],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Deploy | Next: Combined Method &gt; Model</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/","title":"Deploy","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#deploying-a-model-with-practicus-ai-involves-a-sequence-of-steps-designed-to-securely-and-efficiently-transition-a-model-from-development-to-a-production-ready-state-heres-a-step-by-step-explanation-aimed-at-providing-clarity-and-guidance","title":"Deploying a model with Practicus AI involves a sequence of steps designed to securely and efficiently transition a model from development to a production-ready state. Here's a step-by-step explanation, aimed at providing clarity and guidance:","text":"<pre><code>import practicuscore as prt\nregion = prt.get_region() # The region where the deployments are stored\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#defining-parameters","title":"Defining parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#this-section-defines-key-parameters-for-the-notebook-parameters-control-the-behavior-of-the-code-making-it-easy-to-customize-without-altering-the-logic-by-centralizing-parameters-at-the-start-we-ensure-better-readability-maintainability-and-adaptability-for-different-use-cases","title":"This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.","text":"<pre><code>_deployment_key = None # e.g. \"llm-depl\"\n_prefix = None # e.g. \"llm-models\"\n_model_name = None # e.g. \"llama-1b-chain-test\"\n</code></pre> <pre><code>assert _deployment_key and _prefix and _model_name, \"Please enter your deployment parameters.\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code># Let's list our model deployments and select one of them.\nmy_model_deployments = region.model_deployment_list\ndisplay(my_model_deployments.to_pandas())\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#deploying-the-model","title":"Deploying the Model","text":"<pre><code>prt.models.deploy(\n    deployment_key=_deployment_key,\n    prefix=_prefix, \n    model_name=_model_name, \n    model_dir=None # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#model-deployment-a-call-to-deploy-initiates-the-deployment-process-it-requires-the-host-url-the-obtained-auth_token-and-other-previously-defined-parameters","title":"Model Deployment: A call to deploy() initiates the deployment process. It requires the host URL, the obtained auth_token, and other previously defined parameters.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#feedback-upon-successful-deployment-youll-receive-a-confirmation-if-authentication-fails-or-other-issues-arise-youll-be-prompted-with-an-error-message-to-help-diagnose-and-resolve-the-issue","title":"Feedback: Upon successful deployment, you'll receive a confirmation. If authentication fails or other issues arise, you'll be prompted with an error message to help diagnose and resolve the issue.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#this-process-encapsulates-a-secure-and-structured-approach-to-model-deployment-in-practicus-ai-leveraging-the-datapipeline-for-effective-model-management-by-following-these-steps-you-ensure-that-your-model-is-deployed-to-the-right-environment-with-the-appropriate-configurations-ready-for-inference-at-scale-this-systematic-approach-not-only-simplifies-the-deployment-process-but-also-emphasizes-security-and-organization-critical-factors-for-successful-ai-project-implementations","title":"This process encapsulates a secure and structured approach to model deployment in Practicus AI, leveraging the DataPipeline for effective model management. By following these steps, you ensure that your model is deployed to the right environment with the appropriate configurations, ready for inference at scale. This systematic approach not only simplifies the deployment process but also emphasizes security and organization, critical factors for successful AI project implementations.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/deploy/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item['content'] for item in payload['messages']])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0]['generated_text']\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload['lang_model'],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Model Json | Next: Consume Parallel</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/","title":"Model.json","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#the-provided-modeljson-snippet-exemplifies-how-configuration-files-are-used-to-specify-operational-parameters-for-deploying-and-running-large-language-models-llms-within-an-ecosystem-like-practicus-ai-this-json-configuration-plays-a-critical-role-in-streamlining-the-deployment-process-enhancing-model-management-and-ensuring-the-model-operates-efficiently-within-its-environment-heres-an-explanation-of-why-this-modeljson-content-is-significant","title":"The provided model.json snippet exemplifies how configuration files are used to specify operational parameters for deploying and running Large Language Models (LLMs) within an ecosystem like Practicus AI. This JSON configuration plays a critical role in streamlining the deployment process, enhancing model management, and ensuring the model operates efficiently within its environment. Here's an explanation of why this model.json content is significant:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#specifying-resource-locations","title":"Specifying Resource Locations","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#download_files_from-cachellama-1b-instruct","title":"\"download_files_from\": \"cache/llama-1b-instruct/\":","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#this-key-value-pair-indicates-the-directory-or-path-from-which-the-necessary-model-files-should-be-downloaded-in-the-context-of-deploying-an-llm-these-files-could-include-the-model-weights-tokenizer-files-and-any-other-dependencies-required-for-the-model-to-run-this-parameter-ensures-that-the-deployment-system-knows-where-to-fetch-the-models-components-which-is-crucial-for-initializing-the-model-in-the-target-environment","title":"This key-value pair indicates the directory or path from which the necessary model files should be downloaded. In the context of deploying an LLM, these files could include the model weights, tokenizer files, and any other dependencies required for the model to run. This parameter ensures that the deployment system knows where to fetch the model's components, which is crucial for initializing the model in the target environment.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#customizable-download-target","title":"Customizable Download Target","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#_comment-you-can-also-define-download_files_to-otherwise-varpracticuscache-is-used-this-comment-within-the-json-highlights-an-optional-parameter-that-could-be-specified-in-a-similar-json-configuration-file-if-the-download_files_to-parameter-is-provided-it-would-dictate-the-destination-directory-on-the-local-system-where-the-downloaded-files-should-be-stored-in-the-absence-of-this-parameter-a-default-location-varpracticuscache-is-used-this-flexibility-allows-for-adaptability-to-different-deployment-environments-and-configurations-ensuring-that-the-files-are-stored-in-a-location-that-is-accessible-and-appropriate-for-the-models-operation","title":"\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\": This comment within the JSON highlights an optional parameter that could be specified in a similar JSON configuration file. If the download_files_to parameter is provided, it would dictate the destination directory on the local system where the downloaded files should be stored. In the absence of this parameter, a default location (/var/practicus/cache) is used. This flexibility allows for adaptability to different deployment environments and configurations, ensuring that the files are stored in a location that is accessible and appropriate for the model's operation.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#modeljson_1","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model-json/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item['content'] for item in payload['messages']])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0]['generated_text']\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload['lang_model'],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Model | Next: Deploy</p>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/","title":"Preparation of Model File","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#this-section-explains-the-code-for-deploying-a-langchain-compatible-large-language-model-llm-api-endpoint-using-the-practicusai-sdk-the-modelpy-handles-model-initialization-handling-payloads-and-generating-responses-below-is-a-breakdown-of-each-segment","title":"This section explains the code for deploying a LangChain-compatible Large Language Model (LLM) API endpoint using the PracticusAI SDK. The model.py handles model initialization, handling payloads, and generating responses. Below is a breakdown of each segment:","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#import-statements","title":"Import Statements","text":"<pre><code>import sys \nfrom datetime import datetime\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#global-variables","title":"Global Variables","text":"<pre><code>generator = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#generator-holds-the-model-instance-initialized-as-none-and-later-assigned-the-llm-object","title":"generator: Holds the model instance. Initialized as None and later assigned the LLM object.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#sys-used-for-interacting-with-the-interpreter-including-adding-paths-for-python-to-search-for-modules","title":"sys: Used for interacting with the interpreter, including adding paths for Python to search for modules.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#datetime-facilitates-recording-timestamps-useful-for-performance-monitoring","title":"datetime: Facilitates recording timestamps, useful for performance monitoring.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#initialization-function","title":"Initialization Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#the-init-function-attempts-to-import-the-llama-library-and-build-the-model-with-specified-parameters","title":"The <code>init</code> function  attempts to import the LLaMA library and build the model with specified parameters.","text":"<pre><code>async def init(model_meta=None, *args, **kwargs):\n    global generator\n    # Checks if the `generator` is already initialized to avoid redundant model loading.\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    # If `generator` is not already initialised, builds the generator by loading the desired LLM\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\" # for details check 02_model_json\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#cleanup-function","title":"Cleanup Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#this-function-is-designed-to-free-up-resources-once-theyre-no-longer-needed-setting-generator-back-to-none-and-clearing-the-gpu-memory-cache-to-prevent-memory-leaks-crucial-for-maintaining-performance","title":"This function is designed to free up resources once they're no longer needed, setting generator back to None and clearing the GPU memory cache to prevent memory leaks, crucial for maintaining performance.","text":"<pre><code>async def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n    global generator\n    generator = None\n    from torch import cuda\n    cuda.empty_cache()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#prediction-wrapper-function","title":"Prediction Wrapper Function","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#the-predict-function-processes-user-input-and-generates-responses-using-the-llm-key-steps-include","title":"The <code>predict</code> function processes user input and generates responses using the LLM. Key steps include:","text":"<pre><code>async def _predict(payload_dict: dict, **kwargs):\n\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item['content'] for item in payload['messages']])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0]['generated_text']\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload['lang_model'],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#this-modelpy-script-outlines-a-robust-framework-for-deploying-and-interacting-with-a-llm-in-a-scalable-asynchronous-manner-it-highlights-essential-practices-like-dynamic-library-loading-concurrent-processing-with-threads-resource-management-and-detailed-logging-for-performance-monitoring-this-setup-is-adaptable-to-various-models-and-can-be-tailored-to-fit-specific-requirements-of-different-llm-deployments","title":"This model.py script outlines a robust framework for deploying and interacting with a LLM in a scalable, asynchronous manner. It highlights essential practices like dynamic library loading, concurrent processing with threads, resource management, and detailed logging for performance monitoring. This setup is adaptable to various models and can be tailored to fit specific requirements of different LLM deployments.","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/llama-1b-instruct/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/deploying-llm/langchain-deployment/model/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport json\n\ngenerator = None\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    model_cache = \"/var/practicus/cache\"\n    if model_cache not in sys.path:\n        sys.path.insert(0, model_cache)\n\n    try:\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n    except Exception as e:\n        raise print(f\"Failed to import required libraries: {e}\")\n\n    # Initialize the local LLM model using transformers:\n\n    def load_local_llm(model_path):\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForCausalLM.from_pretrained(model_path)\n        model.to('cpu') # Change with cuda or auto to use gpus.\n        return pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=200)\n\n    try:\n        generator = load_local_llm(model_cache)\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\nasync def predict(payload_dict: dict, **kwargs):\n\n    from practicuscore.gen_ai import PrtLangRequest, PrtLangResponse\n\n    # The payload dictionary is validated against PrtLangRequest.\n    practicus_llm_req = PrtLangRequest.model_validate(payload_dict)\n\n    # Converts the validated request object to a dictionary.\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True)\n    payload = json.loads(data_js)\n\n    # Joins the content field from all messages in the payload to form the prompt string.\n    prompt = \" \".join([item['content'] for item in payload['messages']])\n\n    # Generate a response from the model\n    response = generator(prompt)\n    answer = response[0]['generated_text']\n\n    # Creates a PrtLangResponse object with the generated content and metadata about the language model and token usage\n    resp = PrtLangResponse(\n        content=answer,\n        lang_model=payload['lang_model'],\n        input_tokens=0,\n        output_tokens=0,\n        total_tokens=0,\n        # additional_kwargs={\n        #     \"some_additional_info\": \"test 123\",\n        # },\n    )\n\n    return resp\n</code></pre> <p>Previous: Consume Parallel | Next: Model Json</p>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/","title":"Mail E-Assistant","text":""},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None # 'E.g. company.practicus.com'\nembedding_model_path = None\nvector_store = None \nmilvus_uri = None # E.g. 'company.practicus.milvus.com'\n</code></pre> <pre><code>assert host, \"Please enter your host url\" \nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert vector_store in ['ChromaDB', 'MilvusDB'], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"\nif vector_store == 'MilvusDB': \n    assert 'milvus_uri', \"Please enter your milvus connection uri\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#firstly-we-need-install-transformers-and-torch","title":"Firstly we need install transformers and torch","text":"<p>Run at terminal:</p> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#pip-install-transformers-sentence-transformers-langchain-langchain-community-chromadb","title":"pip install transformers sentence-transformers langchain langchain-community chromadb","text":"<ul> <li> <p>Transformers: It allows you to easily use Transformer-based models (such as BERT, GPT, etc.).-</p> </li> <li> <p>Sentence-Transformers: It produces vector representations of sentences using Transformer models.</p> </li> <li> <p>LangChain: It is used to manage more complex workflows with language models. </p> </li> <li> <p>Langchain-Community: Contains additional modules and components developed by the community for the LangChain library.</p> </li> <li> <p>ChromaDB: Used as a vector database. It is optimized for embeddings and similarity searches.</p> </li> <li> <p>PyPDF: A library used to process PDF files with Python. </p> </li> </ul>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#pip-install-torch-index-url-httpsdownloadpytorchorgwhlcpu","title":"pip install torch --index-url https://download.pytorch.org/whl/cpu","text":"<p>This command is used to install the PyTorch library with CPU support.</p> <p>Details:</p> <ul> <li>Torch (PyTorch): A library used for developing machine learning and deep learning models. It offers features such as tensor computations, automatic differentiation, and advanced modeling.</li> <li>--index-url https://download.pytorch.org/whl/cpu: This parameter uses a specific index URL to download the CPU version of PyTorch. If you do not want to install a GPU-specific version, this URL is used.</li> </ul> <pre><code># Prepare Data\nimport os\nimport requests\n\nrepo_owner = \"practicusai\"\nrepo_name = \"sample-data\"\nfile_path = \"hr_assistant\"\nbranch = \"main\"\n\n\nurl = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{file_path}?ref={branch}\"\n\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    files = response.json()  \n\n    for file in files:\n        file_url = file['download_url'] \n        file_name = file['name']  \n\n        file_response = requests.get(file_url)\n        if file_response.status_code == 200:\n\n            with open(file_name, 'wb') as f:\n                f.write(file_response.content)\n            print(f\"'{file_name}' successfully downloaded.\")\n        else:\n            print(f\"'{file_name}' not successfully downloaded.\")\nelse:\n    print(f\"HTTP Status: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#import-libraries","title":"Import Libraries","text":"<pre><code>from langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre> <p>This code initializes and validates the required variables for setting up the environment, including the host URL, embedding model path, and vector store configuration.</p> <ol> <li>Host URL:</li> <li><code>host = None</code>:<ul> <li>Placeholder for the host URL (e.g., <code>'company.practicus.com'</code>).</li> </ul> </li> <li> <p><code>assert host, \"Please enter your host url\"</code>:</p> <ul> <li>Ensures that a valid host URL is provided. Raises an error with the message <code>\"Please enter your host url\"</code> if <code>host</code> is not defined.</li> </ul> </li> <li> <p>Embedding Model Path:</p> </li> <li><code>embedding_model_path = None</code>:<ul> <li>Placeholder for the embedding model's file path.</li> </ul> </li> <li> <p><code>assert embedding_model_path, \"Please enter your embedding model path.\"</code>:</p> <ul> <li>Ensures that a valid embedding model path is provided. Raises an error if the path is not set.</li> </ul> </li> <li> <p>Vector Store Selection:</p> </li> <li><code>vector_store = None</code>:<ul> <li>Placeholder for selecting a vector store (<code>'ChromaDB'</code> or <code>'MilvusDB'</code>).</li> </ul> </li> <li> <p><code>assert vector_store in ['ChromaDB', 'MilvusDB'], \"Vector store must be 'ChromaDB' or 'MilvusDB'.\"</code>:</p> <ul> <li>Ensures that the vector store is either <code>'ChromaDB'</code> or <code>'MilvusDB'</code>. Raises an error if an invalid option is set.</li> </ul> </li> <li> <p>MilvusDB Configuration (Conditional):</p> </li> <li><code>if vector_store == 'MilvusDB':</code>:<ul> <li>Executes the following block only if <code>vector_store</code> is set to <code>'MilvusDB'</code>.</li> </ul> </li> <li><code>milvus_uri = None</code>:<ul> <li>Placeholder for the Milvus connection URI (e.g., <code>'company.practicus.milvus.com'</code>).</li> </ul> </li> <li><code>assert 'milvus_uri', \"Please enter your milvus connection uri\"</code>:<ul> <li>Ensures that a valid <code>milvus_uri</code> is provided when <code>MilvusDB</code> is used.</li> </ul> </li> </ol>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#define-llm-api-function-and-call-chatpracticus-in-this-function","title":"Define llm api function and call ChatPracticus in this function","text":"<p>Function interacts with the ChatPracticus API to send input data and retrieve a response using the provided API URL and token.</p> <ol> <li>Initialize ChatPracticus:</li> <li> <p><code>chat = ChatPracticus(...)</code>:</p> <ul> <li>Creates a <code>ChatPracticus</code> object with the following parameters:</li> <li><code>endpoint_url</code>: The API endpoint URL.</li> <li><code>api_token</code>: The token for authentication.</li> <li><code>model_id</code>: Currently ignored but reserved for future model-specific configurations.</li> </ul> </li> <li> <p>Invoke the API:</p> </li> <li> <p><code>response = chat.invoke(input=inputs)</code>:</p> <ul> <li>Sends the input data to the API and retrieves the response.</li> </ul> </li> <li> <p>Return the Response:</p> </li> <li><code>return(response.content)</code>:<ul> <li>Extracts and returns the content</li> </ul> </li> </ol> <pre><code>def call_llm_api(inputs, api_url, api_token):\n    # We need to give input to 'generate_response'. This function will use our 'api_token' and 'endpoint_url' and return the response.\n\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n\n    return(response.content)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#get-all-mails-and-use-seperator-for-split-questions","title":"Get all mails and use seperator for split questions","text":"<p>This code reads a CSV file containing email data, extracts a specific column, and concatenates all messages into a single string, separated by a custom delimiter <code>//m-n-m//</code>.</p> <ol> <li>Import pandas:</li> <li> <p><code>import pandas as pd</code>:</p> <ul> <li>Imports the pandas library for data manipulation and analysis.</li> </ul> </li> <li> <p>Read the CSV File:</p> </li> <li> <p><code>df = pd.read_csv('single_sender_1k.csv')</code>:</p> <ul> <li>Reads the CSV file <code>single_sender_1k.csv</code> into a pandas DataFrame named <code>df</code>.</li> </ul> </li> <li> <p>Select Relevant Column:</p> </li> <li> <p><code>df = df[['message_sent']]</code>:</p> <ul> <li>Filters the DataFrame to include only the <code>message_sent</code> column, which contains the email messages.</li> </ul> </li> <li> <p>Initialize Merged String:</p> </li> <li> <p><code>merged_mails = ''</code>:</p> <ul> <li>Initializes an empty string <code>merged_mails</code> to store the concatenated messages.</li> </ul> </li> <li> <p>Concatenate Messages:</p> </li> <li><code>for message in df['message_sent']:</code>:<ul> <li>Iterates over each message in the <code>message_sent</code> column.</li> </ul> </li> <li><code>merged_mails = merged_mails + '//m-n-m//' + message</code>:<ul> <li>Appends each message to <code>merged_mails</code>, preceded by the delimiter <code>//m-n-m//</code>.</li> </ul> </li> </ol> <pre><code>import pandas as pd\ndf = pd.read_csv('single_sender_1k.csv')\n\ndf = df[['message_sent']]\n\nmerged_mails = ''\n\nfor message in df['message_sent']:\n    merged_mails = merged_mails + '//m-n-m//' + message\n</code></pre> <p>This function takes a concatenated string of emails, splits it into individual email documents, and further divides the documents into smaller chunks using a specified chunk size and overlap.</p> <ol> <li>Initialize the Text Splitter:</li> <li> <p><code>CharacterTextSplitter</code> is configured with:</p> <ul> <li><code>separator=\"//m-n-m//\"</code>: The delimiter used to separate email strings.</li> <li><code>chunk_size</code>: The size of each chunk.</li> <li><code>chunk_overlap</code>: Overlapping characters between chunks for better context retention.</li> <li><code>length_function=len</code>: Uses the <code>len</code> function to determine text length.</li> <li><code>is_separator_regex=False</code>: Indicates the separator is a literal string, not a regex.</li> </ul> </li> <li> <p>Split Emails:</p> </li> <li> <p><code>emails = merged_mails.split('//m-n-m//')</code>:</p> <ul> <li>Splits the concatenated email string into individual emails using the delimiter.</li> </ul> </li> <li> <p>Transform Emails into Documents:</p> </li> <li> <p><code>documents = [Document(page_content=email.strip()) for email in emails if email.strip()]</code>:</p> <ul> <li>Creates <code>Document</code> objects for each email, excluding empty or whitespace-only emails.</li> </ul> </li> <li> <p>Chunk Documents:</p> </li> <li> <p><code>split_docs = text_splitter.split_documents(documents)</code>:</p> <ul> <li>Splits each document into smaller chunks based on the configured <code>chunk_size</code> and <code>chunk_overlap</code>.</li> </ul> </li> <li> <p>Aggregate Results:</p> </li> <li> <p><code>all_docs.extend(split_docs)</code>:</p> <ul> <li>Adds the resulting chunks to the <code>all_docs</code> list.</li> </ul> </li> <li> <p>Return Chunks:</p> </li> <li>Returns the <code>all_docs</code> list containing the split chunks.</li> </ol> <pre><code>def load_and_split_mails(merged_mails, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load and split email strings into chunks.\n\n    :param merged_mails: A single string containing all email contents, separated by '//m-n-m//'.\n    :param chunk_size: The maximum number of characters in each text chunk. \n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = CharacterTextSplitter(\n        separator=\"//m-n-m//\",  # Defines the separator used to split the text.\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False\n    )\n\n    # Split mails\n    emails = merged_mails.split('//m-n-m//')\n\n    # Transform to Document\n    documents = [Document(page_content=email.strip()) for email in emails if email.strip()]\n\n    # Split docs\n    split_docs = text_splitter.split_documents(documents) \n    all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#define-pdf-array","title":"Define pdf array","text":"<p>This code calls the <code>load_and_split_mails</code> function to process a concatenated string of emails and generate text chunks for further processing.</p> <ol> <li>Function Call:</li> <li> <p><code>text_chunks = load_and_split_mails(merged_mails)</code>:</p> <ul> <li>Passes the <code>merged_mails</code> string to the <code>load_and_split_mails</code> function.</li> </ul> </li> <li> <p>Processing Inside the Function:</p> </li> <li> <p>The <code>load_and_split_mails</code> function:</p> <ul> <li>Splits the concatenated email string into individual email documents.</li> <li>Further divides each document into smaller chunks based on a specified chunk size and overlap.</li> </ul> </li> <li> <p>Assign the Output:</p> </li> <li>The resulting list of chunks is assigned to the variable <code>text_chunks</code>.</li> </ol>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#output","title":"Output","text":"<ul> <li><code>text_chunks</code>:   A list of <code>Document</code> objects, where each object contains a smaller chunk of email text.</li> </ul> <pre><code># Create our chunks\ntext_chunks = load_and_split_mails(merged_mails)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#create-the-vector-store-and-model-path-is-given","title":"Create the vector store and model path is given","text":"<p>This code generates embeddings for email chunks and creates a ChromaDB vector store to facilitate document retrieval based on similarity.</p> <ol> <li>Check for ChromaDB:</li> <li> <p><code>if vector_store == 'ChromaDB':</code>:</p> <ul> <li>Executes the following block only if <code>vector_store</code> is set to <code>'ChromaDB'</code>.</li> </ul> </li> <li> <p>Define <code>create_chroma_vector_store</code> Function:</p> </li> <li> <p>This function handles the creation of a vector store using ChromaDB.</p> </li> <li> <p>Create Vector Store and Retriever:</p> </li> <li><code>retriever_mail = create_chroma_vector_store(text_chunks, embeddings_model_path)</code>:<ul> <li>Calls the <code>create_chroma_vector_store</code> function to generate embeddings and create the vector store.</li> </ul> </li> </ol> <pre><code># Generate embeddings and create vector store\nif vector_store == 'ChromaDB':\n    def create_chroma_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings( # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path, # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={'device': 'cpu'}, # Configuration for running model on cpu.\n            encode_kwargs={'normalize_embeddings': False})\n\n        db_name = str(random.random())\n        vectorstore = Chroma.from_documents(collection_name=db_name, documents=chunks, embedding=embeddings) # This method creates a vector store from the provided documents (chunks) and embeddings.\n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_mail = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) \n        return retriever_mail\n\n    retriever_mail = create_chroma_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#optional-milvus-vector-db","title":"(OPTIONAL) Milvus Vector DB","text":"<p>This code sets up a vector store using MilvusDB to store embeddings for email chunks and enables similarity-based document retrieval.</p>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#workflow","title":"Workflow","text":"<ol> <li>Check for MilvusDB:</li> <li> <p><code>if vector_store == 'MilvusDB':</code></p> <ul> <li>Executes the following block only if <code>vector_store</code> is set to <code>'MilvusDB'</code>.</li> </ul> </li> <li> <p>Define <code>create_milvus_vector_store</code> Function:</p> </li> <li> <p>This function creates a vector store using MilvusDB.</p> </li> <li> <p>Create Vector Store and Retriever:</p> </li> <li><code>retriever_mail = create_milvus_vector_store(text_chunks, embeddings_model_path)</code>:<ul> <li>Calls the <code>create_milvus_vector_store</code> function to set up the Milvus vector store and create the retriever.</li> </ul> </li> </ol> <pre><code>from langchain_milvus import Milvus\nfrom pymilvus import connections\n\nif vector_store == 'MilvusDB':\n    def create_milvus_vector_store(chunks, embeddings_model_path):\n        embeddings = HuggingFaceEmbeddings( # This class is used to generate embeddings for the text chunks.\n            model_name=embeddings_model_path, # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n            model_kwargs={'device': 'cpu'}, # Configuration for running model on cpu.\n            encode_kwargs={'normalize_embeddings': False})\n\n        connections.connect(\"default\", host=milvus_uri, port=\"19530\")\n\n        vectorstore = Milvus.from_documents(\n            documents=chunks,\n            embedding=embeddings,\n            collection_name=\"langchain_example\",\n            connection_args={\n                \"uri\": f\"https://{milvus_uri}:19530\"\n            },\n            drop_old=True,  # Drop the old Milvus collection if it exists\n        ) \n\n        # 'search_type' parameter defines the method to use while searching the most relevant documents to the prompt and 'k' parameter defines the number of documents which will include within context\n        retriever_mail = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}) \n        return retriever_mail\n\n    retriever_mail = create_milvus_vector_store(text_chunks, embedding_model_path)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#define-format_docs-for-join-all-chunks","title":"Define format_docs for join all chunks","text":""},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#description","title":"Description","text":"<p>This function processes a list of document objects and combines their content into a single string, with each document's content separated by two newline characters (<code>\\n\\n</code>) for readability.</p> <ol> <li>Retrieve Content:</li> <li> <p>Iterates through the <code>docs</code> list to access the <code>page_content</code> attribute of each document.</p> </li> <li> <p>Join Content:</p> </li> <li>Combines all retrieved content into a single string.</li> <li> <p>Adds two newline characters (<code>\\n\\n</code>) between each document's content to improve separation and readability.</p> </li> <li> <p>Return Result:</p> </li> <li>The concatenated string is returned.</li> </ol> <pre><code>def format_docs(docs):\n     # Retrieves the content of each document in the `docs` list and joins the content of all documents into a single string, with each document's content separated by two newline characters.\n     return \"\\n\\n\".join(doc.page_content for doc in docs) \n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#all-chains-merged-into-each-other-at-this-function","title":"All chains merged into each other at this function","text":""},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#description_1","title":"Description","text":"<p>This function retrieves relevant documents related to a given question from a retriever, formats them into a prompt, and queries a language model (LLM) API to generate a response.</p> <ol> <li>Define Prompt Template:</li> <li>Creates a <code>PromptTemplate</code> object to format the input for the language model.</li> <li> <p>Template Details:</p> <ul> <li>Includes retrieved context (relevant documents).</li> <li>Contains the question.</li> <li>Instructs the model to respond with \"I don't know\" if it cannot answer.</li> </ul> </li> <li> <p>Retrieve Relevant Documents:</p> </li> <li><code>retriever.get_relevant_documents(question)</code>:<ul> <li>Retrieves the most relevant documents for the given question.</li> </ul> </li> <li> <p><code>format_docs(docs)</code>:</p> <ul> <li>Formats the retrieved documents into a single string, separated by two newline characters (<code>\\n\\n</code>).</li> </ul> </li> <li> <p>Format the Prompt:</p> </li> <li> <p><code>prompt_template.format(context=context, question=question)</code>:</p> <ul> <li>Generates a formatted prompt by inserting the context and question into the template.</li> </ul> </li> <li> <p>Query the LLM API:</p> </li> <li> <p><code>call_llm_api(prompt, api_url, api_token)</code>:</p> <ul> <li>Sends the formatted prompt to the LLM API and retrieves the response.</li> </ul> </li> <li> <p>Extract the Answer:</p> </li> <li> <p>Extracts the answer from the API response by splitting the output on the keyword <code>Answer:</code> and removing extra whitespace.</p> </li> <li> <p>Return the Answer:</p> </li> <li>Returns the extracted answer as a string.</li> </ol> <pre><code># Query the PDF using the API-based LLM\ndef query_mail(retriever, question, api_url, api_token):\n    \"\"\"\n    this function is used for returning response by using all of the chains we defined above\n\n    :param retriever : An instance of a retriever used to fetch relevant documents.\n    :param question : The question to be asked about the PDF content.\n    \"\"\"\n\n    prompt_template = PromptTemplate( # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=( # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        )\n    )\n\n    docs = retriever.get_relevant_documents(question) # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs) # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question) # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split('Answer:')[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/email-e-assistant/mail-e-assistant/#chat-examples","title":"Chat Examples","text":"<p>This code initializes the Practicus environment by retrieving the current region using the <code>practicuscore</code> library.</p> <ol> <li>Import PracticusCore:</li> <li> <p><code>import practicuscore as prt</code>:</p> <ul> <li>Imports the <code>practicuscore</code> library and assigns it the alias <code>prt</code> for easier usage.</li> </ul> </li> <li> <p>Retrieve Region:</p> </li> <li><code>region = prt.get_region()</code>:<ul> <li>Calls the <code>get_region</code> function from the <code>practicuscore</code> module.</li> <li>This function retrieves the region configuration or information relevant to the Practicus environment.</li> <li>The <code>region</code> object typically contains details like available resources, models, or configurations tied to a specific geographic or logical region.</li> </ul> </li> </ol> <pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre> <p>This code retrieves a list of available models in the current region, displays them in a tabular format, and selects the first model for further use.</p> <ol> <li>Retrieve Model List:</li> <li> <p><code>my_model_list = region.model_list</code>:</p> <ul> <li>Accesses the <code>model_list</code> attribute of the <code>region</code> object.</li> <li>This attribute contains a list of available models in the region.</li> </ul> </li> <li> <p>Display Model List:</p> </li> <li> <p><code>display(my_model_list.to_pandas())</code>:</p> <ul> <li>Converts the <code>model_list</code> to a pandas DataFrame using the <code>.to_pandas()</code> method for better visualization.</li> <li>Displays the DataFrame, allowing the user to view details such as model names, versions, and statuses.</li> </ul> </li> <li> <p>Select First Model:</p> </li> <li> <p><code>model_name = my_model_list[0].name</code>:</p> <ul> <li>Accesses the first model in the list using index <code>0</code> and retrieves its name using the <code>.name</code> attribute.</li> </ul> </li> <li> <p>Print the Selected Model Name:</p> </li> <li><code>print(\"Using first model name:\", model_name)</code>:<ul> <li>Outputs the name of the selected model to the console for verification.</li> </ul> </li> </ol> <pre><code>my_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\nmodel_name = my_model_list[0].name\nprint(\"Using first model name:\", model_name)\n</code></pre> <p>This code retrieves a list of available model prefixes in the current region, displays them in a tabular format, and selects the first prefix for further use.</p> <ol> <li>Retrieve Model Prefix List:</li> <li> <p><code>my_model_prefixes = region.model_prefix_list</code>:</p> <ul> <li>Accesses the <code>model_prefix_list</code> attribute of the <code>region</code> object.</li> <li>This attribute contains a list of model prefixes available in the region.</li> </ul> </li> <li> <p>Display Model Prefix List:</p> </li> <li> <p><code>display(my_model_prefixes.to_pandas())</code>:</p> <ul> <li>Converts the <code>model_prefix_list</code> to a pandas DataFrame using the <code>.to_pandas()</code> method for better visualization.</li> <li>Displays the DataFrame, allowing the user to view details such as prefix keys and descriptions.</li> </ul> </li> <li> <p>Select First Prefix:</p> </li> <li> <p><code>model_prefix = my_model_prefixes[0].key</code>:</p> <ul> <li>Accesses the first prefix in the list using index <code>0</code> and retrieves its key using the <code>.key</code> attribute.</li> </ul> </li> <li> <p>Print the Selected Prefix Key:</p> </li> <li><code>print(\"Using first prefix:\", model_prefix)</code>:<ul> <li>Outputs the selected prefix key to the console for verification.</li> </ul> </li> </ol> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\nmodel_prefix = my_model_prefixes[0].key\nprint(\"Using first prefix:\", model_prefix)\n</code></pre> <p>This code constructs an API URL for accessing a model and generates a session token for authenticated communication.</p> <ol> <li>Construct the API URL:</li> <li> <p><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"</code>:</p> <ul> <li>Uses Python's f-string formatting to dynamically create the API URL.</li> <li>Components:</li> <li><code>host</code>: The base hostname of the API server.</li> <li><code>model_prefix</code>: The selected model prefix.</li> <li><code>model_name</code>: The name of the selected model.</li> <li>The resulting URL points to the specific endpoint for the desired model.</li> </ul> </li> <li> <p>Generate a Session Token:</p> </li> <li><code>token = prt.models.get_session_token(api_url=api_url)</code>:<ul> <li>Calls the <code>get_session_token</code> method from the <code>practicuscore.models</code> module.</li> <li>Parameters:</li> <li><code>api_url</code>: The constructed API URL.</li> <li>The session token is returned and stored in the <code>token</code> variable.</li> </ul> </li> </ol> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <p>Get answer with our function </p> <pre><code># Example query\nanswer = query_mail(retriever = retriever_mail, question=\"How can business meetings be made more productive, and what are some alternative travel suggestions?\", api_url = api_url,api_token = token)\nprint(answer)\n</code></pre> <p>Previous: Consume Parallel | Next: Data Analysis &gt; Plot &gt; Introduction</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/","title":"Hosting of LLM application on AppHost without using front-end.","text":"<p>In this example we will try to host an LLM application which only used by API requests. This application will use an already deployed llm model.</p> <pre><code>import practicuscore as prt\nimport requests \nimport json\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#api-python-script","title":"API Python script","text":"<p>First of all we need to create a Python scripts which will be invoked by using requests. For this instance we should create an 'apis' folder which will contain the Python scripts. Then we can create our scripts within the folder.</p> <p>You can check-out sample api script simple_api.py</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None # E.g. 'company.practicus.com'\nlang_model=  None # E.g. 'LLAMA-3-70b'\napp_name = None # E.g. 'api-chatbot'\nmodel_name = None\nmodel_prefix = None\ndeployment_setting_key = None\napp_prefix = None\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code># Let's list our app deployments and select one of them.\nmy_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\n</code></pre> <pre><code># Let's list our app prefixes and select one of them.\nmy_app_prefix_list = region.app_prefix_list\ndisplay(my_app_prefix_list.to_pandas())\n</code></pre> <pre><code>assert host, \"Please enter your host url\"\nassert lang_model, \"Please select a model\"\nassert app_name, \"Please enter application name\"\nassert model_name, \"Please enter model_name\"\nassert model_prefix, \"Please enter model_prefix\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#testing-scripts","title":"Testing scripts","text":"<p>We can call our api scripts withIn this example and test them</p> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <pre><code>from apis.simple_api import Messages, ModelRequest, run\n\n# Let's test our message class\nmessages = Messages(\n    content=\"Who is einstein?\", \n    role=\"human\"\n)\n\nmessages\n</code></pre> <pre><code># Let's test our Model request class\nmodelreq = ModelRequest(\n    messages = messages,\n    lang_model = lang_model,\n    streaming = False,\n    api_token = token,\n    end_point = api_url\n)\n\ndict(modelreq)\n</code></pre> <pre><code># Let's test our prediction function\nresponse = run(modelreq)\n\ndict(response)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#deployment-of-api","title":"Deployment of API","text":"<p>After testing our python scripts, and make sure they work, we can deploy them by using our prefix and SDK</p> <pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#prediction-by-using-api","title":"Prediction by using API","text":"<p>After the deployment process we can consume the api url by using the code cell down below</p> <pre><code>api_url = f\"https://{host}/{app_prefix}/{app_name}/api/simple_api/\"\ntoken = prt.apps.get_session_token(api_url=api_url)\n</code></pre> <pre><code>headers = {\n    \"Authorization\": f\"Bearer {token}\",\n    \"content-type\": 'application/json'}\n\n\ndata_js = modelreq.model_dump_json(indent=2)\n\nresp = requests.post(api_url, json=data_js, headers=headers)\n\nif resp.ok:\n    print(f\"Response text:\", resp.text)\nelse:\n    print(resp.status_code, resp.text)\n</code></pre> <p>After deployment, comprehensive documentation for the API service is automatically generated. You can review this documentation and easily share it with your colleagues and other team members for seamless collaboration and onboarding.</p> <pre><code>documentation_url = f\"https://{host}/{app_prefix}/{app_name}/api/redoc/\"\nprint(f\"Your documentation url:{documentation_url}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/api-llm-apphost/build/#apissimple_apipy","title":"apis/simple_api.py","text":"<pre><code>from pydantic import BaseModel\nfrom practicuscore.gen_ai import PrtLangRequest, PrtLangMessage\nfrom requests import get\nimport json\n\n''' We are defining classes for taken inputs from api call. Using classes allows you to enforce type safety. \nThis means you can be sure that the data your functions receive has the correct types and structure, \nreducing the likelihood of runtime errors. But you don't have to use classes while creating api scripts.'''\n\n# Holds a message's content and an optional role for model to consume prompts.\nclass Messages(PrtLangMessage):\n    content: str\n    role: str | None = None\n\n# Stores details for a language model request, including the message, model type, and API information.\nclass ModelRequest(BaseModel):\n    messages: Messages\n    lang_model: str | None = 'None'\n    streaming: bool | None = False\n    end_point: str\n    api_token: str\n\n# We need to define a 'run' function to process incoming data to API\ndef run(payload: ModelRequest, **kwargs):\n\n    # Set up authorization headers using the API token from the payload\n    headers = {'authorization': f'Bearer {payload.api_token}'}\n\n    # Create a language model request object with message, model, and streaming options\n    practicus_llm_req = PrtLangRequest(\n        messages=[payload.messages],\n        lang_model=payload.lang_model,\n        streaming=payload.streaming,\n        llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"} # (Optional) Additional parameters for the language model could be added here\n        )\n\n    # Convert the request object to a JSON string, excluding unset fields\n    data_js = json.loads(practicus_llm_req.model_dump_json(indent=2, exclude_unset=True))\n\n    # Send the HTTP GET request to the specified endpoint with the headers and JSON data\n    r = get(payload.end_point, headers=headers, json=data_js)\n\n    # Parse the JSON response text into a Python dictionary\n    parsed = json.loads(r.text)\n\n    # Return the parsed response dictionary\n    return parsed\n</code></pre> <p>Previous: Lang Chain LLM Model | Next: Sdk LLM Apphost &gt; Non Stream &gt; Sdk Streamlit Hosting</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/","title":"Flow hosting of Langflow by using Streamlit","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#defining-parameters-from-region","title":"Defining parameters from region.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre> <pre><code>app_name = None # E.g. 'api-chatbot'\ndeployment_setting_key = None\napp_prefix = None\napp_dir = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code>my_app_prefix_list = region.app_prefix_list\ndisplay(my_app_prefix_list.to_pandas())\napp_prefix = my_app_prefix_list[0].prefix\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>my_app_list = region.app_list\ndisplay(my_app_list.to_pandas())\napp_name = my_app_list[0].name\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\ndeployment_setting_key = my_app_settings[1].key\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre> <pre><code>assert app_name, \"Please enter application name\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#testing-app","title":"Testing App","text":"<p>First of all we need to create a \"Basic Prompting (Hello, World)\" flow at langflow and export the json of it.</p> <p>After exporting the json and save it within current directory of this tutorial, you should test it if it's working.</p> <pre><code>from langflow.load import run_flow_from_json\n\nresult = run_flow_from_json(\n    flow=\"Flow.json\",\n    input_value=\"What is the capital of Australia?\"\n)\n\nrun_output = result[0]\nresult_data = run_output.outputs[0]\nmessage_obj = result_data.results['message']\nmessage_text = message_obj.data['text']\n\nmessage_text\n</code></pre> <p>Now we can create our own stream-lit app and use json of our flow within stream-lit's front-end. You can check-out our streamlit_app.py:</p> <p>View streamlit_app.py</p> <p>After creating/editing stream_app.py we could test it by hosting it as test by using our SDK:</p> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#deploying-app","title":"Deploying App","text":"<pre><code>import practicuscore as prt\n\nprt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None # Current dir\n)\n</code></pre> <p>After the deployment process completed we could enter UI url (e.g. https://dev.practicus.io/apps/langflow-json-test/v1/) to show-case our app.</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/langflow-llm-apphost/langflow-streamlit-hosting/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\nfrom langflow.load import run_flow_from_json\n\n# The below will secure the page by authenticating and authorizing users with Single-Sign-On.\n# Please note that security code is only activate when the app is deployed.\n# Pages are always secure, even without the below, during development and only the owner can access them.\nprt.apps.secure_page(\n    page_title=\"Hello World App\",\n    must_be_admin=False,\n)\n\n\ndef main():\n    # The below is standard Streamlit code..\n    st.title(\"My App on Practicus AI\")\n\n    st.markdown(\"##### Welcome to the front-end of your flow\")\n\n    # Initialize session state to store chat messages if not already initialized.\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display all messages stored in session state in the chat interface.\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    # When the user inputs a message, add it to the chat history and display it.\n    if prompt := st.chat_input(\"I'm your flow, how may I help you?\"):\n        # Add user message to chat history\n        st.session_state.messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        )\n        # Display user message in chat message container\n        with st.chat_message(\n                \"user\"\n        ):\n            st.write(prompt)\n        # Display assistant response in chat message container\n        with st.chat_message(\n                \"assistant\"\n        ):\n            message_placeholder = st.empty()\n            with st.spinner(text=\"Thinking...\"):\n                assistant_response = generate_response(prompt)\n                message_placeholder.write(assistant_response)\n        # Add assistant response to chat history\n        st.session_state.messages.append(\n            {\n                \"role\": \"assistant\",\n                \"content\": assistant_response\n            }\n        )\n\n\ndef run_flow(message, flow_json):\n    result = run_flow_from_json(flow=flow_json,\n                                input_value=message,\n                                fallback_to_env_vars=True)  # False by default\n    return result\n\n\n# Function to generate a response from the flow based on the user's input.\ndef generate_response(prompt):\n    # Log the user's question.\n    # logging.info(f\"question: {prompt}\")\n\n    # Run the flow to get the response.\n    response = run_flow(message=prompt, flow_json='Flow.json')\n\n    run_output = response[0]\n    result_data = run_output.outputs[0]\n    message_obj = result_data.results['message']\n    message_text = message_obj.data['text']\n\n    try:\n        # Log and return the assistant's response.\n        #logging.info(f\"answer: {message_obj}\")\n        return message_text\n    except Exception as exc:\n        # Log any errors and return a fallback message.\n        #logging.error(f\"error: {exc}\")\n        return \"Sorry, there was a problem finding an answer for you.\"\n\n\n# Run the main function to start the Streamlit app.\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Sdk Streamlit Hosting | Next: Milvus Embedding And LangChain &gt; Milvus Chain</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/","title":"Hosting of LLM which is built by using SDK","text":"<pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#test-app","title":"Test App","text":"<pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>app_name = None #E.g. 'api-chatbot'\ndeployment_setting_key = None\napp_prefix = None\napp_dir = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code>my_app_prefixes = region.app_prefix_list\ndisplay(my_app_prefixes.to_pandas())\n</code></pre> <pre><code>my_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\n</code></pre> <pre><code>assert app_name, \"Please enter application name\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#deploying-app","title":"Deploying App","text":"<pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key, # Deployment Key, ask admin for deployment key\n    prefix=app_prefix, # Apphost deployment extension\n    app_name=app_name, \n    app_dir=None # Directory of files that will be deployed ('None' for current directory)\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/non-stream/sdk-streamlit-hosting/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code># The below is official Streamlit + Langchain demo.\n\nimport streamlit as st\nimport practicuscore as prt\n\nfrom langchain_practicus import ChatPracticus\n\nprt.apps.secure_page(\n    page_title=\"\ud83e\udd9c\ud83d\udd17 Quickstart App\" # Give page title\n)\n\nst.title(\"\ud83e\udd9c\ud83d\udd17 Quickstart App v1\") # Give app title\n\n\n# This function use our 'api_token' and 'endpoint_url' and return the response.\ndef generate_response(input_text, endpoint, api):\n\n    model = ChatPracticus(\n        endpoint_url=endpoint, # Give model url\n        # Give api token , ask your admin for api\n        api_token=api,\n        model_id=\"model\",\n        verify_ssl=True,\n    )    \n\n    st.info(model.invoke(input_text).content) # We are give the input to model and get content\n\n\nwith st.form(\"my_form\"): # Define our question\n    endpoint = st.text_input('Enter your end point url:')\n    api = st.text_input('Enter your api token:')\n    text = st.text_area(\n        \"Enter text:\",\n        \"Who is Einstein ?\",\n    )\n    submitted = st.form_submit_button(\"Submit\") # Define the button\n\n    if submitted:\n        generate_response(text, endpoint, api) # Return the response\n</code></pre> <p>Previous: Build | Next: Stream &gt; Sdk Streamlit Hosting</p>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/stream/sdk-streamlit-hosting/","title":"Hosting LLM APIs and Apps","text":"<pre><code>import practicuscore as prt\n</code></pre> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre> <p>After testing our application we can set our configurations and start the deployment process.</p> <pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/stream/sdk-streamlit-hosting/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>app_name = None # E.g. 'api-chatbot'\ndeployment_setting_key = None\napp_prefix = None\napp_dir = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/stream/sdk-streamlit-hosting/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code>my_app_list = region.app_list\ndisplay(my_app_list.to_pandas())\n\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_app_prefix_list = region.app_prefix_list\ndisplay(my_app_prefix_list.to_pandas())\n\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>my_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\n\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre> <pre><code>assert app_name, \"Please enter application name\"\nassert deployment_setting_key, \"Please enter deployment_setting_key\"\nassert app_prefix, \"Please enter app_prefix\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/stream/sdk-streamlit-hosting/#deploying-app","title":"Deploying app","text":"<pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key, # Deployment Key, ask admin for deployment key\n    prefix=app_prefix, # Apphost deployment extension\n    app_name=app_name, \n    app_dir=None # Directory of files that will be deployed ('None' for current directory)\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/stream/sdk-streamlit-hosting/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/generative-ai/llm-apps/sdk-llm-apphost/stream/sdk-streamlit-hosting/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code># The below is official Streamlit + Langchain demo.\n\nimport streamlit as st\nimport practicuscore as prt\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest\nimport requests\n\n\nprt.apps.secure_page(\n    page_title=\"\ud83e\udd9c\ud83d\udd17 Quickstart App\" # Give page title\n)\n\n\nst.title(\"\ud83e\udd9c\ud83d\udd17 Quickstart App v2\") # Give app title\n\n\n# This function use our 'api_token' and 'endpoint_url' and return the response.\ndef generate_response(messages, model):\n\n    api_url = \"Enter your model api\"\n    token =\"Enter your model token\"\n\n    practicus_llm_req = PrtLangRequest( # This class need message and model and if u want to stream u should change streaming value false to true\n        messages=messages, # Our contest\n        lang_model= model, #\"gpt-4o\", # Select model\n        streaming=True, # Streaming mode\n        llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"} # If we have an extra parameters at model.py we can add them here\n    )\n\n    headers = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'application/json'\n    }\n\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True) # Convert our returned parameter to json\n\n    with requests.post(api_url, headers=headers, data=data_js, stream=True) as r: \n        for word in r.iter_content(1024):\n            yield word.decode(\"utf-8\")\n\ndef reset_chat():\n    st.session_state[\"messages\"] = []\n\n\n# Save chat history\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = []\n\n# Input chat\nuser_message = st.chat_input(\"Write message\")\nif user_message:\n    # Add user message to history\n    st.session_state.messages.append({\"role\": \"user\", \"content\": user_message})\n\n    human_msg = PrtLangMessage(\n        content=user_message,\n        role = \"human\"\n    )\n\n    # Show messages\n    for msg in st.session_state.messages:\n        st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n\n    with st.chat_message(\"assistant\"):\n        response = st.write_stream(generate_response([human_msg], \"gpt-4o\"))\n\n    # Show answer\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n# Create a container to hold the button at the bottom\nwith st.container():\n    st.write(\"\")  # Add some empty space to push the button to the bottom\n    if st.button(\"Clear history\"):\n        reset_chat()\n        st.success(\"History has been cleaned\")\n</code></pre> <p>Previous: Sdk Streamlit Hosting | Next: Langflow LLM Apphost &gt; Langflow Streamlit Hosting</p>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/","title":"Milvus Chain","text":""},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#firstly-we-need-install-transformers-and-torch","title":"Firstly we need install transformers and torch","text":"<p>In this tutorial we will see how to create a MILVUS vector store for encoded documents and how to store the embeddings in the desired vector store using Practicus AI SDK.</p> <p>Focuses: - Preparing test documents - Creating Milvus Vector Store - Creating an index - Inserting embeddings - Using the vector store within RAG pipeline</p> <p>Necessary libraries:</p> <pre><code>! pip install transformers sentence-transformers langchain langchain-community langchain-milvus chromadb pypdf\n! pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre> <pre><code>import practicuscore as prt\nfrom transformers import pipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.docstore.document import Document\nfrom langchain_community.vectorstores import Chroma\n\nfrom langchain_practicus import ChatPracticus\n\nimport chromadb\nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>host = None # E.g. company.practicus.com'\nembedding_model_path = None\nmilvus_uri = None # E.g. 'company.practicus.milvus.com'\nmilvus_port = None # E.g. '19530'\nmodel_name = None\nmodel_prefix = None\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#if-you-dont-know-your-prefixes-and-deployments-you-can-check-them-out-by-using-the-sdk-like-down-below","title":"If you don't know your prefixes and deployments you can check them out by using the SDK like down below:","text":"<pre><code># Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n\nprint(\"Using first model name:\", model_name)\n# Let's list our prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n\nprint(\"Using first prefix:\", model_prefix)\n</code></pre> <pre><code>assert host, \"Please enter your host url\"\nassert embedding_model_path, \"Please enter your embedding model path.\"\nassert milvus_uri, \"Please enter your milvus connection uri\"\nassert milvus_port, \"Please enter your milvus connection port\"\nassert model_name, \"Please enter your model_name\"\nassert model_prefix, \"Please enter your model_prefix\"\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#prepare-test-document","title":"Prepare test document","text":"<pre><code>import requests\n\nurl = 'https://raw.githubusercontent.com/practicusai/sample-data/refs/heads/main/small_rag_document/test_document_v1.pdf'\n\noutput_file = \"test_document_v1.pdf\"\n\n# Sending a GET request to the URL\nresponse = requests.get(url)\n\n# Checking if the request was successful\nif response.status_code == 200:\n    # Writing the file to the specified path\n    with open(output_file, \"wb\") as file:\n        file.write(response.content)\n    print(f\"File downloaded successfully and saved as {output_file}\")\nelse:\n    print(f\"Failed to download file. HTTP status code: {response.status_code}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#define-llm-api-function-and-call-chatpracticus-in-this-function","title":"Define llm api function and call ChatPracticus in this function","text":"<pre><code>def call_llm_api(inputs, api_url, api_token):\n    \"\"\"\n    This function will use our 'api_token' and 'api_url' and return the response.\n\n    :params inputs: The input to be sent to the API.\n    :params api_url: The endpoint URL of the ChatPracticus API.\n\n    \"\"\"\n\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=api_token,\n        model_id=\"current models ignore this\",\n    )\n\n    response = chat.invoke(input=inputs)\n    # response = chat.invoke(\"What is Capital of France?\")  # This also works\n\n    return(response.content)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#get-all-pdf-files-and-use-seperator-for-split-questions","title":"Get all pdf files and use seperator for split questions","text":"<pre><code>def load_and_split_pdfs(pdf_files, chunk_size=500, chunk_overlap=50):\n    \"\"\"\n    Load all pdf files and split with using the 'seperator'.\n\n    :param pdf_files: A list of paths to the PDF files to be processed.\n    :param chunk_size: The maximum number of characters in each text chunk. \n    :param chunk_overlap: The number of characters to overlap between consecutive chunks.\n    \"\"\"\n    all_docs = []\n    text_splitter = CharacterTextSplitter( # Langchain method used to separate documents, there are different ways\n        separator=\"\\n\", # Defines the separator used to split the text.\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False)\n\n    for pdf_file in pdf_files:\n        loader = PyPDFLoader(pdf_file) # PDF loader compatible with langchain\n        documents = loader.load_and_split()\n        split_docs = text_splitter.split_documents(documents)\n        all_docs.extend(split_docs)\n\n    return all_docs\n</code></pre> <pre><code># Define pdf array\npdf_list = ['test_document_v1.pdf']\n\ntext_chunks = load_and_split_pdfs(pdf_list, chunk_size=500)\n</code></pre> <pre><code>context_array = []\nfor i, row in enumerate(text_chunks):\n    context_array.append(row.page_content)\n</code></pre> <pre><code>embedding_model = HuggingFaceEmbeddings( # This class is used to generate embeddings for the text chunks.\n        model_name=embedding_model_path, # Specifies the path to the pre-trained embeddings model used for generating embeddings.\n        model_kwargs={'device': 'cpu'}, # Configuration for running model on cpu.\n        encode_kwargs={'normalize_embeddings': False})\n</code></pre> <pre><code>embeddings = embedding_model.embed_documents(context_array)\n\nvector_dimension = len(embeddings[0])\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#create-the-vector-store-by-using-given-embedding-model","title":"Create the vector store by using given embedding model","text":"<pre><code>from pymilvus import (\n    connections,\n    utility,\n    FieldSchema, CollectionSchema, DataType,\n    Collection,\n)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#1-connect-to-milvus","title":"1. connect to Milvus","text":"<p>Add a new connection alias <code>default</code> for Milvus server in <code>localhost:19530</code>. </p> <p>Actually the <code>default</code> alias is a building in PyMilvus. If the address of Milvus is the same as <code>localhost:19530</code>, you can omit all parameters and call the method as: <code>connections.connect()</code>.</p> <p>Note: the <code>using</code> parameter of the following methods is default to \"default\".</p> <pre><code>connections.connect(\"default\", host=milvus_uri, port=milvus_port)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#2-create-collection","title":"2. create collection","text":"<p>We need to create collection with desired fields, in this example our collection will be like down below:</p> field name field type other attributes field description 1 \"pk\" VARCHAR is_primary=True, auto_id=False \"primary key field\" 2 \"companyInfo\" VARCHAR max_length=65535 \"our text field\" 3 \"embeddings\" FloatVector dim, equals to embedding dimension \"vector field\" <pre><code>fields = [\n    # Id field of vectors\n    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n    # Embedded texts field\n    FieldSchema(name=\"companyInfo\", dtype=DataType.VARCHAR, max_length=65535), # https://milvus.io/docs/limitations.md\n    # Embedded vectors field\n    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=vector_dimension)\n]\n\n# Creating schema and collection\nschema = CollectionSchema(fields, \"dummy_info is a basic example demonstrating document embedding.\")\ndummy_company_collection = Collection(\"dummy_info\", schema, consistency_level=\"Strong\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#3-insert-data","title":"3. insert data","text":"<p>We need to define our entities which will be inserted into our 'dummy_info' collection and insert them.</p> <p>The insert() method returns: - either automatically generated primary keys by Milvus if auto_id=True in the schema; - or the existing primary key field from the entities if auto_id=False in the schema.</p> <pre><code>entities = [\n    # provide the pk field because `auto_id` is set to False\n    [str(i) for i in range(len(text_chunks))], # Will be inserted to first field, 'pk'\n    context_array,  # Will be inserted to second field, 'companyInfo'\n    embeddings,    # Will be inserted to third field, 'embeddings'\n]\n</code></pre> <pre><code>dummy_company_collection.insert(entities)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#4-create-index","title":"4. Create index","text":"<p>We need to index our inserted entities to creates queries.</p> <p>In this example we will use Ecludian Distance (L2) and Quantization-based index (IVF_FLAT) as indexing options.</p> <p>You can check what options do you have from Milvus Documentation</p> <pre><code>index = {\n    \"index_type\": \"IVF_FLAT\",\n    \"metric_type\": \"L2\",\n    \"params\": {\"nlist\": 128},\n}\n\ndummy_company_collection.create_index(\"embeddings\", index)\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#test-the-created-milvus-db-collection","title":"Test the created Milvus DB Collection","text":"<p>After data were inserted into Milvus and indexed, you can perform: - search based on vector similarity - query based on scalar filtering(boolean, int, etc.) - hybrid search based on vector similarity and scalar filtering.</p> <p>Before conducting a search or a query, you need to load the data in <code>dummy_info</code> into memory.</p> <pre><code>dummy_company_collection.load()\n</code></pre> <pre><code># The query down below returns 'companyInfo' field of entities that has any 'pk' information, which means, all of the entities.\nentities = dummy_company_collection.query(expr=\"pk != ''\", output_fields=['companyInfo'], limit=100)\n\n# Print out the retrieved entities\nfor entity in entities:\n    print(entity)\n</code></pre> <pre><code># Now we can embed a text and querry it on our collection\nvector_to_search = embedding_model.embed_documents(['What is the company name?'])\n</code></pre> <pre><code>import time\n\nsearch_params = {\n    \"metric_type\": \"L2\",\n    \"params\": {\"nprobe\": 10},\n}\n\nstart_time = time.time()\nresult = dummy_company_collection.search(vector_to_search, \"embeddings\", search_params, limit=1, output_fields=[\"companyInfo\"])\nend_time = time.time()\n\nfor hits in result:\n    for hit in hits:\n        print(f\"hit: {hit}, random field: {hit.entity.get('random')}\")\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#integration-of-milvus-db-to-rag-pipeline","title":"Integration of Milvus DB to RAG pipeline","text":"<p>We use LangChain to integrate our Milvus vector collection into the RAG pipeline for efficient retrieval.</p> <pre><code>from langchain.vectorstores import Milvus\n\ndef initialize_milvus_retriever():\n\n    # Connect to the existing collection in Milvus without recreating it\n    milvus_retriever = Milvus(embedding_model, connection_args={\"uri\": f'https://{milvus_uri}:{milvus_port}'}, collection_name=\"dummy_info\", text_field='companyInfo', vector_field='embeddings')\n    return milvus_retriever\n</code></pre> <pre><code>milvus_retriever = initialize_milvus_retriever()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#define-format_docs-for-join-all-chunks","title":"Define format_docs for join all chunks","text":"<pre><code>def format_docs(docs):\n     # Retrieves the content of each document in the `docs` list and joins the content of \n     # all documents into a single string, with each document's content separated by two newline characters.\n     return \"\\n\\n\".join(doc.page_content for doc in docs) \n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#all-chains-merged-into-each-other-at-this-function","title":"All chains merged into each other at this function","text":"<pre><code># Query the PDF using the API-based LLM\ndef query_pdf(retriever, question, api_url, api_token):\n    \"\"\"\n    this function is used for returning response by using all of the chains we defined above\n\n    :param retriever : An instance of a retriever used to fetch relevant documents.\n    :param question : The question to be asked about the PDF content.\n    \"\"\"\n\n    prompt_template = PromptTemplate( # Defines a template for the prompt sent to the LLM.\n        input_variables=[\"context\", \"question\"],\n        template=( # The format of the prompt.\n            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \"\n            \"If you don't know the answer, just say that you don't know.\\n\"\n            \"Question: {question}\\nContext: {context}\\nAnswer:\"\n        )\n    )\n\n    docs = retriever.similarity_search(question, k=3) # Uses the retriever to get relevant documents based on the question.\n    context = format_docs(docs) # Formats the retrieved documents\n\n    prompt = prompt_template.format(context=context, question=question) # Formats the prompt\n\n    answer = call_llm_api(prompt, api_url, api_token)\n\n    return answer.strip().split('Answer:')[-1].strip()\n</code></pre>"},{"location":"technical-tutorial/extras/generative-ai/milvus-embedding-and-langchain/milvus-chain/#chat-example","title":"Chat Example","text":"<pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <pre><code># Example query\nanswer = query_pdf(retriever = milvus_retriever, \n                   question=\"What is the name of company?\", \n                   api_url = api_url,\n                   api_token = token)\nprint(answer)\n</code></pre> <pre><code># Deleting collection after tutorial\nutility.drop_collection(\"dummy_info\")\n</code></pre> <p>Previous: Langflow Streamlit Hosting | Next: Cv Assistant &gt; Cv Assistant</p>"},{"location":"technical-tutorial/extras/modeling/AutoML/","title":"Predicting Insurance Charges with Automated Machine Learning (AutoML)","text":"<p>In the insurance sector, accurately forecasting the costs associated with policyholders is crucial for pricing policies competitively while ensuring profitability. For insurance companies, the ability to predict these costs helps in tailoring individual policies, identifying key drivers of insurance costs, and ultimately enhancing customer satisfaction by offering policies that reflect a customer's specific risk profile and needs.</p>"},{"location":"technical-tutorial/extras/modeling/AutoML/#objective","title":"Objective","text":"<p>The primary goal of this notebook is to showcase how Practicus AI users can leverage AutoML to swiftly and proficiently develop a predictive model, minimizing the need for extensive manual modeling work. By engaging with this notebook, you'll acquire knowledge on how to:</p> <ul> <li> <p>Load the dataset specific to insurance costs</p> </li> <li> <p>Using AutoML to train and tune a predictive model tailored for insurance charges prediction.</p> </li> <li> <p>Assess the model's performance accurately</p> </li> </ul> <p>Let's embark on this journey!</p> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>service_key = None # Eg. Mlflow\nexperiment_name = None # Eg. automl-experiment-test\n</code></pre> <pre><code># If you don't know experiment service key and name you can checkout down below\n\naddon_list = region.addon_list\ndisplay(addon_list.to_pandas())\n</code></pre> <pre><code>assert service_key, \"Please select a service_key\"\nassert experiment_name, \"Please select a experiment_name\"\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-1-setting-up-the-environment","title":"Step 1: Setting Up the Environment","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#first-we-need-to-set-up-our-python-environment-with-the-necessary-libraries-pycaret-an-automl-library-simplifies-the-machine-learning-workflow-enabling-us-to-efficiently-develop-predictive-models","title":"First, we need to set up our Python environment with the necessary libraries. PyCaret, an AutoML library, simplifies the machine learning workflow, enabling us to efficiently develop predictive models.","text":"<pre><code># Standard libraries for data manipulation and numerical operations\nimport pandas as pd\nimport numpy as np\nfrom pycaret.regression import *  # Importing PyCaret's regression module\n# Extras\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-2-loading-the-dataset","title":"Step 2: Loading the Dataset","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#the-dataset-consists-of-1337-observations-and-7-variables-with-charges-being-the-target-variable-we-aim-to-predict-this-dataset-is-a-common-benchmark-in-insurance-cost-predictions","title":"The dataset consists of 1,337 observations and 7 variables, with 'charges' being the target variable we aim to predict. This dataset is a common benchmark in insurance cost predictions.","text":"<pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n\nproc = worker.load(data_set_conn, engine='AUTO') \n\ndf = proc.get_df_copy()\ndisplay(df)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-3-initializing-the-automl-experiment","title":"Step 3: Initializing the AutoML Experiment","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#pycarets-regression-module-is-utilized-here-for-predicting-a-continuous-target-variable-ie-insurance-costs-we-begin-by-initializing-our-automl-experiment","title":"PyCaret's regression module is utilized here for predicting a continuous target variable, i.e., insurance costs. We begin by initializing our AutoML experiment.","text":"<pre><code>from pycaret.regression import RegressionExperiment, load_model, predict_model\n\nexp = RegressionExperiment()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#this-step-sets-up-our-environment-within-pycaret-allowing-for-automated-feature-engineering-model-selection-and-more","title":"This step sets up our environment within PyCaret, allowing for automated feature engineering, model selection, and more.","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#step-4-configuring-the-experiment","title":"Step 4: Configuring the Experiment","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#well-configure-our-experiment-with-a-specific-name-making-it-easier-to-manage-and-reference","title":"We'll configure our experiment with a specific name, making it easier to manage and reference.","text":"<pre><code>prt.experiments.configure(service_key=service_key, experiment_name=experiment_name)\n# No experiment service selected, will use MlFlow inside the Worker. To configure manually:\n# configure_experiment(experiment_name=experiment_name, service_name='Experiment service name')\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-5-preparing-data-with-pycarets-setup","title":"Step 5: Preparing Data with PyCaret's Setup","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#a-critical-step-where-we-specify-our-experiments-details-such-as-the-target-variable-session-id-for-reproducibility-and-whether-to-log-the-experiment-for-tracking-purposes","title":"A critical step where we specify our experiment's details, such as the target variable, session ID for reproducibility, and whether to log the experiment for tracking purposes.","text":"<pre><code>setup_params = {'normalize': True, 'normalize_method': 'minmax'}\n</code></pre> <pre><code>exp.setup(data=df, target='charges', session_id=42, \n          log_experiment=True, feature_selection=True, experiment_name=experiment_name, **setup_params)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-6-model-selection-and-tuning","title":"Step 6: Model Selection and Tuning","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#this-command-leverages-automl-to-compare-different-models-automatically-selecting-the-one-that-performs-best-according-to-a-default-or-specified-metric-its-a-quick-way-to-identify-a-strong-baseline-model-without-manual-experimentation","title":"This command leverages AutoML to compare different models automatically, selecting the one that performs best according to a default or specified metric. It's a quick way to identify a strong baseline model without manual experimentation.","text":"<pre><code>best_model = exp.compare_models()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#once-a-baseline-model-is-selected-this-step-fine-tunes-its-hyperparameters-to-improve-performance-the-use-of-tune-sklearn-and-hyperopt-indicates-an-advanced-search-across-the-hyperparameter-space-for-optimal-settings-which-can-significantly-enhance-model-accuracy","title":"Once a baseline model is selected, this step fine-tunes its hyperparameters to improve performance. The use of tune-sklearn and hyperopt indicates an advanced search across the hyperparameter space for optimal settings, which can significantly enhance model accuracy.","text":"<pre><code>tune_params = {}\n</code></pre> <pre><code>tuned_model = exp.tune_model(best_model, **tune_params)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-7-finalizing-the-model","title":"Step 7: Finalizing the Model","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#after-tuning-the-model-is-finalized-meaning-its-retrained-on-the-entire-dataset-including-the-validation-set-this-step-ensures-the-model-is-as-generalized-as-possible-before-deployment","title":"After tuning, the model is finalized, meaning it's retrained on the entire dataset, including the validation set. This step ensures the model is as generalized as possible before deployment.","text":"<pre><code>final_model = exp.finalize_model(tuned_model)\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#step-8-predictions-and-saving-the-model","title":"Step 8: Predictions and Saving the Model","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#we-predict-insurance-costs-using-our-final-model-and-save-it-for-future-use-ensuring-operational-scalability","title":"We predict insurance costs using our final model and save it for future use, ensuring operational scalability.","text":"<pre><code>predictions = exp.predict_model(final_model, data=df)\ndisplay(predictions)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#the-last-step-involves-saving-the-trained-model-for-future-use-such-as-deployment-in-a-production-environment-or-further-evaluation-it-ensures-the-models-availability-beyond-the-current-session-facilitating-operationalization-and-scalability","title":"The last step involves saving the trained model for future use, such as deployment in a production environment or further evaluation. It ensures the model's availability beyond the current session, facilitating operationalization and scalability.","text":"<pre><code>exp.save_model(final_model, 'model')\n</code></pre> <pre><code>loaded_model = load_model('model')\n\npredictions = predict_model(loaded_model, data=df)\ndisplay(predictions)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#summary","title":"Summary","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#by-following-these-steps-insurance-companies-can-develop-a-predictive-model-for-insurance-costs-using-practicus-ais-automl-capabilities-this-approach-reduces-the-need-for-extensive-manual-modeling-enabling-insurers-to-efficiently-adapt-to-changing-market-conditions-and-customer-profiles","title":"By following these steps, insurance companies can develop a predictive model for insurance costs using Practicus AI's AutoML capabilities. This approach reduces the need for extensive manual modeling, enabling insurers to efficiently adapt to changing market conditions and customer profiles.","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/AutoML/#bank_marketingsnippetslabel_encoderpy","title":"bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#bank_marketingsnippetsone_hotpy","title":"bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multi collinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#model_observabilitymodelpy","title":"model_observability/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#model_trackingmodel_driftmodelpy","title":"model_tracking/model_drift/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlice_creammodeljson","title":"sparkml/ice_cream/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlice_creammodelpy","title":"sparkml/ice_cream/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlspark_with_jobjobpy","title":"sparkml/spark_with_job/job.py","text":"<pre><code>import practicuscore as prt \nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit, max, min, stddev, corr\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\n\nspark = SparkSession.builder \\\n    .appName(\"Advanced Data Processing\") \\\n    .getOrCreate()\n\nfile_path = \"/home/ubuntu/samples/insurance.csv\"\ndata = spark.read.csv(file_path, header=True, inferSchema=True)\nmissing_data = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n\ncategorical_columns = ['sex', 'smoker', 'region']\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n\ndata = data.withColumn(\"bmi_category\", \n                       when(col(\"bmi\") &lt; 18.5, lit(\"underweight\"))\n                       .when((col(\"bmi\") &gt;= 18.5) &amp; (col(\"bmi\") &lt; 25), lit(\"normal\"))\n                       .when((col(\"bmi\") &gt;= 25) &amp; (col(\"bmi\") &lt; 30), lit(\"overweight\"))\n                       .otherwise(lit(\"obese\")))\n\nfeature_columns = ['age', 'bmi', 'children', 'sex_encoded', 'smoker_encoded', 'region_encoded']\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\ndata = pipeline.fit(data).transform(data)\n\noutput_path = \"/home/ubuntu/my/processed_insurance_data.parquet/\"\n\ndata.write.parquet(output_path, mode=\"overwrite\")\n\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlspark_with_jobrun2c741eprt_dist_jobjson","title":"sparkml/spark_with_job/run/2c741e/prt_dist_job.json","text":"<pre><code>{\"job_type\":\"spark\",\"job_dir\":\"~/my/02_batch_job/\",\"initial_count\":2,\"coordinator_port\":7077,\"additional_ports\":[4040,7078,7079],\"terminate_on_completion\":false,\"py_file\":\"job.py\",\"executors\":[{\"rank\":0,\"instance_id\":\"5cf16b71\"},{\"rank\":1,\"instance_id\":\"63e80dc8\"}]}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlspark_with_jobrun2c741erank_0json","title":"sparkml/spark_with_job/run/2c741e/rank_0.json","text":"<pre><code>{\"rank\":0,\"instance_id\":\"5cf16b71\",\"state\":\"completed\",\"used_ram\":1187,\"peak_ram\":1187,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#sparkmlspark_with_jobrun2c741erank_1json","title":"sparkml/spark_with_job/run/2c741e/rank_1.json","text":"<pre><code>{\"rank\":1,\"instance_id\":\"63e80dc8\",\"state\":\"running\",\"used_ram\":284,\"peak_ram\":293,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#xgboostmodelpy","title":"xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/AutoML/#xgboostmodel_custom_dfpy","title":"xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: XGBoost | Next: Shap Analysis</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/","title":"Shap Analysis","text":"<pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport shap\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import plot_tree\n</code></pre> <pre><code># load the csv file as a data frame\ndf = pd.read_csv('samples/iris.csv')\n</code></pre> <pre><code>label_encoder = LabelEncoder()\ndf['species'] = label_encoder.fit_transform(df['species'])\n</code></pre> <pre><code># Separate Features and Target Variables\nX = df.drop(columns='species')\ny = df['species']\n</code></pre> <pre><code># Create Train &amp; Test Data\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,\n                                                    stratify =y,\n                                                    random_state = 13)\n\n# Build the model\nrf_clf = RandomForestClassifier(max_features=2, n_estimators =100 ,bootstrap = True)\n\nrf_clf.fit(X_train, y_train)\n</code></pre> <pre><code>y_pred = rf_clf.predict(X_test)\nprint(classification_report(y_pred, y_test))\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#iris-data-set-feature-importance","title":"Iris Data Set Feature Importance","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview","title":"Overview","text":"<p>The following visualization presents a ranked bar chart depicting the relative importance of each feature used by our machine learning model to predict Iris flower species. In this analysis, we observe the features derived from the dimensions of the flower's petals and sepals: <code>petal_width</code>, <code>petal_length</code>, <code>sepal_length</code>, and <code>sepal_width</code>.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#interpretation","title":"Interpretation","text":"<ul> <li>Petal Width (petal_width): This feature has the highest relative importance score, indicating its strong predictive power in distinguishing between Iris species. The model heavily relies on petal width, suggesting that this attribute significantly influences the model's decision-making process.</li> <li>Petal Length (petal_length): Following petal width, petal length also shows substantial influence on the model's predictions. Its prominence implies that the length of the petal is another defining characteristic in species classification.</li> <li>Sepal Length (sepal_length) &amp; Sepal Width (sepal_width): These features hold less significance compared to petal measurements. Their lower importance scores may reflect their reduced discriminative ability in the context of this specific model and dataset.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#implications-for-model-refinement","title":"Implications for Model Refinement","text":"<p>We leverage this feature importance chart to guide feature selection and model simplification efforts. In high-dimensional datasets, reducing model complexity and computational load by pruning less significant features is crucial. Additionally, the chart enhances model interpretability by highlighting which features predominantly drive predictions, providing insights into potential dependencies within the dataset.</p> <p>For instance, if petal measurements are more influential than sepal measurements, it could indicate that petals play a more decisive role in identifying the Iris species. </p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion","title":"Conclusion","text":"<p>This graphical analysis is instrumental for data-driven decision-making, ensuring that our model is both efficient and interpretable. By focusing on the most informative features, we can streamline the model while maintaining or even enhancing its accuracy.</p> <pre><code>importances = rf_clf.feature_importances_\nindices = np.argsort(importances)\nfeatures = df.columns\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='y', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n</code></pre> <pre><code># compute SHAP values\nexplainer = shap.TreeExplainer(rf_clf)\nshap_values = explainer.shap_values(X)\n\nclass_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#random-forest-component-analysis-decision-tree-visualization","title":"Random Forest Component Analysis: Decision Tree Visualization","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_1","title":"Overview","text":"<p>This visualization illustrates a single decision tree from a Random Forest classifier trained on the Iris dataset. It's a visual representation of how the algorithm makes decisions and classifies different Iris species: setosa, versicolor, and virginica.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#specific-observations","title":"Specific Observations","text":"<ul> <li>The first split is made with the condition <code>petal_width &lt;= 0.75</code>, perfectly separating Iris setosa from the other species with a Gini score of 0.0 and 25 samples at the node.</li> <li>Subsequent splits focus on distinguishing Iris versicolor and Iris virginica, proceeding until reaching nodes with lower Gini scores.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#importance-for-model-interpretation","title":"Importance for Model Interpretation","text":"<ul> <li>Such visualizations are crucial for understanding the decision-making process of the model and determining which features are most influential during the classification task.</li> <li>The explainable nature of decision trees allows us to clearly communicate how the model works and when specific features become significant, enhancing the transparency and reliability of the AI model.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#implications-for-stakeholders","title":"Implications for Stakeholders","text":"<ul> <li>The clear delineation of decision paths provides stakeholders with insight into the model's reasoning, facilitating trust in the predictions made by the AI system.</li> <li>It underscores the model's dependence on petal measurements, potentially informing feature engineering and data collection priorities for future modeling efforts.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_1","title":"Conclusion","text":"<p>This decision tree is a testament to the power of Random Forest in handling complex classification tasks. By breaking down the decision process step by step, we gain a granular understanding of feature importance and model behavior, laying a foundation for informed model refinement and application.</p> <pre><code>from sklearn.tree import plot_tree\n\n\nfeature_names = list(X_train.columns)\n\n# Select one of the trees from your random forest\ntree_to_plot = rf_clf.estimators_[0]\n\n# Plot the selected tree\nfig = plt.figure(figsize=(25,20))\n_ = plot_tree(tree_to_plot,\n              feature_names=feature_names,\n              class_names=class_names,\n              filled=True)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-value-summary-feature-importance-for-iris-classification","title":"SHAP Value Summary: Feature Importance for Iris Classification","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_2","title":"Overview","text":"<p>This graph utilizes SHAP (SHapley Additive explanations) values to provide a summary of feature importance within our Iris species classification model. The length of the bars represents the average impact of each feature on the model's predictions, across all instances in the dataset.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#detailed-feature-contributions","title":"Detailed Feature Contributions","text":"<ul> <li>Petal Width (petal_width): Stands out as the feature with the most significant positive impact on model output, particularly for Iris-virginica predictions. The prominence of this bar suggests that petal width is a critical factor in the classification.</li> <li>Petal Length (petal_length): Exhibits a notable positive influence as well, especially for Iris-setosa. Its impact underlines the importance of petal length in distinguishing this particular species.</li> <li>Sepal Measurements (sepal_length and sepal_width): While having a lesser effect compared to petal features, these still contribute to the model's decision-making process, with sepal_length showing some influence on Iris-versicolor classification.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#model-insight-and-adjustments","title":"Model Insight and Adjustments","text":"<ul> <li>This visualization is a powerful tool for identifying potential biases or over-reliance on specific features. The predominance of petal-related features may suggest the need for balance through feature engineering or model hyperparameter tuning.</li> <li>The insight provided by this summary plot is critical for enhancing model fairness, balance, and ultimately, the trustworthiness of its predictions.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_2","title":"Conclusion","text":"<p>The SHAP summary plot offers an in-depth understanding of how each feature influences the classification model. It is essential for developers and stakeholders to make data-driven decisions regarding feature selection and to grasp the relative importance of attributes within the dataset.</p> <pre><code>shap.summary_plot(shap_values, X.values, plot_type=\"bar\", class_names= class_names, feature_names = X.columns)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-summary-plot-for-iris-dataset-model-interpretation","title":"SHAP Summary Plot for Iris Dataset Model Interpretation","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_3","title":"Overview","text":"<p>This SHAP (SHapley Additive exPlanations) summary plot provides a visual representation of the feature impact within our classification model for the Iris dataset. It quantifies the marginal contribution of each feature to the prediction made by the model, offering insights into the decision-making process.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#interpretation-of-shap-values","title":"Interpretation of SHAP Values","text":"<ul> <li>Positive and Negative Impacts: The distribution of SHAP values on the x-axis reveals how each feature affects the model output for individual observations. Red points indicate higher feature values, while blue points represent lower values, demonstrating the directional impact of features on the model output.</li> <li>Feature Contributions: For instance, <code>petal_width</code> is shown to have a predominantly positive impact on the model's predictions\u2014most red points lie to the right of the zero line, suggesting that higher petal width values tend to increase the likelihood of a particular Iris species prediction.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#data-point-analysis","title":"Data Point Analysis","text":"<ul> <li>Each point on the graph corresponds to a unique observation in the dataset. The spread of these points allows us to discern how the model differentiates between the samples, particularly noting the variability of effects across the range of feature values.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#utility-of-shap-summary","title":"Utility of SHAP Summary","text":"<ul> <li>Model Interaction: The SHAP values elucidate interactions between features and their directional influence on predictions, offering a reliable method to transparently showcase how feature variations influence the model's decisions.</li> <li>Insights for Model Improvement: This analysis is instrumental in enhancing our understanding of the model and explaining the heterogeneity and complexity present in the predictions. It aids in identifying areas for model improvement, guiding feature engineering, and ensuring robust prediction performance.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_3","title":"Conclusion","text":"<p>By employing the SHAP summary plot, we provide a granular view of feature influences, enhancing interpretability and trust in our model. It serves as a valuable tool for stakeholders alike, enabling data-driven decision-making and promoting a thorough comprehension of the model's predictive dynamics.</p> <pre><code>shap.summary_plot(shap_values[1], X.values, feature_names = X.columns)\n</code></pre> <pre><code>shap.summary_plot(shap_values[0], X.values, feature_names = X.columns)\n</code></pre> <pre><code>shap.summary_plot(shap_values[2], X.values, feature_names = X.columns)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-dependence-plot-sepal-lengths-influence-on-iris-classification","title":"SHAP Dependence Plot: Sepal Length's Influence on Iris Classification","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_4","title":"Overview","text":"<p>This SHAP dependence plot showcases the relationship between <code>sepal_length</code> and the model output, while also highlighting the impact of another feature, <code>sepal_width</code>, using a color gradient. We analyze how variations in sepal dimensions influence the classification predictions made by the model.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#key-observations","title":"Key Observations","text":"<ul> <li>Sepal Length Impact: As <code>sepal_length</code> increases, we observe a general trend of decreasing SHAP values, indicating a potentially negative influence on the model's confidence in predicting a particular Iris species.</li> <li>Interaction Effect: The color coding represents the <code>sepal_width</code> values, with warmer colors (red) indicating larger sepal widths. Notably, data points with larger <code>sepal_width</code> often correspond to higher SHAP values, suggesting that a wider sepal might counteract the negative impact of longer sepal length.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#insights-for-feature-interaction","title":"Insights for Feature Interaction","text":"<ul> <li>This plot allows us to discern not only the individual effects of features but also how they might interact with each other. For instance, while longer sepals (<code>sepal_length</code>) tend to decrease prediction confidence, this effect might be moderated by the width of the sepals (<code>sepal_width</code>).</li> <li>The variability in SHAP values across different <code>sepal_length</code> measurements, especially when colored by <code>sepal_width</code>, provides an understanding of how feature combinations affect model predictions.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#implications-for-model-refinement_1","title":"Implications for Model Refinement","text":"<ul> <li>Such insights are valuable for stakeholders when considering how to optimize features and adjust the model. </li> <li>Recognizing the influence of feature interactions is crucial for developing more robust and accurate classification models and can lead to more nuanced data preprocessing and feature engineering strategies.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_4","title":"Conclusion","text":"<p>The dependence plot is a vital interpretability tool, allowing stakeholders to grasp the complex dynamics of feature interactions within the model. This understanding is imperative for fine-tuning the model to enhance predictive performance and ensure that it generalizes well to new data.</p> <pre><code>shap.dependence_plot(0, shap_values[0], X.values, feature_names=X.columns)\n</code></pre> <pre><code>shap.dependence_plot(1, shap_values[0], X.values, feature_names=X.columns)\n</code></pre> <pre><code>shap.dependence_plot(2, shap_values[0], X.values, feature_names=X.columns)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-waterfall-plot-analysis-for-individual-prediction","title":"SHAP Waterfall Plot Analysis for Individual Prediction","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#overview_5","title":"Overview","text":"<p>The displayed SHAP waterfall plot is an interpretative tool used to break down the contribution of each feature to a specific prediction made by our machine learning model. It details the individual and cumulative impact of features on a single prediction.</p>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#feature-contributions-explained","title":"Feature Contributions Explained","text":"<ul> <li>Petal Measurements (petal_width and petal_length): These features exhibit a strong positive effect on the model's output. </li> <li>Sepal Measurements (sepal_length and sepal_width): While <code>sepal_length</code> shows a small positive contribution, <code>sepal_width</code> has a slight negative influence. The limited impact of <code>sepal_width</code> in this instance may suggest it plays a lesser role in the classification for this specific prediction.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#addressing-feature-impact","title":"Addressing Feature Impact","text":"<ul> <li>The substantial influence of petal measurements raises concerns about the model's reliance on a narrow set of features, which could lead to overfitting. This phenomenon occurs when a model learns patterns specific to the training data, impacting its ability to generalize to unseen data.</li> <li>To mitigate over-reliance and enhance generalization, regularization techniques may be employed, or the model could be adjusted to give more weight to other features in the dataset.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#conclusion_5","title":"Conclusion","text":"<p>Understanding which features the model prioritizes and the potential outcomes of this prioritization is central to data scientists' efforts to enhance the model's reliability and applicability. These analyses are crucial for maintaining a balanced performance of the model, ensuring that it remains robust across different scenarios.</p> <pre><code>row = 8\nshap.waterfall_plot(shap.Explanation(values=shap_values[0][row], \n                                        base_values=explainer.expected_value[0], data=X_test.iloc[row],  \n                                        feature_names=X_test.columns.tolist()))\n</code></pre> <pre><code>row = 42\nshap.waterfall_plot(shap.Explanation(values=shap_values[0][row], \n                                        base_values=explainer.expected_value[0], data=X_test.iloc[row],  \n                                        feature_names=X_test.columns.tolist()))\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-force-plot","title":"SHAP Force Plot","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#this-shap-force-plot-illustrates-the-impact-of-each-feature-on-our-models-classification-prediction-the-base-value-represents-our-average-reference-prediction-while-the-fx-100-value-is-the-definitive-prediction-made-by-our-model-for-this-instance-red-bars-petal-length-45-petal-width-13-and-sepal-length-57-indicate-factors-that-increase-the-models-prediction-these-three-features-have-elevated-the-prediction-with-petal-length-having-the-most-significant-impact-sepal-width-28-presents-a-slight-negative-effect-causing-a-minimal-decrease-in-the-models-prediction-overall-in-light-of-these-values-our-model-robustly-classifies-the-given-data-point-into-a-specific-category","title":"This SHAP Force Plot  illustrates the impact of each feature on our model's classification prediction. The \"base value\" represents our average reference prediction, while the \"f(x) = 1.00\" value is the definitive prediction made by our model for this instance. Red bars (petal length: 4.5, petal width: 1.3, and sepal length: 5.7) indicate factors that increase the model's prediction. These three features have elevated the prediction, with petal length having the most significant impact. Sepal width (2.8) presents a slight negative effect, causing a minimal decrease in the model's prediction. Overall, in light of these values, our model robustly classifies the given data point into a specific category.","text":"<pre><code>shap.plots.force(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0, :], matplotlib = True)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#shap-decision-plot","title":"SHAP Decision Plot","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#the-shap-decision-plot-visualizes-the-impact-of-individual-features-on-the-output-of-a-machine-learning-model-this-particular-plot-is-generated-using-the-shapdecision_plot-function-with-parameters-corresponding-to-the-expected-value-of-the-model-shap-values-for-a-set-of-predictions-and-feature-names-from-the-test-dataset","title":"The SHAP decision plot visualizes the impact of individual features on the output of a machine learning model. This particular plot is generated using the shap.decision_plot function with parameters corresponding to the expected value of the model, SHAP values for a set of predictions, and feature names from the test dataset:","text":"<p>X-axis: The model output value after accounting for the impact of each feature. Lines: Represent the shift in the model output due to the impact of the corresponding feature from the base value. Petal length: Shows a consistently negative impact on the model output. Petal width: Generally contributes towards an increase in the model output. Sepal length and sepal width: Exhibit variable impacts on the model output. This plot is instrumental in pinpointing the most influential features for a prediction and understanding their collective impact on the final model output.</p> <pre><code>#For class 0\nshap.decision_plot(explainer.expected_value[0], shap_values[0], X_test.columns)\n</code></pre> <pre><code># For class 1\nshap.decision_plot(explainer.expected_value[1], shap_values[1], X_test.columns)\n</code></pre> <pre><code># For class 2\nshap.decision_plot(explainer.expected_value[2], shap_values[2], X_test.columns)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/shap-analysis/#bank_marketingsnippetslabel_encoderpy","title":"bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#bank_marketingsnippetsone_hotpy","title":"bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multi collinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#model_observabilitymodelpy","title":"model_observability/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#model_trackingmodel_driftmodelpy","title":"model_tracking/model_drift/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlice_creammodeljson","title":"sparkml/ice_cream/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlice_creammodelpy","title":"sparkml/ice_cream/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlspark_with_jobjobpy","title":"sparkml/spark_with_job/job.py","text":"<pre><code>import practicuscore as prt \nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit, max, min, stddev, corr\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\n\nspark = SparkSession.builder \\\n    .appName(\"Advanced Data Processing\") \\\n    .getOrCreate()\n\nfile_path = \"/home/ubuntu/samples/insurance.csv\"\ndata = spark.read.csv(file_path, header=True, inferSchema=True)\nmissing_data = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n\ncategorical_columns = ['sex', 'smoker', 'region']\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n\ndata = data.withColumn(\"bmi_category\", \n                       when(col(\"bmi\") &lt; 18.5, lit(\"underweight\"))\n                       .when((col(\"bmi\") &gt;= 18.5) &amp; (col(\"bmi\") &lt; 25), lit(\"normal\"))\n                       .when((col(\"bmi\") &gt;= 25) &amp; (col(\"bmi\") &lt; 30), lit(\"overweight\"))\n                       .otherwise(lit(\"obese\")))\n\nfeature_columns = ['age', 'bmi', 'children', 'sex_encoded', 'smoker_encoded', 'region_encoded']\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\ndata = pipeline.fit(data).transform(data)\n\noutput_path = \"/home/ubuntu/my/processed_insurance_data.parquet/\"\n\ndata.write.parquet(output_path, mode=\"overwrite\")\n\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlspark_with_jobrun2c741eprt_dist_jobjson","title":"sparkml/spark_with_job/run/2c741e/prt_dist_job.json","text":"<pre><code>{\"job_type\":\"spark\",\"job_dir\":\"~/my/02_batch_job/\",\"initial_count\":2,\"coordinator_port\":7077,\"additional_ports\":[4040,7078,7079],\"terminate_on_completion\":false,\"py_file\":\"job.py\",\"executors\":[{\"rank\":0,\"instance_id\":\"5cf16b71\"},{\"rank\":1,\"instance_id\":\"63e80dc8\"}]}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlspark_with_jobrun2c741erank_0json","title":"sparkml/spark_with_job/run/2c741e/rank_0.json","text":"<pre><code>{\"rank\":0,\"instance_id\":\"5cf16b71\",\"state\":\"completed\",\"used_ram\":1187,\"peak_ram\":1187,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#sparkmlspark_with_jobrun2c741erank_1json","title":"sparkml/spark_with_job/run/2c741e/rank_1.json","text":"<pre><code>{\"rank\":1,\"instance_id\":\"63e80dc8\",\"state\":\"running\",\"used_ram\":284,\"peak_ram\":293,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#xgboostmodelpy","title":"xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/shap-analysis/#xgboostmodel_custom_dfpy","title":"xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: AutoML | Next: Bank Marketing &gt; Bank Marketing</p>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/","title":"Bank marketing Sample","text":"<p>A banking company wants to develop a model to predict the customers who will subscribe to time deposits and also wants to reach customers who are likely to subscribe to time deposits by using the call center resource correctly.</p> <p>In the data set to be studied, variables such as demographic information, balance information and previous campaign information of the customers will be used to predict whether they will subscribe to time deposits.</p> <p>Using the App - You can open the dataset in the Practicus AI  by loading all data - Then in the analysis phase, you can start with profiling the data - Then Graph &gt; Boxplot &gt; Age - Groupby &gt; Age, Job,  Balance Mean &amp; Median - Analyze &gt; Graph &gt; Plot -&gt; Job -&gt; Balance Mean Add Layer, Balance Median Add Layer</p> <p>Using Notebook - You can find detailed preprocessing steps in the notebook made with SDK - The main idea here is that the model is built by filtering the -1s in the pdays variable, that is, they are not included in the model. - In addition, the poutcome variable should be deleted to prevent Data Leakage. - Then the model Feature Selection was selected as 95% and the Setup params were set to fix_imbalance: True.</p> <pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None # Eg. bank_test_1\nexperiment_tracking_service = None # Eg. MlFlow\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code>my_deployment_list = region.model_deployment_list\ndisplay(my_deployment_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>my_addon_list = region.addon_list\ndisplay(my_addon_list.to_pandas())\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please select a model_name\"\nassert experiment_tracking_service, \"Please select an experiment tracking service, or skip this cell\"\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n</code></pre> <pre><code>data_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/bank_marketing.csv\"\n}\n\nproc = worker.load(data_conn) \nproc.show_head() \n</code></pre> <pre><code>proc.delete_columns(['poutcome']) \n</code></pre> <pre><code>proc.run_snippet( \n    'one_hot', \n    text_col_list=['marital', 'default', 'housing', 'loan', ],  \n    max_categories=25,  \n    dummy_option='Drop First Dummy',  \n    result_col_suffix=[],  \n    result_col_prefix=[],  \n) \n</code></pre> <pre><code>proc.delete_columns(['default']) \n</code></pre> <pre><code>proc.delete_columns(['housing', 'loan']) \n</code></pre> <pre><code>proc.delete_columns(['marital']) \n</code></pre> <pre><code>proc.run_snippet( \n    'label_encoder', \n    text_col_list=['job', 'education', 'contact', 'month', 'deposit', ],  \n) \n</code></pre> <pre><code>filter_expression = ''' \ncol[pdays] != -1 \n''' \nproc.filter(filter_expression) \n</code></pre> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#building-a-model-using-automl","title":"Building a model using AutoML","text":"<p>The below code is generated. You can update the code to fit your needs, or re-create it by building a model with Practicus AI app first and then view it's jupter notebook oncethe model building is completed.</p> <pre><code>from pycaret.classification import ClassificationExperiment, load_model, predict_model\n\nexp = ClassificationExperiment()\n</code></pre> <pre><code>experiment_name = 'Bank-marketing'\nprt.experiments.configure(service_name=experiment_tracking_service, experiment_name=experiment_name)\n</code></pre> <pre><code>setup_params = {'fix_imbalance': True}\n</code></pre> <pre><code>exp.setup(data=df, target='deposit', session_id=7272, \n          log_experiment=True, experiment_name=experiment_name, **setup_params)\n</code></pre> <pre><code>best_model = exp.compare_models()\n</code></pre> <pre><code>tune_params = {}\n</code></pre> <pre><code>tuned_model = exp.tune_model(best_model, **tune_params)\n</code></pre> <pre><code>final_model = exp.finalize_model(tuned_model)\n</code></pre> <pre><code>predictions = exp.predict_model(final_model, data=df)\ndisplay(predictions)\n</code></pre> <pre><code>exp.save_model(final_model, 'model')\n</code></pre> <pre><code>loaded_model = load_model('model')\n\npredictions = predict_model(loaded_model, data=df)\ndisplay(predictions)\n</code></pre> <pre><code># Deploy to current Practicus AI region\nprt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#prediction-by-using-model-api","title":"Prediction by using model API","text":"<pre><code>region = prt.current_region()\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>import requests \nimport pandas as pd\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#prediction-by-using-sdk","title":"Prediction by using SDK","text":"<pre><code>proc.predict( \n    api_url=f\"{region.url}/{prefix}/{model_name}/\", \n    column_names=['Buildgnage', 'job', 'education', 'balance', 'contact', 'day', 'month',\n       'duration', 'campaign', 'pdays', 'previous', 'deposit',\n       'marital_married', 'marital_single', 'default_yes', 'housing_yes',\n       'loan_yes'], \n    new_column_name='predicted_deposit' \n) \n</code></pre> <pre><code>df = proc.get_df_copy()\ndf.columns\n</code></pre> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre> <pre><code>df_predicted = proc.get_df_copy()\n</code></pre> <pre><code>df_predicted\n</code></pre> <pre><code>df_predicted['predicted_deposit'].head()\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#snippetslabel_encoderpy","title":"snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/bank-marketing/bank-marketing/#snippetsone_hotpy","title":"snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multi collinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre> <p>Previous: Shap Analysis | Next: Workflows &gt; Task Parameters</p>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/","title":"Model Observability and Monitoring","text":""},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#scenario-model-drift","title":"Scenario: Model Drift","text":"<p>In this example, we'll deploy a model on an income dataset. Our main goal withIn this example is deploying pre-processes into the \"model.pkl\".</p> <ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Preparing pre-process function</p> </li> <li> <p>Preparing train pipeline and deploy a model on income dataset</p> </li> <li> <p>Making predictions with deployed model without making any pre-process</p> </li> </ol> <pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None # Eg. bank_test_1\npracticus_url = None # Eg. http://practicus.company.com\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code>my_model_deployment_list = region.model_deployment_list\ndisplay(my_model_deployment_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please select a model_name\"\nassert practicus_url, \"Please select practicus_url\"\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#creating-preprocess-and-the-pipeline","title":"Creating Preprocess and the Pipeline","text":"<p>Loading the data set</p> <pre><code>import practicuscore as prt\nimport pandas as pd \n\ndf = pd.read_csv(\"/home/ubuntu/samples/income.csv\")\n\ndf.head()\n</code></pre> <p>Creating pre-process function for new features</p> <pre><code>from sklearn.preprocessing import FunctionTransformer\n\ndef add_features(df):    \n\n    for column in df.select_dtypes(include='object'):  # Selecting columns which has type as object\n        mode_value = df[column].mode()[0]  # Find mode\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):  # Selecting columns which has type as in64\n        mean_value = df[column].mean()  # Find median\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):  # Selecting columns which has type as float64\n        mean_value = df[column].mean()  # Find Median\n        df[column] = df[column].fillna(mean_value)\n\n    return df\n\nadd_features_transformer = FunctionTransformer(add_features, validate=False)\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>df.dtypes\n</code></pre> <p>Defining categorical and numerical features</p> <pre><code>numeric_features = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\ncategorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n</code></pre> <p>Creating preprocessor object for the pipeline to apply scaling and one hot encoding</p> <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n</code></pre> <p>Creating the pipeline</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[\n    ('add_features', add_features_transformer),\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier())\n])\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#model-training","title":"Model Training","text":"<p>Train test split</p> <pre><code>X = df.drop(['income &gt;50K'], axis=1)\ny = df['income &gt;50K']\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42);\n</code></pre> <p>Fitting the model</p> <pre><code>pipeline.fit(X_train, y_train)\n</code></pre> <pre><code>score = pipeline.score(X_test, y_test)\nprint(f'Accuracy Score of the model: {score}')\n</code></pre> <p>Importing the model by using cloudpickle</p> <pre><code>import cloudpickle\n\nwith open('model.pkl', 'wb') as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre> <pre><code>import joblib\nimport pandas as pd\n\n# Load the saved model\nmodel = joblib.load('model.pkl')\n\n# Making predictions\npredictions = model.predict(X)\n\n# Converting predictions to a DataFrame\npredictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#deployment-of-the-model","title":"Deployment of the model","text":"<pre><code>prt.models.deploy(\n    deployment_key=deployment_key, \n    prefix=prefix, \n    model_name=model_name, \n    model_dir=None # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#prediction","title":"Prediction","text":"<pre><code># Let's construct the REST API url.\n# Please replace the below url with your current Practicus AI address\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"https://{practicus_url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <p>Making predictions without making any pre-process on the income dataset</p> <pre><code>import requests\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <pre><code>pred_df.value_counts()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/model-observability/model-observability/#modelpy","title":"model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre> <p>Previous: Model Drift | Next: XGBoost &gt; XGBoost</p>"},{"location":"technical-tutorial/extras/modeling/model-tracking/experiment-tracking/experiment-tracking-logging/","title":"Experiment Tracking Logging","text":"<pre><code>import practicuscore as prt\nimport os\nimport mlflow\n\nregion = prt.current_region()\n</code></pre> <pre><code># Defining parameters\n\n# You need to configure using the service unique key and name\nservice_name = None\nservice_key =  None\n\n# Optionally, you can provide experiment name to create a new experiment while configuring\nexperiment_name = None\n</code></pre> <pre><code># If you don't know service key and name you can checkout down below\n\naddon_list = region.addon_list\ndisplay(addon_list.to_pandas())\n</code></pre> <pre><code>assert service_name, \"Please select a service_name\"\nassert service_key, \"Please select a service_key\"\nassert experiment_name, \"Please enter a experiment_name\"\n</code></pre> <pre><code>prt.experiments.configure(service_name=service_name, service_key=service_key, experiment_name=experiment_name)\n</code></pre> <pre><code># Set experiment name, if you haven't already while configuring the service\nmlflow.set_experiment(\"my experiment\")\n\n# Prefer unique run names, or leave empty to auto generate unique names\nrun_name = \"My ML experiment run 123\"\n\n# Start an MLflow run and log params, metrics and artifacts\nwith mlflow.start_run(run_name=run_name):\n    # Log parameters\n    mlflow.log_param(\"param1\", 5)\n    mlflow.log_param(\"param2\", \"test\")\n\n    # Log metrics\n    mlflow.log_metric(\"metric1\", 0.85)\n\n    # Create an artifact (e.g., a text file)\n    artifact_path = \"artifacts\"\n    if not os.path.exists(artifact_path):\n        os.makedirs(artifact_path)\n    file_path = os.path.join(artifact_path, \"output.txt\")\n\n    with open(file_path, \"w\") as f:\n        f.write(\"This is a test artifact.\")\n\n    # Log the artifact\n    mlflow.log_artifacts(artifact_path)\n\n    # Optional: Print the run ID\n    print(\"Run ID:\", mlflow.active_run().info.run_id)\n\n# Explicitly close the active MLflow run, if you are not using the above with keyword\n# mlflow.end_run()\n</code></pre> <p>Previous: Spark Tutorial | Next: Experiment Tracking Model Training</p>"},{"location":"technical-tutorial/extras/modeling/model-tracking/experiment-tracking/experiment-tracking-model-training/","title":"Experiment Tracking Model Training","text":"<pre><code>import practicuscore as prt\nimport os\nimport mlflow\nimport xgboost as xgb\nimport cloudpickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nregion = prt.current_region()\n</code></pre> <pre><code># Defining parameters\n\n# You need to configure using the service unique key and name\nservice_name = None\nservice_key =  None\n\n# Optionally, you can provide experiment name to create a new experiment while configuring\nexperiment_name = None\n</code></pre> <pre><code># If you don't know service key and name you can checkout down below\n\naddon_list = region.addon_list\ndisplay(addon_list.to_pandas())\n</code></pre> <pre><code>assert service_name, \"Please select a service_name\"\nassert service_key, \"Please select a service_key\"\nassert experiment_name, \"Please select a experiment_name\"\n</code></pre> <pre><code>prt.experiments.configure(service_name=service_name, service_key=service_key, experiment_name=experiment_name)\n</code></pre> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/ice_cream.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn) \n\ndata = proc.get_df_copy()\ndata.head()\n</code></pre> <pre><code># Set experiment name, if you haven't already while configuring the service\nmlflow.set_experiment(\"XGBoost Experiment\")\n\n# Loading the dataset\nX = data.Temperature\ny = data.Revenue\n</code></pre> <pre><code># Test and Train split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# XGBoost parameters\nparams = {\n    'max_depth': 3,\n    'eta': 0.1,\n    'objective': 'reg:squarederror',\n}\n</code></pre> <pre><code># Creation of DMatrix\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n</code></pre> <pre><code># Training of model by using mlflow\nwith mlflow.start_run():\n    mlflow.log_params(params)\n    model = xgb.train(params, dtrain, num_boost_round=200)\n    # Prediction process\n    predictions = model.predict(dtest)\n    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"rmse\", rmse)\n    # Saving the model in MLFlow\n    artifact_path = \"model\"\n    if not os.path.exists(artifact_path):\n        os.makedirs(artifact_path)\n    model_path = os.path.join(artifact_path, \"xgboost_model.pkl\")\n    with open(model_path, \"wb\") as f:\n        cloudpickle.dump(model, f)\n    # Saving the serialised model in MLflow\n    mlflow.log_artifacts(artifact_path)\n    mlflow.log_artifacts(artifact_path)\n    # Printing out the run id\n    print(\"Run ID:\", mlflow.active_run().info.run_id)\n</code></pre> <pre><code># Ending MLFlow\nmlflow.end_run()\n</code></pre> <p>Previous: Experiment Tracking Logging | Next: Model Drift &gt; Model Drift</p>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/","title":"Model Observability and Monitoring","text":""},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#scenario-model-drift","title":"Scenario: Model Drift","text":"<p>In this example, we'll deploy a model on an insurance dataset to make two predictions. Introducing intentional drifts in the BMI and Age columns, we aim to observe their impact on the model's predictions.</p> <ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Train and deploy a model on insurance dataset</p> </li> <li> <p>Making predictions with deployed model</p> </li> <li> <p>Multiplying the BMI and Age columns to create Drifts on features and predictions</p> </li> <li> <p>Observing the model drift plots</p> </li> </ol>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None\npracticus_url = None # Example http://company.practicus.com\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please enter a model_name\"\nassert practicus_url, \"Please enter practicus_url\" \n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#model-development","title":"Model Development","text":"<p>Loading and preparing the dataset</p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code># Loading the dataset to worker\n\nimport practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn) \n\nproc.show_head()\n</code></pre> <pre><code># Pre-process\n\nproc.categorical_map(column_name='sex', column_suffix='category') \nproc.categorical_map(column_name='smoker', column_suffix='category') \nproc.categorical_map(column_name='region', column_suffix='category') \nproc.delete_columns(['region', 'smoker', 'sex']) \n</code></pre> <pre><code># Taking the dataset into csv\n\ndf = proc.get_df_copy()\ndf.head()\n</code></pre> <p>Model Training</p> <pre><code>X = df.drop('charges', axis=1)\ny = df['charges']\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <pre><code>import xgboost as xgb\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('model', xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100))\n])\npipeline.fit(X_train, y_train)\n</code></pre> <pre><code># Exporting the model\n\nimport cloudpickle\n\nwith open('model.pkl', 'wb') as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#model-deployment","title":"Model Deployment","text":"<pre><code>prt.models.deploy(\n    deployment_key=deployment_key, \n    prefix=prefix, \n    model_name=model_name, \n    model_dir=None # Current Dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#prediction","title":"Prediction","text":"<pre><code># Loading the dataset to worker\n\nimport practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn) \n\nproc.categorical_map(column_name='sex', column_suffix='category') \nproc.categorical_map(column_name='smoker', column_suffix='category') \nproc.categorical_map(column_name='region', column_suffix='category') \nproc.delete_columns(['region', 'smoker', 'sex']) \n\ndf = proc.get_df_copy()\ndf.head()\n</code></pre> <pre><code># Let's construct the REST API url.\n# Please replace the below url with your current Practicus AI address\n# e.g. http://practicus.company.com\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"https://{practicus_url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code>import requests\nimport pandas as pd\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <ul> <li>After you make the first prediction, please wait for 5 minutes to see a clearer picture on the drift plot</li> <li>When we look at Model Drifts Dashboard at Grafana we will see plots with the model drift visible</li> </ul>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#hand-made-model-drift","title":"Hand-Made Model Drift","text":"<pre><code>df['age'] = df['age'] * 2\n</code></pre> <pre><code>df['bmi'] = df['bmi'] * 3\n</code></pre> <pre><code>display(df)\n</code></pre> <pre><code>headers = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <ul> <li>After you make the second prediction, please wait for 2 minutes to see a clearer picture on the drift plot</li> <li>When we look at Model Drifts Dashboard at Grafana we will see plots with the model drift visible</li> </ul>"},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/model-tracking/model-drift/model-drift/#modelpy","title":"model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Experiment Tracking Model Training | Next: Model Observability &gt; Model Observability</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/","title":"SparkML Ice Cream","text":""},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#end-to-end-sparkml-model-development-and-deployment","title":"End-to-end SparkML Model development and deployment","text":"<p>This sample notebook outlines the process of deploying a SparkML model on the Practicus AI platform and making predictions from the deployed model using various methods.</p> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None\npracticus_url = None # E.g. http://company.practicus.com\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code>my_model_deployment_list = region.model_deployment_list\ndisplay(my_model_deployment_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please enter a model_name\"\nassert practicus_url, \"Please enter practicus_url\"\n</code></pre> <pre><code>import pandas as pd\n\n# Step 1: Read CSV with Pandas\ndata = pd.read_csv(\"/home/ubuntu/samples/ice_cream.csv\")\n</code></pre> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.linalg import Vectors\n\n# Step 2: Convert to Spark DataFrame with Explicit Schema\nspark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n\n# Define schema for Spark DataFrame\nschema = StructType([\n    StructField(\"label\", DoubleType(), True),\n    StructField(\"features\", DoubleType(), True)\n])\n\nspark_data = spark.createDataFrame(\n    data.apply(lambda row: (float(row['Revenue']), Vectors.dense(float(row['Temperature']))), axis=1),\n    schema=[\"label\", \"features\"]\n)\n</code></pre> <pre><code>from pyspark.ml.regression import LinearRegression\n\n# Step 3: Train Linear Regression Model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\nmodel = lr.fit(spark_data)\n</code></pre> <pre><code># Step 4: Make Predictions\npredictions = model.transform(spark_data)\n\npredictions.select(\"features\", \"label\", \"prediction\").show()\n</code></pre> <pre><code># Step 5: Save Model\nmodel.save(model_name)\n</code></pre> <pre><code># Optional: Stop Spark session when done\nspark.stop()\n</code></pre> <pre><code># Prediction, you can run this in another notebook \n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\ndef predict(df: pd.DataFrame) -&gt; pd.DataFrame:\n    spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    model = LinearRegressionModel.load(model_name)\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"/home/ubuntu/samples/ice_cream.csv\")\n</code></pre> <pre><code>prediction = predict(data)\n\nprediction\n</code></pre> <pre><code># Optional: Stop Spark session when done\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#deploying-the-sparkml-model","title":"Deploying the SparkML Model","text":""},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#step-1-upload-to-object-storage","title":"Step 1) Upload to object storage","text":"<ul> <li>Before you start deploying the model, please upload SparkML model files to the object storage that your Practicus AI model deployment is using.</li> <li>Why? Unlike scikit-learn, XGBoost etc., Spark ML model files are not a file such as model.pkl, but a folder. </li> </ul>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#sample-object-storage-cache-folder","title":"Sample object storage cache folder","text":"<ul> <li>E.g. s3://my-models-bucket/cache/ice_cream_sparkml_model/ice_cream_sparkml_model/ [ add your model folders here ] </li> <li>Why use same folder name twice? ice_cream_sparkml_model/ice_cream_sparkml_model </li> <li>Practicus AI will download all cache files defined in model.json.</li> <li>If you select cache/ice_cream_sparkml_model and don't use the second folder, all of your model files will be downloaded under /var/practicus/cache/</li> <li>If you are caching one model only this would work fine, but if you deploy multiple models their files can override each other.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#step-2-verify-you-practicus-ai-model-host-is-sparkml-compatible","title":"Step 2) Verify you Practicus AI model host is SparkML compatible","text":"<ul> <li>Please make sure you are using a Practicus AI model deployment image with SparkML support.</li> <li>You can use the default SparkML image: ghcr.io/practicusai/practicus-modelhost-sparkml:{add-version-here}</li> <li>Not sure how? Please consult your admin and ask for the deployment key with SparkML.</li> </ul>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#step-3-deploy-the-model","title":"Step 3) Deploy the model","text":"<ul> <li>Please follow the below steps to deploy the model as usual</li> </ul> <pre><code>import practicuscore as prt \n\n# Deploying model as an API\n# Please review model.py and model.json files\n\nprt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None # Current dir  \n)\n</code></pre> <pre><code>region = prt.current_region()\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code>token = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code># Caution: Due to initial Spark session creation process, first prediction can be quite slow.\n\nimport pandas as pd\nimport requests \n\ndf = pd.read_csv(\"/home/ubuntu/samples/ice_cream.csv\")\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#modeljson","title":"model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/ice-cream/sparkml-ice-cream/#modelpy","title":"model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre> <p>Previous: Use Polars | Next: Spark With Job &gt; Batch Job</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/","title":"Spark Tutorial","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit, max, min, stddev, corr\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#sparksession-creation","title":"SparkSession Creation","text":"<p>This code creates a <code>SparkSession</code> object, which is the entry point for any Spark application. It is used to configure and initialize a Spark application for data processing and analysis.</p> <pre><code>spark = SparkSession.builder \\\n    .appName(\"Advanced Data Processing\") \\\n    .getOrCreate()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#reading-a-csv-file-with-spark","title":"Reading a CSV File with Spark","text":"<p>A CSV file is loaded into a Spark DataFrame using Spark's <code>read.csv()</code> method. This allows the data to be processed and analyzed efficiently within the Spark environment.</p> <pre><code>file_path = \"/home/ubuntu/samples/insurance.csv\"\ndata = spark.read.csv(file_path, header=True, inferSchema=True)\n</code></pre> <pre><code>print(\"First rows:\")\ndata.show(5)\n</code></pre> <pre><code>print(\"Data Schema:\")\ndata.printSchema()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#checking-for-null-values-in-a-spark-dataframe","title":"Checking for Null Values in a Spark DataFrame","text":"<p>Null values in a DataFrame are counted for each column, and the results are displayed to assess data completeness.</p> <pre><code>print(\"Check Nulls:\")\nmissing_data = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\nmissing_data.show()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#data-summary-and-statistical-analysis","title":"Data Summary and Statistical Analysis","text":"<p>Statistical measures such as descriptive statistics, minimum, maximum, standard deviation, and correlation are computed to analyze the data.</p> <pre><code>data.describe().show()\n\nprint(\"Min, Max and Std:\")\ndata.select(\n    [min(c).alias(f\"{c}_min\") for c in data.columns if data.schema[c].dataType != \"StringType\"] +\n    [max(c).alias(f\"{c}_max\") for c in data.columns if data.schema[c].dataType != \"StringType\"] +\n    [stddev(c).alias(f\"{c}_stddev\") for c in data.columns if data.schema[c].dataType != \"StringType\"]\n).show()\n\n\nprint(\"Correlation Analysis:\")\ndata.select(corr(\"age\", \"charges\").alias(\"age_charges_corr\"), \n            corr(\"bmi\", \"charges\").alias(\"bmi_charges_corr\")).show()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#handling-categorical-variables","title":"Handling Categorical Variables","text":"<p>This code transforms categorical variables into numerical representations, preparing the data for machine learning algorithms.</p> <pre><code>categorical_columns = ['sex', 'smoker', 'region']\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#bmi-categorization-and-grouping","title":"BMI Categorization and Grouping","text":"<p>BMI values are categorized into specific groups, and the data is grouped to count the number of entries in each category.</p> <pre><code>data = data.withColumn(\"bmi_category\", \n                       when(col(\"bmi\") &lt; 18.5, lit(\"underweight\"))\n                       .when((col(\"bmi\") &gt;= 18.5) &amp; (col(\"bmi\") &lt; 25), lit(\"normal\"))\n                       .when((col(\"bmi\") &gt;= 25) &amp; (col(\"bmi\") &lt; 30), lit(\"overweight\"))\n                       .otherwise(lit(\"obese\")))\n\ndata.groupBy(\"bmi_category\").count().show()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#feature-vector-assembly","title":"Feature Vector Assembly","text":"<p>The specified numerical and encoded categorical columns are combined into a single feature vector column, which is essential for machine learning models.</p> <pre><code>feature_columns = ['age', 'bmi', 'children', 'sex_encoded', 'smoker_encoded', 'region_encoded']\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#scaling-features-and-applying-a-pipeline","title":"Scaling Features and Applying a Pipeline","text":"<p>A pipeline is created to process data by combining categorical encoding, feature vector assembly, and feature scaling into a sequential workflow. This ensures a clean and efficient transformation of raw data into a format suitable for machine learning.</p> <pre><code>scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\ndata = pipeline.fit(data).transform(data)\n\ndata.select(\"features\", \"scaled_features\").show(5, truncate=False)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-for-ds/spark-tutorial/#writing-processed-data-to-a-parquet-file-and-stopping-spark-session","title":"Writing Processed Data to a Parquet File and Stopping Spark Session","text":"<p>The processed DataFrame is saved in a Parquet format, and the Spark session is gracefully terminated to release resources.</p> <pre><code>output_path = \"/home/ubuntu/my/processed_insurance_data.parquet\"\ndata.write.parquet(output_path, mode=\"overwrite\")\n\nspark.stop()\n</code></pre> <p>Previous: Batch Job | Next: Model Tracking &gt; Experiment Tracking &gt; Experiment Tracking Logging</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/","title":"Executing batch jobs in Spark Cluster","text":"<p>In this example we will: - Create a Spark cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"spark\" under your \"~/my\" folder</li> <li>And copy job.py under this (spark_with_job) folder</li> </ul> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/spark_with_job/\"\n\n\ndistributed_config = prt.DistJobConfig(\n    job_type = prt.DistJobType.spark,\n    job_dir = job_dir,\n    py_file = \"job.py\",\n    worker_count = 2,\n    terminate_on_completion = False\n)\n\nworker_config = prt.WorkerConfig(\n    worker_size=\"Small\",\n    distributed_config=distributed_config,\n    log_level=\"DEBUG\"\n)\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n\n)\n</code></pre> <pre><code># You can view the logs during or after the job is completed\n# To view coordinator (master) set rank = 0\nrank = 0\n# To view other workers set rank = 1,2, ..\n\nprt.distributed.view_log(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n    rank=rank\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt \nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, count, when, lit, max, min, stddev, corr\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml import Pipeline\n\nspark = SparkSession.builder \\\n    .appName(\"Advanced Data Processing\") \\\n    .getOrCreate()\n\nfile_path = \"/home/ubuntu/samples/insurance.csv\"\ndata = spark.read.csv(file_path, header=True, inferSchema=True)\nmissing_data = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n\ncategorical_columns = ['sex', 'smoker', 'region']\nindexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_columns]\nencoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_encoded\") for col in categorical_columns]\n\ndata = data.withColumn(\"bmi_category\", \n                       when(col(\"bmi\") &lt; 18.5, lit(\"underweight\"))\n                       .when((col(\"bmi\") &gt;= 18.5) &amp; (col(\"bmi\") &lt; 25), lit(\"normal\"))\n                       .when((col(\"bmi\") &gt;= 25) &amp; (col(\"bmi\") &lt; 30), lit(\"overweight\"))\n                       .otherwise(lit(\"obese\")))\n\nfeature_columns = ['age', 'bmi', 'children', 'sex_encoded', 'smoker_encoded', 'region_encoded']\nassembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n\npipeline = Pipeline(stages=indexers + encoders + [assembler, scaler])\ndata = pipeline.fit(data).transform(data)\n\noutput_path = \"/home/ubuntu/my/processed_insurance_data.parquet/\"\n\ndata.write.parquet(output_path, mode=\"overwrite\")\n\nspark.stop()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#run2c741eprt_dist_jobjson","title":"run/2c741e/prt_dist_job.json","text":"<pre><code>{\"job_type\":\"spark\",\"job_dir\":\"~/my/02_batch_job/\",\"initial_count\":2,\"coordinator_port\":7077,\"additional_ports\":[4040,7078,7079],\"terminate_on_completion\":false,\"py_file\":\"job.py\",\"executors\":[{\"rank\":0,\"instance_id\":\"5cf16b71\"},{\"rank\":1,\"instance_id\":\"63e80dc8\"}]}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#run2c741erank_0json","title":"run/2c741e/rank_0.json","text":"<pre><code>{\"rank\":0,\"instance_id\":\"5cf16b71\",\"state\":\"completed\",\"used_ram\":1187,\"peak_ram\":1187,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/sparkml/spark-with-job/batch-job/#run2c741erank_1json","title":"run/2c741e/rank_1.json","text":"<pre><code>{\"rank\":1,\"instance_id\":\"63e80dc8\",\"state\":\"running\",\"used_ram\":284,\"peak_ram\":293,\"total_ram\":3200,\"gpus\":0,\"used_vram\":0,\"peak_vram\":0,\"reserved_vram\":0,\"total_vram\":0}\n</code></pre> <p>Previous: SparkML Ice Cream | Next: Spark For Ds &gt; Spark Tutorial</p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/","title":"XGBoost","text":""},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#end-to-end-custom-xgboost-model-development-and-deployment","title":"End-to-end custom XGBoost Model development and deployment","text":"<p>This sample notebook outlines the process of deploying a custom XGBoost model on the Practicus AI platform and making predictions from the deployed model using various methods.</p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>deployment_key = None\nprefix = None\nmodel_name = None\npracticus_url = None # E.g. http://company.practicus.com\n</code></pre> <pre><code>assert deployment_key, \"Please select a deployment key\"\nassert prefix, \"Please select a prefix\"\nassert model_name, \"Please enter a model_name\"\nassert practicus_url, \"Please enter practicus_url\"\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#data-preparation","title":"Data Preparation","text":"<p>We will be using Practicus AI to prepare data, but you can also do it manually using just Pandas. The rest of the model building and deployment steps would not change. </p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\":\"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn) \n\nproc.show_head()\n</code></pre> <pre><code>proc.categorical_map(column_name='sex', column_suffix='category') \nproc.categorical_map(column_name='smoker', column_suffix='category') \nproc.categorical_map(column_name='region', column_suffix='category') \nproc.delete_columns(['region', 'smoker', 'sex']) \n</code></pre> <pre><code>df = proc.get_df_copy()\ndf.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#building-the-model","title":"Building the model","text":"<p>Let's build a model with XGBoost</p> <pre><code>X = df.drop('charges', axis=1)\ny = df['charges']\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <pre><code>import xgboost as xgb\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('model', xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100))\n])\npipeline.fit(X_train, y_train)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#saving-your-model","title":"Saving your model","text":"<ul> <li>After training your model, you can save it in <code>.pkl</code> format.</li> <li>Although not mandatory, please prefer to use cloudpickle so your custom code part of the model (e.g. preprocessing steps) is more portable.</li> </ul> <pre><code>import cloudpickle\n\nwith open('model.pkl', 'wb') as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre> <pre><code>import joblib \nimport pandas as pd\n\n# Load the saved model\nmodel = joblib.load('model.pkl')\n\n# Making predictions\npredictions = model.predict(X)\n\npred_df = pd.DataFrame(predictions, columns=['Predictions'])\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#deploying-your-model-as-an-api-endpoint","title":"Deploying your model as an API endpoint","text":"<pre><code># Let's locate the Kubernetes model deployment to deploy our model\nif len(region.model_deployment_list) == 0:\n    raise SystemError(\"You do not have any model deployment systems. \"\n                      \"Please contact your system admin.\")\nelif len(region.model_deployment_list) &gt; 1:\n    print(\"You have more than one model deployment systems. \"\n          \"Will use the first one\")\n\nprint(f\"Will deploy '{prefix}/{model_name}' to '{deployment_key}' kubernetes deployment\")\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#create-modelpy-that-predicts-using-modelpkl","title":"Create model.py that predicts using model.pkl","text":"<p>View sample model.py code</p> <pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#making-predictions-using-the-model-api","title":"Making predictions using the model API","text":"<pre><code># *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#getting-a-session-token-for-the-model-api","title":"Getting a session token for the model API","text":"<ul> <li>You can programmatically get a short-lived (~4 hours) model session token (recommended)</li> <li>You can also get an access token that your admin provided with a custom life span, e.g. months.</li> </ul> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#posting-data","title":"Posting data","text":"<ul> <li>There are multiple ways to post your data and construct the DataFrame in your model.py code.</li> <li>If you add content-type header, Practicus AI model hosting system will automatically convert your csv data into a Pandas DataFrame, and pass on to model.py predict() method.</li> <li>If you would like to construct the Dataframe yourself, skip passing content-type header and construct using Starlette request object View sample code</li> </ul> <pre><code>import requests \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#compressing-api-traffic","title":"Compressing API traffic","text":"<ul> <li>Practicus AI currently supports 'lz4' (recommended), 'zlib', 'deflate' and 'gzip' compression algorithms.</li> <li>Compressing to and from the API endpoint can increase performance for large datasets, low network bandwidth.</li> </ul> <pre><code>import lz4 \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv',\n    'content-encoding': 'lz4'\n}\ndata_csv = X.to_csv(index=False)\ncompressed = lz4.frame.compress(data_csv.encode())\nprint(f\"Request compressed from {len(data_csv)} bytes to {len(compressed)} bytes\")\n\nr = requests.post(api_url, headers=headers, data=compressed)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\ndecompressed = lz4.frame.decompress(r.content)\nprint(f\"Response de-compressed from {len(r.content)} bytes to {len(decompressed)} bytes\")\n\npred_df = pd.read_csv(BytesIO(decompressed))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#detecting-drift","title":"Detecting drift","text":"<p>If enabled, Practicus AI allows you to detect feature and prediction drift and visualize in an observability platform such as Grafana. </p> <pre><code># Let's create an artificial drift for BMI feature, which will also affect charges\ndf[\"bmi\"] = df[\"bmi\"] * 1.2\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#recommended-adding-model-metadata-to-your-api","title":"(Recommended) Adding model metadata to your API","text":"<ul> <li>You can create and upload model.json file that defines the input and output schema of your model and potentially other metadata too.</li> <li>This will explain how to consume your model efficiently and make it accessible to more users.</li> <li>Practicus AI uses MlFlow model input/output standard to define the schema</li> <li>You can build the model.json automatically, or let Practicus AI build it for you using the dataframe.</li> </ul> <pre><code>model_config = prt.models.create_model_config(\n    df=df,\n    target=\"charges\",\n    model_name=\"My XG Boost Model\",\n    problem_type=\"Regression\",\n    version_name=\"2024-02-15\",\n    final_model=\"xgboost\",\n    score=123\n)\nmodel_config.save(\"model.json\")\n# You also can directly instantiate ModelConfig class to provide more metadata elements\n# model_config = prt.models.ModelConfig(...)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#adding-a-new-api-version","title":"Adding a new API version","text":"<ul> <li>You can add as many model version as you need.</li> <li>Your admin can then route traffic as needed, Including for A/B testing.</li> </ul> <pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=None # Current dir\n)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#reading-model-metadata","title":"Reading model metadata","text":"<p>You can add the query string '?get_meta=true' to any model to get the metadata.</p> <pre><code>headers = {'authorization': f'Bearer {token}'}\nr = requests.get(api_url + '?get_meta=true', headers=headers)\n\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nimport json\nfrom pprint import pprint\n\nmodel_config_dict = json.loads(r.text)\nprint(\"Model metadata:\")\npprint(model_config_dict)\nschema_dict = json.loads(model_config_dict[\"model_signature_json\"])\nprint(\"Model input/output schema:\")\npprint(schema_dict)\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#optional-consuming-custom-model-apis-from-practicus-ai-app","title":"(Optional) Consuming custom model APIs from Practicus AI App","text":"<p>Practicus AI App users can consume any REST API model. </p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#end-user-api-view","title":"End user API view","text":"<p>If you add metadata to your models, App users will be able to view your model details in the UI.</p> <p></p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#feature-matching","title":"Feature matching","text":"<p>If your model has an input/output schema, the App will try to match them to current dataset.</p> <p></p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#viewing-the-prediction-result","title":"Viewing the prediction result","text":"<p>Note: Please consider adding categorical mapping to your models as a pre-processing step for improved user experience.</p>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#recommended-clean-up","title":"(Recommended) Clean-up","text":"<p>Explicitly calling kill() on your processes will free un-used resources on your worker faster.</p> <pre><code>proc.kill()\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#modelpy","title":"model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"technical-tutorial/extras/modeling/xgboost/xgboost/#model_custom_dfpy","title":"model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Model Observability | Next: AutoML</p>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/","title":"API Triggers with HttpSensor in Apache Airflow","text":"<p>In this example, we will demonstrate how to use the <code>HttpSensor</code> in Apache Airflow to poll a REST API. This can be particularly useful if your pipeline depends on external API data or statuses.</p>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#overview","title":"Overview","text":"<ol> <li>Create or configure an Airflow Connection: Under Admin \u2192 Connections, set up your HTTP connection (<code>Conn Id</code>) with the base URL and optionally store your token securely.</li> <li>Reference the connection in your DAG: Use the <code>http_conn_id</code> in the <code>HttpSensor</code> (or define headers directly if you prefer).</li> <li>Set up your sensor: The <code>HttpSensor</code> will regularly check (poke) an endpoint until it finds an expected value or times out.</li> </ol>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#example-dag","title":"Example DAG","text":"<p>Below is a minimal example of a DAG using <code>HttpSensor</code>. When you place this code in an Airflow DAG file, it will: - Start from <code>days_ago(1)</code>. - Use the <code>HttpSensor</code> to GET from <code>my/api/endpoint</code>. - Send an authorization Bearer token in the header. - Check if the response contains <code>expected_value</code>. If yes, the task succeeds; if not, it keeps retrying until the timeout.</p> <pre><code>from airflow.sensors.http_sensor import HttpSensor\nfrom airflow import DAG\nfrom airflow.utils.dates import days_ago\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': days_ago(1),\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\nwith DAG(\n    dag_id='example_http_sensor',\n    default_args=default_args,\n    schedule_interval='@once',\n) as dag:\n\n    http_sensor_task = HttpSensor(\n        task_id='http_sensor_task',\n        http_conn_id='my_http_conn_id',  # Must match the connection ID in Airflow\n        endpoint='my/api/endpoint',\n        method='GET',\n        headers={\n            'Authorization': 'Bearer YOUR_BEARER_TOKEN_HERE'\n        },\n        response_check=lambda response: \"expected_value\" in response.text,\n        poke_interval=30,  # how often to ping the endpoint (in seconds)\n        timeout=60,        # how long to wait before failing the task\n        mode='reschedule'  # or 'poke' depending on your preference\n    )\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/api-triggers-for-airflow/#conclusion","title":"Conclusion","text":"<p>By configuring an <code>HttpSensor</code> (or a custom sensor around <code>HttpHook</code>) with the correct headers, you can easily poll REST APIs that require Bearer tokens. This approach is handy for triggering downstream tasks only when external services have the data or status you need.</p> <p>Previous: Task Parameters | Next: Generative AI &gt; Advanced LangChain &gt; Lang Chain LLM Model</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/","title":"Customizing Task Parameters","text":"<p>This example demonstrates how to customize and pass parameters to a Practicus AI worker. You can do so by defining environment variables in the <code>WorkerConfig</code> object.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#why-environment-variables","title":"Why Environment Variables?","text":"<p>Environment variables are often the easiest way to inject small pieces of configuration or parameters into a script. Practicus AI automatically sets these environment variables in the worker's environment when your script runs.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#basic-example","title":"Basic Example","text":"<p>In this simple example, we show how to set environment variables in <code>WorkerConfig</code> and then access them in a Python script.</p> <pre><code># task.py\nimport os\n\nprint(\"First Param:\", os.environ[\"MY_FIRST_PARAM\"])\nprint(\"Second Param:\", os.environ[\"MY_SECOND_PARAM\"])\n</code></pre>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#defining-workerconfig-with-environment-variables","title":"Defining <code>WorkerConfig</code> with Environment Variables","text":"<p>Below, we create a <code>WorkerConfig</code> object. Notice how we specify environment variables in the <code>env_variables</code> dictionary. Practicus AI will ensure these variables are set when <code>task.py</code> is executed.</p> <pre><code>import practicuscore as prt\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",  # The base container image\n    worker_size=\"X-Small\",     # Size configuration\n    env_variables={\n        \"MY_FIRST_PARAM\": \"VALUE1\",\n        \"MY_SECOND_PARAM\": 123\n    },\n)\n\nworker, success = prt.run_task(\n    file_name=\"task.py\",          # The name of the script to run\n    worker_config=worker_config,\n)\n\nprint(\"Task finished with status:\", success)\n</code></pre> <p>When this code runs, it will print out the values of <code>MY_FIRST_PARAM</code> and <code>MY_SECOND_PARAM</code> from within <code>task.py</code>.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#airflow-integration","title":"Airflow Integration","text":"<p>To integrate with Practicus AI Airflow, you can utilize the same approach. Airflow tasks can inject environment variables by writing out a <code>WorkerConfig</code> JSON file that Practicus AI can pick up.</p> <p>For example: 1. Define your <code>worker_config</code> in Python. 2. Serialize it to JSON. 3. Store it in a file named after your task (e.g., <code>task_worker.json</code> if your script is <code>task.py</code>).</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#example-writing-task_workerjson","title":"Example: Writing <code>task_worker.json</code>","text":"<pre><code>worker_config_json = worker_config.model_dump_json(exclude_none=True)\n\nwith open(\"task_worker.json\", \"wt\") as f:\n    f.write(worker_config_json)\n\nprint(\"Generated worker_config JSON:\\n\", worker_config_json)\n</code></pre> <p>An example <code>task_worker.json</code> might look like:</p> <pre><code>{\n  \"worker_image\": \"practicus\",\n  \"worker_size\": \"X-Small\",\n  \"additional_params\": \"eyJlbnZfdmFyaWFibGVzIjogeyJNWV9GSVJTVF9QQVJBTSI6ICJWQUxVRTEiLCAiTVlfU0VDT05EX1BBUkFNIjogMTIzfX0=\"\n}\n</code></pre> <p>Note that <code>additional_params</code> is base64-encoded data containing:</p> <p><pre><code>{\n  \"env_variables\": {\n    \"MY_FIRST_PARAM\": \"VALUE1\",\n    \"MY_SECOND_PARAM\": 123\n  }\n}\n</code></pre> This is how Practicus AI stores your configuration behind the scenes.</p>"},{"location":"technical-tutorial/extras/workflows/task-parameters/#summary","title":"Summary","text":"<ul> <li>Defining environment variables: Use <code>env_variables</code> within <code>WorkerConfig</code> to inject parameters.</li> <li>Accessing parameters: In your Python script (e.g., <code>task.py</code>), read them from <code>os.environ</code>.</li> <li>Airflow: Write out the <code>worker_config</code> to a JSON file. Practicus AI automatically picks that up.</li> </ul> <p>By following these steps, you can effectively pass custom parameters and configurations to your Practicus AI tasks, making your data pipelines more dynamic and flexible.</p> <p>Previous: Bank Marketing | Next: API Triggers For Airflow</p>"},{"location":"technical-tutorial/generative-ai/introduction/","title":"Generative AI with Practicus AI","text":"<p>Generative AI (GenAI) is revolutionizing how we interact with and utilize AI models, enabling applications that can generate text, answer questions, and provide intelligent, human-like responses. Practicus AI provides a streamlined and powerful platform to build, deploy, and scale Generative AI workflows and applications. This section of the tutorial focuses on leveraging Practicus AI for cutting-edge GenAI use cases.</p>"},{"location":"technical-tutorial/generative-ai/introduction/#key-areas-of-focus","title":"Key Areas of Focus","text":""},{"location":"technical-tutorial/generative-ai/introduction/#building-visual-applications-for-genai","title":"Building Visual Applications for GenAI","text":"<p>With Practicus AI, you can create visually rich, interactive applications that leverage GenAI capabilities. You can integrate Generative AI models into applications to provide functionalities such as:</p> <ul> <li>Chatbots for customer service or technical support.</li> <li>Creative content generators for marketing or entertainment.</li> <li>Educational tools that dynamically respond to user inputs.</li> </ul> <p>These applications are easy to deploy and scale using Practicus AI's infrastructure, ensuring smooth performance and seamless user experience.</p>"},{"location":"technical-tutorial/generative-ai/introduction/#building-flows-with-langchain","title":"Building Flows with LangChain","text":"<p>LangChain is a framework designed to build sequential chains of prompts and actions that utilize large language models (LLMs). Practicus AI integrates seamlessly with LangChain including private LLM endpoints to enable the following:</p> <ul> <li>Chains: Create chains of prompts that interact with LLMs, memory, or tools.</li> <li>Tools and Agents: Use LangChain's built-in tools to interact with APIs or databases. Build agents that intelligently choose which actions to take.</li> <li>Memory: Add memory to your LLM applications, enabling context-aware interactions that persist across conversations.</li> </ul> <p>By combining LangChain's rich abstraction with Practicus AI's scalable execution environment, you can build sophisticated, dynamic AI-driven workflows.</p>"},{"location":"technical-tutorial/generative-ai/introduction/#retrieval-augmented-generation-rag-with-vector-databases","title":"Retrieval-Augmented Generation (RAG) with Vector Databases","text":"<p>RAG enhances GenAI applications by combining the power of LLMs with the precision of vector search. Practicus AI supports RAG workflows by:</p> <ul> <li>Integrating with vector databases to perform semantic search.</li> <li>Using your domain-specific data for fine-tuned, accurate responses.</li> <li>Combining LLM capabilities with real-time document retrieval for high-quality, contextually relevant outputs.</li> </ul> <p>This approach is ideal for creating chatbots, document assistants, and any application requiring up-to-date and context-aware AI-generated responses.</p>"},{"location":"technical-tutorial/generative-ai/introduction/#deploying-custom-genai-models","title":"Deploying Custom GenAI Models","text":"<p>Practicus AI enables you to deploy custom fine-tuned LLMs tailored to your specific needs. Whether you're building models for healthcare, finance, or creative tasks, the platform supports:</p> <ul> <li>Fine-Tuning: Train custom models using Practicus AI's distributed computing capabilities, leveraging frameworks like DeepSpeed and FairScale.</li> <li>Deployment: Host your fine-tuned LLMs with auto-scaling, secure endpoints, and service mesh integration.</li> <li>Integration: Connect your custom models to your applications, workflows, and APIs, ensuring seamless access.</li> </ul> <p>Previous: Generating Wokflows | Next: Apps &gt; Building Visual Apps</p>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/","title":"Building Visual Apps","text":""},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#building-and-managing-visual-applications-with-practicus-ai","title":"Building and Managing Visual Applications with Practicus AI","text":"<p>Practicus AI enables the development, deployment, and management of secure, enterprise-grade visual applications built on Streamlit. While Streamlit itself is a simple framework for creating interactive, web-based apps, Practicus AI provides enterprise-level features on top of it:</p> <ul> <li>Enterprise Single Sign-On (SSO) integration with LDAP or other enterprise authentication methods.</li> <li>User and Group-Based Customization: Dynamically tailor pages and content based on user roles and permissions.</li> <li>Administrative Pages: Add interfaces to manage users, groups, and application settings.</li> <li>Version and Traffic Management: Seamlessly handle multiple versions of your app, route traffic to staging or production, and roll out changes gradually.</li> </ul> <p>This example demonstrates how to leverage Practicus AI to deploy a basic Streamlit app, integrate it with APIs, and manage it with enterprise security and versioning features.</p>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#sample-streamlit-application-overview","title":"Sample Streamlit Application Overview","text":"<p>This sample Streamlit application demonstrates a variety of features and best practices for building secure, multi-page apps within the Practicus AI environment. It includes the following components:</p> <p>Home.py: The main entry point that authenticates and authorizes users, sets a page title, and displays a basic counter. It illustrates how to secure pages in both development and production modes.</p> <p>pages/: A collection of additional Streamlit pages showcasing different functionalities:</p> <ul> <li>01_First_Page.py: A secured child page with its own counter.</li> <li>02_Second_Page.py: A non-secured page accessible publicly when deployed.</li> <li>03_Mixed_Content.py: A page demonstrating mixed access levels, where certain sections are only visible to administrators.</li> <li>04_Cookies.py: A page handling cookie creation, retrieval, and deletion.</li> <li>05_App_Meta.py: A page displaying application metadata and user details, differing in development vs. deployed scenarios.</li> <li>06_Settings.py: An admin-only settings page.</li> </ul> <p>apis/:</p> <ul> <li>say_hello.py: A simple API endpoint that returns a personalized greeting.</li> </ul> <p>shared/helper.py: Contains reusable helper functions for shared logic.</p> <p>This sample code serves as a reference for building, testing, and deploying secure, multi-page Streamlit apps on Practicus AI, with integrated authentication, logging, and developer-focused features.</p>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <pre><code>app_deployment_key = None\napp_prefix = None\n\ntest_app = True\n</code></pre> <p>If you don't know your prefixes and deployments you can check them out by using the SDK like down below:</p> <pre><code>my_app_settings = region.app_deployment_setting_list\n\nprint(\"Application deployment settings I have access to:\")\ndisplay(my_app_settings.to_pandas())\n</code></pre> <pre><code>my_app_prefixes = region.app_prefix_list\n\nprint(\"Application prefixes (groups) I have access to:\")\ndisplay(my_app_prefixes.to_pandas())\n</code></pre> <pre><code>assert app_deployment_key, \"Please select an app deployment setting.\"\nassert app_prefix, \"Please select an app prefix.\"\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#before-you-continue","title":"Before You Continue","text":"<p>To ensure proper execution, run the code below on a Practicus AI GenAI or a compatible container image. Make sure to use the GenAI Jupyter kernel (the <code>practicus_genai</code> virtual environment).</p>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#testing-applications-in-design-time","title":"Testing Applications in Design Time","text":"<p>You can launch a sample Streamlit application directly within the Practicus AI worker to test it before deploying to a Practicus AI AppHost system.</p>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#testing-on-vs-code","title":"Testing on VS Code","text":"<p>If you are using VS Code, click on the printed URL to view the application.</p>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#testing-on-jupyter","title":"Testing on Jupyter","text":"<p>If you are using Jupyter, we recommend using Practicus AI Studio, which has built-in GenAI app visualization. After running the code below, navigate to Explore, right-click on the worker, and select GenAI App.</p> <pre><code>if test_app:\n    prt.apps.test_app()\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#testing-apis-in-design-time","title":"Testing APIs in design time","text":"<pre><code>import apis.say_hello\nfrom apis.say_hello import Person, SayHelloRequest, SayHelloResponse\nfrom pydantic import BaseModel\n\nperson = Person(name=\"Alice\", email=\"alice@wonderland.com\")\npayload = SayHelloRequest(person=person)\n\nprint(issubclass(type(payload), BaseModel))\n\nresponse: SayHelloResponse = prt.apps.call_api(apis.say_hello, payload)\nprint(\"Greeting message:\", response.greeting_message)\nprint(\"Email:\", response.for_person.email)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#deploying-the-app","title":"Deploying the App","text":"<p>Once our development and tests are over, we can deploy the app as a new version.</p> <pre><code>app_name=\"my-first-app\"\nvisible_name = \"My First App\"\ndescription = \"A very useful app..\"\nicon = \"rocket\"\n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=app_deployment_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None,\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n\nprint(\"App deployed:\", app_url)\nprint(\"API endpoint:\", api_url)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#understanding-app-versions","title":"Understanding App Versions","text":"<p>Practicus AI supports multiple app versions and provides different URLs for each environment:</p> <ul> <li>Default route: <code>https://practicus.company.com/apps/my-first-app/</code> routes to the latest or production version.</li> <li>Specific versions:</li> <li>Production: <code>/prod/</code></li> <li>Staging: <code>/staging/</code></li> <li>Latest: <code>/latest/</code></li> <li>Exact version: <code>/v[version]/</code></li> </ul> <pre><code>import requests\n\ntoken = prt.apps.get_session_token(api_url=api_url)\nsay_hello_api_url = f\"{api_url}say-hello/\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n    \"content-type\": \"application/json\"\n}\n\njson_data = payload.model_dump_json(indent=2)\nprint(f\"Sending below JSON to: {say_hello_api_url}\")\nprint(json_data)\n\nresp = requests.post(say_hello_api_url, json=json_data, headers=headers)\n\nif resp.ok:\n    print(\"Response text:\")\n    print(resp.text)\n    response_obj = SayHelloResponse.model_validate_json(resp.text)\n    print(\"Response object:\")\n    print(response_obj)\nelse:\n    print(\"Error:\", resp.status_code, resp.text)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#deleting-apps-or-app-versions","title":"Deleting Apps or App Versions","text":"<p>You can delete entire apps or specific versions if you have the appropriate permissions:</p> <ul> <li>Delete an app: removes all versions.</li> <li>Delete a particular version: cannot delete the latest version.</li> </ul> <p>Permissions can be granted by:</p> <ul> <li>Being the app owner.</li> <li>Having admin privileges for the app's prefix.</li> <li>Being a system admin.</li> </ul> <pre><code>print(\"Listing all apps and their versions I have access to:\")\nregion.app_list.to_pandas()\n</code></pre> <pre><code># If you don't know the app_id you can use prefix and app_name\nregion.delete_app(prefix=\"apps\", app_name=\"my-first-app\")\n\ntry:\n    # Deleting an app and all it's versions\n    region.delete_app(app_id=123)\nexcept:\n    pass\n</code></pre> <pre><code>try:\n    # Deleting a particular version of an app\n    region.delete_app_version(app_id=123, version=4)\n\n    # If you don't know the app_id you can use prefix and app_name\n    region.delete_app_version(prefix=\"apps\", app_name=\"my-first-app\", version=4)\nexcept:\n    pass\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#homepy","title":"Home.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n\n# The below will secure the page by authenticating and authorizing users with Single-Sign-On.\n# Please note that security code is only activate when the app is deployed.\n# Pages are always secure, even without the below, during development and only the owner can access them.\nprt.apps.secure_page(\n    page_title=\"Hello World App\",\n    must_be_admin=False,\n)\n\n# The below is standard Streamlit code..\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello!\")\nst.write(\"This is a text from the code inside the page.\")\n\nst.write(some_function())\n\nif 'counter' not in st.session_state:\n    st.session_state.counter = 0\n\nincrement = st.button('Increment Counter')\nif increment:\n    current = st.session_state.counter\n    new = current + 1\n    st.session_state.counter = new\n    prt.logger.info(f\"Increased counter from {current} to {new}\")\n\nst.write('Counter = ', st.session_state.counter)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#apissay_hellopy","title":"apis/say_hello.py","text":"<pre><code>from pydantic import BaseModel\n\n\nclass Person(BaseModel):\n    name: str\n    email: str | None = None\n\n\nclass SayHelloRequest(BaseModel):\n    person: Person\n\n\nclass SayHelloResponse(BaseModel):\n    greeting_message: str\n    for_person: Person\n\n\ndef run(payload: SayHelloRequest, **kwargs):\n    return SayHelloResponse(greeting_message=f\"Hello {payload.person.name}\", for_person=payload.person)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#pages01_first_pagepy","title":"pages/01_First_Page.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n\n# Child pages must also request to be secured.\n# Or else, they will be accessible by everyone after deployment.\n\nprt.apps.secure_page(\n    page_title=\"My first child page\",\n)\n\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello from first page!\")\n\nst.write(some_function())\n\nif 'page_1_counter' not in st.session_state:\n    st.session_state.page_1_counter = 0\n\nincrement = st.button('Increment Counter +2')\nif increment:\n    st.session_state.page_1_counter += 2\n\nst.write('Counter = ', st.session_state.page_1_counter)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#pages02_second_pagepy","title":"pages/02_Second_Page.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n\n# Since this page is not secured, it will be public after deployment.\n# During development, it is still only accessible to the owner, and only from Practicus AI Studio.\n# If the home page is secured, a public child page will only be accessible if directly requested.\n# prt.apps.secure_page(\n#     page_title=\"My second child page\"\n# )\n\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello from my second page!\")\nst.write(\"This page is not secured and will be open to public.\")\n\nst.write(some_function())\n\nif 'page_2_counter' not in st.session_state:\n    st.session_state.page_2_counter = 0\n\nincrement = st.button('Increment Counter +4')\nif increment:\n    st.session_state.page_2_counter += 4\n\nst.write('Counter = ', st.session_state.page_2_counter)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#pages03_mixed_contentpy","title":"pages/03_Mixed_Content.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nprt.apps.secure_page(\n    page_title=\"Mixed content page\",\n)\n\nst.title(\"Mixed content page\")\n\nst.write(\"Everyone will see this part of the page.\")\nst.write(\"If you see nothing below, you are not an admin.\")\n\n\n# Only admins will see this\nif prt.apps.user_is_admin():\n    st.subheader(\"Admin Section\")\n    st.write(\"If you see this part, you are an admin, owner of the app, or in development mode.\")\n\n    # Input fields\n    admin_input1 = st.text_input(\"Admin Input 1\")\n    admin_input2 = st.text_input(\"Admin Input 2\")\n\n    admin_action = st.button('Admin Button')\n    if admin_action:\n        st.write(\"Performing some dummy admin action..\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#pages04_cookiespy","title":"pages/04_Cookies.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\n# Secure the page using the provided SDK\nprt.apps.secure_page(\n    page_title=\"Using Cookies\"\n)\n\nst.title(\"Cookies Management\")\n\n# Inputs for cookie operations\ncookie_name = st.text_input(\"Cookie Name\", placeholder=\"Enter cookie name\")\ncookie_value = st.text_input(\"Cookie Value\", placeholder=\"Enter cookie value\")\nmax_age = st.number_input(\"Max Validity (seconds)\", min_value=None, value=None, step=60, placeholder=\"Leave empty for 30 days\")\npath = st.text_input(\"Cookie path\", placeholder=\"Leave empty for /\")\n\n# Add Cookie\nif st.button(\"Add Cookie\"):\n    if cookie_name and cookie_value:\n        prt.apps.set_cookie(name=cookie_name, value=cookie_value, max_age=max_age, path=path)\n        st.success(f\"Cookie '{cookie_name}' has been set!\")\n    else:\n        st.error(\"Please provide both a cookie name and value.\")\n\n# Get Cookie Value\nif st.button(\"Get Cookie Value\"):\n    if cookie_name:\n        cookie_value = prt.apps.get_cookie(name=cookie_name)\n        if cookie_value:\n            st.success(f\"The value of cookie '{cookie_name}' is: {cookie_value}\")\n        else:\n            st.warning(f\"No cookie found with the name '{cookie_name}'.\")\n    else:\n        st.error(\"Please provide a cookie name to retrieve its value.\")\n\n# Delete Cookie\nif st.button(\"Delete Cookie\"):\n    if cookie_name:\n        prt.apps.delete_cookie(name=cookie_name)\n        st.success(f\"Cookie '{cookie_name}' has been deleted!\")\n    else:\n        st.error(\"Please provide a cookie name to delete.\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#pages05_app_metapy","title":"pages/05_App_Meta.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nprt.apps.secure_page(\n    page_title=\"Application Metadata\"\n)\n\nst.title(\"Application Metadata\")\n\nif prt.apps.development_mode():\n    st.subheader(\"Development Mode\")\n    st.markdown(\n        \"\"\"\n        You are in **development mode**, and application metadata is only available after deploying the app.\n\n        **Developer Information:**\n        \"\"\"\n    )\n    st.write({\n        \"Email\": prt.apps.get_user_email(),\n        \"Username\": prt.apps.get_username(),\n        \"User ID\": prt.apps.get_user_id(),\n    })\nelse:\n    st.subheader(\"Deployed App Metadata\")\n    col1, col2 = st.columns(2)\n\n    with col1:\n        st.markdown(\"**Application Details**\")\n        st.write({\n            \"Name\": prt.apps.get_app_name(),\n            \"Prefix\": prt.apps.get_app_prefix(),\n            \"Version\": prt.apps.get_app_version(),\n            \"App ID\": prt.apps.get_app_id(),\n        })\n\n    with col2:\n        st.markdown(\"**User Information**\")\n        st.write({\n            \"Email\": prt.apps.get_user_email(),\n            \"Username\": prt.apps.get_username(),\n            \"User ID\": prt.apps.get_user_id(),\n        })\n\nif st.button(\"View User Groups\"):\n    st.write(\n        prt.apps.get_user_groups()\n    )\n    # User groups are cached. If you need reset you can call:\n    # reload = True\n    # prt.apps.get_user_groups(reload)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#pages06_settingspy","title":"pages/06_Settings.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\n# User must have admin privileges to view this page (must_be_admin=True)\nprt.apps.secure_page(\n    page_title=\"Settings Page\",\n    must_be_admin=True,\n)\n\nst.title(\"Settings page\")\n\nst.write(\"If you see this, you are an admin, owner of the app, or in development mode.\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/apps/building-visual-apps/#sharedhelperpy","title":"shared/helper.py","text":"<pre><code>def some_function():\n    return \"And, this text is from a shared function.\"\n</code></pre> <p>Previous: Introduction | Next: LangChain &gt; LangChain Basics</p>"},{"location":"technical-tutorial/generative-ai/langchain/langchain-basics/","title":"LangChain Pipeline Development","text":"<p>The <code>ChatPracticus</code> library seamlessly integrates Practicus AI\u2019s hosted private LLM models into the LangChain framework. This allows you to easily interact with language models that are privately hosted and secured within the Practicus AI environment.</p> <p>To get started with <code>ChatPracticus</code>, you will need the following parameters:</p> <ul> <li>endpoint_url: The API endpoint for the LLM model host.</li> <li>api_token: A secret key required to authenticate with the model host API.</li> <li>model_id: The identifier of the target LLM model.</li> </ul> <p>Once these parameters are set, you can define a <code>chat</code> instance using <code>ChatPracticus</code> and invoke it with any prompt of your choice.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <pre><code>model_name = None\nmodel_prefix = None\nhost = None # e.g. 'company.practicus.io'\n</code></pre> <p>If you don't know your prefixes and models you can check them out by using the SDK like down below:</p> <pre><code>my_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>assert model_name, \"Please select an LLM model.\"\nassert model_prefix, \"Please select the prefix LLM deployed.\"\nassert host, \"Please enter your host\"\n</code></pre> <p>Now we can define our API url and it's token.</p> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <p>Below is an example of how to create and use a <code>ChatPracticus</code> instance in your code.</p> <pre><code>from langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_practicus import ChatPracticus\n\ndef test_langchain_practicus(api_url, token, inputs):\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=token,\n        model_id=\"some models will ignore this\",\n        stream = True\n    )\n\n    response = chat.invoke(input=inputs)\n\n    print(\"\\n\\nReceived response:\\n\", response)\n    print(\"\\n\\nReceived Content:\\n\", response.content)\n</code></pre> <p>Async calls also work using the below (but not on jupyter)</p> <pre><code>import asyncio\nasyncio.run(llm.ainvoke([sys_input, human_input1, human_input2]))\nprint(response)\n</code></pre> <p>After defining your token and API URL, you can easily incorporate prompts into your workflow. By using the <code>langchain</code> library, you can structure your messages as follows:</p> <ul> <li>System Messages: Use <code>SystemMessage</code> to provide overarching instructions or context that guide the LLM\u2019s behavior.</li> <li>Human Messages: Wrap user prompts and queries with <code>HumanMessage</code> to represent the user\u2019s input.</li> </ul> <p>This structured approach ensures that the model receives clear, role-specific instructions, enhancing the quality and relevance of its responses.</p> <pre><code>human_input1 = HumanMessage(\"Capital of United Kingdom?\")\nhuman_input2 = HumanMessage(\"And things to do there?\")\nsystem_message = SystemMessage(\"Less 50 words.\")\n\ninputs = [human_input1, human_input2, system_message]\n</code></pre> <pre><code>test_langchain_practicus(api_url, token, ['who is einstein'])\n</code></pre>"},{"location":"technical-tutorial/generative-ai/langchain/langchain-basics/#received-json-response","title":"Received json response:","text":"<pre><code>{\n    content=\"Albert Einstein was a theoretical physicist born on March 14, 1879, in Ulm, in the Kingdom of W\u00fcrttemberg in the German Empire. He is best known for developing the theory of relativity, which revolutionized the understanding of space, time, and energy. His most famous equation, E=mc\u00b2, expresses the equivalence of mass and energy.\\n\\nEinstein's work laid the foundation for much of modern physics and he received the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect, which was pivotal in the development of quantum theory. Beyond his scientific contributions, Einstein was also known for his philosophical views, advocacy for civil rights, and his involvement in political and humanitarian causes. He passed away on April 18, 1955, in Princeton, New Jersey, USA.\" \n\n    ... with additional metadata \n}       \n</code></pre> <p>Previous: Building Visual Apps | Next: Streaming</p>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/","title":"LangChain with Streaming","text":"<p>This example demonstrates how to provide human and system messages to a language model and receive a streamed response. The primary steps include:</p> <ol> <li>Defining the URL and authentication token.</li> <li>Initializing and interacting with the LLM model.</li> <li>Converting responses into JSON format.</li> <li>Streaming the LLM\u2019s output to your environment.</li> </ol>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/#context","title":"Context","text":"<p>The <code>PrtLangMessage</code> object stores content and associated roles within a dictionary. This structure serves as your conversation context. To interact with the chat model, you simply create messages\u2014both system-level and user-level\u2014and assign the appropriate role to each.</p>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre> <pre><code>method = None\n\n# For model APIs\nif method == 'llm_model':\n    model_name = None\n    model_prefix = None\n\n# For App APIs\nelif method == 'llm_app':\n    app_name = None\n    app_prefix = None\n\nhost = None # e.g. 'company.practicus.io'\n</code></pre> <p>If you don't know your prefixes and models or apps you can check them out by using the SDK like down below:</p> <pre><code>my_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n</code></pre> <pre><code>my_app_list = region.app_list\ndisplay(my_app_list.to_pandas())\n</code></pre> <pre><code>my_app_prefix_list = region.app_prefix_list\ndisplay(my_app_prefix_list.to_pandas())\n</code></pre> <pre><code>assert method in ('llm_app', 'llm_model'), \"Please select a valid method ('llm_app' or 'llm_model').\"\n\nif method == 'llm_model':\n    assert model_name, \"Please select an LLM.\"\n    assert model_prefix, \"Please select the prefix LLM  deployed.\"\n\nelif method == 'llm_app':\n    assert app_name, \"Please select an LLM app.\"\n    assert app_prefix, \"Please select the prefix LLM app deployed.\"\n\nassert host, \"Please enter your host\"\n</code></pre> <p>Now we can define our API url and it's token.</p> <pre><code>if method == 'llm_model':\n    api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\nelif method == 'llm_app':\n    api_url = f\"https://{host}/{app_prefix}/{app_name}/api/\"\n\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <pre><code>from practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport requests\n</code></pre> <pre><code>human_msg = PrtLangMessage(\n    content=\"Who is einstein? \",\n    role = \"human\"\n)\n\nsystem_msg = PrtLangMessage(\n    content=\"Give me answer less than 100 words.\",\n    role = \"system\"\n)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/#request-llm","title":"Request LLM","text":"<ul> <li>The purpose of PrtLangRequest is to keep the messages, lang_model and streaming mode.</li> <li>If you need to data json you can use 'model_dump_json'. This function will return json.</li> </ul> <pre><code># This class need message and model and if you want to stream, \n# you should change streaming value false to true\npracticus_llm_req = PrtLangRequest( \n    # Our context\n    messages=[human_msg, system_msg], \n    # Select a model, leave empty for default\n    lang_model=\"\", \n    # Streaming mode\n    streaming=True, \n    # If we have a extra parameters at model.py we can add them here \n    llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"} \n)\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'application/json'\n}\n\n# Convert our returned parameter to json\ndata_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True) \n</code></pre> <pre><code>with requests.post(api_url, headers=headers, data=data_js, stream=True) as r: \n    for response_chunk in r.iter_content(1024): \n        print(response_chunk.decode(\"utf-8\"), end = '')\n</code></pre>"},{"location":"technical-tutorial/generative-ai/langchain/streaming/#sample-streaming-output","title":"Sample streaming output","text":"<p>Albert Einstein was a theoretical physicist born in 1879 in Germany. He is best known for developing the theory of relativity, particularly the equation (E=mc^2), which describes the equivalence of energy (E) and mass (m) with (c) being the speed of light. His work revolutionized the understanding of space, time, and gravity. Einstein received the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect. He is considered one of the most influential scientists of the 20th century.</p> <p>Previous: LangChain Basics | Next: Vector Databases &gt; Sample Vector Db</p>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/","title":"Sample Vector Db","text":""},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#milvus-vector-database-sample","title":"Milvus vector database sample","text":"<p>This example demonstrates the basic operations of PyMilvus, a Python SDK of Milvus.</p>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#before-you-begin","title":"Before you begin","text":"<p>Please make sure that you have a running Milvus instance.</p> <pre><code>milvus_host = None\nmilvus_port = None\n</code></pre> <pre><code>assert milvus_host, \"Please enter your Milvus connection uri.\"\nassert milvus_port, \"Please enter your Milvus port.\"\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#steps","title":"Steps","text":"<ol> <li>connect to Milvus</li> <li>create collection</li> <li>insert data</li> <li>create index</li> <li>search, query, and hybrid search on entities</li> <li>delete entities by PK</li> <li>drop collection</li> </ol> <pre><code>import numpy as np\nimport time\n\nfrom pymilvus import (\n    connections,\n    utility,\n    FieldSchema, CollectionSchema, DataType,\n    Collection,\n)\n\nfmt = \"\\n=== {:30} ===\\n\"\nsearch_latency_fmt = \"search latency = {:.4f}s\"\nnum_entities, dim = 3000, 8\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#1-connect-to-milvus","title":"1. connect to Milvus","text":"<p>Add a new connection alias <code>default</code> for Milvus server in <code>localhost:19530</code>. </p> <p>Actually the <code>default</code> alias is a building in PyMilvus. If the address of Milvus is the same as <code>localhost:19530</code>, you can omit all parameters and call the method as: <code>connections.connect()</code>.</p> <p>Note: the <code>using</code> parameter of the following methods is default to \"default\".</p> <pre><code>connections.connect(\"default\", host=milvus_host, port=milvus_port)\n\nhas = utility.has_collection(\"hello_milvus\")\nprint(f\"Does collection hello_milvus exist in Milvus: {has}\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#2-create-collection","title":"2. create collection","text":"<p>We're going to create a collection with 3 fields.</p> field name field type other attributes field description 1 \"pk\" VARCHAR is_primary=True, auto_id=False \"primary field\" 2 \"random\" Double \"a double field\" 3 \"embeddings\" FloatVector dim=8 \"float vector with dim 8\" <pre><code>fields = [\n    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n    FieldSchema(name=\"random\", dtype=DataType.DOUBLE),\n    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n]\n\nschema = CollectionSchema(fields, \"hello_milvus is the simplest demo to introduce the APIs\")\n\nhello_milvus = Collection(\"hello_milvus\", schema, consistency_level=\"Strong\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#3-insert-data","title":"3. insert data","text":"<p>We are going to insert 3000 rows of data into <code>hello_milvus</code>. Data to be inserted must be organized in fields.</p> <p>The insert() method returns: - either automatically generated primary keys by Milvus if auto_id=True in the schema; - or the existing primary key field from the entities if auto_id=False in the schema.</p> <pre><code>rng = np.random.default_rng(seed=19530)\nentities = [\n    # provide the pk field because `auto_id` is set to False\n    [str(i) for i in range(num_entities)],\n    rng.random(num_entities).tolist(),  # field random, only supports list\n    rng.random((num_entities, dim)),    # field embeddings, supports numpy.ndarray and list\n]\n\ninsert_result = hello_milvus.insert(entities)\n\nprint(f\"Number of entities in Milvus: {hello_milvus.num_entities}\")  # check the num_entites\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#4-create-index","title":"4. create index","text":"<p>We are going to create an IVF_FLAT index for hello_milvus collection.</p> <p>create_index() can only be applied to <code>FloatVector</code> and <code>BinaryVector</code> fields.</p> <pre><code>index = {\n    \"index_type\": \"IVF_FLAT\",\n    \"metric_type\": \"L2\",\n    \"params\": {\"nlist\": 128},\n}\n\nhello_milvus.create_index(\"embeddings\", index)\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#5-search-query-and-hybrid-search","title":"5. search, query, and hybrid search","text":"<p>After data were inserted into Milvus and indexed, you can perform: - search based on vector similarity - query based on scalar filtering(boolean, int, etc.) - hybrid search based on vector similarity and scalar filtering.</p> <p>Before conducting a search or a query, you need to load the data in <code>hello_milvus</code> into memory.</p> <pre><code>hello_milvus.load()\n</code></pre> <p>Search based on vector similarity</p> <pre><code>vectors_to_search = entities[-1][-2:]\nsearch_params = {\n    \"metric_type\": \"L2\",\n    \"params\": {\"nprobe\": 10},\n}\n\nstart_time = time.time()\nresult = hello_milvus.search(vectors_to_search, \"embeddings\", search_params, limit=3, output_fields=[\"random\"])\nend_time = time.time()\n\nfor hits in result:\n    for hit in hits:\n        print(f\"hit: {hit}, random field: {hit.entity.get('random')}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre> <p>Query based on scalar filtering(boolean, int, etc.)</p> <p>Start querying with <code>random &gt; 0.5</code></p> <pre><code>start_time = time.time()\nresult = hello_milvus.query(expr=\"random &gt; 0.5\", output_fields=[\"random\", \"embeddings\"])\nend_time = time.time()\n\nprint(f\"query result:\\n-{result[0]}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre> <p>Hybrid search</p> <p>Start hybrid searching with <code>random &gt; 0.5</code></p> <pre><code>start_time = time.time()\nresult = hello_milvus.search(vectors_to_search, \"embeddings\", search_params, limit=3, expr=\"random &gt; 0.5\", output_fields=[\"random\"])\nend_time = time.time()\n\nfor hits in result:\n    for hit in hits:\n        print(f\"hit: {hit}, random field: {hit.entity.get('random')}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#6-delete-entities-by-pk","title":"6. delete entities by PK","text":"<p>You can delete entities by their PK values using boolean expressions.</p> <pre><code>ids = insert_result.primary_keys\nexpr = f'pk in [\"{ids[0]}\", \"{ids[1]}\"]'\n\nresult = hello_milvus.query(expr=expr, output_fields=[\"random\", \"embeddings\"])\nprint(f\"query before delete by expr=`{expr}` -&gt; result: \\n-{result[0]}\\n-{result[1]}\\n\")\n\nhello_milvus.delete(expr)\n\nresult = hello_milvus.query(expr=expr, output_fields=[\"random\", \"embeddings\"])\nprint(f\"query after delete by expr=`{expr}` -&gt; result: {result}\\n\")\n</code></pre>"},{"location":"technical-tutorial/generative-ai/vector-databases/sample-vector-db/#7-drop-collection","title":"7. drop collection","text":"<p>Finally, drop the hello_milvus collection</p> <pre><code>utility.drop_collection(\"hello_milvus\")\n</code></pre> <p>Previous: Streaming | Next: Distributed Computing &gt; Introduction</p>"},{"location":"technical-tutorial/getting-started/add-ons/","title":"Practicus AI Add-ons","text":"<p>Practicus AI Add-ons represent external services integrated into your Practicus AI environment. These can include </p> <ul> <li>Airflow for workflows</li> <li>MLflow for experiment tracking</li> <li>Observability tools</li> <li>Analytics Services</li> <li>and more..</li> </ul> <p>In addition to Practicus AI Home and AI Sudio, you can also view them, and open their interfaces directly from the SDK.</p> <pre><code>import practicuscore as prt\n\n# Add-ons are tied to a Practicus AI region\nregion = prt.current_region()\n\n# Ensure that add-ons are available\nif len(region.addon_list) == 0:\n    raise NotImplementedError(\"No add-ons installed.\")\n</code></pre> <pre><code># Iterate over available add-ons\nfor addon in region.addon_list:\n    print(\"Add-on key:\", addon.key)\n    print(\"  URL:\", addon.url)\n</code></pre> <pre><code># Printing addon_list directly returns a CSV-like formatted text\nprint(region.addon_list)\n</code></pre> <pre><code># Convert addon_list to a pandas DataFrame for convenience\nregion.addon_list.to_pandas()\n</code></pre> <pre><code>first_addon = region.addon_list[0]\n# Accessing an add-on object prints its details\nfirst_addon\n</code></pre> <pre><code># Open the add-on in your default browser\nfirst_addon.open()\n</code></pre> <pre><code># If you know the add-on key, you can open it directly from the region\naddon_key = first_addon.key\nregion.open_addon(addon_key)\n</code></pre> <pre><code># Search for a specific add-on by key\naddon_key = first_addon.key\nfound_addon = region.get_addon(addon_key)\nassert found_addon, f\"Addon {addon_key} not found\"\nfound_addon.open()\n</code></pre> <p>Previous: Workspaces | Next: Modeling &gt; Introduction</p>"},{"location":"technical-tutorial/getting-started/introduction/","title":"Introduction","text":"<p>Getting started with Practicus AI is straightforward.</p>"},{"location":"technical-tutorial/getting-started/introduction/#typical-practicus-ai-usage","title":"Typical Practicus AI usage","text":"<p>The following steps outline a typical scenario for users who write code:</p> <ol> <li>Log in to your chosen region (e.g., <code>https://practicus.your-company.com</code>).</li> <li>Create one or more workers with the desired features and resource capacities.</li> <li>Start an IDE, such as JupyterLab or VS Code, within a worker.</li> <li>Develop models, applications, and process data as usual.</li> <li>Deploy models, applications, or use add-ons (e.g., create Airflow workflows).</li> <li>Observe metrics, logs, events, errors. Create alerts.</li> </ol> <p></p>"},{"location":"technical-tutorial/getting-started/introduction/#leveraging-documentation-and-developer-tooling","title":"Leveraging Documentation and Developer Tooling","text":"<p>1. Access the SDK Documentation:    Experienced coders understand that having immediate access to detailed SDK references accelerates the development lifecycle. You can refer to the Practicus AI SDK Documentation to understand package structures, classes, methods, and parameters. This robust, searchable reference ensures you can quickly find the API calls needed to interact with Practicus AI resources programmatically.</p> <p>2. Utilize IntelliSense and Contextual Help in JupyterLab or VS Code:    When working within JupyterLab or VS Code, take advantage of built-in IntelliSense (auto-completion) capabilities. As you type, your IDE will surface method signatures, docstrings, and parameter hints\u2014especially helpful for complex ML pipelines or when invoking intricate model-serving APIs.  </p>"},{"location":"technical-tutorial/getting-started/introduction/#contextual-help-with-jupyter-lab","title":"Contextual Help with Jupyter Lab","text":"<ul> <li> <p>Contextual Tooltips: Hover over classes and methods to see in-line docstrings and parameter descriptions. This \u201cjust-in-time\u201d help enables you to craft pipelines, preprocess data, or orchestrate model inference steps without constantly switching between your IDE and external docs.  </p> </li> <li> <p>Shift-Tab: Inside a Jupyter notebook, pressing <code>Shift+Tab</code> while your cursor is within a function call will reveal type hints, default values, and docstrings. This immediate feedback reduces trial-and-error and makes coding more efficient and error-free.</p> </li> </ul> <p></p> <p>Combining direct SDK reference materials with IntelliSense-driven guidance ensures data scientists spend more time crafting robust models and less time hunting down syntax or function definitions.</p> <ul> <li>Contextual Help Tab: You can also right-click on a cell, select \"Show Contextual Help\" to leave the help tab always open.</li> </ul> <p></p>"},{"location":"technical-tutorial/getting-started/introduction/#contextual-help-with-vs-code","title":"Contextual Help with VS Code","text":"<ul> <li>Ctrl+Space for Inline Help: In VS Code, pressing <code>Ctrl+Space</code> triggers IntelliSense to display inline suggestions, completion items, and parameter hints. This built-in guidance makes it easy to discover available functions, understand their expected parameters, and review docstrings\u2014all without leaving your editor window.</li> </ul> <ul> <li>Jupyter Panel: You can also keep the Jupyter panel open in VS Code for continuous, context-sensitive help as you work. This panel remains visible as you code, providing an always-on reference for classes, methods, and type hints.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-platform-components","title":"Practicus AI Platform Components","text":"<p>Below are the primary components you will interact with when using Practicus AI.</p>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-workers","title":"Practicus AI Workers","text":"<p>Practicus AI Workers are dedicated compute environments that run ML, data processing, and other tasks.</p> <p>Key characteristics include:</p> <ul> <li>On-demand: Request as many workers as you need, available within seconds.</li> <li>Interactive: Launch JupyterLab or VS Code for hands-on experimentation.</li> <li>Batch-capable: Run tasks or jobs in non-interactive mode as well.</li> <li>Isolated: Issues in other workers or systems do not affect your worker.</li> <li>Configurable: Each worker is defined by a container image, which can be chosen from the provided options or customized.</li> <li>Flexible Resources: Assign a specific amount of CPU, memory, and GPU resources.</li> <li>Ephemeral: Workers can be replaced easily. Since each restart resets the file system, save important files in <code>~/my</code> or <code>~/shared</code> to preserve them, or push to a source control system such as git.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-modelhost","title":"Practicus AI ModelHost","text":"<p>Practicus AI ModelHost deployments run classic ML and LLM models, optimized for CPUs and GPUs.</p> <ul> <li>Shared deployments can host thousands of models, each with up to 100 versions.</li> <li>Isolated deployments allow you to create a dedicated environment for a set of models.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-apphost","title":"Practicus AI AppHost","text":"<p>Practicus AI AppHost deployments are used to build visual Gen AI applications or microservices focused on ML workflows.</p>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-add-ons","title":"Practicus AI Add-ons","text":"<p>Practicus AI Add-ons, such as Airflow or MLflow, extend the platform\u2019s core functionality. They integrate seamlessly, allowing you to manage and orchestrate complex workflows and track experiments.</p>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-regions","title":"Practicus AI Regions","text":"<p>Practicus AI is a multi-region environment, where each region is a separate deployment and isolated Kubernetes namespace. Regions can differ by geography, cloud vendor, lifecycle stage, department, or security requirements.</p> <p>For example, you might have:</p> <ul> <li>One region in a certain geographic location and another in a different one.</li> <li>Regions across different cloud vendors (e.g., AWS, Azure, on-premises).</li> <li>Separate regions for production, development, or testing.</li> <li>Regions dedicated to different departments or security contexts.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#practicus-ai-clients","title":"Practicus AI Clients","text":"<p>Practicus AI clients enable you to connect to multiple regions seamlessly, allowing you to develop in one region and deploy in another. Common client options include:</p> <ul> <li>Browser: Access the platform via a standard web interface to launch JupyterLab, VS Code, and manage workloads.</li> <li>AI Studio: A desktop application for Windows, macOS, and Linux that connects to multiple regions for unified management.</li> <li>SDK: Install the SDK (<code>pip install practicuscore</code>) to interact programmatically with any Practicus AI region.</li> <li>CLI: With the SDK installed, use the <code>prtcli</code> command-line tool to manage tasks and resources.</li> </ul>"},{"location":"technical-tutorial/getting-started/introduction/#example-a-multi-region-setup","title":"Example: A Multi-Region Setup","text":"<p>Below is an example of a deployment where a customer utilizes three regions in two geographies, accessible through various clients.</p> <p></p> <p>Next: Workers</p>"},{"location":"technical-tutorial/getting-started/workers/","title":"Practicus AI Workers","text":"<p>This example demonstrates a typical workflow for using Practicus AI Workers:</p> <p>Create a Worker:  </p> <p>Request a new worker with the required resources (CPU, RAM, GPU) from Practicus AI. This usually takes a few seconds.</p> <p>Open JupyterLab or VS Code:</p> <p>Once the worker is ready, launch JupyterLab or VS Code directly on it. Within this environment, you can:</p> <ul> <li>Develop and run code interactively</li> <li>Explore and process data</li> <li>Train and evaluate machine learning models</li> </ul> <p>Perform Tasks:  </p> <p>Inside JupyterLab or VS Code, run Python notebooks, scripts, or leverage integrated libraries and frameworks.</p> <p>Terminate the Worker:  </p> <p>After finishing your tasks, stop or delete the worker. You can always start a new one later, ensuring a clean environment each time.</p> <p>This approach provides an isolated, on-demand environment for efficient development, scaling, and maintaining a clean slate for each new task.</p>"},{"location":"technical-tutorial/getting-started/workers/#creating-a-worker-with-default-settings","title":"Creating a Worker with Default Settings","text":"<p>Let's start by creating a worker with the default configuration.</p> <pre><code># Import the Practicus AI SDK\nimport practicuscore as prt\n</code></pre> <pre><code># Create a worker using default settings\nworker = prt.create_worker()\n</code></pre> <pre><code># Start JupyterLab on the worker and open it in a new browser tab\nworker.open_notebook()\n\n# To use VS Code instead of JupyterLab, uncomment the line below:\n# worker.open_vscode()\n</code></pre> <pre><code># After using the worker, terminate it:\nworker.terminate()\n\n# If you're inside a worker environment, you can self-terminate by running:\n# prt.get_local_worker().terminate()\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#creating-a-customized-worker","title":"Creating a Customized Worker","text":"<p>Now let's create a worker with a custom configuration, specifying a custom image, size, and a startup script.</p> <pre><code># Define a custom worker configuration\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus-genai\",\n    worker_size=\"X-Small\",\n    startup_script=\"\"\"\n    echo \"Hello Practicus AI\" &gt; ~/hello.txt\n    \"\"\",\n)\n\n# Create the worker with the custom configuration\nworker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Verify that hello.txt exists in the home directory:\nworker.open_notebook()\n</code></pre> <pre><code>worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#working-with-the-region-class","title":"Working with the Region Class","text":"<ul> <li>You can interact with multiple regions and perform most operations by using the Region class.</li> <li>If running inside a worker, you can easily access the current region and perform actions directly.</li> </ul> <pre><code># Get the current region\nregion = prt.current_region()\n\n# You could also connect to a different region\n# region = prt.get_region(\"username@some-other-region.com\")\n</code></pre> <pre><code>print(\"Current region:\")\nregion\n</code></pre> <p>It will print something like:</p> <pre><code>key: my-user-name@practicus.your-company.com\nurl: https://practicus.your-company.com\nusername: my-user-name\nemail: my-user-name@your-company.com\nis_default: True\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#worker-sizes","title":"Worker Sizes","text":"<p>Worker sizes define the CPU, RAM, GPU, and other resources allocated to a worker.</p> <pre><code># List available worker sizes\nfor worker_size in region.worker_size_list:\n    print(worker_size.name)\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#smart-listing-with-prtlist","title":"Smart Listing with PrtList","text":"<p>PrtList is a specialized list type that can be toggled as read-only and easily converted to CSV, DataFrame, or JSON. Many results returned by the SDK are <code>PrtList</code> objects.</p> <pre><code># Convert worker sizes to a pandas DataFrame\ndf = region.worker_size_list.to_pandas()\ndisplay(df)\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#worker-images","title":"Worker Images","text":"<p>Worker images define the base container image and features available on the worker.</p> <pre><code>df = region.worker_image_list.to_pandas()\nprint(\"Available worker images:\")\ndisplay(df)\n</code></pre>"},{"location":"technical-tutorial/getting-started/workers/#worker-logs","title":"Worker Logs","text":"<p>You can view the logs of a worker to debug issues or review activities.</p> <pre><code>if prt.running_on_a_worker():\n    print(\"Code is running on a worker, will use 'self' (local worker).\")\n    worker = prt.get_local_worker()\nelse:\n    print(\"Code not running on a worker, creating a new one.\")\n    worker = prt.create_worker()\n\nprint(\"Worker logs:\")\nworker.view_logs()\n\nworker_logs = worker.get_logs()\nif \"some error\" in worker_logs:\n    print(\"Found 'some error' in logs\")\n</code></pre> <p>Previous: Introduction | Next: Workspaces</p>"},{"location":"technical-tutorial/getting-started/workspaces/","title":"Practicus AI Workspaces","text":"<p>Practicus AI Workspaces provide a web-based remote desktop environment with Practicus AI Studio and numerous pre-installed tools including an open-source Office suite.</p> <p></p> <p>You can create, use, and then terminate these Workspaces from Practicus AI Home page, or programmatically through a few simple commands. This ensures a clean, reproducible environment that you can spin up on-demand.</p>"},{"location":"technical-tutorial/getting-started/workspaces/#simple-workspace-usage","title":"Simple Workspace Usage","text":"<pre><code>import practicuscore as prt\n\n# Obtain the current region\nregion = prt.current_region()\n\n# Create a new workspace\nworkspace = region.create_workspace()\n</code></pre> <pre><code># View workspace details\nworkspace\n</code></pre> <pre><code># Both workers and workspaces are managed similarly, differentiated by their 'service_type' attribute.\nregion.worker_list.to_pandas()\n\n# You can select an existing workspace like this:\n# workspace = region.worker_list[0]\n</code></pre> <pre><code># Retrieve workspace login credentials\nusername, token = workspace.get_workspace_credentials()\n\nprint(\"Opening Workspace in your browser\")\nprint(f\"Please log in with username: {username} and password: {token}\")\n\nlogin_url = workspace.open_workspace()\n\n# If you only need the URL, without opening a browser:\n# login_url = workspace.open_workspace(get_url_only=True)\n</code></pre> <pre><code># Terminate the workspace when you're finished\nworkspace.terminate()\n</code></pre> <p>Previous: Workers | Next: Add-Ons</p>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/","title":"Advanced GPU Resource Management with Practicus AI","text":"<p>This document provides an overview of how to configure partial NVIDIA GPUs (MIG), along with high-level steps for AMD and Intel GPU support, and how Practicus AI platform facilitates GPU management.</p>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#topics-covered-in-this-document","title":"Topics covered in this document","text":"<ul> <li>Partial NVIDIA GPUs (MIG)</li> <li>AMD GPUs</li> <li>Intel GPUs</li> </ul>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#1-partial-nvidia-gpus-mig","title":"1. Partial NVIDIA GPUs (MIG)","text":""},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#how-mig-works","title":"How MIG Works","text":"<ul> <li>NVIDIA's <code>Multi-Instance GPU (MIG)</code> feature allows you to split a single physical GPU (e.g., NVIDIA A100m, H100, H200) into multiple independent GPU instances.</li> <li>Each MIG instance provides dedicated memory and compute resources, ideal for running multiple workloads on the same GPU.</li> </ul>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#setup-steps","title":"Setup Steps","text":"<ol> <li>Enable MIG Mode on the GPU:</li> <li>Log into the GPU node and enable MIG using <code>nvidia-smi</code>:      <pre><code>sudo nvidia-smi -i 0 --mig-enable\nsudo reboot\n</code></pre></li> <li> <p>After reboot, confirm MIG mode is enabled:      <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Create MIG Instances:</p> </li> <li>Use <code>nvidia-smi</code> to create MIG profiles. For example, split a GPU into 7 instances:      <pre><code>sudo nvidia-smi mig -i 0 -cgi 0,1,2,3,4,5,6\nsudo nvidia-smi mig -i 0 -cci\n</code></pre></li> <li> <p>Check the configuration:      <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Expose MIG Resources in Kubernetes:</p> </li> <li>Deploy the NVIDIA Device Plugin:      <pre><code>kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/main/deployments/k8s-device-plugin-daemonset.yaml\n</code></pre></li> <li> <p>Verify available resources:      <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre></p> </li> <li> <p>To learn more please visit:</p> <ul> <li>https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html</li> </ul> </li> </ol>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#2-custom-gpu-configuration-in-practicus-ai","title":"2. Custom GPU Configuration in Practicus AI","text":"<p>Practicus AI simplifies advanced GPU management through the intuitive management UI:</p> <p>Open Practicus AI Management Console: - Access the platform's web console for infrastructure management.</p> <p>Select Worker Sizes: - Choose from predefined worker sizes or create a new one to include GPU capacity :    - <code>Number of GPUs</code>    - <code>Amount of Video RAM (VRAM)</code></p> <p>Enter GPU Type Selector: - Specify the custom GPU type you need:     - Specify for Nvidia MIG (e.g. <code>nvidia.com/mig-1g.5gb</code>) you defined in the above step.     - Specify for other vendors (e.g., <code>amd.com/gpu</code> or <code>intel.com/gpu</code>).     - Leave empty for the default, which will use entire NVIDIA GPUs without fractions.</p> <p>Deploy workloads as usual: - Deploy end user workers, model hosts, app hosts etc. as usual with the worker size you defined above. - The platform will dynamically manage the resources with the selected GPU configuration.</p> <p>Example Configuration</p> <ul> <li>If you set GPU count to 2, with a GPU type selector of <code>nvidia.com/mig-1g.5gb</code> running on a single <code>NVIDIA H100 GPU</code>, the end user could get two separate GPU instances, each with 1/7th of the GPU's compute and memory resources (1 compute slice and 5 GB of memory per instance).</li> <li>This configuration allows the same physical GPU to handle multiple workloads independently, providing dedicated resources for each workload without interference. </li> <li>This setup is ideal for lightweight GPU workloads, such as inference or smaller-scale training tasks, that do not require the full power of an entire GPU.</li> </ul> <p>Important Note - Please note that the VRAM setting in the Practicus AI Management Console does not dictate how much VRAM a user gets. It is only used to measure usage and ensure a user is kept within their designated daily/weekly/monthly usage limits.  - To actually enforce VRAM limits, you must use NVIDIA MIG profiles (e.g., <code>nvidia.com/mig-1g.5gb</code>) or equivalent to <code>apply resource constraints at the hardware level.</code></p>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#3-high-level-steps-for-amd-gpus","title":"3. High-Level Steps for AMD GPUs","text":"<p>AMD GPUs (e.g., using ROCm) require setup similar to NVIDIA but with their own tools and configurations:</p> <ol> <li>Install AMD ROCm Drivers:</li> <li> <p>Install ROCm drivers on the nodes with AMD GPUs.</p> </li> <li> <p>Deploy AMD Device Plugin:</p> </li> <li>Use the AMD ROCm Kubernetes device plugin to expose AMD GPU resources:      <pre><code>kubectl apply -f https://github.com/RadeonOpenCompute/k8s-device-plugin\n</code></pre></li> </ol> <p>The rest is the same as NVIDIA MIG, define a new worker size and use the GPU Type Selector <code>amd.com/gpu</code></p>"},{"location":"technical-tutorial/how-to/configure-advanced-gpu/#4-high-level-steps-for-intel-gpus","title":"4. High-Level Steps for Intel GPUs","text":"<p>Intel GPUs can be managed using the Intel GPU Device Plugin:</p> <ol> <li>Install Intel GPU Drivers:</li> <li> <p>Install Intel drivers and libraries for iGPU or discrete GPU support.</p> </li> <li> <p>Deploy Intel Device Plugin:</p> </li> <li>Use the Intel GPU plugin to expose GPU resources:      <pre><code>kubectl apply -f https://github.com/intel/intel-device-plugins-for-kubernetes\n</code></pre> The rest is the same as NVIDIA MIG, define a new worker size and use the GPU Type Selector <code>intel.com/gpu</code></li> </ol> <p>Previous: Customize Templates | Next: Configure Workspaces</p>"},{"location":"technical-tutorial/how-to/configure-workspaces/","title":"Configure Workspaces","text":""},{"location":"technical-tutorial/how-to/configure-workspaces/#advanced-workspace-configuration","title":"Advanced Workspace Configuration","text":"<p>You can specify additional parameters to customize the workspace environment.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_region()\n</code></pre> <pre><code>import json\nimport base64\n\nadditional_params = {\n    \"timezone\": \"America/Los_Angeles\",\n    # Setting a custom password is possible, but not recommended.\n    \"password\": \"super_secret\"\n}\n\nadditional_params_str = json.dumps(additional_params)\nadditional_params_b64 = str(base64.b64encode(bytes(additional_params_str, encoding=\"utf-8\")), \"utf-8\")\n</code></pre> <pre><code># Create a workspace with these additional parameters\nworkspace = region.create_workspace(\n    worker_config={\n        \"additional_params\": additional_params_b64\n    }\n)\n</code></pre> <pre><code>username, token = workspace.get_workspace_credentials()\n\nprint(\"Opening Workspace in your browser.\")\nprint(f\"Please log in with username: {username} and password: {token}\")\n\nlogin_url = workspace.open_workspace()\n\n# The workspace should now use US Pacific Time, per the timezone setting.\n</code></pre> <pre><code>workspace.terminate()\n</code></pre>"},{"location":"technical-tutorial/how-to/configure-workspaces/#shared-folders","title":"Shared Folders","text":"<p>Administrators can define <code>my</code> and <code>shared</code> folders accessible from your Workspaces. Your <code>my</code> folder is dedicated to your environment, while <code>shared</code> folders can be used for collaboration with other users.</p> <p>Sharing between Workers and Workspaces: Sometimes, you may have shared folders accessible by both Workspaces and Workers. In such cases, permission issues might arise due to different default users (e.g., practicus vs. ubuntu). To fix this, simply adjust file ownership on the respective environment:</p> <p>Fixing file permission issues of a shared folder:</p> <p>On a Workspace</p> <pre><code>sudo chown practicus:practicus some_file.txt\n</code></pre> <p>On a Worker</p> <pre><code>sudo chown ubuntu:ubuntu some_file.txt\n</code></pre> <p>Previous: Configure Advanced Gpu | Next: Share Workers</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/","title":"Creating new virtual environments","text":"<p>Practicus AI workers allow you to create new Python Virtual environments using the python default venv module. Please follow the below steps to create and use new virtual environments.</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#option-1-re-use-base-python-packages","title":"Option 1) Re-use base python packages","text":"<ul> <li>With this option you can save disk space with fewer package installations </li> <li>If you need to change a python package version of base image you can simply pip install the different version.</li> </ul> <pre><code># Create new venv \npython3 -m venv $HOME/.venv/new_venv --system-site-packages --symlinks\n# Activate\nsource $HOME/.venv/new_venv/bin/activate\n# Add to Jupyter \npython3 -m ipykernel install --user --name new_venv --display-name \"My new Python\"\n# Install packages, these will 'override' parent python package versions\npython3 -m pip install some_package\n</code></pre>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#option-2-fresh-install","title":"Option 2) Fresh install","text":"<ul> <li>With the fresh install you will have to install all packages, including Practicus AI</li> </ul> <pre><code># Create new venv \npython3 -m venv $HOME/.venv/new_venv\n# Activate\nsource $HOME/.venv/new_venv/bin/activate\n# Install Jupyter Kernel\npython3 -m pip install ipykernel\n# Add to Jupyter \npython3 -m ipykernel install --user --name new_venv --display-name \"My new Python\"\n# Install packages\npython3 -m pip install practicuscore\n</code></pre>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#creating-new-notebooks","title":"Creating new Notebooks","text":"<p>On Jupyter click File &gt; New &gt; Notebook. And your new virtual environment should show up.</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#creating-new-virtual-environments-for-older-python-versions-using-uv","title":"Creating New Virtual Environments for Older Python Versions Using <code>uv</code>","text":"<p>If you need to use a Python version older than the one provided by default, you can leverage <code>uv</code> to install that version and then use Python\u2019s built-in <code>venv</code> module to create and manage your virtual environments.</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#important-note","title":"Important Note","text":"<p>Practicus AI may not fully support older, deprecated Python versions. While you can still run these versions in a Practicus AI worker or notebook, certain features (like the Practicus AI SDK) may not function as expected.</p>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#installing-a-specific-python-version","title":"Installing a Specific Python Version","text":"<p>Since <code>uv</code> is already installed, you can use it to install an alternate Python version. For example, to install Python 3.7:</p> <pre><code>uv python install 3.7\n</code></pre>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#creating-a-virtual-environment","title":"Creating a Virtual Environment","text":"<p>Use the newly installed Python version to create a virtual environment:</p> <pre><code>uv venv ~/.venv/test --python 3.7\n</code></pre> <p>Activate the virtual environment:</p> <pre><code>source ~/.venv/test/bin/activate\n</code></pre>"},{"location":"technical-tutorial/how-to/create-virtual-envs/#installing-packages","title":"Installing Packages","text":"<p>Within the activated environment, install your desired packages:</p> <pre><code>uv pip install pandas\n</code></pre> <p>Previous: View Stats | Next: Model Tokens</p>"},{"location":"technical-tutorial/how-to/customize-templates/","title":"Practicus AI Code Generation Templates","text":"<p>Practicus AI workers use Jinja2 templates in ~/practicus/templates folder to streamline and automate code generation for key components such as Python libraries, Airflow DAGs and Jupyter notebooks. This approach ensures consistency, scalability, and customization across your projects. Below is an overview of how these templates work and how you can use them effectively.</p>"},{"location":"technical-tutorial/how-to/customize-templates/#1-overview-of-jinja2-templates","title":"1. Overview of Jinja2 Templates","text":"<p>Jinja2 is a powerful templating engine for Python, enabling dynamic creation of files by inserting variables and logic into predefined structures. In our platform, templates are configured to: - Standardize code structure and style. - Dynamically generate project-specific code. - Simplify the onboarding process for new workflows. - To learn more please visit:     - https://jinja.palletsprojects.com/en/stable/intro/</p>"},{"location":"technical-tutorial/how-to/customize-templates/#2-how-to-customize-templates","title":"2. How to Customize Templates","text":"<ul> <li>Start a worker and view templates in ~/practicus/templates folder</li> <li>Customize templates by editing the Jinja2 syntax with your specific project details.</li> <li>Templates can be rendered to test programmatically, see below a sample.</li> <li>Once your testing is completed, you can create a new Practicus AI worker container image and override existing template files.</li> </ul>"},{"location":"technical-tutorial/how-to/customize-templates/#3-best-practices","title":"3. Best Practices","text":"<ul> <li>Test Before Use: Validate generated code in a test environment.</li> <li>Document Changes: If you modify templates, document the changes for future reference.</li> <li>Leverage Variables and Logic: Use Jinja2's advanced features like loops and conditionals for maximum flexibility.</li> </ul> <pre><code># Jinja templates in action..\n\nfrom jinja2 import Template\n\ntemplate_content = \"\"\"\ndef {{ function_name }}(x):\n    return x ** 2\n\"\"\"\n\ntemplate = Template(template_content)\nrendered_code = template.render(function_name=\"square\")\nprint(rendered_code)\n\n# prints\n# def square(x):\n#    return x ** 2\n</code></pre> <p>Previous: Use Custom Metrics | Next: Configure Advanced Gpu</p>"},{"location":"technical-tutorial/how-to/integrate-git/","title":"Git source control integration","text":"<p>You can use git commands from the terminal, or with the native Git extension from the Jupyter notebook.</p>"},{"location":"technical-tutorial/how-to/integrate-git/#setting-up-using-terminal-recommended","title":"Setting up using terminal (recommended)","text":"<ul> <li>Using the main menu open a terminal. File &gt; New &gt; Terminal</li> <li>Run the below commands</li> </ul> <pre><code># 1) Navigate to the directory to clone \nmkdir ~/projects\ncd ~/projects\n\n# 2) Clone a git repo \ngit clone https://github.com/username/repository.git\n\n# 3) Enter your username and password\n# Note: Many git systems such as GitHub only allow \"personal access tokens\" as password\n# Please check below this notebook to learn how to create a personal access token\n\n# 4) Add your email and username information. \n# This will prevent entering it each time you want to commit.\ngit config --global user.email \"alice@wonderland.com\"\ngit config --global user.name \"Alice\"\n\n# 5) (Optional) Save your credentials\ngit config --global credential.helper cache\n\n# If you already cloned a git repo and need to save credentials again\ncd ~/projects/name_of_the_repository\ngit pull\n# Enter credentials, and repeat after step 4)\n</code></pre> <p>After cloning a git repo and saving credentials, you can use both the terminal and the git UI elements inside Jupyter Notebook</p>"},{"location":"technical-tutorial/how-to/integrate-git/#setting-up-using-the-jupyter-notebook","title":"Setting up using the Jupyter Notebook","text":"<ul> <li>You can open the Git section and click clone a repository.</li> <li>Please note that with this method you might have to enter your password each time you want sync with your git repo.</li> </ul>"},{"location":"technical-tutorial/how-to/integrate-git/#creating-git-personal-access-tokens","title":"Creating Git Personal Access Tokens","text":"<ul> <li>Github<ul> <li>https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens</li> </ul> </li> <li>Bitbucket<ul> <li>https://support.atlassian.com/bitbucket-cloud/docs/create-a-repository-access-token/</li> </ul> </li> <li>GitLab<ul> <li>https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html</li> </ul> </li> </ul> <p>Previous: Work With Connections | Next: Use Security Tokens</p>"},{"location":"technical-tutorial/how-to/model-tokens/","title":"Using model access tokens","text":"<ul> <li>In this example we will show how to find model prefixes, models and get short lived session tokens.</li> </ul>"},{"location":"technical-tutorial/how-to/model-tokens/#anatomy-of-a-model-url","title":"Anatomy of a model url","text":"<ul> <li>Practicus services follow this pattern:<ul> <li>[ primary service url ] / [ model prefix ] / [ model name ] / &lt; optional version &gt; /</li> </ul> </li> <li>Sample model addresses:<ul> <li>https://service.practicus.io/models/practicus/diamond-price/</li> <li>https://service.practicus.io/models/practicus/diamond-price/v3/</li> </ul> </li> <li>Please note that Practicus AI model urls always end with a \"/\" </li> </ul> <pre><code>import practicuscore as prt \n\nregion = prt.regions.get_default_region()\n</code></pre>"},{"location":"technical-tutorial/how-to/model-tokens/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>model_name = None # E.g. \"diamond-price\"\nmodel_prefix = None #  E.g. 'models/practicus'\n</code></pre> <pre><code>assert model_name, \"Please enter your model_name \"\nassert model_prefix, \"Please enter your model_prefix.\"\n</code></pre> <pre><code># Let's get model prefixes dataframe\n# We can also use the list form with: region.model_prefix_list \nmodel_prefix_df = region.model_prefix_list.to_pandas()\n\nprint(\"Current model prefixes:\")\nprint(\"Note: we will use the 'prefix' column in the API urls, and not the 'key'.\")\n\ndisplay(model_prefix_df)\n</code></pre> <pre><code>print(\"Current models:\")\nprint(\"Note: we will use the 'name' column in the API urls, and not 'model_id'\")\n\ndf = region.model_list.to_pandas()\ndisplay(df)\n</code></pre> <pre><code># You can use regular pandas filters\n# E.g. let's search for models with a particular model prefix, \n# and remove all models that are not deployed (hs no version)\n\n\nfiltered_df = df[(df['prefix'] == model_prefix) &amp; (df['versions'].notna())]\ndisplay(filtered_df)\n</code></pre> <pre><code>assert model_name, \"Please enter your model_name\"\nassert model_prefix, \"Please enter your model_prefix\"\n</code></pre> <pre><code>api_url = f\"{region.url}/{model_prefix}/{model_name}/\"\n\nprint(\"Getting Model API session token for:\", api_url)\ntoken = prt.models.get_session_token(api_url)\n\nprint(\"Model access token with a short life:\")\nprint(token)\n</code></pre>"},{"location":"technical-tutorial/how-to/model-tokens/#getting-model-api-session-token-using-rest-api-calls","title":"Getting Model API session token using REST API calls","text":"<p>If your end users do not have access to Practicus AI SDK, they can simply make the below REST API calls with any programming language to get a Model API session token.</p> <pre><code># \"No Practicus SDK\" sample to get a session token\n\nimport requests\n\ntry :\n    console_api_url = \"http://local.practicus.io/console/api/\"\n\n    # Option 1 - Use password auth every time you need tokens\n    print(\"[Not Recommended] Getting console API access token using password.\")\n    email = \"admin@admin.com\"\n    password = \"admin\"\n\n    data = {\"email\": email, \"password\": password}\n    console_login_api_url = f\"{console_api_url}auth/\"\n    r = requests.post(console_login_api_url, headers=headers, json=data)\n    if not r.ok:\n        raise ConnectionError(r.status_code)\n    body = r.json()\n    refresh_token = body[\"refresh\"]  # Keep refresh tokens safe!\n    console_access_token = body[\"access\"] \n\n    # Option 2 - Get a refresh token once, and only use that until it expires in ~3 months\n    print(\"[Recommended] Getting console API access token using refresh token\")\n    console_access_api_url = f\"{console_api_url}auth/refresh/\"\n    headers = {\"authorization\": f\"Bearer {refresh_token}\"}\n    data = {\"refresh\": refresh_token}\n    r = requests.post(console_access_api_url, headers=headers, json=data)\n    if not r.ok:\n        raise ConnectionError(r.status_code)\n    body = r.json()\n    console_access_token = body[\"access\"]\n    headers = {\"authorization\": f\"Bearer {console_access_token}\"}\n\n    # Console API access tokens expire in ~30 minutes\n    print(\"Console API access token:\", console_access_token)\n\n    # Locating model id\n    print(\"Getting model id.\")\n    print(\"Note: you can also view model id using Open API documentation (E.g. https://../models/redoc/), or using Practicus AI App.\")\n    r = requests.get(api_url + \"?get_meta=true\", headers=headers, data=data)\n    if not r.ok:\n        raise ConnectionError(r.status_code)\n    model_id = int(r.headers[\"x-prt-model-id\"])\n    print(\"Model id:\", model_id)\n\n    # Getting model access token, expires in ~4 hours\n    print(\"Getting a model API session token using the console API access token\") \n    console_model_token_api_url = f\"{console_api_url}modelhost/model-auth/\"\n    data = {\"model_id\": model_id}\n    r = requests.get(console_model_token_api_url, headers=headers, data=data)\n    if not r.ok:\n        raise ConnectionError(r.status_code)\n    body = r.json()\n    model_api_token = body[\"token\"]\n    print(\"Model API session token:\", model_api_token) \nexcept:\n    pass\n</code></pre> <p>Previous: Create Virtual Envs | Next: Use Polars</p>"},{"location":"technical-tutorial/how-to/share-workers/","title":"Sharing Workers","text":"<p>To allow another user to access a Practicus AI worker you\u2019ve created, you can run the code below and then share the connection details. This provides a quick way to collaborate on the same environment, including data and code.</p> <p>Important Note: Any user you share a worker with will have access to the contents of your <code>~/my</code> and <code>~/shared</code> directories.</p> <pre><code>import practicuscore as prt \n\n# To share the worker you are currently using,\n#  first get a reference to 'self'\nworker = prt.get_local_worker()\n\n# To start and share a worker, create a worker as usual, e.g.\n# worker = prt.create_worker(worker_config)\n\n# The rest of the code will be the same\n</code></pre> <pre><code># To share using Jupyter Lab\nurl = worker.open_notebook(get_url_only=True)\n\nprint(\"Jupyter Lab login url:\", url)\n</code></pre> <pre><code># To share using VS Code\nurl, token = worker.open_vscode(get_url_only=True)\n\nprint(\"VS Code login url:\", url)\nprint(\"VS Code token    :\", token)\n</code></pre> <p>Previous: Configure Workspaces | Next: View Stats</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/","title":"Using Custom Observability Metrics","text":"<p>Practicus AI\u2019s model hosting and app hosting system allows you to create and track custom Observability metrics via Prometheus or another time-series database. Follow the steps below to begin publishing your own metrics.</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#step-1-enable-observability","title":"Step 1) Enable Observability","text":"<p>Verify with your administrator that the model hosting deployment or app hosting deployment setting you plan to use has observability enabled. If not, an admin can open the Practicus AI Admin Console, select the relevant model deployment or app deployment setting and enable it.</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#step-2-for-model-hosting-modify-modelpy","title":"Step 2) For model hosting, modify <code>model.py</code>","text":"<p>Initialize Prometheus counters, histograms, etc. as usual, and then update your metrics accordingly. An example is shown below.</p> <pre><code># model.py\n\n# ... your existing imports\nfrom prometheus_client import Counter, REGISTRY\n\nmy_counter = None\n\nasync def init(*args, **kwargs):\n    # ... your existing model init code\n\n    global my_counter\n\n    metric_name = \"my_test_counter\"\n    try:\n        my_counter = Counter(\n            name=metric_name,\n            documentation=\"My test counter\",\n            labelnames=[\"my_first_dimension\", \"my_second_dimension\"]\n        )\n    except ValueError:\n        # Metric is already defined; retrieve it from the registry\n        my_counter = REGISTRY._names_to_collectors.get(metric_name)\n        if not my_counter:\n            raise  # Something else went wrong\n\n\nasync def predict(df, *args, **kwargs):\n    # ... your existing prediction code\n\n    # Increment the counter for the chosen labels\n    my_counter.labels(\n        my_first_dimension=\"abc\",\n        my_second_dimension=\"xyz\",\n    ).inc()\n\n    # ... your existing code returning prediction\n</code></pre>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#step-2-for-app-hosting-modify-api-py-files","title":"Step 2) For App Hosting, modify API .py files","text":"<p>You can select any api .py file and initialize Prometheus counters, histograms, etc. as usual, and then update your metrics accordingly. An example is shown below.</p> <pre><code># apis/some_api_method.py\n\n# ... your existing imports\nfrom prometheus_client import Counter, REGISTRY\n\nmy_counter = None\n\ndef init_counter():\n    global my_counter\n\n    metric_name = \"my_test_counter\"\n    try:\n        my_counter = Counter(\n            name=metric_name,\n            documentation=\"My test counter\",\n            labelnames=[\"my_first_dimension\", \"my_second_dimension\"]\n        )\n    except ValueError:\n        # Metric is already defined; retrieve it from the registry\n        my_counter = REGISTRY._names_to_collectors.get(metric_name)\n        if not my_counter:\n            raise  # Something else went wrong\n\ndef increment_my_counter():\n    if not my_counter:\n        init_counter()\n\n    # Increment the counter for the chosen labels\n    my_counter.labels(\n        my_first_dimension=\"abc\",\n        my_second_dimension=\"xyz\",\n    ).inc()\n\n\ndef run(payload: SomePayloadRequest, **kwargs):\n    # ... api code\n\n    # observe any metric\n    increment_my_counter()\n\n    # ... your existing code returning api result\n</code></pre>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#note-on-metric-exporters","title":"Note on Metric Exporters","text":"<p>Prometheus is a pull-based monitoring system, which means it regularly scrapes endpoints for metrics rather than having metrics pushed to it. To accommodate this, you can use a Practicus AI application as a centralized Metric Exporter for your other systems outside of Practicus AI.</p> <p>In practice, you would: 1. Build a minimal Practicus AI app with no UI (i.e., no <code>Home.py</code>), focusing solely on exposing API endpoints. 2. Define metric-specific endpoints, for example:    - <code>increase_counter_x</code> \u2014 increments a particular counter.    - <code>observe_histogram_y</code> \u2014 records a value for a given histogram.</p> <p>And the Prometheus-compatible <code>/metrics</code> endpoint would be automatically created, assuming your admin enabled observability for app hosting setting you use.</p> <p>This design pattern allows external systems to call your Practicus AI app to update metrics, while Prometheus simply pulls the metrics from one central place.</p> <p>Important Considerations: - Single Replica: Ensure your app deployment setting uses a single replica (1 Kubernetes pod). This makes your metrics consistent and avoids synchronization issues. - Shared Cache/Database: If multiple replicas are required for scaling or redundancy, use a shared cache (e.g., Redis) or a central database (e.g., PostgreSQL) to ensure that a metric update to one pod in the app is visible to all pods.</p> <p>By following this pattern, you centralize metric updates in a single service, keeping your application code cleaner and leveraging Prometheus\u2019s native pull-based model.</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#step-3-observe","title":"Step 3) Observe","text":"<p>After a few minutes, the observations from your custom metric should appear in your monitoring system (for example, Grafana).</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#important-note-on-performance-and-storage-costs","title":"Important note on performance and storage costs","text":"<p>Please be aware that using too many unique labels or values (high cardinality) can quickly degrade performance and inflate storage costs in Prometheus. Labels that change frequently or include user IDs, timestamps, or other unique identifiers are particularly problematic. Always consult your system administrator or MLOps/DevOps team on best practices for label management and on setting up thresholds or limits to maintain a healthy, stable monitoring environment.</p>"},{"location":"technical-tutorial/how-to/use-custom-metrics/#troubleshooting","title":"Troubleshooting","text":"<p>If you do not see your metric in the monitoring system, check the following:</p> <ul> <li>Observability Disabled? Make sure observability is enabled for your model hosting deployment or app hosting deployment setting.</li> <li>Verify via Pod Metrics Endpoint: If you have the necessary Kubernetes permissions, connect to the pod running your code is hosted on and run:</li> </ul> <pre><code>curl http://localhost:8000/metrics/\n</code></pre> <p>You should see custom metrics like:</p> <pre><code># HELP my_test_counter_total My test counter\n# TYPE my_test_counter_total counter\nmy_test_counter_total{my_first_dimension=\"abc\",my_second_dimension=\"xyz\"} 4.0\n# HELP my_test_counter_created My test counter\n# TYPE my_test_counter_created gauge\nmy_test_counter_created{my_first_dimension=\"abc\",my_second_dimension=\"xyz\"} 1.737049965621189e+09\n</code></pre> <ul> <li>If no metrics appear at all, observability may not be enabled.</li> <li> <p>If only your metric is missing, ensure you are using the model version or api call that contains your custom metrics code.</p> </li> <li> <p>Check Prometheus Scraping: If the metrics are available in the pod but not in Prometheus, verify the <code>defaultServiceMonitor</code> settings in your Kubernetes namespace. This determines whether Prometheus is correctly scraping the metrics from your deployment.</p> </li> </ul> <p>Previous: Work With Data Catalog | Next: Customize Templates</p>"},{"location":"technical-tutorial/how-to/use-polars/","title":"Using Polars for High-Performance Data Processing","text":"<p>Polars is a high-performance DataFrame library designed for efficient and fast data manipulation. Built in Rust and leveraging Apache Arrow, Polars provides a modern, user-friendly API for working with structured data. It offers several advantages over traditional libraries like Pandas and Dask:</p>"},{"location":"technical-tutorial/how-to/use-polars/#why-use-polars","title":"Why Use Polars?","text":""},{"location":"technical-tutorial/how-to/use-polars/#key-benefits","title":"Key Benefits:","text":"<ol> <li>Speed: Written in Rust and optimized for performance, Polars is significantly faster for many operations compared to Pandas.</li> <li>Memory Efficiency: Polars uses Arrow memory structures, which are compact and designed for zero-copy interprocess communication.</li> <li>Parallelism: Automatically leverages multiple CPU cores for computations.</li> <li>Lazy Evaluation: Allows defining a series of operations that are only computed when needed, improving efficiency for complex workflows.</li> <li>Interoperability: Easy to switch between Polars and Pandas, allowing incremental adoption.</li> </ol>"},{"location":"technical-tutorial/how-to/use-polars/#example-basic-polars-operations-with-diamondcsv","title":"Example: Basic Polars Operations with <code>diamond.csv</code>","text":"<p>In this notebook, we will: 1. Load the <code>diamonds.csv</code> dataset using Polars. 2. Explore the dataset with basic info commands. 3. Perform simple data manipulations. 4. Showcase interoperation between Polars and Pandas.</p> <pre><code># Import Polars\nimport polars as pl\n\n# Read the dataset using Polars\ndf = pl.read_csv(\"/home/ubuntu/samples/diamond.csv\")\n\n# Basic exploration\nprint(\"Shape of the dataset:\", df.shape)\nprint(\"First few rows of the dataset:\")\nprint(df.head())\n</code></pre> <pre><code># Summary statistics\nprint(\"Summary statistics:\")\nprint(df.describe())\n</code></pre> <pre><code># Filter rows where carat is greater than 2\nfiltered_df = df.filter(pl.col(\"Carat Weight\") &gt; 2)\nprint(\"Filtered rows where Carat Weight &gt; 2:\")\nprint(filtered_df)\n</code></pre> <pre><code># Convert to Pandas DataFrame\npandas_df = df.to_pandas()\nprint(\"Converted to Pandas DataFrame:\")\ndisplay(pandas_df.head())\n</code></pre> <pre><code># Convert back to Polars DataFrame\npolars_df = pl.from_pandas(pandas_df)\nprint(\"Converted back to Polars DataFrame:\")\nprint(polars_df.head())\n</code></pre> <p>Previous: Model Tokens | Next: Extras &gt; Modeling &gt; SparkML &gt; Ice Cream &gt; SparkML Ice Cream</p>"},{"location":"technical-tutorial/how-to/use-security-tokens/","title":"Access and Refresh JWT tokens","text":"<ul> <li>In this example we will show how to login to a Practicus AI region and get access and or refresh tokens.</li> <li>Access tokens are short lived, refresh tokens are long.</li> <li>Refresh tokens allow you the ability to store your login credentials without actually storing your password</li> <li>JWT tokens are human readable, you can visit jwt.io and view what is inside the token.<ul> <li>Is this secure? Yes, jwt.io does not store tokens and decryption happens with javascript on your browser.</li> <li>Who can create JWT tokens? Practicus AI tokens are asymmetric, one can read what is inside a token but cannot create a new one without the secret key. Only your system admin has access to secrets.</li> <li>Can I use a token created for one Practicus AI region for another region? By default, no. If your admin deployed the regions in \"federated\" mode, yes.</li> </ul> </li> </ul> <pre><code>import practicuscore as prt \nimport getpass\n\nregion = prt.regions.get_default_region()\n</code></pre>"},{"location":"technical-tutorial/how-to/use-security-tokens/#defining-parameters","title":"Defining parameters.","text":"<p>This section defines key parameters for the notebook. Parameters control the behavior of the code, making it easy to customize without altering the logic. By centralizing parameters at the start, we ensure better readability, maintainability, and adaptability for different use cases.</p> <pre><code>practicus_url = None # E.g.\"https://practicus.your-company.com\" \nemail = None # E.g. \"your-email@your-company.com\"\npassword = None\n</code></pre> <pre><code>assert practicus_url, \"Please enter your practicus_url \"\nassert email, \"Please enter your email.\"\nassert password, \"Plese enter your password\"\n</code></pre> <pre><code># Method 1) If you are already logged in, or if you are running this code on a Practicus AI Worker.\n\n\n# Get tokens for the current region.\nrefresh_token, access_token = region.get_refresh_and_access_token()\n\n# Will print long strings like eyJ...\nprint(\"Refresh token:\", refresh_token)\nprint(\"Access token:\", access_token)\n</code></pre> <pre><code># Method 2) You are logging in using the SDK on your laptop,\n#   Or, you are running this code on a worker in a region, but logging in to another region.\n\n# Optionally, you can log-out first\n# prt.auth.logout(all_regions=True)\n\n\n# Tip: region.url shows the current Practicus AI service URL that you are logged-in to.\n\n\n\nprint(f\"Please enter the password for {email} to login {practicus_url}\")\nif not password:\n    password = getpass.getpass()\n\nsome_practicus_region = prt.auth.login(\n    url=practicus_url,\n    email=email,\n    password=password,\n    # Optional parameters:\n    # Instead of using a password, you can login using a refresh token or access token\n    #   refresh_token = ... will keep logged in for many days\n    #   access_token = ... will keep logged in for some minutes\n    # By default, your login token is stored for future use under ~/.practicus/core.conf, to disable:\n    #   save_config = False\n    # By default, your password is not saved under ~/.practicus/core.conf, to enable:\n    #   save_password = True\n)\n\n# Now you can get as many refresh/access tokens as you need.\nrefresh_token, access_token = some_practicus_region.get_refresh_and_access_token()\n\nprint(\"Refresh token:\", refresh_token)\nprint(\"Access token:\", access_token)\n</code></pre> <pre><code># If you just need an access token.\naccess_token = region.get_access_token()\n\nprint(\"Access token:\", access_token)\n</code></pre> <p>Previous: Integrate Git | Next: Work With Processes</p>"},{"location":"technical-tutorial/how-to/view-stats/","title":"System Resource Usage","text":"<p>Below is a sample code snippet that retrieves and displays system resource usage of memory, disk, and GPU statistics. This helps you monitor available system resources at a glance:</p> <ul> <li>Memory: Reports total, free, and percentage of free memory.</li> <li>Disk: Reports total, free, and percentage of free disk space.</li> <li>GPU: Shows detailed GPU memory usage (used, reserved, total).</li> </ul> <pre><code>from practicuscore.util import Stats\n\ntotal_mem, used_mem = Stats.get_memory_stats()\n\nfree_mem = total_mem - used_mem\ngb = 1024**3\ntotal_mem_gb = round(total_mem / gb, 2)\nfree_mem_gb = round(free_mem / gb, 2)\nfree_mem_percent = round(free_mem/total_mem*100, 2)\n\nprint(f\"Worker RAM : {total_mem_gb} GB\")\nprint(f\"Free RAM : {free_mem_gb} GB ({free_mem_percent}%)\\n\")\n\ntotal_disk, free_disk = Stats.get_disk_stats()\n\ntotal_disk_gb = round(total_disk / gb, 2)\nfree_disk_gb = round(free_disk / gb, 2)\nfree_disk_percent = round(free_disk/total_disk*100, 2)\n\nprint(f\"Worker Disk : {total_disk_gb} GB\")\nprint(f\"Free Disk : {free_disk_gb} GB ({free_disk_percent}%)\")\n\nprint(\"Note: The above views the physical disk capacity of the node.\")\nprint(\"The ephemeral disk capacity that your admin allowed for this Worker can be lower.\\n\")\n\ntry:\n    gpu_stats = Stats.get_gpu_stats()\n    for gpu_id, (used, reserved, total) in enumerate(gpu_memory):\n        print(f\"GPU usage for gpu: {gpu_id} used: {used} reserved: {reserved} total: {total}\")                        \nexcept:\n    print(\"No GPUs detected\")\n</code></pre> <p>Previous: Share Workers | Next: Create Virtual Envs</p>"},{"location":"technical-tutorial/how-to/work-with-connections/","title":"Practicus AI Connections","text":""},{"location":"technical-tutorial/how-to/work-with-connections/#displaying-available-connections","title":"Displaying Available Connections","text":"<p>Below is a quick way to list all available connections you have. Run the code to view your configured data sources, storage endpoints, and other services.</p> <pre><code>import practicuscore as prt\n\nprt.connections.get_all().to_pandas()\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-connections/#creating-a-new-connection","title":"Creating a New Connection","text":"<p>The <code>conn_conf</code> parameter takes a configuration object from a base class that  supports many data sources, including:</p> <ul> <li>S3ConnConf</li> <li>SqLiteConnConf</li> <li>MYSQLConnConf</li> <li>PostgreSQLConnConf</li> <li>RedshiftConnConf</li> <li>SnowflakeConnConf</li> <li>MSSQLConnConf</li> <li>OracleConnConf</li> <li>HiveConnConf</li> <li>ClouderaConnConf</li> <li>AthenaConnConf</li> <li>ElasticSearchConnConf</li> <li>OpenSearchConnConf</li> <li>TrinoConnConf</li> <li>DremioConnConf</li> <li>HanaConnConf</li> <li>TeradataConnConf</li> <li>Db2ConnConf</li> <li>DynamoDBConnConf</li> <li>CockroachDBConnConf</li> <li>CustomDBConnConf</li> </ul> <p>Here we use <code>SqLiteConnConf</code> to create a new SQLite connection by providing the database file path.</p> <pre><code>import practicuscore as prt\n\n_name = \"New SQLite Connection\"\n_conn_conf = prt.connections.SqLiteConnConf(file_path=\"/home/ubuntu/samples/chinook.db\")\n\nnew_conn_uuid = prt.connections.create(name=_name, conn_conf=_conn_conf)\nprint(new_conn_uuid)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-connections/#retrieving-a-specific-connection","title":"Retrieving a Specific Connection","text":"<p>Retrieve an existing connection by UUID or name to reuse previously configured settings without reconfiguring them each time.</p> <pre><code>import practicuscore as prt\n\n_conn_id_or_name = None\nassert _conn_id_or_name is not None\n\nconn = prt.connections.get(_conn_id_or_name)\nprint(conn)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-connections/#updating-an-existing-connection","title":"Updating an Existing Connection","text":"<p>After retrieving a connection, you can update its configuration\u2014changing  file paths, credentials, endpoints, or its name. Provide <code>_new_conn_conf</code>  and <code>_updated_name</code> to apply the changes.</p> <pre><code>import practicuscore as prt\n\n_conn_id_or_name = None\nassert _conn_id_or_name is not None\nconn = prt.connections.get(_conn_id_or_name)\n\n_new_conn_conf = None  # prt.connections.SqLiteConnConf(file_path=\"/home/ubuntu/samples/chinook.db\")\n_updated_name = None\nassert _new_conn_conf and _updated_name\n\nprt.connections.update(conn_uuid=conn.uuid, name=_updated_name, conn_conf=_new_conn_conf)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-connections/#data-upload-to-s3-compatible-storage","title":"Data Upload to S3-Compatible Storage","text":"<p>Upload local data to S3-compatible storage (e.g., Amazon S3, MinIO) using the Practicus SDK. Provide your credentials, region, endpoints, and prefixes as needed. This approach simplifies managing datasets remotely, ensures reproducibility, and makes sharing data easier.</p> <p>Set the parameters below and run the code to transfer files.</p> <pre><code>import practicuscore as prt\n\n_aws_access_key_id = None   # AWS Access Key ID or compatible service key\n_aws_secret_access_key = None  # AWS Secret Access Key or compatible service secret\n_bucket = None  # The name of your target bucket, e.g. \"my-data-bucket\"\n\n# Ensure that essential parameters are provided\nassert _aws_access_key_id and _aws_secret_access_key and _bucket\n\n_aws_session_token = None  # (Optional) AWS session token\n_aws_region = None         # (Optional) AWS region\n_endpoint_url = None       # (Optional) S3-compatible endpoint (e.g., MinIO)\n\n_prefix = None  # (Optional) Prefix for organizing objects within the bucket\n_folder_path = None  # The local folder path with files to upload\n_source_path_to_cut = None  # (Optional) Remove a leading folder path portion from object keys\n\n# Ensure the folder path is provided\nassert _folder_path\n\n_upload_conf = prt.connections.UploadS3Conf(\n    bucket=_bucket,\n    prefix=_prefix,\n    folder_path=_folder_path,\n    source_path_to_cut=_source_path_to_cut,\n    aws_access_key_id=_aws_access_key_id,\n    aws_secret_access_key=_aws_secret_access_key\n)\n\nprt.connections.upload_to_s3(_upload_conf)\n</code></pre> <p>Previous: Automated Code Quality | Next: Integrate Git</p>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/","title":"Practicus AI Data Catalog","text":"<p>Practicus AI provides a Data Catalog where you, or an administrator can save data source connection information. </p> <p>Data sources can be Data Lakes, Object Storage (E.g. S3), Data Warehouses (E.g. Snowflake), Databases (e.g. Oracle) ...</p> <p>Data catalog info does not include details like the actual SQL queries to run, S3 keys to read etc., but just the info such as host address, port (if needed), user name, password etc. You can think of them as a \"connection string\" in most programming languages. </p>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#practicus-ai-data-catalog_1","title":"Practicus AI Data Catalog","text":"<p>Practicus AI provides a Data Catalog where you, or an admininstrator can save data source connection information. </p> <p>Data sources can be Data Lakes, Object Storage (E.g. S3), Data Warehouses (E.g. Snowflake), Databases (e.g. Oracle) ...</p> <p>Data catalog info does not include details like the actual SQL queries to run, S3 keys to read etc., but just the info such as host address, port (if needed), user name, password etc. You can think of them as a \"connection string\" in most programming languages. </p> <pre><code>import practicuscore as prt\n\n# Connections are saved under a Practicus AI region\nregion = prt.current_region()\n\n# You can also connect to a remote region instead of the default one\n# region = prt.regions.get_region(..)\n</code></pre> <pre><code># Let's get connections that we have access to \n# If a connection is missing, please ask your admin to be granted access,\n# OR, create new connections using the Practicus AI App or SDK\nconnections = region.connection_list\n\nif len(connections) == 0:\n    raise ConnectionError(\n        \"You or an admin has not defined any connections yet. \"\n        \"This notebook will not be meaningful..\")\n</code></pre> <pre><code># Let's view our connections as a Pandas DF for convenience\nconnections.to_pandas()\n</code></pre> <pre><code># Lets view the first connection\nfirst_connection = connections[0]\nfirst_connection\n</code></pre> <pre><code># Is the data source read-only? \nif first_connection.can_write:\n    print(\"You can read from, and write to this data source.\")\nelse:\n    print(\"Data source is read-only. You cannot write to this data-source.\")\n    # Note: read-only data sources are created by Practicus AI admins \n    # and shared with users or user groups using Management Console.\n</code></pre> <pre><code># You can search a connection using it's uuid\nprint(\"Searching with connection uuid:\", first_connection.uuid)\nfound_connection = region.get_connection(first_connection.uuid)\nprint(\"Found:\", found_connection)\n</code></pre> <pre><code># You can also search using the connection name.\n# Please note that connection names can be updated later,\n#   and they are not unique in the Data Catalog.\n# Please prefer to search using a connection uuid for production deployments.\nprint(\"Searching with connection name:\", first_connection.name)\nfound_connection = region.get_connection(first_connection.name)\nprint(\"Found:\", found_connection)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#deep-dive-into-connections","title":"Deep dive into connections","text":"<p>There are multiple ways to load data into a Practicus AI process. </p> <p>Lets' start with the simplest, just using a dictionary, and then we will discuss other options including the Data Catalog.</p>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#loading-data-without-the-data-catalog","title":"Loading data without the data catalog","text":"<p>This is the simplest option and does not use a central data catalog to store connections. If you have the database credentials, you can read from that database.</p> <pre><code># Let's get a worker to use, one that you are already working on, or a remote one.\ntry:\n    worker = region.get_local_worker()\nexcept: \n    workers = region.worker_list\n    if len(workers) == 0:\n        raise ConnectionError(\n            \"Please run this code on a Practicus AI worker, or have at least one active worker\") \n    worker = workers[0]\n</code></pre> <pre><code># Let's load using the sample SQLLite DB that comes pre-installed with Practicus AI\nsql_query = \"\"\"\n  select artists.Name, albums.Title \n  from artists, albums \n  where artists.ArtistId = albums.ArtistId \n  limit 1000\n\"\"\"\n\n# Let's configure a connection\nconn_conf_dict = {\n    \"connection_type\": \"SQLITE\",\n    \"file_path\": \"/home/ubuntu/samples/chinook.db\",\n    \"sql_query\": sql_query,\n}\n\nproc = worker.load(conn_conf_dict)\nproc.show_head()\nproc.kill()\n</code></pre> <pre><code># Connection configuration can be a json\nimport json\n\nconn_conf_json = json.dumps(conn_conf_dict)\n\nproc = worker.load(conn_conf_json)\nproc.show_head()\nproc.kill()\n</code></pre> <pre><code># Connection configuration can be path to a json file\nwith open(\"my_conn_conf.json\", \"wt\") as f:\n    f.write(conn_conf_json)\n\nproc = worker.load(\"my_conn_conf.json\")\nproc.show_head()\nproc.kill()\n\nimport os \nos.remove(\"my_conn_conf.json\")\n</code></pre> <pre><code># You can use the appropriate conn conf class, \n#   which can offer some benefits such as intellisense in Jupyter or other IDE.\n# The below will use an Oracle Connection Configuration Class\n\nfrom practicuscore.api_base import OracleConnConf, PRTValidator\n\noracle_conn_conf = OracleConnConf(\n    db_host=\"my.oracle.db.address\",\n    service_name=\"my_service\",\n    sid=\"my_sid\",\n    user=\"alice\",\n    password=\"in-wonderland\",\n    sql_query=\"select * from my_table\",\n\n    # Wrong port !!\n    db_port=100_000,\n)\n\n# We deliberately entered the wrong Oracle port. Let's validate, and fail\nfield_name, issue = PRTValidator.validate(oracle_conn_conf)\nif issue:\n    print(f\"'{field_name}' field has an issue: {issue}\")\n# Will print:\n# 'db_port' field has an issue: Port must be between 1 and 65,535\n\n# With the right Oracle db connection info, you would be able to load\n# proc = worker.load(oracle_conn_conf)\n# df = proc.get_df_copy()\n</code></pre> <pre><code># Practicus AI conn conf objects are easy to convert to a dictionary\nprint(\"Oracle conn dict:\", oracle_conn_conf.model_dump())\n</code></pre> <pre><code># Or to json\noracle_conn_conf_json = oracle_conn_conf.model_dump_json()\nprint(\"Oracle conn json:\", oracle_conn_conf_json)\n</code></pre> <pre><code># And, vice versa. from a dict or json back to class the instance\n# This can be very convenient, e.g. save to a file, including the SQL Query,\n# and reuse later, e.g. scheduled every night in Airflow.\nreloaded_oracle_conn_conf = OracleConnConf.model_validate_json(oracle_conn_conf_json)\ntype(reloaded_oracle_conn_conf)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#loading-using-the-data-catalog","title":"Loading using the Data Catalog","text":"<p>When you read a connection from Practicus AI Data Catalog, you also download it's \"base\" connection configuration class.</p> <p>But instead of the database credentials like user name, password etc. you will load a \"reference\" (uuid) to the Data Catalog. </p> <p>Practicus AI workers will load data intelligently;  - if there are database credentials in the conn conf, these will be used. - Or else, the worker will sue your credentials to \"fetch\" connection credentials from the Data Catalog, and by using the reference.</p> <pre><code># Accessing the conn_conf will print the json\nconn_conf_object = first_connection.conn_conf\nconn_conf_object\n</code></pre> <p>In most cases, accessing the \"conn_conf\" of a connection that you load from the Data Catalog will just have:</p> <ul> <li>Connection type, e.g. Oracle</li> <li>And the unique reference uuid</li> </ul> <p>For relational DBs, you can just supply a SQL query and you're good to go. Practicus AI will take care of the rest.</p> <pre><code># The conn_conf is actually a child class of ConnConf\ntype(conn_conf_object)\n# e.g. OracleConnConf\n</code></pre> <pre><code># Let's make sure we use a connection type that can run a SQL statement, \n#   which will be a child class of Relational DB class RelationalConnConf. \nfrom practicuscore.api_base import RelationalConnConf\n\nif not isinstance(conn_conf_object, RelationalConnConf):\n    raise ConnectionError(\"The rest of the notebook needs a conn type that can run SQL\")\n</code></pre> <pre><code># With the below code, you can see that the conn conf class has many advanced properties\n# dir(conn_conf_object)\n# We just need to use sql_query property\nconn_conf_object.sql_query = \"Select * from Table\"\n</code></pre> <pre><code># In addition to a dict, json or json file, we can also use a conn conf object to read data\n# proc = worker.load(conn_conf_object)\n</code></pre>"},{"location":"technical-tutorial/how-to/work-with-data-catalog/#summary","title":"Summary","text":"<p>Let's summarize some of the common options to load data.</p> <pre><code>region = prt.current_region()\n\nprint(\"My connections:\", region.connection_list)\n\npostgres_conn = region.get_connection(\"My Team/My Postgres\")\nif postgres_conn:\n    postgres_conn.sql_query = \"Select * from Table\"\n    proc = worker.load(postgres_conn)\n\nredshift_conn = region.get_connection(\"Some Department/Some Project/Some Redshift\")\nif redshift_conn:\n    conn_dict = redshift_conn.model_dump()\n    conn_dict[\"sql_query\"] = \"Select * from Table\"\n    proc = worker.load(redshift_conn)\n\nconn_with_credentials = {\n    \"connection_type\": \"SNOWFLAKE\",\n    \"db_name\": \"my.snowflake.com\",\n    # add warehouse etc. \n    \"user\": \"bob\",\n    \"password\": \"super-secret\",\n    \"sql_query\": \"Select * from Table\"\n}\n# proc = worker.load(conn_with_credentials)\n\n# And lastly, which can include all the DB credentials + SQL\n#  or a reference to the data catalog + SQL\n# proc = worker.load(\"path/to/my_conn.json\")\n</code></pre> <p>Previous: Work With Processes | Next: Use Custom Metrics</p>"},{"location":"technical-tutorial/how-to/work-with-processes/","title":"Practicus AI Processes","text":"<p>Practicus AI Processes live in Practicus AI Workers and you can create, work with and kill them as the below.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>worker = prt.get_local_worker()\n</code></pre> <pre><code># We can also create a remote worker by running the below, either from your computer,\n# or from an existing Practicus AI worker.\nworker_config = {\n    \"worker_size\": \"Small\",\n    \"worker_image\": \"practicus\",\n}\n# worker = prt.create_worker(worker_config) \n</code></pre> <pre><code># Processes start by loading some data.  \nworker_file_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/ice_cream.csv\"\n}\n\nproc_1 = worker.load(worker_file_conn)\nproc_1.show_head()\n</code></pre> <pre><code># Let's create another process\nsqlite_conn = {\n    \"connection_type\": \"SQLITE\",\n    \"file_path\": \"/home/ubuntu/samples/chinook.db\",\n    \"sql_query\": \"select * from artists\"\n}\n\nproc_2 = worker.load(sqlite_conn)\nproc_2.show_head()\n</code></pre> <pre><code># Let's iterate over processes\nfor proc in worker.proc_list:\n    print(f\"Process id: {proc.proc_id} connection: {proc.conn_info}\")\n</code></pre> <pre><code># Converting the proc_list into string will give you a csv \nprint(worker.proc_list)\n</code></pre> <pre><code># You can also get current processes as a pandas DataFrame for convenience\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># Simply accessing the proc_list will also print a formatted table string \nworker.proc_list\n</code></pre> <pre><code>first_proc = worker.proc_list[0]\n# Accessing a process object will print it's details\nfirst_proc\n</code></pre> <pre><code># You can ask a process to kill itself and free resources\nproc_1.kill()\n</code></pre> <pre><code># It's parent worker will be updated\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># You can also ask a worker to kill one of it's processes\nremaining_proc_id = worker.proc_list[0].proc_id\nworker.kill_proc(remaining_proc_id)\n</code></pre> <pre><code># Will return empty\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># You can also ask a worker to kill all its processes to free up resources faster\nworker.kill_all_procs()\n</code></pre> <p>Previous: Use Security Tokens | Next: Work With Data Catalog</p>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/","title":"Executing Notebooks","text":""},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#automating-notebook-execution","title":"Automating Notebook Execution","text":"<p>Practicus AI allows you to execute notebooks in an automated fashion, which can be particularly useful for <code>testing</code> and  <code>automated workflows</code>.</p>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#executing-notebooks-from-practicus-ai","title":"Executing Notebooks from Practicus AI","text":""},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#1-create-a-notebook-file","title":"1) Create a Notebook File","text":"<p>Begin by creating a file named <code>sample_notebook.ipynb</code>.</p>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#2-add-a-parameters-cell-optional","title":"2) Add a Parameters Cell (Optional)","text":"<p>Add a cell with the following code to define parameters that can be updated dynamically later: <pre><code># Notebook parameters - can be dynamically updated later\nsome_param = 1\nsome_other_param = 2\n</code></pre></p> <p>For Jupyter: 1. Select the newly added cell. 2. In the top-right section of JupyterLab, open the property inspector. 3. Click on <code>Add Tag</code> and enter <code>parameters</code>, then click <code>+</code>. 4. The cell now has the <code>parameters</code> tag.</p> <p></p> <p>For VS Code: 1. Select the cell you just created. 2. Click the <code>...</code> in the cell's upper-right corner. 3. Add a cell tag named <code>parameters</code>. 4. The cell is now tagged.</p> <p></p>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#3-add-your-code-as-usual","title":"3) Add Your Code as Usual","text":"<p>Continue writing code in the notebook as you normally would. For example, add the following code to print messages and validate your parameters:</p> <pre><code>print(\"Starting to run sample notebook\")\n\nif some_param &lt;= 0:\n    raise ValueError(\"some_param must be &gt; 0\")\nelse:\n    print(\"some_param value is acceptable:\", some_param)\n\nprint(\"Finished running sample notebook\")\n</code></pre>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#4-execute-the-notebook","title":"4) Execute the Notebook","text":"<p>Once you\u2019ve set up the notebook and its parameters, you can execute it using Practicus AI\u2019s automated run capabilities.</p> <pre><code>import practicuscore as prt\n\n# This will run just fine, \n# and save the resulting output to sample_notebook_output.ipynb\nprt.notebooks.execute_notebook(\"sample_notebook\")\n</code></pre> <pre><code># The below *will FAIL* since some_param cannot be 0\nprt.notebooks.execute_notebook(\n    \"sample_notebook\",\n    parameters={\n        \"some_param\": 0 \n    }\n)\n</code></pre>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#advanced-features","title":"Advanced features","text":"<pre><code># Advanced Notebook automation parameters\ndefault_output_folder=\"~/tests\"  # If none, writes notebook output to same folder as notebook\ndefault_failed_output_folder=\"~/tests_failed\"  # If not none, collects failed notebook results \n\n# By calling configure you can save notebook results to central location\n# Please note that you can do this to a shared/ folder daily where all of our members have access to\nprt.notebooks.configure(\n    default_output_folder=default_output_folder,\n    default_failed_output_folder=default_failed_output_folder,\n    add_time_stamp_to_output=True,\n)\n\n# This will work\nprt.notebooks.execute_notebook(\"sample_notebook\")\n\n# This will fail but does not stop the execution of the notebook\nprt.notebooks.execute_notebook(\n    \"sample_notebook\",\n    parameters={\n        \"some_param\": 0 \n    }\n)\n\n# Calling validate_history() will raise an exception IF any of the previous notebooks failed\n# This is useful to have a primary \"orchestration\" notebook that executes other child notebooks,\n# And then finally fails itself if there was a mistake. \n# You can then report the result, essentially creating a final report. \nprt.notebooks.validate_history()\n\n# You can view the passed and failed execution results in \n# ~/tests and ~/tests_failed with a time stamp (optional)\n</code></pre>"},{"location":"technical-tutorial/how-to/automate-notebooks/executing-notebooks/#executing-notebooks-from-terminal","title":"Executing notebooks from terminal","text":"<p>You can run the below command to execute a notebook from the terminal or an .sh script.</p> <pre><code>prtcli execute-notebook -p notebook=my-notebook.ipynb \n</code></pre> <p>Previous: Build Custom Images | Next: Improve Code Quality &gt; Automated Code Quality</p>"},{"location":"technical-tutorial/how-to/improve-code-quality/automated-code-quality/","title":"Automated Code Quality","text":"<p>You can check for code quality issues, fix and format your files automatically.</p> <pre><code>import practicuscore as prt\n\n# Let's check for code quality issues in this folder\nsuccess = prt.quality.check()\n</code></pre> <pre><code># Some issues like 'unused imports' can be fixed automatically\nif not success:\n    prt.quality.check(fix=True)\n</code></pre> <pre><code># Let's also format code to improve quality and readability\nprt.quality.format()\n</code></pre> <pre><code># Still errors? open bad_code.py and delete the wrong code\n# Final check, this should pass\nprt.quality.check()\n</code></pre> <pre><code># We can add a \"No QA\" tag to ignore checking a certain type of issue\n# E.g. to ignore an unused imports for a line of code\nimport pandas  # noqa: F401\n# To ignore all QA checks (not recommended)\nimport numpy  # noqa\n</code></pre>"},{"location":"technical-tutorial/how-to/improve-code-quality/automated-code-quality/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/how-to/improve-code-quality/automated-code-quality/#bad_codepy","title":"bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is a code formatting issue.\")\n</code></pre> <p>Previous: Executing Notebooks | Next: Work With Connections</p>"},{"location":"technical-tutorial/modeling/introduction/","title":"Practicus AI Model Hosting Foundations","text":"<p>Practicus AI provides a robust model deployment infrastructure that allows you to organize, host, and manage machine learning models with scalability, security, and flexibility.</p>"},{"location":"technical-tutorial/modeling/introduction/#model-prefixes","title":"Model Prefixes","text":"<p>Models in Practicus AI are logically grouped using model prefixes. A model prefix serves as a namespace, often reflecting organizational or access boundaries. For example:</p> <pre><code>https://practicus.company.com/models/marketing/churn/v6/\n</code></pre> <p>In this URL:</p> <ul> <li><code>models/marketing</code> is the model prefix</li> <li><code>churn</code> is the model name</li> <li><code>v6</code> is an optional version</li> </ul> <p>These prefixes can also be tied to security and access controls, ensuring that only authorized user groups (e.g., an LDAP group for finance) can access certain prefixes and the models under them.</p>"},{"location":"technical-tutorial/modeling/introduction/#model-deployments","title":"Model Deployments","text":"<p>While model prefixes handle logical grouping and naming, model deployments represent the physical Kubernetes deployments that host these models. Model deployments:</p> <ul> <li>Run on Kubernetes</li> <li>Can host multiple models and model versions</li> <li>Part of an integrated service mesh and load balancing</li> <li>Support advanced techniques like A/B testing between model versions</li> <li>Can be auto-scaled</li> </ul> <p>This architecture enables flexible model hosting where you can deploy multiple versions of a model under the same prefix but on different deployments, ensuring isolation, performance optimization, and continuous integration and delivery of models.</p> <p>You can use the Practicus AI App, CLI, or this SDK to list, navigate, and interact with model prefixes, deployments, models, and their versions.</p> <pre><code>import practicuscore as prt\n\n# Connect to the current region\nregion = prt.current_region()\n</code></pre> <pre><code># List the model prefixes defined by your admin and accessible to you\nregion.model_prefix_list.to_pandas()\n</code></pre> <pre><code># List the model deployments defined by your admin and accessible to you\nregion.model_deployment_list.to_pandas()\n</code></pre>"},{"location":"technical-tutorial/modeling/introduction/#re-creating-a-model-deployment","title":"Re-creating a Model Deployment","text":"<p>If you have admin privileges, you can re-create a model deployment. Re-creating involves deleting and then re-launching the deployment using the existing configuration. This is helpful in development or testing scenarios to ensure a fresh start. However, it\u2019s not typically advised in production environments since model versions hosted by that deployment will be temporarily unavailable during the process.</p> <p>Note: You must have admin privileges for this operation.</p> <pre><code>model_deployment_key = \"development-model-deployment\"\nregion.recreate_model_deployment(model_deployment_key)\n</code></pre> <p>Previous: Add-Ons | Next: Sample Modeling &gt; Build And Deploy</p>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/","title":"Building and deploying an XGBoost model","text":"<p>In this example, we will walk through the process of building a simple XGBoost model using a small dataset. We\u2019ll begin by training and saving the model, and then demonstrate how to deploy it as a REST API endpoint. Finally, we\u2019ll make some predictions by calling the deployed API.</p> <p>By the end of this example, you will have a clear understanding of how to:</p> <ol> <li>Prepare and train a basic XGBoost model.</li> <li>Save the trained model to a file.</li> <li>Deploy the model as a simple API service.</li> <li>Make predictions by sending requests to the API.</li> </ol> <p>This approach can be extended and adapted to more complex models and larger systems, giving you a foundation for building scalable machine learning services.</p>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#sample-xgboost-model","title":"Sample XGBoost model","text":"<p>Let's build a simple XGBoost model</p> <pre><code>import pandas as pd\nfrom xgboost import XGBRegressor\n\n# Load the ice cream dataset that come pre-installed in Workers\ndf = pd.read_csv(\"/home/ubuntu/samples/ice_cream.csv\")\n\n# Separate features and target\nX = df[[\"Temperature\"]]\ny = df[\"Revenue\"]\n\n# Create and train the XGBoost regressor\nmodel = XGBRegressor(n_estimators=50)\nmodel.fit(X, y)\n\n# Save the model using the recommended XGBoost .ubj format\nmodel.save_model(\"model.ubj\")\n\nprint(\"Model saved as model.ubj\")\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#how-to-query-for-deployments-and-prefixes","title":"How to Query for Deployments and Prefixes","text":"<p>If you are unsure about which model deployment (the underlying infrastructure) or which logical address group (prefix) to use, you can run the code below to dynamically query the available options.</p> <p>If you already have the required information, you can define it directly as shown here and skip the query step:</p> <pre><code>deployment_key = \"some-deployment\"\nprefix = \"models\"\n</code></pre> <p>The code below demonstrates how to programmatically identify the first available model deployment system and model prefix. These values are then used to construct a URL for deploying and accessing a model\u2019s REST API.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\n\n# Identify the first available model deployment system\nif len(region.model_deployment_list) == 0:\n    raise SystemError(\n        \"No model deployment systems are available. \"\n        \"Please contact your system administrator.\"\n    )\nelif len(region.model_deployment_list) &gt; 1:\n    print(\"Multiple model deployment systems found. Using the first one.\")\n\nmodel_deployment = region.model_deployment_list[0]\ndeployment_key = model_deployment.key\n\n# Identify the first available model prefix\nif len(region.model_prefix_list) == 0:\n    raise SystemError(\n        \"No model prefixes are available. \"\n        \"Please contact your system administrator.\"\n    )\nelif len(region.model_prefix_list) &gt; 1:\n    print(\"Multiple model prefixes found. Using the first one.\")\n\nprefix = region.model_prefix_list[0].key\n\nmodel_name = \"my-xgboost-model\"\nmodel_dir = None  # Use the current directory by default\n\n# All Practicus AI model APIs follow this URL convention:\nexpected_api_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Note: Ensure the URL ends with a slash (/) to support correct routing.\n\nprint(\"Expected Model REST API URL:\", expected_api_url)\nprint(\"Using model deployment:\", deployment_key)\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#modelpy","title":"model.py","text":"<p>Review the <code>model.py</code> file to see how the XGBoost model is integrated and consumed within the environment.</p>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#deploy-the-model-as-an-api","title":"Deploy the model as an API","text":"<pre><code># This function can be called multiple times to deploy additional versions.\napi_url, api_version_url, api_meta_url = prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=model_dir\n)\n</code></pre> <pre><code>print(\"Which model API URL to use:\")\nprint(\"If you prefer the system admin dynamically route\")\nprint(\"  between model versions (recommended), use the below:\")\nprint(api_url)\nprint(\"If you prefer to use exactly this version, use the below:\")\nprint(api_version_url)\nprint(\"If you prefer to get the metadata of this version, use the below:\")\nprint(api_meta_url)\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#making-predictions-using-the-model-api","title":"Making predictions using the model API","text":"<pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view samples in the extras section\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code># Now let's consume the Rest API to make the prediction\nimport requests \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df[\"Temperature\"].to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text} - {r.headers}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\ndisplay(pred_df)\n</code></pre>"},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/modeling/sample-modeling/build-and-deploy/#modelpy_1","title":"model.py","text":"<pre><code>import os\nimport pandas as pd\nfrom xgboost import XGBRegressor\n\nmodel = None\n\n\nasync def init(*args, **kwargs):\n    print(\"Initializing model\")\n    global model\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.ubj')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model = XGBRegressor()\n    model.load_model(model_file)\n\n\nasync def predict(df, *args, **kwargs):\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    X = df[[\"Temperature\"]]\n\n    # Generate predictions\n    predictions = model.predict(X)\n\n    # Return predictions as a new DataFrame\n    return pd.DataFrame({\"predictions\": predictions})\n</code></pre> <p>Previous: Introduction | Next: Workflows &gt; Introduction</p>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/","title":"Automated Git Sync for Workers","text":"<p>This example demonstrates how to securely configure Git in Practicus AI, ensuring you can clone or pull a repository both locally (within a running notebook) and automatically when a worker starts.</p>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#overview","title":"Overview","text":"<ol> <li>Create a Personal Access Token (PAT) on your chosen Git platform.</li> <li>Store the PAT as a Personal Secret in the Practicus AI Vault.</li> <li>Configure and Sync the Repository via the Practicus AI Git Helper.</li> <li>Auto-Clone a Repo on Worker Startup to have your code ready immediately when the worker launches.</li> </ol>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#step-1-create-a-personal-access-token","title":"Step 1: Create a Personal Access Token","text":"<p>Log into your Git system (e.g., Practicus AI Git, GitHub, GitLab, ..) and generate a personal access token. Make sure to include any necessary permissions (e.g., read/write to repositories if needed).</p>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#step-2-store-the-token-as-a-personal-secret","title":"Step 2: Store the Token as a Personal Secret","text":"<p>Store the new token in the Practicus AI Vault to keep it secure and avoid hardcoding credentials in your code.</p> <pre><code>import practicuscore as prt\nfrom getpass import getpass\n\n# Define a name for the stored secret\ngit_secret_name = \"MY_GIT_SECRET\"\n\n# Prompt for your personal access token\nkey = getpass(\"Enter your Git personal access token:\")\n\n# Store or update the token in the Practicus AI Vault\nprt.vault.create_or_update_secret(name=git_secret_name, key=key)\nprint(f\"Successfully saved secret '{git_secret_name}' in Practicus AI Vault.\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#step-3-configure-git-and-synchronize-the-repository","title":"Step 3: Configure Git and Synchronize the Repository","text":"<p>Specify details such as the repository URL, which secret to use, and optional parameters like branch name, fetch depth, or sparse checkout folders.</p> <pre><code>import os\n\nremote_url = \"https://git.practicus.my-company.com/myuser/myrepo.git\"  # Example repository URL\n\n# Create a GitConfig object\ngit_config = prt.GitConfig(\n    remote_url=remote_url,   # Repository to clone or pull\n    secret_name=git_secret_name,  # Name of the secret containing the PAT\n    # Optional configurations:\n    # username=\"your-username\",  # If the Git username differs from your Practicus AI username\n    # save_secret=True,\n    # local_path=\"~/some-path-on-worker\",\n    # branch=\"main\",\n    # sparse_checkout_folders=[\"folder1\", \"folder2\"],\n    # fetch_depth=1,\n)\n\n# For demonstration, retrieve the token in this local notebook (avoid printing it!)\nos.environ[git_secret_name], age = prt.vault.get_secret(git_secret_name)\nprint(f\"Retrieved secret '{git_secret_name}' which is {age} days old.\")\n\n# Sync the repository locally in this current environment\nprt.git.sync_repo(git_config)\nprint(\"Repository synced locally.\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#step-4-auto-clone-or-pull-the-repo-on-worker-startup","title":"Step 4: Auto-Clone or Pull the Repo on Worker Startup","text":"<p>Using <code>git_configs</code> in the <code>WorkerConfig</code>, you can automatically clone or pull the repository when the worker is created. This ensures your environment has the code ready immediately.</p> <pre><code># Configure a new worker to automatically clone your repository\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    personal_secrets=[git_secret_name],\n    git_configs=[git_config],\n)\n\n# Create and start the worker\nworker = prt.create_worker(worker_config)\n\n# Open a notebook on the new worker\nworker.open_notebook()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-git-sync/#cleanup","title":"Cleanup","text":"<p>Terminate the worker when you are finished.</p> <pre><code>worker.terminate()\n</code></pre> <p>Previous: Automated Worker Init | Next: Git Integrated CICD</p>"},{"location":"technical-tutorial/unified-devops/automated-worker-init/","title":"Automated Worker Initialization with Secrets","text":"<p>In this example, we'll show how to pass both standard environment variables and secure secrets from Practicus AI vault (personal and shared) to a Practicus AI Worker. This is especially useful for setting up required configurations during job startup without hardcoding sensitive information.</p>"},{"location":"technical-tutorial/unified-devops/automated-worker-init/#overview","title":"Overview","text":"<ul> <li>Environment Variables: Set standard OS-level environment variables to be available inside the worker.</li> <li>Personal Secrets: These secrets are private to you and will be injected as environment variables.</li> <li>Shared Secrets: These secrets can be shared across projects or team members and also appear as environment variables in the worker.</li> <li>Worker Interaction: We\u2019ll launch a worker and open a notebook on that worker. Within the worker\u2019s terminal, we can verify that the environment variables and secrets are set correctly (though remember never to log actual secret values in production scenarios!).</li> </ul> <pre><code>import practicuscore as prt\n\n# Configure the worker\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    env_variables={\n        \"MY_FIRST_ENV\": \"123\",       # Regular environment variable\n        \"MY_SECOND_ENV\": 123          # Another environment variable\n    },\n    # Personal and shared secrets to be passed as environment variables\n    personal_secrets=[\"MY_PERSONAL_SECRET_1\"],\n    shared_secrets=[\"SHARED_SECRET_1\"],\n)\n\n# Create and start the worker\nworker = prt.create_worker(worker_config)\n\n# Open a Jupyter notebook on the new worker\nworker.open_notebook()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/automated-worker-init/#verifying-environment-variables-and-secrets","title":"Verifying Environment Variables and Secrets","text":"<p>Inside the worker\u2019s terminal (not here), you can run: <pre><code>echo \"MY_FIRST_ENV is: $MY_FIRST_ENV\"\necho \"MY_SECOND_ENV is: $MY_SECOND_ENV\"\n# Avoid printing actual secrets in plaintext!\necho \"MY_PERSONAL_SECRET_1 length:\" $(echo $MY_PERSONAL_SECRET_1 | wc -m)\necho \"SHARED_SECRET_1 length:\" $(echo $SHARED_SECRET_1 | wc -m)\n</code></pre></p> <p>The above commands will help confirm that the environment variables and secrets have been successfully injected into the worker's environment. Note that we never display the actual secret values directly.</p> <pre><code># Terminate the worker when you're finished.\nworker.terminate()\n</code></pre> <p>Previous: Secrets With Vault | Next: Automated Git Sync</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/","title":"Building and Using Custom Container Images","text":"<p>In this example, we'll demonstrate how to build custom container images using the Practicus AI platform. You\u2019ll see how to:</p> <ol> <li>Start a Worker with container-building capabilities.</li> <li>Create and push a custom container image to a private registry.</li> <li>Use your custom container to run new Practicus AI Workers.</li> </ol> <p>We\u2019ll also cover:</p> <ul> <li>Configuring the builder to allocate a percentage of your Worker\u2019s resources.</li> <li>Pushing images to the Practicus AI container registry or other registries.</li> <li>Granting security permissions to allow custom images.</li> <li>Advanced topics like privileged builds, custom builder images, and separating credentials for pulling vs. pushing.</li> </ul> <p>If you\u2019re using Practicus AI CI/CD (as explained in previous examples), you can fully automate these steps\u2014building and pushing images whenever changes are pushed to your Git repository.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#step-1-start-a-worker-with-image-building-capabilities","title":"Step 1: Start a Worker with Image-Building Capabilities","text":"<p>First, we'll create a Practicus AI Worker that can build container images. This Worker uses two containers:</p> <ol> <li>Primary Container (Ubuntu Linux): Contains your main environment.</li> <li>Builder Container (Alpine Linux): Builds container images under the hood.</li> </ol> <p>By setting <code>builder=True</code> in <code>ImageConfig</code>, you opt to start this secondary container. You can also specify how much of your Worker\u2019s CPU/RAM/GPU the builder can use.</p> <pre><code>import practicuscore as prt\n\n# Configure a Worker that can build container images.\nimage_config = prt.ImageConfig(\n    builder=True,  # Enables the builder container.\n    # Optional: allocates 60% of CPU/RAM/GPU to the builder.\n    # builder_capacity=60,\n    # Optional: use HTTP instead of HTTPS if your on-prem registry doesn't support HTTPS\n    # insecure_registries=\"my-registry.practicus.my-company.com\",\n)\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus-minimal\",\n    worker_size=\"Small-restricted\",\n    image_config=image_config,\n)\n\n# Create and open the new Worker.\nbuilder_worker = prt.create_worker(worker_config)\nbuilder_worker.open_notebook()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#step-2-build-and-push-your-custom-image","title":"Step 2: Build and Push Your Custom Image","text":"<p>Within the <code>builder_worker</code> notebook, you'll have access to the <code>~/build</code> directory\u2014shared by the primary container and the builder container. Follow these steps to build your image:</p> <ol> <li>Add files (e.g., application code, Dockerfile, config files) to <code>~/build</code>.</li> <li>Run <code>build_image</code> with any necessary credentials.</li> <li>Optionally push the image to your registry.</li> </ol> <p>Below, we create a small text file and a simple Dockerfile, then build and push the image.</p> <pre><code># Create a text file in ~/build\nwith open(\"/home/ubuntu/build/hello.txt\", \"wt\") as f:\n    f.write(\"Hello from the custom container image!\")\n</code></pre> <pre><code># Create a Dockerfile in ~/build\ndockerfile_content = \"\"\"\n# Use a base image from your private registry or a public source\nFROM my-registry.practicus.my-company.com/practicusai/practicus-minimal:24.8.3\n\n# Copy in our text file\nCOPY hello.txt /home/ubuntu/hello.txt\n\n# (Optional) Install packages via apt or pip if needed.\n# RUN sudo apt-get update &amp;&amp; sudo apt-get install -y &lt;packages&gt;\n# RUN pip install &lt;libraries&gt;\n\"\"\"\n\nwith open(\"/home/ubuntu/build/Dockerfile\", \"wt\") as f:\n    f.write(dockerfile_content)\n</code></pre> <pre><code># Build and push the image, run on a builder Worker.\nimport practicuscore as prt\n\nrepo_username = \"some-user-name\"\nrepo_password = \"some-token\"\n\nsuccess = prt.containers.build_image(\n    name=\"my-registry.practicus.my-company.com/demo/practicus-minimal-my\",\n    tag=\"24.8.3\",\n    push_image=True,\n    registry_credentials=[\n        (\"my-registry.practicus.my-company.com\", repo_username, repo_password),\n    ],\n    # Optional: use HTTP instead of HTTPS if you are using an on-prem registry and haven't installed certificates to the builder image.\n    # insecure_registry=True,\n)\n\nprint(\"Successful!\" if success else \"Failed..\")\n</code></pre> <p>prtcli build-container-image -p \\     name=\"my-registry.practicus.my-company.com/demo/practicus-minimal-my\" \\     tag=\"24.8.3\" \\     push_image=True \\     registry_credentials=\"[(\\\"my-registry.practicus.my-company.com\\\", \\\"my-robot-user\\\", \\\"token\\\")]\" \\     insecure_registry=True</p> <p>Once your image is successfully pushed, you can reference it in any future Practicus AI Worker by its full name, for example:</p> <pre><code>my-registry.practicus.my-company.com/demo/practicus-minimal-my:24.8.3\n</code></pre>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#step-3-use-the-custom-image","title":"Step 3: Use the Custom Image","text":"<p>Switch back to your original environment (the one where you created <code>builder_worker</code>). Now you can create a new Worker that runs on your freshly built image. Make sure your user or group has permissions to use custom container images. If you\u2019re an admin, you can grant this permission in the Practicus AI admin console under Infrastructure &gt; Container Images.</p> <p>Below is an example of how to run a Worker with the newly built image:</p> <pre><code>import practicuscore as prt\n\nimage_config = prt.ImageConfig(\n    repo_username=\"some-user-name\",\n    repo_password=\"some-token\"\n)\n\nworker_config = prt.WorkerConfig(\n    # If desired, specify a tag explicitly, \n    # worker_image=\"my-registry.practicus.my-company.com/demo/practicus-minimal-my:24.8.3\",\n    # or else, the current Practicus AI platform version is used if the tag is omitted.\n    worker_image=\"my-registry.practicus.my-company.com/demo/practicus-minimal-my\",\n    worker_size=\"Small-restricted\",\n    image_config=image_config,\n)\n\nworker_from_custom_image = prt.create_worker(worker_config)\nworker_from_custom_image.open_notebook()\n</code></pre> <p>With that, you have a Practicus AI Worker running your custom container image. You can install packages, train models, or run any workload you need.</p>"},{"location":"technical-tutorial/unified-devops/build-custom-images/#advanced-topics","title":"Advanced Topics","text":"<ul> <li> <p>Privileged vs. Restricted Builder: Practicus AI offers a restricted builder (<code>ghcr.io/practicusai/practicus-builder</code>) by default. If you need lower-level system access (e.g., building certain drivers), admins can enable a privileged builder (<code>ghcr.io/practicusai/practicus-builder-privileged</code>) via the Worker Size Security Context.</p> </li> <li> <p>Custom Builder Images: You can create a custom Alpine builder image (e.g., to install specialized dependencies or certificates) and set <code>custom_builder_url</code> in <code>ImageConfig</code>. This allows more flexibility for specialized builds.</p> </li> <li> <p>Pull vs. Push Credentials: If you need separate credentials for pulling a base image and pushing a built image, you can:</p> </li> <li>Build (and cache) the base image locally without pushing (<code>push_image=False</code>).</li> <li>Change your credentials.</li> <li> <p>Re-run the build with <code>push_image=True</code>.</p> </li> <li> <p>Integrating with CI/CD: You can include container builds in your Practicus AI CI/CD workflows. For instance, automatically build and push your image upon each commit by referencing these steps in your <code>.github/workflows/</code> YAML.</p> </li> <li> <p>Building with CLI: You can also build and push containers using the Practicus AI CLI, which can be helpful in automation scenarios.</p> </li> </ul> <p>Example CLI command: <pre><code>prtcli build-container-image -p \\\n    name=\"my-registry.practicus.my-company.com/my-project/my-image\" \\\n    tag=\"24.8.3\" \\\n    build_args=\"{\\\"ENV_VAR\\\": \\\"value\\\"}\" \\\n    registry_credentials=\"[(\\\"my-registry.practicus.my-company.com\\\", \\\"my-robot-user\\\", \\\"token\\\")]\" \\\n    insecure_registry=true\n</code></pre></p> <pre><code># Clean up\nbuilder_worker.terminate()\nworker_from_custom_image.terminate()\n</code></pre> <p>Previous: Git Integrated CICD | Next: How To &gt; Automate Notebooks &gt; Executing Notebooks</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/","title":"Git-Integrated CI/CD","text":"<p>Practicus AI offers CI/CD runners that are compatible with GitHub Actions workflows. These runners let you integrate code pushes, tests, and automated tasks seamlessly with Practicus AI\u2019s infrastructure\u2014even in air-gapped or private deployments.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#overview","title":"Overview","text":"<ol> <li>Add a Workflow Configuration: Create a <code>.github/workflows/</code> directory in your repository and add a YAML configuration to define your workflows.</li> <li>Specify a Runner: Use <code>runs-on: practicus-XX.X.X</code> to leverage Practicus AI runners.</li> <li>Add Secrets: Store Practicus AI credentials (like a refresh token) or other required secrets in your Git repository settings.</li> <li>Create CI/CD Task Scripts: Use Python scripts (or the Practicus AI CLI) to launch tasks (like spinning up workers or building containers) on Practicus AI.</li> </ol> <p>Below are some examples of how to set up and run these workflows.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#basic-workflow-example","title":"Basic Workflow Example","text":"<p>Create a workflow file (for example, <code>test.yaml</code>) inside <code>.github/workflows/</code> in your repository:</p> <pre><code># .github/workflows/test.yaml\nname: Test CICD actions\n\non:\n  - push\n\njobs:\n  Sample-Job:\n    runs-on: practicus-24.8.3\n    steps:\n      - name: Say Hello\n        run: echo \"Hello\"\n</code></pre> <p>Whenever you push code, this workflow will run on the Practicus AI runner. You can view logs and status on the repository\u2019s Actions page.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#using-practicus-ai-in-a-workflow","title":"Using Practicus AI in a Workflow","text":"<p>You can trigger Practicus AI workers, tasks, or container builds directly in your workflow. Let\u2019s walk through an example.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#step-1-obtain-a-practicus-ai-refresh-token","title":"Step 1: Obtain a Practicus AI Refresh Token","text":"<p>In Practicus AI, generate a refresh token for your account, then store it privately.</p> <pre><code>import practicuscore as prt\n\n# Retrieve your personal refresh token\nrefresh_token = prt.auth.get_refresh_token()\nprint(\"Your refresh token is:\", refresh_token)\nprint(\"Keep this token private and clear the cell/terminal output immediately!\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#step-2-save-the-token-as-a-repository-secret","title":"Step 2: Save the Token as a Repository Secret","text":"<p>Go to your Git repository settings (e.g., Settings &gt; Secrets &gt; Actions on GitHub) and add a new secret named <code>PRT_TOKEN</code>. Paste the refresh token you obtained above.</p> <p>Note: Do not store a short-lived access token. It will expire quickly, causing your CI/CD workflows to fail. Use the refresh token instead.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#step-3-create-a-task-file-in-the-repository","title":"Step 3: Create a Task File in the Repository","text":"<p>Below is a simple Python script (<code>cicd/start_worker.py</code>) that spins up a Practicus AI worker, runs a command, and terminates it.</p> <pre><code># cicd/start_worker.py\nimport practicuscore as prt\n\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    startup_script=\"\"\"\n    echo \"Hello from Practicus AI unified DevOps\"\n    \"\"\"\n)\n\nprint(\"Starting worker...\")\nworker = prt.create_worker(worker_config)\n\nprint(\"Terminating worker...\")\nworker.terminate()\n</code></pre>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#step-4-reference-the-task-file-in-your-workflow","title":"Step 4: Reference the Task File in Your Workflow","text":"<p>Add a workflow file (e.g., <code>.github/workflows/using_workers.yaml</code>) that sets the appropriate environment variables and runs your script.</p> <p><pre><code># .github/workflows/using_workers.yaml\nname: Using Practicus AI Workers\n\n# This action will run each time code is pushed to the repository\non:\n  - push\n\nenv:\n  # Update with your Practicus AI URL\n  PRT_URL: https://practicus.my-company.com\n  PRT_TOKEN: ${{ secrets.PRT_TOKEN }}\n\njobs:\n  Explore-PracticusAI-Actions:\n    runs-on: practicus-24.8.3\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Perform some task on a Practicus AI Worker\n        run: python cicd/start_worker.py\n\n      - name: View Practicus AI CLI help\n        run: prtcli --help\n</code></pre> After saving and pushing these changes, the Practicus AI runner will automatically execute your workflow.</p>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#more-complex-task-examples","title":"More Complex Task Examples","text":""},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#example-1-upload-and-execute-a-task-file","title":"Example 1: Upload and Execute a Task File","text":"<p>Use <code>prt.run_task(...)</code> to start a worker, upload specified files, and run them automatically. This requires that all the files your task depends on are in the directory you provide.</p> <pre><code># cicd/task.py\nprint(\"Hello from a simple task\")\n</code></pre> <pre><code># cicd/run_task.py\nimport practicuscore as prt\n\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n)\n\ntask_file = \"task.py\"  # The file to run\n\nprint(f\"Starting {task_file} on a new worker.\")\nworker, success = prt.run_task(\n    file_name=task_file,\n    files_path=\"cicd\",       # Local folder containing task.py\n    worker_config=worker_config,\n)\n\nif success:\n    print(\"Task execution finished successfully.\")\nelse:\n    raise SystemError(f\"{task_file} failed.\")\n</code></pre> <p>You can tie this script to a workflow file, for example:</p> <pre><code># .github/workflows/run_task.yaml\nname: Run a task on Practicus AI Worker\n\n# This action will run each time code is pushed to the repository\non:\n  - push\n\nenv:\n  # Update the below url\n  PRT_URL: https://practicus.my-company.com\n  PRT_TOKEN: ${{ secrets.PRT_TOKEN }}\n\njobs:\n  Explore-PracticusAI-Actions:\n    runs-on: practicus-24.8.3\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Run task.py on a Practicus AI Worker\n        run: python cicd/run_task.py\n</code></pre>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#example-2-use-git-instead-of-uploading-files","title":"Example 2: Use Git Instead of Uploading Files","text":"<p>If your code exists in a Git repository, you can clone or pull the repo on the worker automatically and then execute a file directly from the cloned repo. First, ensure you have a Git personal access token saved in the Practicus AI Vault.</p> <pre><code>import practicuscore as prt\nfrom getpass import getpass\n\npersonal_secret_name = \"MY_GIT_SECRET\"\nkey = getpass(f\"Enter key for {personal_secret_name}:\")\n\nprt.vault.create_or_update_secret(name=personal_secret_name, key=key)\nprint(f\"Git access token saved as secret '{personal_secret_name}'.\")\n</code></pre> <pre><code># cicd/run_from_git_repo.py\nimport practicuscore as prt\n\nremote_git_url = \"http://git.practicus.my-company.com/myuser/myproject\"\ngit_secret_name = \"MY_GIT_SECRET\"\nlocal_path_for_git = \"~/myproject\"\n\n# Configure the Git repository and secret\ngit_config = prt.GitConfig(\n    remote_url=remote_git_url,\n    secret_name=git_secret_name,\n    local_path=local_path_for_git,\n)\n\n# Create a WorkerConfig that auto-clones the repo\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    personal_secrets=[git_secret_name],\n    git_configs=[git_config],\n)\n\n# Path to the script we want to run inside the worker, will be cloned from Git.\ntask_file_on_worker = f\"{local_path_for_git}/cicd/task.py\"\n\nprint(f\"Starting {task_file_on_worker} on a new worker.\")\nworker, success = prt.run_task(\n    upload_files=False,              # We'll rely on Git instead of uploading files\n    file_path_on_worker=task_file_on_worker,\n    worker_config=worker_config,\n)\n\nif success:\n    print(\"Task finished successfully.\")\nelse:\n    raise SystemError(f\"{task_file_on_worker} failed.\")\n</code></pre> <p>And reference it in a workflow file, for example:</p> <pre><code># .github/workflows/run_from_git_repo.yaml\nname: Running tasks from Git Repo\n\n# This action will run each time code is pushed to the repository\non:\n  - push\n\nenv:\n  # Update the below url\n  PRT_URL: https://practicus.my-company.com\n  PRT_TOKEN: ${{ secrets.PRT_TOKEN }}\n\njobs:\n  Explore-PracticusAI-Actions:\n    runs-on: practicus-24.8.3\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Run a file from Git on a Practicus AI Worker\n        run: python cicd/run_from_git_repo.py\n</code></pre>"},{"location":"technical-tutorial/unified-devops/git-integrated-cicd/#conclusion","title":"Conclusion","text":"<p>By combining Practicus AI\u2019s worker orchestration with your Git repository\u2019s CI/CD configuration, you can automate virtually any workflow\u2014ranging from running tests and building containers to complex data processing jobs. Just push your changes, and Practicus AI takes care of the rest.</p> <p>You can track the status of each workflow run in your repository\u2019s \u201cActions\u201d tab (or equivalent, depending on your Git service).</p> <p>Previous: Automated Git Sync | Next: Build Custom Images</p>"},{"location":"technical-tutorial/unified-devops/introduction/","title":"Introduction to Practicus AI Unified DevOps","text":"<p>In modern data and software engineering, teams often grapple with fragmented tools and workflows when attempting to integrate development, security, and operations. Unified DevOps is a methodology that brings these components together into a single, cohesive environment\u2014reducing complexity, boosting collaboration, and accelerating releases. </p>"},{"location":"technical-tutorial/unified-devops/introduction/#why-does-unified-devops-matter","title":"Why Does Unified DevOps Matter?","text":"<ul> <li>Streamlined Collaboration: Development, IT operations, and data science teams can collaborate in one platform. This leads to fewer context switches and more efficient handoffs.</li> <li>Faster Delivery Cycles: Automated CI/CD pipelines reduce the time from code commit to production deployment.</li> <li>Security and Compliance: A unified platform offers consistent security controls across every step of the development lifecycle, from managing secrets to controlling infrastructure access.</li> <li>Scalability and Flexibility: With on-demand resources and containerized workflows, teams can scale when they need, without being locked into rigid infrastructure.</li> </ul>"},{"location":"technical-tutorial/unified-devops/introduction/#what-is-practicus-ai-unified-devops","title":"What Is Practicus AI Unified DevOps?","text":"<p>Practicus AI provides a single platform that integrates all the capabilities of Unified DevOps\u2014combining secrets management, containerization, CI/CD, and more. This empowers teams to handle everything from day-to-day development to full-scale production deployments. Here are some of the key features:</p> <ol> <li>Secrets Management: Securely store, rotate, and retrieve sensitive data with an integrated Vault. This ensures that passwords, tokens, and access keys are never exposed in plain text.</li> <li>Automated Worker Initialization: Spin up ephemeral computing environments with all required environment variables and secrets already injected. No more manual configuration.</li> <li>Git Integration: Easily clone or pull repositories during Practicus AI Worker startup or on-demand, using personal or shared access tokens stored in Vault.</li> <li>CI/CD Workflows: Leverage GitHub Actions\u2013compatible workflows that run on Practicus AI Runners. Execute tasks like testing, building, and deploying with minimal overhead.</li> <li>Custom Container Builds: Use built-in container builders to build and push your images to a private or public registry. You can then run new Workers on these custom images\u2014ensuring a fully customized runtime environment.</li> </ol>"},{"location":"technical-tutorial/unified-devops/introduction/#how-to-get-started","title":"How to Get Started","text":"<p>In the following examples, you\u2019ll learn how to:</p> <ul> <li>Store and Retrieve Secrets with Practicus AI\u2019s Vault.</li> <li>Configure Worker Environments by setting environment variables and injecting personal or shared secrets.</li> <li>Set Up Git Repositories to automatically clone or pull code inside Practicus AI.</li> <li>Create CI/CD Workflows that run each time you push code to a repository.</li> <li>Build and Use Custom Container Images for specialized tasks, ensuring each ephemeral Worker can run precisely the environment you need.</li> </ul> <p>By the end, you\u2019ll see how these capabilities combine into a single, streamlined DevOps pipeline\u2014one that unifies data science, engineering, and operations into a shared, secure, and scalable process.</p> <p>Previous: Use Cluster | Next: Secrets With Vault</p>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/","title":"Practicus AI Secrets with Built-in Vault","text":"<p>This notebook demonstrates how to work with Practicus AI\u2019s built-in Vault to securely store, retrieve, and manage secrets. Practicus AI Vault offers encryption at rest, key rotation, and other security features to protect sensitive data.</p>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#overview","title":"Overview","text":"<ul> <li>Create or Update a Secret: You can create or update a personal secret, prompting for sensitive input rather than hardcoding it.</li> <li>Retrieve a Secret: Retrieve the stored secret along with its age in days.</li> <li>Shared Secrets: Access secrets marked as shared (useful if multiple team members or projects need a common secret).</li> <li>Delete a Secret: Remove the secret from the Vault if it\u2019s no longer needed.</li> </ul> <pre><code>import practicuscore as prt\nfrom getpass import getpass\n\n# Create or update a personal secret\npersonal_secret_name = \"MY_PERSONAL_SECRET_1\"\nkey = getpass(f\"Enter key for {personal_secret_name}:\")\n\nprt.vault.create_or_update_secret(name=personal_secret_name, key=key)\n</code></pre>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#retrieving-the-personal-secret","title":"Retrieving the Personal Secret","text":"<p>Next, we fetch the secret we just stored in the Vault, ensuring it was saved correctly.</p> <pre><code>key, age = prt.vault.get_secret(\n    name=personal_secret_name,\n)\nprint(f\"Retrieved personal secret {personal_secret_name} key: ****, length is {len(key)} chars, which is {age} days old.\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#accessing-a-shared-secret","title":"Accessing a Shared Secret","text":"<p>You can also retrieve secrets that are created by administrators and shared with you or one of your groups.</p> <pre><code>shared_secret_name = \"SHARED_SECRET_1\"\n\nkey, age = prt.vault.get_secret(name=shared_secret_name, shared=True)\nprint(f\"Retrieved shared secret {shared_secret_name} key: ****, length is {len(key)} chars, which is {age} days old.\")\n</code></pre>"},{"location":"technical-tutorial/unified-devops/secrets-with-vault/#deleting-a-secret","title":"Deleting a Secret","text":"<p>Finally, if you no longer need the secret, you can delete it from the Vault.</p> <pre><code>prt.vault.delete_secret(name=personal_secret_name)\nprint(f\"Deleted personal secret: {personal_secret_name}\")\n</code></pre> <p>Previous: Introduction | Next: Automated Worker Init</p>"},{"location":"technical-tutorial/workflows/introduction/","title":"Introduction","text":""},{"location":"technical-tutorial/workflows/introduction/#introduction-to-practicus-ai-workflows-and-tasks","title":"Introduction to Practicus AI Workflows and Tasks","text":"<p>In Practicus AI, a workflow is essentially a structured sequence of tasks\u2014units of work that run independently on isolated Practicus AI Workers (Kubernetes pods). By designing workflows as collections of tasks, you gain flexibility, scalability, and maintainability. Each task executes in its own fresh environment, ensuring that resource usage is optimized and preventing interference between tasks.</p> <p>Key points to remember:</p> <ul> <li>Tasks are the fundamental building blocks of workflows. They encapsulate any operation, such as running a Python script or executing a shell command.</li> <li>Isolation: Every task runs inside its own Worker, a dedicated pod with specified CPU, memory, and optional GPU resources. This ensures consistent performance, security, and an easy path to debug or scale individual components.</li> <li>Workflows bring these tasks together, defining their order, parallelism, and dependencies. Using orchestrators like Airflow, you can schedule, monitor, and manage these tasks collectively.</li> </ul> <p>With this approach, Practicus AI ensures that both simple jobs and complex enterprise-level workflows benefit from automated resource management, improved reliability, and streamlined development-to-production workflows.</p> <p>Modern data pipelines often need to implement two key concepts: Atomicity and Idempotency. Practicus AI naturally supports these concepts by breaking complex workflows into atomic and idempotent tasks, ultimately improving the consistency and reliability of your pipelines.</p>"},{"location":"technical-tutorial/workflows/introduction/#modern-data-pipelines-atomicity-and-idempotency","title":"Modern Data Pipelines: Atomicity and Idempotency","text":""},{"location":"technical-tutorial/workflows/introduction/#atomicity","title":"Atomicity","text":"<p>In traditional ETL scenarios, pipelines would process data in small batches due to hardware constraints, potentially leaving destinations partially updated if a failure occurred mid-process. Atomicity ensures that changes are applied fully or not at all, preventing partial and inconsistent updates.</p> <p>Non-Atomic Pipeline Example (not recommended):</p> <p></p> <p>Here, a failure in the middle of the process leaves the target data source in an <code>inconsistent state</code>.</p> <p>Atomic Pipeline Example (recommended):</p> <p></p> <p>An atomic pipeline ensures that a task\u2019s output is only considered complete and available once the entire operation finishes successfully. If the task fails, no partial data is committed, and the system <code>remains consistent</code>.</p>"},{"location":"technical-tutorial/workflows/introduction/#idempotency","title":"Idempotency","text":"<p>Idempotence means that running a task multiple times yields the same final state. For example, pressing an \u201con\u201d button repeatedly on an idempotent device doesn\u2019t change its state after the first press\u2014it stays \u201con.\u201d</p> <p>Idempotency Example:</p> <p></p> <p>When tasks are idempotent, <code>retries or repeated executions due to transient failures do not corrupt or alter the final outcome unexpectedly</code>. This is especially valuable in large-scale data processing, where intermittent failures are common.</p>"},{"location":"technical-tutorial/workflows/introduction/#practicus-ais-approach","title":"Practicus AI\u2019s Approach","text":"<p>Practicus AI makes it straightforward to create workflows that are both atomic and idempotent by default. </p>"},{"location":"technical-tutorial/workflows/introduction/#default-anatomy-of-a-practicus-ai-data-pipeline","title":"Default Anatomy of a Practicus AI Data Pipeline","text":"<p>Practicus AI workflows are represented as a DAG, describing the order, parallelism, and dependencies of tasks. Consider an example workflow:</p> <ol> <li>Load and transform table1, then export as table1_new.</li> <li>Load and transform table2, then export as table2_new.</li> <li>Load table3 and join it with table1_new and table2_new outputs.</li> <li>Load table4 (no changes needed).</li> <li>In table3\u2019s task, now also join table4.</li> <li>Export the final processed data to final_table.</li> </ol> <p>Because each task exports its results upon completion, <code>every subsequent join relies on stable, already-written data</code>, promoting both <code>atomicity</code> and <code>idempotency</code>. Parallel tasks (like table1 and table2) <code>can run simultaneously</code>, and if either needs to be re-run, the final state remains <code>consistent</code> and <code>predictable</code>.</p> <p></p> <p>Sample Airflow DAG:</p> <ul> <li>Process table1 and table 2 in parallel, and independent of each other.</li> <li>Upon completion, if both are successful, process to create the final table.</li> </ul> <pre><code>dag_flow = \"[process_table_1, process_table_2] &gt;&gt; process_final_table\"\n</code></pre>"},{"location":"technical-tutorial/workflows/introduction/#flexibility-and-customization","title":"Flexibility and Customization","text":"<p>While the default approach encourages atomicity and idempotency, you retain full control. You can modify DAGs, customize tasks, and change how data is read, transformed, and saved. For example, if you prefer to capture or skip certain logs, or run tasks in different regions for geo-distributed workflows, you can update worker configurations and DAG definitions accordingly.</p> <p>In short, Practicus AI\u2019s model of defining workflows as collections of independent, atomic, and idempotent tasks ensures that your data pipelines are robust, resilient to failures, and easy to understand and maintain. This sets a solid foundation for building more advanced workflows with orchestration tools like Airflow, enabling you to confidently move from simple scripts to enterprise-grade automation.</p> <p>Previous: Build And Deploy | Next: Tasks &gt; Task Basics</p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/","title":"Generating Wokflows","text":""},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#using-practicus-ai-studio-with-airflow","title":"Using Practicus AI Studio with Airflow","text":"<p>You can use Practicus AI Studio for the following tasks for Airflow workflows.</p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#practicus-ai-studio-functionality-for-airflow","title":"Practicus AI Studio functionality for Airflow","text":"<ul> <li>Explore data sources such as Data Lakes, Data Warehouses and Databases</li> <li>Transform data</li> <li>Join data from different data sources</li> <li>Export the result to any data source</li> <li>Perform these tasks on individual Workers or on distributed Spark cluster</li> <li>Generate data processing steps as Python code</li> <li>Auto-detect dependencies between tasks</li> <li>Generate the DAG code </li> <li>Export data connection files separately so you can change them later</li> </ul>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#sample-scenario","title":"Sample scenario","text":"<ul> <li>Load some_table from a Database A<ul> <li>Make changes</li> <li>Save to Database B</li> </ul> </li> <li>Load some_other_table from a Data Lake C<ul> <li>Make changes</li> <li>Save to Data Warehouse D</li> </ul> </li> <li>Load final_table from Database E<ul> <li>Join to some_table</li> <li>Join to some_other_table</li> <li>Make other changes</li> <li>Save to Data Lake F</li> <li>Export everything to Airflow</li> </ul> </li> </ul> <p>Let's take a quick look on the experience.</p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#joining-data-sources","title":"Joining data sources","text":"<ul> <li>Left joining final_table with column ID to some_other_table column ID</li> </ul>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#exporting-to-airflow","title":"Exporting to Airflow","text":"<ul> <li>Practicus AI automatically detects the dependency:</li> <li>Operations on some_table and some_other_table can execute in parallel since they do not depend on each other</li> <li>If both are successful, operations on final_table can happen including joins</li> </ul>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#viewing-the-exported-code","title":"Viewing the exported code","text":"<ul> <li>After the code export is completed you can update 4 types of files:</li> <li><code>.py files:</code> Each are tasks that include the data processing steps, SQL etc.</li> <li><code>.._worker.json files:</code> Defines the worker that each task will run on.<ul> <li>Container image to use, worker capacity (CPU, GPU, RAM) ..</li> </ul> </li> <li><code>.._conn.json files:</code> Defines how to read data for each task.<ul> <li>Note: Data source credentials can be stored in the Practicus AI data catalog.</li> </ul> </li> <li><code>.._save_conn.json files:</code> Defines how to write data for each task.<ul> <li>Note: Data source credentials can be stored in the Practicus AI data catalog.</li> </ul> </li> <li><code>.._join_.._conn.json files:</code> Defines how each join operation will work: how to read data and where to join.</li> <li><code>.._dag.py file:</code> The DAG file that brings everything together.</li> </ul> <p>Sample view from the embedded Jupyter notebook inside Practicus AI Studio.</p> <p></p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#airflow-deployment-options","title":"Airflow deployment options","text":"<p>You have 2 options to deploy to Airflow from Practicus AI Studio.</p>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#self-service","title":"Self-service","text":"<ul> <li>Select the schedule and deploy directly to Airflow add-on service that an admin gave you access to.</li> <li>This will instantly start the Airflow schedule.</li> <li>You can then view your DAGs using Practicus AI and monitor the state of your workflows.</li> <li>You can also manually trigger DAGs.</li> </ul>"},{"location":"technical-tutorial/workflows/AI-Studio/generating-wokflows/#working-with-a-data-engineer-recommended-for-sensitive-data","title":"Working with a Data Engineer (recommended for sensitive data)","text":"<ul> <li>Just export the code and share with a Data Engineer, so they can:</li> <li>Validate your steps (.py files)</li> <li>Update data sources for production databases (conn.json files)</li> <li>Select appropriate Worker capacity (worker.json files)</li> <li>Select appropriate Worker user credentials (worker.json files)</li> <li>Deploy to Airflow </li> <li>Define the necessary monitoring steps with automation (e.g. with Practicus AI observability)</li> </ul> <p>Previous: Deploying On Airflow | Next: Generative AI &gt; Introduction</p>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/","title":"Workflows with the Practicus AI Airflow Add-on","text":"<p>Practicus AI integrates seamlessly with Airflow to orchestrate workflows. By leveraging Airflow as an add-on, you can:</p> <ul> <li>Define complex directed acyclic graphs (DAGs) to manage task order and parallelism.</li> <li>Schedule tasks to run at specific times or intervals.</li> <li>Use Airflow's UI and ecosystem for monitoring and managing workflows.</li> </ul> <p>For more details on Airflow concepts, see the official Airflow documentation.</p>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#creating-tasks-dags-and-supporting-files","title":"Creating Tasks, DAGs, and Supporting Files","text":"<p>When building workflows, start by designing your tasks as independently executable units. Each task runs on its own Practicus AI Worker. You can group related actions into a single task for simplicity. Practicus AI provides utilities to generate starter files and DAG definitions.</p> <p>Example DAG Flow:</p> <pre><code>dag_flow = \"my_1st_task &gt;&gt; [my_2nd_task, my_3rd_task] &gt;&gt; my_4th_task\"\n</code></pre> <p>This means:</p> <ul> <li><code>my_1st_task</code> runs first.</li> <li>On success, <code>my_2nd_task</code> and <code>my_3rd_task</code> run in parallel.</li> <li>After both complete, <code>my_4th_task</code> runs.</li> </ul> <pre><code>import practicuscore as prt\n\n# Define a DAG flow with 4 tasks, where two run in parallel.\ndag_flow = \"my_1st_task &gt;&gt; [my_2nd_task, my_3rd_task] &gt;&gt; my_4th_task\"\n\n# Default worker configuration\ndefault_worker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",\n    worker_size=\"X-Small\",\n)\n\n# Custom configuration for the second task\nmy_2nd_task_worker_config = prt.WorkerConfig(\n    worker_image=\"practicus-genai\",\n    worker_size=\"Small\",\n)\n\ncustom_worker_configs = [\n    ('my_2nd_task', my_2nd_task_worker_config),\n]\n\ndag_key = \"my_workflow\"\nschedule_interval = None  # Set a cron string or '@daily' as needed\nretries = 0  # 0 for dev/test, increase for production\n\nprt.workflows.generate_files(\n    dag_key=dag_key,\n    dag_flow=dag_flow,\n    files_path=None,  # Current dir\n    default_worker_config=default_worker_config,\n    custom_worker_configs=custom_worker_configs,\n    save_credentials=True,\n    overwrite_existing=False,\n    schedule_interval=schedule_interval,\n    retries=retries,\n)\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#understanding-the-generated-files","title":"Understanding the Generated Files","text":"<p>Task Python Scripts (e.g., <code>my_1st_task.py</code>):</p> <ul> <li>Contain the logic for each task.</li> <li>Each runs in its own isolated Worker.</li> </ul> <p><code>default_worker.json</code>:</p> <ul> <li>Stores default worker configuration (image, size, credentials).</li> <li>Credentials can be set globally by an admin or passed at runtime.</li> </ul> <p><code>my_2nd_task_worker.json</code>:</p> <ul> <li>Overrides the worker config for <code>my_2nd_task</code>.</li> </ul> <p><code>my_workflow_dag.py</code>:</p> <ul> <li>The Airflow DAG file that ties tasks together.</li> </ul>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#test-your-tasks-before-deploying","title":"Test Your Tasks Before Deploying","text":"<p>It's wise to test tasks locally or via Practicus AI before deploying to Airflow. You can intentionally insert errors in your task code to verify error handling.</p> <p>For more details on tasks, see the tasks sample notebook. </p> <pre><code>successful_task_workers, failed_task_workers = prt.workflows.test_tasks(\n    dag_flow=dag_flow,\n    task_list=None,  # Test all tasks in the DAG\n    files_path=None,  # Current dir\n    default_worker_config=default_worker_config,\n    custom_worker_configs=custom_worker_configs,\n    terminate_on_success=True,   # Automatically terminate successful tasks\n    terminate_on_failed=False,  # Keep failed tasks alive for debugging\n)\n</code></pre> <pre><code># Investigate successful or failed tasks\nfor worker in successful_task_workers:\n    # If you had terminate_on_success=False, you could open the notebook to review logs.\n    print(f\"Opening notebook on successful task worker: {worker.name}\")\n    worker.open_notebook()\n\nfor worker in failed_task_workers:\n    print(f\"Opening notebook on failed task worker: {worker.name}\")\n    worker.open_notebook()\n</code></pre> <pre><code># After analysis, terminate remaining workers\nfor worker in successful_task_workers:\n    print(f\"Terminating successful task worker: {worker.name}\")\n    worker.terminate()\n\nfor worker in failed_task_workers:\n    print(f\"Terminating failed task worker: {worker.name}\")\n    worker.terminate()\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#optional-locating-an-airflow-service-add-on","title":"(Optional) Locating an Airflow Service Add-on","text":"<p>If you don't know your Airflow service key, you can list available add-ons and identify it. For instructions on add-ons and their usage, see the Practicus AI documentation.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\naddons_df = region.addon_list.to_pandas()\n\nprint(\"Add-on services accessible to you:\")\ndisplay(addons_df)\n\nairflow_services_df = addons_df[addons_df[\"service_type\"] == \"Airflow\"]\nprint(\"Airflow services you can access:\")\ndisplay(airflow_services_df)\n\nif airflow_services_df.empty:\n    raise RuntimeError(\"No Airflow service access. Contact your admin.\")\n\nservice_key = airflow_services_df.iloc[0][\"key\"]\nservice_url = airflow_services_df.iloc[0][\"url\"]\n\nprint(\"Selected Airflow Service:\")\nprint(f\"- Service Key: {service_key}\")\nprint(f\"- Service URL: {service_url}\")\n</code></pre> <pre><code># Alternatively, you can set service_key manually if you know it:\n# service_key = 'my-airflow-service-key'\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#deploying-the-workflow-to-airflow","title":"Deploying the Workflow to Airflow","text":"<p>Once your tasks, DAG, and configurations are ready and tested, deploy them to Airflow. This pushes your code and configuration to the underlying version control system used by the Airflow service, making the workflow visible and runnable via the Airflow UI.</p> <pre><code>prt.workflows.deploy(\n    service_key=service_key,\n    dag_key=dag_key,\n    files_path=None,  # Current directory\n)\n</code></pre>"},{"location":"technical-tutorial/workflows/airflow/deploying-on-airflow/#additional-notes-and-customizations","title":"Additional Notes and Customizations","text":"<ul> <li>Running Shell Scripts: Tasks don't have to be Python files; <code>.sh</code> scripts also work.</li> <li>Manual Worker Config Files: Instead of passing parameters to <code>generate_files()</code> or <code>deploy()</code>, you can manually manage the <code>.json</code> worker config files.</li> <li>Credential Management: For security, consider storing credentials globally at the Airflow environment level. Avoid embedding sensitive info in local files.</li> <li>Multi-Region Deployments: You can create workflows that run tasks in different regions. Just ensure the worker config <code>.json</code> files point to the correct <code>service_url</code>, <code>email</code>, and <code>refresh_key</code>.</li> <li>Customizing the DAG: Edit the generated DAG file to change default parameters, logging settings, or to add custom logic. For complex scenarios (e.g., different logging strategies), you can customize the <code>run_airflow_task</code> calls as shown in the example snippet.</li> </ul> <p>Previous: Task Basics | Next: AI Studio &gt; Generating Wokflows</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/","title":"Running Tasks on Practicus AI Workers","text":"<p>Before exploring orchestrations like Airflow, it\u2019s important to grasp the fundamentals of tasks in Practicus AI and how they run on workers.</p> <p>Practicus AI executes tasks by creating isolated, on-demand Kubernetes pods (Workers). Each task runs in its own Worker environment, ensuring scalability, isolation, and resource-efficiency.</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#what-is-a-task","title":"What is a Task?","text":"<p>A task is a unit of work, typically a Python script or shell script, that you want to execute in a controlled environment. Tasks can be composed into larger workflows or run as standalone jobs. They form the building blocks for repeatable, automated processes.</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#how-practicus-ai-executes-tasks","title":"How Practicus AI Executes Tasks","text":"<ol> <li>Task Submission: You define the code (e.g., a Python file <code>task_1.py</code>) along with any parameters.</li> <li>Worker Creation: Practicus AI provisions a dedicated Worker pod to run your task.</li> <li>Task Execution: The Worker runs your code, capturing stdout and stderr logs by default.</li> <li>Termination: After completion, the Worker is automatically terminated to free up resources.</li> </ol> <p>This pattern ensures that every task runs in a fresh environment and that resources are only consumed for as long as they are needed.</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#running-a-task-using-the-practicus-ai-sdk","title":"Running a Task Using the Practicus AI SDK","text":"<p>Using the Practicus AI SDK, you can easily submit tasks to be executed on Workers. The SDK handles provisioning, running, and cleaning up the Worker, so you can focus on your code.</p> <pre><code>import practicuscore as prt\n\n# This call:\n#  - Creates a new Worker\n#  - Uploads files from the current directory by default\n#  - Runs 'task_1.py'\n#  - Captures and prints the script output\n#  - Terminates the Worker after completion\nprt.run_task(file_name=\"task_1.py\")\n\nprint(\"Task completed and worker is terminated.\")\n</code></pre> <pre><code># Running a shell script task\nprt.run_task(file_name=\"task_2.sh\")\nprint(\"Shell task completed and worker is terminated.\")\n</code></pre> <pre><code># Customizing the worker environment\n\n# Example startup script (uncomment to use):\n# startup_script = \"\"\"\n# sudo apt-get update\n# sudo apt-get install -y some-package\n# pip install some-library\n# \"\"\"\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",\n    worker_size=\"X-Small\",\n    # startup_script=startup_script,\n)\n\n# This task intentionally fails to demonstrate debugging\nworker, success = prt.run_task(\n    file_name=\"task_with_error.py\",\n    worker_config=worker_config,\n    terminate_on_completion=False,  # Keep the worker alive to inspect\n)\n\nprint(\"Task execution finished, next cell to investigate...\")\n</code></pre> <pre><code># If the task failed, inspect the environment.\n# Uploaded files and logs are in ~/practicus/task/\n\nif not success:\n    # Opens Jupyter notebook in a new browser tab to troubleshoot\n    worker.open_notebook()\n</code></pre> <pre><code>print(f\"Done analyzing, now terminating {worker.name}.\")\nworker.terminate()\n</code></pre>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#tasks-with-jupyter-notebooks","title":"Tasks with Jupyter Notebooks","text":"<p>You can also run tasks defined as Jupyter notebooks: - Dynamically pass parameters to notebooks. - Collect and manage output artifacts.</p> <p>Create <code>task_with_notebook.ipynb</code> with the below code that raises an error.</p> <pre><code>print(\"This task runs inside a notebook.\")\n\nraise SystemError(\"Simulated error\")\n</code></pre> <p>Create <code>task_with_notebook.py</code> that triggers the notebook we just created.</p> <pre><code>import practicuscore as prt\n\nprt.notebooks.execute_notebook(\n    \"task_with_notebook\",\n    raise_on_failure=True,\n)\n</code></pre> <p>To run the notebook as a task:</p> <pre><code>print(\"Running a notebook as task\")\nworker, success = prt.run_task(\n    file_name=\"task_with_notebook.py\",\n    terminate_on_completion=False,\n)\nprint(\"Task was successful.\" if success else \"Task failed.\")\n</code></pre> <pre><code>print(f\"Opening Jupyter notebook on {worker.name}.\")\nprint(\"Check ~/practicus/task/ for output notebooks, e.g. task_with_notebook_output.ipynb\")\nworker.open_notebook()\n</code></pre> <pre><code>print(\"Terminating worker.\")\nworker.terminate()\n</code></pre>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#additional-tips","title":"Additional Tips","text":"<ul> <li>Already logging task output?</li> <li> <p>Disable additional capture using <code>prt.run_task(..., capture_task_output=False)</code>.</p> </li> <li> <p>Need a custom virtual environment?</p> </li> <li> <p>Create or specify a Python venv under <code>~/.venv/</code> and run tasks with <code>python_venv_name=\"your_venv\"</code>.</p> </li> <li> <p>Running automated notebooks from CLI?</p> </li> <li>Use <code>prtcli</code> commands and <code>.sh</code> scripts to trigger notebook executions.</li> </ul> <p>These options give you flexibility in how tasks run, interact with environments, and handle their outputs.</p>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#supplementary-files","title":"Supplementary Files","text":""},{"location":"technical-tutorial/workflows/tasks/task-basics/#task_1py","title":"task_1.py","text":"<pre><code>print(\"Hello from simple task 1\")\n</code></pre>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#task_2sh","title":"task_2.sh","text":"<pre><code>echo \"Hello from simple task 2\"\n</code></pre>"},{"location":"technical-tutorial/workflows/tasks/task-basics/#task_with_errorpy","title":"task_with_error.py","text":"<pre><code>import practicuscore as prt\n\n\ndef main():\n    print(\"Starting task..\")\n\n    # Code as usual:\n    # - Process data\n    # - Train models\n    # - Make predictions\n    # - Orchestrate other tasks\n    # - ...\n\n    try:\n        raise NotImplementedError(\"Still baking..\")\n    except Exception as ex:\n        # Psudo detail log\n        with open(\"my_log.txt\", \"wt\") as f:\n            f.write(str(ex))\n        raise ex\n\n    print(\"Finished task..\")\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Previous: Introduction | Next: Airflow &gt; Deploying On Airflow</p>"}]}