{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Practicus AI Documentation","text":""},{"location":"#about-practicus-ai","title":"About Practicus AI","text":"<p>Practicus AI is a unified platform for Generative AI and Data Intelligence. It helps you move from initial concepts to production-grade AI and Data solutions. Whether you are an experienced engineer or new to Data and AI, the platform is designed to be both reliable and easy to navigate.</p>"},{"location":"#select-benefits","title":"Select Benefits","text":"<p>Practicus AI brings together Generative AI, self-service AI and Data tools, MLOps, Orchestration, Kubernetes infrastructure management, and Observability into a single platform. This integrated, but flexible approach simplifies the process of working with AI and Data at scale.</p> <p></p>"},{"location":"#platform-components","title":"Platform Components","text":"<p>The Practicus AI Platform is built on an enterprise-grade core, with optional add-ons to extend functionality. This structure supports consistent deployment, governance, and scalability. It also provides a unified environment for managing applications, runtimes, and advanced analytics tools.</p> <p></p>"},{"location":"#core-platform","title":"Core Platform","text":"<p>The core platform establishes the foundation for a secure, scalable, and versatile AI environment. It consists of four layers: Applications, Mesh, Runtime, and Infrastructure. Each layer offers distinct capabilities\u2014such as creating and sharing GenAI applications, hosting advanced models and microservices, managing service meshes for reliability, and ensuring efficient use of GPUs and other resources.</p> <p></p>"},{"location":"#add-ons","title":"Add-Ons","text":"<p>In addition to the core platform, Practicus AI offer a variety of add-ons that can be selectively incorporated into your environment. These add-ons are protected by enterprise single sign-on and integrate smoothly with core platform and other add-on components. As we continue to expand the add-on library, you\u2019ll have access to a growing range of tools and features, ensuring that your platform remains both adaptable and secure as your AI initiatives evolve.</p>"},{"location":"#practicus-ais-core-principles","title":"Practicus AI's Core Principles","text":"<p>While the core platform and its add-ons define what you can accomplish with Practicus AI, a set of guiding principles ensures that every feature, integration, and improvement remains aligned with our core values of security, flexibility, and usability.</p>"},{"location":"#7-core-principles-of-the-practicus-ai-platform","title":"7 Core Principles of the Practicus AI Platform","text":"<ol> <li> <p>Security &amp; Compliance First:    The platform must maintain enterprise-level security and compliance at all times, enforcing measures such as SSO with MFA/OTP and strict data governance to safeguard sensitive information.</p> </li> <li> <p>Cloud-Native &amp; Vendor-Agnostic:    The platform must remain cloud-native and adhere to open-source and CNCF standards, ensuring it can be deployed in any environment\u2014on-premises, in the cloud, or within air-gapped configurations\u2014without creating vendor lock-in.</p> </li> <li> <p>Ease of Use &amp; Extensibility:    The platform must provide a unified interface that simplifies setup, enables rapid deployment, and supports extensibility, adapting smoothly to evolving requirements.</p> </li> <li> <p>Operational Simplicity for Technical Teams:    The platform must reduce operational complexity, allowing technical users to focus on designing and refining AI solutions rather than managing infrastructure overhead.</p> </li> <li> <p>Real-Time Observability &amp; Scalability:    The platform must offer continuous insights into performance and health, alongside automatic scaling and optimized GPU utilization, ensuring efficiency as workloads grow.</p> </li> <li> <p>Accessibility for Non-Technical Users:    The platform must deliver interfaces and tools that are easy to navigate, allowing non-technical users to engage in data exploration, analysis, and insight generation without requiring advanced expertise.</p> </li> <li> <p>Comprehensive Analytics &amp; Generative AI:    The platform must integrate a full spectrum of analytics capabilities, from traditional to advanced and generative AI, supporting informed decision-making and fostering innovation across diverse use cases.</p> </li> </ol>"},{"location":"#next-steps","title":"Next Steps","text":"<p>As you continue through this documentation, you will find sections tailored to different types of users. Technical teams can explore detailed operational information, while those new to AI can learn the fundamentals. Consider this the starting point for understanding how Practicus AI supports your AI and data initiatives.</p> <p>Next: Getting Started</p>"},{"location":"advanced/","title":"Advanced Features","text":"<p>In this section we will look at some advanced functionality of Practicus AI. </p>"},{"location":"advanced/#production-data-pipelines","title":"Production Data Pipelines","text":"<p>Typically, an ML workflow will depend on several cycles of data preparation steps before the final ML training can be done. This is rarely a one time task, since your ML model might eventually start drifting as time goes by (not predicting accurately anymore). When this happens, the first thing to try is usually re-training with new, fresh data. Which means you have to reload data from raw data sources, and apply data preparation steps again.  Instead of preparing data manually every time you train, some teams create automated data pipelines. These pipelines typically run on a schedule, for instance daily, creating clean and ready-to-train-on data sets. </p> <p>You can easily build data pipelines with Practicus AI, and embed them into existing data integration platforms.  All you need is a platform that can run Python. Please see some performance tips below in the example code </p> <p>Achieving this is straightforward, you can simply \"chain\" data preparation steps using .dp files, and not necessarily the updated Excel files for performance reasons. </p> <p>Example data pipeline:</p> <pre><code>import practicus\n# 1) create a Python .py file for the data pipeline\n# 2) place your data loading code as usual, from databases, S3 etc. \ndf = load_data_as_usual()\n# 3) execute changes in .dp files one after another\npracticus.apply_changes(df, \"data_prep_1.dp\", inplace=True)\npracticus.apply_changes(df, \"data_prep_2.dp\", inplace=True)\n# ...\npracticus.apply_changes(df, \"data_prep_10.dp\", inplace=True)\n# pipeline is completed\nsave_to_final_dest_as_usual(df)\n\n# Performance Tips: \n# 1) Pandas Data Frame will work faster with practicus, especially for very large datasets \n# 2) You can use \"inplace=True\" updates, to avoid creating new data frames \n# 3) you can delete sorting and column reordering commands in the .dp files, since your \n# ML model training will not care. Especially sorting is a relatively expensive operation.  \n# column reordering with Pandas Data Frame will also trigger creating a new df. \n</code></pre> <p>Deployment</p> <p>Once the data pipeline Python code (.py) is ready like the above, you can then \"pip install practicus\" in the data integration platform and run the .py file as usual. Please note that Practicus AI uses Java (&gt;= v8.0) behind the scenes for some complex operations. Although it is unlikely, if there is no Java runtime (JRE) installed on the data integration platform, Practicus AI will download a local copy of the JRE, first time it runs. This local Java installation is purely a \"file download\" operation of a stripped down JRE, and will not need any root/admin privileges.  You can also manually trigger downloading the local JRE operation in advance by calling practicus.install_jre(), or install Java (&gt;= v8.0) yourself. We recommend Open\u00a0JDK.  </p>"},{"location":"advanced/#dp-file-structure","title":"DP file structure","text":"<p>Practicus AI can detect changes made inside an Excel (.xlsx) file when you call detect_changes() or apply_changes()  functions. The recorded changes are then saved to a simple text file with the extension .dp (data prep).</p> <p>The .dp file is intended to be easily consumed by users that are not programmers or data scientists. The goal is to create a happy medium so that different user personas can view the changes detected in the .dp file and collaborate on a data science project.</p> <p>The .dp file does not necessarily need to only include changes that are detected by Practicus AI. As you can read below, you can freely add your own changes or remove existing ones, and then finally ask Practicus AI to run these data prep steps on your data sets to complete data preparation. </p> <p>The below is a simple .dp file example. </p> <p></p>"},{"location":"advanced/#input_columns","title":"Input_Columns","text":"<pre><code>Input_Columns = [Column Name 1], [Column Name 2], ..\n</code></pre> <p>This is an optional section, but it is recommended to use for data validation. When Practicus AI starts analyzing an Excel (.xlsx) file, it will detect all the visible column names and add them in the Input_Columns section.  When you execute apply_changes() with a new data set, Practicus AI will compare the column names of the input data set to Input_Columns and give you a warning if they are not the same. For example, let's assume you have a data set with Col_1, Col_2, Col_3 columns, export to .xlsx, make some changes and then run detect_changes(). You will see Input_Columns = [Col_1], [Col_2], [Col_3]. Let's assume you then go ahead and delete  Col_2 in your data set, and apply the .dp file. You would get a warning that the input columns do not match with the input data set. </p> <p>Example: </p> <pre><code>Input_Columns = [CRIM], [ZN], [INDUS]\n</code></pre>"},{"location":"advanced/#drop_column","title":"Drop_Column","text":"<pre><code>Drop_Column(Col[name of the column to delete])\n</code></pre> <p>Will delete a column from the data set.  Practicus AI will add a Drop_Column command for both deleted and hidden columns in an Excel (.xlsx) file. The only difference is that deleted columns will run early on in the .dp file and hidden columns will be deleted later since formulas or filters can depend on them. It is common that a user creates a new Excel column, uses a formula and then hides the old column that the formula uses. </p> <p>Example: </p> <pre><code>Drop_Column(Col[ZN])\n</code></pre>"},{"location":"advanced/#update","title":"Update","text":"<pre><code>Update(Col[name of the column to update] from_value to to_value)\n</code></pre> <p>Updates all the rows for a certain column from one value to another. For example, updating all missing values to 0. When a manual update of a cell is detected in the Excel file, an Update command is added to the .dp file. You do not need to make the same update to all cells, just one ie enough to apply the change for that column. If the same value is updated to different values manually, only the first detected update will run. Other updates will be commented out. You can review these update commands and make changes on the .dp file as needed.</p> <p>Examples:</p> <pre><code>Update(Col[CHAS] blank to 1.0)\nUpdate(Col[text field] \"abc\" to \"def\")\nUpdate(Col[ZN] 5 to 10)\n# The below Update command is commented out automatically,\n# since the same value (5) was updated to 10 in one Excel cell and to 11 in another\n# Update(Col[ZN] 5 to 11)\n</code></pre>"},{"location":"advanced/#rename_column","title":"Rename_Column","text":"<pre><code>Rename_Column(Col[current name] to Col[new name])\n</code></pre> <p>Will rename a column. Practicus AI will only analyze top n rows to detect potential renames and can miss some of them in complex cases. Please feel free to manually detect these and add them to the .dp file. </p> <p>Example: </p> <pre><code>Rename_Column(Col[petal_width] to Col[petal width])\n</code></pre>"},{"location":"advanced/#functions-and-formulas","title":"Functions and formulas","text":"<pre><code>Col[name of the column] = EXCEL_FUNCTION( .. EMBEDDED_FUNCTIONS(..) .. ) + OPERATORS\n</code></pre> <p>Using functions to create formulas is probably one of the most powerful features of Excel. The same is true for Practicus AI data prep use case as well. Practicus AI currently interprets over 200+ Excel functions and applies them with custom created Python code to your data set to perform the data transformation.  If Practicus AI ever encounters an Excel function that it doesn't understand, it will create a Python template for you to provide the missing functionality. Please read more about this in custom functions' section below.   Practicus AI currently only supports functions and formulas to run on the same row.  For things like averages of all values for  a particular column, you can use separate sheets or pivot tables to do yor analysis, and then finally perform the data preparation steps on individual rows.  </p> <p>Examples:</p> <pre><code># Operators\nCol[A] = Col[A] + 1\nCol[A] = (Col[A] + Col[B]) / 2\n# Mathematical functions\nCol[A] = SQRT(Col[A]) + POWER(Col[B], 2)\n# Statistical\nCol[A] = MAX(Col[B], Col[C], Col[D]) / AVERAGE(Col[B], Col[C], Col[D])\n# Logical\nCol[A] = IF(Col[B] &gt; 1, \"value greater than 1\", \"it is not\")\nCol[A] = IF( AND(Col[B] &gt; 1, Col[C] &gt; 1), \"both values greater than 1\", \"not\")\n# Text\nCol[first name] = UPPER(Col[first name]) \n# splitting text, i.e. John Brown to John in one column and Brown in another \nCol[first name] = LEFT(Col[full name], SEARCH(\" \", Col[full name]) - 1) \nCol[last name] = RIGHT(Col[full name], LEN(Col[full name]) - SEARCH(\" \", Col[full name])) \n</code></pre> <p>Please note that columns with formulas are currently placed in the .dp file in order that they appear in Excel. If a formula column depends on another, but appears prior in Excel you can face execution order issues. For instance, let's assume we have A = B + 1 formula that appears in the 2nd Excel column. And B = [some existing column] + 1 appears in 3rd Excel column. In the .dp file you will see A = B + 1 first, and it's evaluation will fail since B is not defined yet.  To resolve this kind of issue, you can 1) change the order of columns in Excel so that it matches the execution order of formulas, or 2) change the order in .dp file manually.</p>"},{"location":"advanced/#filtering-with-remove_except","title":"Filtering with Remove_Except","text":"<pre><code>Remove_Except(Col[name of the column] criteria)\n</code></pre> <p>Removes (filters) all the rows that do not match the criteria. Criteria can be \"is in [values]\", logical operators like &gt;, &lt;, &gt;=, &lt;=, = and != and corresponding value, \"and\", \"or\".</p> <p>Examples:</p> <pre><code># remove all values on column RAD, and only keep 4, 5 and 6\nRemove_Except(Col[RAD] is in [4, 5, 6])\n# only keep values RAD &gt; 5 and ZN &lt; 10  \nRemove_Except(Col[RAD] &gt; 5 and Col[ZN] &lt;10)\n# remove all 5's   \nRemove_Except(Col[RAD] != 5)\n# keep only 5's   \nRemove_Except(Col[RAD] = 5)\n</code></pre>"},{"location":"advanced/#sort_columns","title":"Sort_Columns","text":"<pre><code>Sort_Columns(Col[name of first column to sort], .., ascending[True|False, ..])\n</code></pre> <p>Sort values for column(s), ascending or descending. You can sort on as many columns as needed. </p> <p>Example: </p> <pre><code>Sort_Columns(Col[AGE], Col[INDUS], ascending[True, False])\n</code></pre> <p>Sorts on column \"AGE\" from smallest to largest first, and then for the same AGE values sorts on column INDUS, largest to smallest.</p>"},{"location":"advanced/#reorder_columns","title":"Reorder_Columns","text":"<pre><code>Reorder_Columns([column name], [another column name], ..)\n</code></pre> <p>All new columns are appended to the end of the data set by default.  Reorder_Columns command changes the order of the columns. Please note that since this command has to create a new data set internally, inplace=True updates for apply_changes() will not work. inplace=False is the default behaviour for apply_changes() function. </p> <p>Example:</p> <pre><code>Reorder_Columns([CRIM], [CHAS], [NOX])\n</code></pre>"},{"location":"advanced/#custom-functions-udfs","title":"Custom Functions (UDFs)","text":"<pre><code>Col[name of the column] = MY_UDF(..)\n</code></pre> <p>Practicus AI supports defining your own User Defined Functions (UDFs) in a .dp file. If Practicus AI.apply_changes() function encounters a function name that it doesn't know, it will create a placeholder Python .py file for you to be completed later. The same will also happen if Practicus AI encounters an Excel function it doesn't recognize. </p> <p>Practicus AI custom function names follow Excel syntax: capitol letters, \"A-Z\" and numbers  \"0-9\", with the exception of an underscore  \"_\". For example MY_FUNCTION_2(). MyFunction2() is not a valid Excel or Practicus AI function name.   </p> <p>Example: </p> <p>Place the below line in \"sample.dp\" file</p> <pre><code>...\nCol[A] = MY_FUNCTION(Col[B], Col[C], 1, 2, 3)\n...\n</code></pre> <p>and run </p> <pre><code>df2 = practicus.apply_changes(df, \"sample.dp\")\n</code></pre> <p>This will create \"sample_dp.py\" file, place the missing function and raise a NotImplemented error. </p> <pre><code>def practicus_my_function(row, *args):\n    # you can define your function and then delete the below line\n    raise NotImplementedError('MY_FUNCTION')\n\n    # 'row' is a Pandas Data Frame row that you can access *any* column\n    result = row['B'] + row['C'] + args[2] + args[3] + args[4]\n    # the below does the same, without accessing row \n    result = args[0] + args[1] + args[2] + args[3] + args[4]\n\n    return result\n\n# please note that the above function template is simplified for documentation purposes\n# what you get will have more lines, including proper exception handling\n</code></pre> <p>After you fill the missing function template, you can re-run practicus.apply_changes() in your notebook, and your UDF MY_FUNCTION() will execute for all the rows on the data frame, in combination with other commands. </p> <p>You can also mix and match different UDFs and Excel functions on the same line as you wish. </p> <p>Example:</p> <pre><code>COL[A] = MY_FUNC( MY_FUNC2(Col[A]), SIN(Col[B]) )\n</code></pre> <p>The above will create UDF .py placeholders for MY_FUNC( ) and MY_FUNC2( ), and also use the standard Excel function SIN( ).  </p>"},{"location":"airflow/","title":"Airflow integration","text":"<p>Create production ready data processing code with one-click, embed into Airflow or other orchestration engines.  Choose your preferred data processing engine including Pandas, DASK, RAPIDS (GPU), RAPIDS + DASK (multi-GPU) and SPARK.</p> <p>After Airflow Code export is completed, you can see, run and stop the DAGs created with the Airflow tab. You can also run and schedule more than one task sequentially.</p> <p></p> <p>You can see and edit your Airflow code exported with Jupyter Notebook and continue your operations programmatically.You can also work on the data pipelines you have created.</p> <p></p> <p>Please view the Modern Data Pipelines section to learn more about out how Practicus AI data pipelines work by default.</p>"},{"location":"analyze/","title":"Analyze","text":"<p>Easily create charts and profile your data with one click.  Quickly understand how your data is distributed and identify hard to find correlations.</p>"},{"location":"analyze/#profile","title":"Profile","text":"<p>Generate the profile of the data you use with the profile feature in the analyze section.</p> <p></p> <p></p>"},{"location":"analyze/#graph","title":"Graph","text":"<p>Let's use the graph feature in the analysis section.</p> <p></p> <p>You can choose the graph style(Plot, Bar, Horizontal Bar, Histogram, Scatter Plot) you want to create from the dialog. Then enter the x and y columns of the graph you want to create.</p> <p></p> <p></p> <p>You can perform operations on the chart with the features in the menu bar, and you can save the chart if you want.</p>"},{"location":"analyze/#quick-stats","title":"Quick Stats","text":"<p>You can see quick statistics of the columns on data.</p> <p></p> <p>It gives you statistical data about the columns such as count, mean, std, min, 25%, 50%, 75% and max.</p> <p></p>"},{"location":"analyze/#quick-pivot","title":"Quick Pivot","text":"<p>Quick pivot allows you to quickly summarize large amounts of data in an interactive way. You can choose Sum, Median, Min, Max, Std. Dev., Variance, Product as Summary Method.</p> <p></p>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#export_data","title":"export_data","text":"<pre><code>export_data(data_set, output_path, columns=None, reindex=False)\n</code></pre> <p>Exports a dataset to an Excel file (.xlsx) to be used with Microsoft Excel, Google Sheets, LibreOffice etc. </p> <p>data_set: Pandas DataFrame (recommended), NumPy Array or Python list. For large datasets, we recommend Pandas DataFrame.</p> <p>output_path: The path where the Excel file will be written. If there is no file extension, Practicus AI will automatically add .xlsx to the file name.  </p> <p>columns: Optional, default is None. We highly recommend providing column names if the data_set does not include them. If no column names are found, Practicus AI will automatically add names for you, such as column_1, column_2 etc. These names will not lead to the best user experience.   </p> <p>reindex: Optional, default is False. If a Pandas DataFrame is used to export_data function, Practicus AI will use the index found in this data frame, or create one if none found. After several cycles of \"exporting and applying changes\", indexes might start jumping from value to value, such as 0, 5, 14 because of  filtering values. Although it is perfectly fine for Practicus AI to work with sparse index values like these, in some cases it can confuse end users. When reindex=True, Practicus AI will reset the index, so it will become like 0, 1, 2 again.</p> <pre><code># Example 1, exporting basic Python Array\n\n# define a basic array\narr = [[1,2,3],\n       [4,5,6],\n       [7,8,9]]\n\nimport practicus\n\n# write to Excel. Since no colum names are provided, practicus will create new names\npracticus.export_data(arr, \"basic_array.xlsx\")\n\n# Now, with column names\npracticus.export_data(arr, \"basic_array_2.xlsx\", columns=['x1', 'x2', 'x3'])\n\n\n# Example 2, exporting NumPy Array\nfrom sklearn.datasets import load_boston\n\ndata_set = load_boston()\nnumpy_array = data_set.data\n\npracticus.export_data(numpy_array, \"boston_house_numpy.xlsx\", \n                  columns=data_set.feature_names)\n\n\n# Example 3, exporting Pandas Data Frame (recommended)\nimport pandas as pd\ndf = pd.DataFrame(data=data_set.data, columns=data_set.feature_names)\n\n# no need to pass columns since the pandas data frame already has them \npracticus.export_data(df, \"boston_house_pandas.xlsx\")\n\n\n# Example 4, reindexing \n\n# let's delete first 5 rows from the data frame \ndf = df.drop(df.index[[0,1,2,3,4]])\n\n# reindexing will start the index from 0 again\npracticus.export_data(df, \"boston_house_pandas_reindexed.xlsx\", reindex=True)\n</code></pre>"},{"location":"api-reference/#apply_changes","title":"apply_changes","text":"<pre><code>data_set2 = apply_changes(data_set, input_path, columns=None, inplace=False, reindex=False)\n</code></pre> <p>Applies detected or passed changes to the data_set. The way apply_changes() work depends on what kind of file is passed in input_path. If an Excel (.xlsx) file is passed, apply_changes() first executes detect_changes() to find out what kind of changes are made to the Excel (.xlsx) file, writes these changed to a .dp file and then applies these changes to the data_set. If input_path is a .dp file, detect_changes() is not called, and updates are applied directly from the .dp file.  </p> <p>data_set: Pandas DataFrame, NumPy Array or Python list. For large datasets, Pandas DataFrame will perform faster.</p> <p>input_path: This can be either an Excel (.xlsx) file with some changes in it, or a data prep (.dp) file that has changes recorded in it.   </p> <p>columns: Optional, default is None. We highly recommend providing column names if the data_set does not include them. If no column names found, Practicus AI will automatically add names for you, such as column_1, column_2 etc.  Being consistent in column names between export_data() and apply_changes() is important since the column names in the Excel file and the data_set we later try to apply changes on should match. </p> <p>inplace: Optional, default is False. If inplace=True, Practicus AI will apply updates directly on the data_set provided as an input, and will not return anything back. Please note that inplace editing only works  if a Pandas DataFrame is provided as an input. inplace editing might have some performance benefits for very large datasets. We discourage using inplace updates unless it is necessary, since it becomes harder to retry code. I.e. if the applied changes do not work as you expect, you can open the auto-generated .dp file, make changes and can re-run apply_changes(). This will not work with inplace=True.</p> <p>reindex: Optional, default is False. Only meaningful if a Pandas Data Frame is used. After several cycles of \"exporting and applying changes\", indexes might start jumping from value to value i.e. 0, 5, 14 due to filtering of values. Although it is perfectly fine for Practicus AI to work with sparse indexes, in some cases these kinds of indexing can confuse end users. When reindex=True, Practicus AI will reset the index so the returned data_set will have indexes reset, i.e. 0,1,2,3. </p> <p>Returns: The exact same type of data_set, with changes applied. If a Pandas DataFrame as passed as input value, this function return a Pandas DataFrame. Same for NumPy Array and Python Lists. If inplace=True, returns nothing.  Other than returning the data_set, apply_changes also writes a new .dp file with changes detected in it, if an Excel (.xlsx) file is passed in input_path. </p> <pre><code># Example 1\n\narr = [[1,2,3],\n       [4,5,6],\n       [7,8,9]]\n\nimport practicus\npracticus.export_data(arr, \"basic_array_2.xlsx\", columns=['x1', 'x2', 'x3'])\n\n# now open the Excel file and delete first column, x1\n</code></pre> <pre><code># running the  below will detect the change we made (drop column), \n# apply it to arr and return a new Array  \narr2 = practicus.apply_changes(arr, \"basic_array.xlsx\", columns=['x1', 'x2', 'x3'])\n# You should see something like the below  in output window.\n</code></pre> <pre><code>Detecting changes made in basic_array.xlsx\nSaved changes to data preparation file basic_array.dp in 0.21 seconds.\n\nApplying changes from basic_array.dp\nInput_Columns = [x1], [x2], [x3]\n Input columns match the DataFrame.\n\n# Columns deleted in Excel \nRunning: Drop_Column(Col[x1])\n Completed. \n\nAll steps completed.\n</code></pre> <pre><code># Example 2, inplace updates. (Pandas data frame is required for this to work)\nimport pandas as pd\ndf = pd.DataFrame(data=arr, columns=['x1', 'x2', 'x3'])\n\n# with inplace=True, apply_changes() doesn't return anything \npracticus.apply_changes(df, \"basic_array.xlsx\", inplace=True)\n# confirm x1 column is dropped\ndisplay(df)\n\n#Example 3, reindexing\n# now open basic_array.xlsx and put a filter on x2, uncheck 2 and leave 5 and 8 only\n# (to filter in Excel, you can click the drop down arrow next to the column name, x2)   \n# reindexing will start the index from 0 again\ndf = pd.DataFrame(data=arr, columns=['x1', 'x2', 'x3'])\ndf2 = practicus.apply_changes(df, \"basic_array.xlsx\", reindex=True)\ndisplay(df2)\n</code></pre> <pre><code>  x2  x3\n0  5   6\n1    8   9\n</code></pre>"},{"location":"api-reference/#detect_changes","title":"detect_changes","text":"<pre><code>detect_changes(input_path, output_path=None)\n</code></pre> <p>This function detects changes a user makes inside the Excel (.xlsx) file and writes the detected changes to a data prep (.dp) file. Once the data prep (.dp) file is created, you can review and make changes as needed and then finally run using apply_changes function.     </p> <p>input_path: Excel (.xlsx) file path with some changes in it. </p> <p>output_path: Optional. The path for the data prep (.dp) file that the detected changes will be written to. If no file name is passed, Practicus AI uses the same name of the input_path and replaces .xlsx with .dp. </p> <p>Returns: The path of the .dp file. If output_path is provided this function returns that name, if output_path was empty, returns the name of the filename that is generated.   </p> <pre><code># Example 1 \n\n# the below code detects all changes made in the Excel file, \n# and writes the result into basic_array.dp\npracticus.detect_changes(\"basic_array.xlsx\")\n\n# open basic_array.dp file, you will see the below\n</code></pre> <pre><code>Input_Columns = [x1], [x2], [x3]\n\n# Columns deleted in Excel \nDrop_Column(Col[x1])\n\n# Filtered rows in Excel \nRemove_Except(Col[x2] is in [5, 8])\n</code></pre> <pre><code># let's assume we don't want to filter anymore. \n# Place a # in front of Remove_Except(..) to comment it out, and save the .dp file\n\n# now let's apply changes to arr, but this time directly from .dp file\n# instead of .xlsx file.\narr2 = practicus.apply_changes(arr, \"basic_array.dp\", columns=['x1', 'x2', 'x3'])\n\ndisplay(arr2)\n# the result will be: [[2, 3], [5, 6], [8, 9]]\n# first column (x1) dropped, but teh value 2 is \"not\" filtered out , \n# since we commented the filter line out with # in the .dp file \n</code></pre>"},{"location":"api-reference/#export_model","title":"export_model","text":"<pre><code>export_model(model, output_path, columns=\"\", target_name=\"\", num_rows=1000)\n</code></pre> <p>Exports a model to an Excel (.xlsx) file, so that users can understand how the model works, and make predictions by entering new values. </p> <p>model: A Python model, pipeline, or the path of a .pmml file. If a model is passed, Practicus AI can read Linear Regression,  Logistic Regression, Decision Trees and Support Vector Machine models. If a pipeline is passed, Practicus AI can read several pre-processing steps like StandardScaler, MinMaxScaler etc., in addition to the model trained as part of the pipeline. Practicus AI can also read .pmml files, especially for models built outside the Python environment like R, KNIME etc.   </p> <p>output_path: The path of the Excel (.xlsx) file to export.</p> <p>columns: Optional, if a .pmml file is passed as model and has column names in it. Otherwise, required. The column names that will be used as input features for the model. For a basic linear model,  y = 2 * x1 + 3 * x2,  'x1' and 'x2' are the input feature names. These names can be used as column names in the Excel file.</p> <p>target_name: Optional, if a .pmml file is passed as model and has column names in it. Otherwise, required. The target column name that the model predicts for. For a basic linear model,  y = 2 * x1 + 3 * x2,  'y' would be the target name.</p> <p>num_rows: Optional, defaults to 1000 rows. Indicates the number of rows to use in the Excel (.xlsx) file to make predictions on. Max 1,048,576 rows are supported, but the performance will depend on model's complexity, and the user's computer speed. Please gradually increase and test number of rows before sending the .xlsx file to others.     </p> <p>Returns: The path of the .dp file. If output_path is provided this function returns that name, if output_path was empty, returns the name of the filename that is generated.   See some limitations below.</p> <pre><code># Example 1, model exporting\n# ... model training code here ... \nsome_model.fit(X, Y)\n\nimport practicus\npracticus.export_model(some_model, output_path=\"some_model.xlsx\",\n                   columns=['X1', 'X2'], target_name=\"Some Target\", num_rows=100)\n\n# Example 2, pipeline exporting\n# ... model and pipeline code here ...\nmy_pipeline = make_pipeline(\n    SomePreProcessing(),\n    SomeOtherPreProcessing(),  \n    SomeModelTraining())\n\npracticus.export_model(my_pipeline, output_path=\"some_model.xlsx\",\n                   columns=['Column 1', 'Column 2'], target_name=\"Target\", num_rows=150)\n\n\n# Example 3, pmml model exporting\n\n# let's assume we have a model trained in R, and saved in a .pmml file\n# let's also assume the .pmml file has column and target names in it\npracticus.export_model(\"some_R_model.pmml\", output_path=\"some_R_model.xlsx\", num_rows=150)\n\n# Please check the Samples' section for more.. \n</code></pre>"},{"location":"api-reference/#export_model-limitations","title":"export_model limitations","text":"<p>Practicus AI is new, and we are looking forward to hearing your feedback on adding new features. some of our current limitations are: </p> <ul> <li> <p>Exported models need to have fewer than 16,384 columns.  </p> </li> <li> <p>Maximum 1 million rows (1,048,576) are supported for both data preparation and model exporting. </p> </li> <li> <p>Model exporting currently work with Linear Regression, Logistic Regression, Decision Trees and Support Vector Machines. According to Kaggle forums, Google search trends and other forums, these are by far the most popular modeling techniques potentially covering 90+% of the predictive ML analytics use cases. Practicus AI currently doesn't target model exporting for cognitive use cases and deep learning frameworks, since the resulting Excel file would become very complicated making  model debugging and model explainability challenging.</p> </li> </ul>"},{"location":"aws-iam/","title":"AWS IAM policies","text":"<p>Create cloud IAM policies that apply to all of our users to create audit trails for data access and processing.</p>"},{"location":"aws-iam/#administrator-access","title":"Administrator Access","text":"<p>User can attach AdministratorAccess policy to their users via AWS IAM. Practicus can be used easily if the user with AdministratorAccess policy is added in the Settings/AWS Cloud User section of the application.</p> <p></p>"},{"location":"aws-iam/#minimum-privileged-aws-user","title":"Minimum Privileged AWS User","text":"<p>You can create a minimum privileged AWS User from the Settings / Create new cloud user section through the application. When the user is created, the policies to be used for Practicus on AWS are attached to the user.</p> <p></p> <p></p>"},{"location":"cloud/","title":"Cloud","text":"<p>A Cloud Worker is like your computer in the cloud. Various configurations of CPU, memory, storage, and networking capacity for your instances, known as instance types. There are many instance types with different features. You can choose different instance types according to your usage area. You can keep your data on Cloud Worker and work on big data quickly.</p>"},{"location":"cloud/#start-new-cloud-worker","title":"Start New Cloud Worker","text":"<p>You create a new Cloud Worker by choosing the type of instance you want to use, the EBS disk size,Name and the Cloud Worker Version.</p> <p></p> <p>You can stop, start, reset, reboot, replicate and terminate the created Cloud Worker.</p>"},{"location":"cloud/#terminal-cloud-worker","title":"Terminal Cloud Worker","text":"<p>Quickly SSH to the Cloud Worker using the terminal feature.</p> <p></p>"},{"location":"cloud/#cloud-worker-log","title":"Cloud Worker Log","text":"<p>It accesses the logs on the Cloud Worker, it can be saved.</p> <p></p>"},{"location":"cloud/#jupyter-notebook-mlflow-airflow","title":"Jupyter Notebook, MLFlow, Airflow","text":"<p>Provides Jupyter Notebook, MLFlow and Airflow connections with the selected Cloud Worker.</p> <p>Jupyter Notebook allows you to code and see the files in the Cloud Worker.</p> <p>MLFlow allows you to see the registered models and predict using these models.</p> <p>Airflow allows you to create and schedule a new DAG.</p>"},{"location":"code-export/","title":"Code Export","text":"<p>Create production ready data processing code with one-click, embed into Airflow or other orchestration engines.  Choose your preferred data processing engine including Pandas, DASK, RAPIDS (GPU), RAPIDS + DASK (multi-GPU) and SPARK. </p>"},{"location":"code-export/#export-code","title":"Export Code","text":"<p>You can export the operations performed when we do data preparation on the data to the code. If you wish, you can continue your operations over the exported code.</p> <p>You can choose Pandas, DASK, RAPIDS, multi-GPU, Spark as data platform, and you can choose Jupyter Notebook, Python Library and Airflow Library as Code Template.</p> <p>Note : You need a ready Cloud Worker for this operation.</p> <p></p> <p>When the process is complete, you can navigate through the files, continue with the exported code and review the code.</p> <p></p>"},{"location":"data-prep/","title":"Data Preparation use case","text":"<p>There are two main use cases for Practicus AI. Data Preparation and model sharing. </p> <p>Data preparation for ML consumes an estimated 80% to 90% of the time for a Data Scientist today. Practicus AI aims to help data preparation for ML by closing the gap between Excel and Python code. Both data scientists and other supporting personas like business analysts can take advantage of the below functionality, and work together to prepare the data for ML training. </p> <p>Basic data preparation use case</p> <p>1) Export a Pandas DataFrame, NumPy array or a Python list to Excel</p> <pre><code>import practicus\n# export data to an Excel file\npracticus.export_data(my_df, \"my_data.xlsx\")\n</code></pre> <p>2) Open the file in Excel, Google Sheets, LibreOffice or any other Spreadsheet platform to analyze and make changes as usual. </p> <p></p> <p>3) After you are finished updating your data in Excel, you can apply all changes made to create a new data set. </p> <pre><code># import back from Excel, detect all the changes made, and apply to the Data Frame  \nmy_df2 = practicus.apply_changes(my_df, \"my_data.xlsx\") \n\n# practicus auto-generates Python code for you, and applies the updates..\n\n# display the result, which will be the same as what you see in Excel\ndisplay(my_df2)\n</code></pre> <p></p> <p>4) (Optional) Practicus AI will automatically create a data prep (.dp) file containing all detected changes, before generating Python code. You can review this file, remove changes you don't like, or add new ones manually as you wish. Once done, you can apply the updates directly from the .dp file. </p> <p></p> <pre><code># apply changes, but this time directly from the .dp file that you reviewed / updated\nmy_df2 = practicus.apply_changes(my_df, \"my_data.dp\")\n</code></pre> <p>5) (Optional) Rinse and repeat... You can continue the above steps, also working with others in a collaborative environment, to keep generating new versions of Excel files and auto-generated data sets. The detected changes (.dp files) can be updated and archived as needed. Outside of Jupyter notebooks, you can also chain multiple .dp files to create complex data preparation / ML pipelines and later embed these data pipelines to a data engineering platform for production purposes.  Any production grade data integration platform that can run Python code will easily run Practicus AI detected changes at scale.   </p>"},{"location":"excel-predict/","title":"Predict with Excel","text":"<p>You can also use pure-Excel for prediction! (only at Practicus AI). Export and embed your AI models into Excel or Google  Sheets, attach to an email and allow your users to make predictions completely offline with no app / plugin / macro required. You can use the AutoML models, or export your own custom models. Currently supporting Linear Regression,  Logistic Regression, SVM and Decision Trees.</p>"},{"location":"excel-predict/#prediction","title":"Prediction","text":"<p>After the model is created with the build excel model option, you select the location where you will save the excel model from the dialog that appears.</p> <p></p> <p>You open the saved excel model and make predictions for the column you want. In this example, the target column was revenue and when we filled the temperature column with different values, the revenue column was filled with the predicted values.</p> <p></p>"},{"location":"excel-prep/","title":"Prepare with Excel","text":"<p>You can also use Excel for data preparation! (only at Practicus AI).  Share pure Excel or Google Sheets with others,  so they can analyze the data and make changes, and send it back to you. Practicus AI can then capture those  changes, so you can apply to any data locally or in the cloud, or export to Python code. Please watch the demo for more! </p>"},{"location":"excel-prep/#export-to-excel","title":"Export to Excel","text":"<p>With Export to Excel, you can save your data on which you have done preparation operations as excel anywhere on your local computer.</p> <p>Note : You need ready Cloud Worker for this operation.</p> <p></p>"},{"location":"excel-prep/#edit-in-excel","title":"Edit in Excel","text":"<p>You can delete columns, change their names, perform sort and filter operations on the exported excel, apply excel formulas and perform other data preparation operations. In this example, the CHAS column has been deleted and the name of the NOX column has been changed.</p> <p></p>"},{"location":"excel-prep/#import-from-excel","title":"Import from Excel","text":"<p>After saving your excel file on which you perform operations, you can import it to the application with Import from Excel.</p> <p></p> <p>The changes we made on the data with Excel were noticed. You can delete and edit them if you wish.</p> <p></p> <p>Thus, we have imported our data on Excel into the application with the changes we have made.</p>"},{"location":"explore/","title":"Explore","text":"<p>Up to 100+ times faster sampling. Random sample 1 billion+ rows down to 1 million in only a few seconds.  Switch between data engines if you need (Pandas, DASK, Spark, GPUs with RAPIDS).  Use S3 like a local drive on your laptop by simple copy/paste operations. Query Databases using SQL.</p>"},{"location":"explore/#local-files","title":"Local Files","text":"<p>You can browse local files in the Explore tab and preview the data you want to work with easily.</p> <p></p> <p>You can create a new file, copy and paste the files on Cloud Worker or S3</p>"},{"location":"explore/#cloud-worker-files","title":"Cloud Worker Files","text":"<p>Firstly you have to launch a new Cloud Worker from the Cloud Tab. Then you can navigate the Cloud Worker content. You can preview the file by clicking on it.</p> <p></p>"},{"location":"explore/#create-folder","title":"Create Folder","text":"<p>Let's create a new folder.</p> <p></p> <p></p> <p>A new folder has been created.</p> <p>You can also download and upload files on Cloud Worker and run the necessary scripts for databases.</p>"},{"location":"explore/#upload","title":"Upload","text":"<p>A directory is selected for the upload process.</p> <p></p> <p>You can select the file you want to upload from the file dialog.</p> <p></p> <p>After making your selection, you will be directed to the file transfer tab.</p> <p></p> <p>It starts the process with start transfer and you can close the tab when the process is finished.</p> <p></p> <p>You can see the uploaded file by navigating on Cloud Worker.</p> <p></p>"},{"location":"explore/#download","title":"Download","text":"<p>Select the files and start the download process.</p> <p></p> <p></p> <p>After completing the download via the file transfer tab, you can navigate to local files and find the file.</p> <p></p>"},{"location":"explore/#amazon-s3","title":"Amazon S3","text":"<p>After the required Cloud Config setup process is completed, you can load the buckets with Amazon S3, then select the bucket and navigate the files. Note : For this operation you need ready Cloud Worker.</p> <p></p> <p>You can also perform the features (copy, paste, upload, download, new folder) in the menu bar.</p>"},{"location":"explore/#relational-databases","title":"Relational Databases","text":"<p>You can connect to the database using relational databases, run SQL queries, and continue your operations on the data. Amazon Redshift, Snowflake, PostgreSQL, MySQL, SQLite, Amazon Athena, Hive(Hadoop), SQLServer, Oracle, ElasticSearch, AWSOpenSearch, OtherDB connections are supported. Amazon Athena, Hive(Hadoop), SQLServer, Oracle, ElasticSearch, AWSOpenSearch databases need driver installation.</p> <p>Note: You need a ready Cloud Worker to access Relational Databases.</p> <p></p>"},{"location":"faq/","title":"Faq","text":"<p>This section of the documentation is work in progress..</p>"},{"location":"feedback/","title":"Feedback","text":""},{"location":"feedback/#general-feedback","title":"General Feedback","text":"<p>We would love to hear your feedback!</p> <p>https://practicus.ai/feedback/</p>"},{"location":"feedback/#report-issue","title":"Report issue","text":"<p>Please use the below form to submit any issues you face. We will do our best to fix it as soon as possible and get back to you. </p> <p>https://practicus.ai/report-issue</p>"},{"location":"feedback/#request-new-feature","title":"Request new feature","text":"<p>We only build features that our users request. Please let us know what you need, and we will prioritize accordingly. </p> <p>https://practicus.ai/feature-request</p>"},{"location":"feedback/#other-questions","title":"Other Questions","text":"<p>If you have any questions regarding how to use Practicus AI, please submit your question on one of the popular forums  such as reddit  or stackoverflow. </p> <p>Since we will not get a notification from the public forum, please let us know the question's URL by using the below. We will get back to you as soon as possible.</p> <p>https://practicus.ai/questions</p>"},{"location":"get-started/","title":"Get started","text":"<p>This page has moved to getting-started</p>"},{"location":"getting-started/","title":"Getting Started with Practicus AI","text":"<p>The Practicus AI platform offers multiple tools for working with AI and data intelligence. Regardless of your role, experience level, or objectives, you can choose an entry point that fits your needs.</p>"},{"location":"getting-started/#personas-practicus-ai-tools-tasks","title":"Personas, Practicus AI Tools &amp; Tasks","text":"<p>The diagram below illustrates various user roles, the tools available to them, and the tasks they support:</p> <p></p>"},{"location":"getting-started/#finding-the-right-tool-for-the-job","title":"Finding the Right Tool for the Job","text":"<p>Your choice of tool depends on your goals, role, and technical skill level:</p> <p></p>"},{"location":"getting-started/#where-to-start","title":"Where to Start","text":"<p>Most users begin at Practicus AI Home, typically accessed at an address like <code>https://practicus.your-company.com</code>. If you do not have access, consult your system administrator. If you are not an existing enterprise user, you can experiment offline with the free Practicus AI Studio, or contact your IT team for enterprise installation options.</p> <p>Next, decide whether you prefer to work with code or not.</p>"},{"location":"getting-started/#if-you-code","title":"If You Code","text":"<ol> <li> <p>Create a Worker:    From Practicus AI Home, create one or more Workers\u2014isolated Kubernetes pods configured with the container image, CPU/RAM, GPU, and other resources you need.</p> </li> <li> <p>Use Jupyter Lab or VS Code:    After selecting a Worker, launch Jupyter Lab or VS Code. You can write code, train and deploy ML models, and interact with data. Sample jupyter notebooks are provided in each Worker's home directory for quick exploration.</p> </li> </ol>"},{"location":"getting-started/#if-you-do-not-code","title":"If You Do Not Code","text":"<ol> <li> <p>Access Practicus AI Studio or Workspaces:    From Practicus AI Home, open a browser-based Workspace or download AI Studio, for a no-code/low-code experience. If local installation isn\u2019t possible, online Workspaces already include Practicus AI Studio, office productivity tools, and other useful applications.</p> </li> <li> <p>Explore &amp; Build with No-Code Tools:    Use visual interfaces and guided workflows to analyze data, generate insights, and create AI solutions\u2014no programming required.</p> </li> </ol> <p>Next: Tutorials</p>"},{"location":"join/","title":"Join","text":"<p>You can use Practicus AI to join data from very different data sources and even physical locations all around the world. This is not something most traditional databases or data warehouses can do. </p>"},{"location":"join/#how-does-it-work","title":"How does it work?","text":"<p>1) Load data normally. We will call this left worksheet </p> <p>2) (Optional) Make changes to the left worksheet as usual.</p> <p>3) Load data to join with. We will call this the right worksheet</p> <p>4) Join left worksheet to right using a single key column. If you need to use multiple columns to join, you can first concatenate them into a single column i.e. 123-abc-456 </p>"},{"location":"join/#changes-to-the-right-worksheet","title":"Changes to the right worksheet","text":"<p>If you need to make changes to the right worksheet right before the join, you need to export your data to some destination first. If you do not export, the original data will be used. </p> <p>A typical join with changes to the right worksheet could work like this: </p> <p>1) Load the left worksheet, i.e. from Redshift, make changes if you need to</p> <p>2) Load the right worksheet, i.e. from Snowflake</p> <p>3) Make your changes to the right worksheet as usual</p> <p>4) Export the right worksheet data, most likely to an intermediary destination i.e. to your data lake on S3</p> <p>5) Join left worksheet to the right, and the Cloud Worker will use the exported data to read it from the intermediary S3 location. The original source on Snowflake will not be used. </p> <p>If you skip the export step #4, the Cloud Worker would ignore your changes, and read original data directly from Snowflake. </p> <p>To learn more about why Practicus AI joins works like the above, please check the below</p> <p>Modern Data Pipelines </p> <p>section, where we explain atomicity, idempotency and big data scalability concepts of Practicus AI.</p>"},{"location":"join/#join-styles","title":"Join Styles","text":"<p>Practicus AI uses traditional join styles: </p> <p></p> <p>Note: Unless you have a very good reason to do so, please do not use Cartesian join since it will result in left worksheet x right worksheet number of rows. </p>"},{"location":"k8s-setup/","title":"Enterprise Cloud setup guide","text":""},{"location":"k8s-setup/#download-helper-files-recommended","title":"Download Helper files (Recommended)","text":"<p>Before you get started, please feel free to download the practicus_setup.zip file that includes sample configuration (.yaml) files for various cloud, on-prem, or local development environments, and helper scripts to accelerate your setup.    </p>"},{"location":"k8s-setup/#overview","title":"Overview","text":"<p>This document will guide you to install Practicus AI Enterprise Cloud Kubernetes backend. </p> <p>There are multiple backend options and Kubernetes is one of them. Please view the detailed comparison table to learn more. </p> <p>Practicus AI Kubernetes backend have some mandatory and optional components.</p>"},{"location":"k8s-setup/#mandatory-components","title":"Mandatory components","text":"<ul> <li>Kubernetes cluster: Cloud (e.g. AWS), on-prem (e.g. OpenShift) or Local (e.g. Docker Desktop)  </li> <li>Kubernetes namespace: Usually named prt-ns</li> <li>Management database: Low traffic database that holds users, connections, and other management components. Can be inside or outside the Kubernetes cluster.</li> <li>Console Deployment: Management console and APIs that the Practicus AI App or SDK communicates with.</li> <li>Istio Service Mesh: Secures, routes and load balances network traffic. You can think of it as an open source and modern NGINX Plus.   </li> </ul>"},{"location":"k8s-setup/#optional-components","title":"Optional components","text":"<ul> <li>Optional Services Deployment: Only required if you would like to enable OpenAI GPT and similar AI services.   </li> <li>Additional Kubernetes clusters: You can use multiple clusters in different geos to build a flexible data mesh architecture.</li> <li>Additional namespaces: You can also deploy a test environment, usually named prt-ns2 or more.  </li> </ul>"},{"location":"k8s-setup/#dynamic-components","title":"Dynamic components","text":"<ul> <li>Cloud worker pods: These are ephemeral (temporary) pods created and disposed by the management console service. E.g. When a user wants to process large data, perform AutoML etc., the management console creates worker pod(s) on the fly, and disposes after the user is done. The dynamic capacity offered to end users is governed by system admins.      </li> </ul>"},{"location":"k8s-setup/#big-picture","title":"Big Picture","text":""},{"location":"k8s-setup/#prerequisites","title":"Prerequisites","text":"<p>Before installing Practicus AI admin console, please make sure you complete the below prerequisite steps. </p> <p>Installation scripts assume you use macOS, Linux or WSL on Windows.  </p>"},{"location":"k8s-setup/#create-or-reuse-a-kubernetes-cluster","title":"Create or reuse a Kubernetes Cluster","text":"<p>Please create a new Kubernetes cluster if you do not already have one. For this document, we will use a new Kubernetes cluster inside Docker Desktop. Min 8GB RAM for Docker engine is recommended. Installation steps are similar for other Kubernetes environments, including cloud ones such as AWS EKS.</p> <p>View Docker Desktop memory settings</p> <p>View Docker Desktop Kubernetes setup guide</p>"},{"location":"k8s-setup/#install-kubectl","title":"Install kubectl","text":"<p>Install kubectl CLI tool to manage Kubernetes clusters </p> Install kubectl for macOS<pre><code>curl -LO \"https://dl.k8s.io/release/v1.27.7/bin/darwin/amd64/kubectl\"\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\nsudo chown root: /usr/local/bin/kubectl\n</code></pre>"},{"location":"k8s-setup/#install-helm","title":"Install Helm","text":"<p>Practicus AI installation is easiest using helm charts. </p> Install Helm<pre><code>curl -fsSL -o get_helm.sh \\\n  https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nrm get_helm.sh\n</code></pre>"},{"location":"k8s-setup/#verify-current-kubectl-context","title":"Verify current kubectl context","text":"<p>Make sure kubectl is pointing to the correct Kubernetes cluster</p> <pre><code>kubectl config current-context\n\n# Switch context to Docker Desktop if required \nkubectl config use-context docker-desktop\n</code></pre>"},{"location":"k8s-setup/#install-istio-service-mesh","title":"Install Istio service mesh","text":"<p>Practicus AI uses Istio to ingest, route traffic, secure and manage the modern data mesh microservices architecture.</p> <p>Istio can be installed on any Kubernetes cluster and designed to run side by side with any number of other production workloads. Istio will not interfere with a namespace unless you ask Istio to do so.</p> <p>Learn more about Istio </p> <p>Note: Some Kubernetes systems come with Istio 'forks' pre-installed, such as Red Hat OpenShift Service Mesh. Practicus AI is designed and tested to work with the original istio only. Istio is designed to be installed and run side-by-side with the forked projects, so you can safely install it on any Kubernetes cluster.  </p> <p>The below script downloads the latest istoctl version, e.g. 1.18. </p> <p>Please update the \"mv istio-... istio\" section below to a newer version if required.   </p> Install Istio<pre><code>cd ~ || exit\n\necho \"Downloading Istio\"\nrm -rf istio\ncurl -L https://istio.io/downloadIstio | sh -\nmv istio-1.20.3 istio || \\\n  echo \"*** Istio version is wrong in this script. \\\n        Please update to the version you just downloaded to your home dir ***\"\ncd ~/istio || exit\nexport PATH=$PWD/bin:$PATH\n\necho \"Analyzing Kubernetes for Istio compatibility\"\nistioctl x precheck \n\necho \"Install istio to your kubernetes cluster\"\nistioctl install --set profile=default -y\n\necho \"Recommended: Add istioctl to path\"\n# Add the below line to .zshrc or alike\n# export PATH=~/istio/bin:$PATH\n</code></pre>"},{"location":"k8s-setup/#preparing-a-kubernetes-namespace","title":"Preparing a Kubernetes namespace","text":"<p>Practicus AI Kubernetes backend is designed to run in a namespace and side-by-side with other production workloads. </p> <p>We strongly suggest you use namespaces, even for testing purposes.</p>"},{"location":"k8s-setup/#create-namespace","title":"Create namespace","text":"<p>You can use multiple namespaces for Practicus AI ,and we will use the name convention: prt-ns (e.g. for production), prt-ns2 (for testing) etc.</p> <p>In this document we will only use one namespace, prt-ns.  </p> <pre><code>echo \"Creating a Kubernetes namespace\"\nkubectl create namespace prt-ns\n</code></pre>"},{"location":"k8s-setup/#add-practicusai-helm-repository","title":"Add practicusai helm repository","text":"<p>Practicus AI helm repository will make installing Practicus AI console backend easier.</p> Add practicusai helm repo<pre><code>helm repo add practicusai https://practicusai.github.io/helm\nhelm repo update\necho \"Viewing charts in practicusai repo\"\nhelm search repo practicusai\n</code></pre>"},{"location":"k8s-setup/#create-or-reuse-postgresql-database","title":"Create or reuse PostgreSQL Database","text":"<p>Practicus AI management console uses PostgreSQL database to store some configuration info such user credentials, database connections and more.</p> <p>This is a management database with low transaction requirements even for production purposes.    </p>"},{"location":"k8s-setup/#creating-a-production-database","title":"Creating a production database","text":"<p>You can reuse an existing PostgreSQL Server or create a new one using one of the cloud vendors.</p> <p>Please make sure your kubernetes cluster will have access network access to the server.</p> <p>Once the PostgreSQL Server is ready, you can create a new database using a tool such as PgAdmin following the below steps:</p> <ul> <li>Login to PostgreSQL Server </li> <li>Crate a new database, E.g. console</li> <li>Create a new login, E.g. console_user and note its password</li> <li>Right-click the database (console) and go to properties &gt; Security &gt; Privileges &gt; hit + and add the login (e.g. console_user) as Grantee, \"All\" as Privileges.</li> <li>Expand the database items, go to Schemas &gt; public &gt; right click &gt; properties &gt; Security &gt; hit + and add the login (e.g. console_user) as Grantee, \"All\" as privileges. </li> </ul>"},{"location":"k8s-setup/#optional-creating-a-sample-test-database","title":"(Optional) Creating a sample test database","text":"<p>For testing or PoC purposes, you can create a sample PostgreSQL database in your Kubernetes cluster. </p> <p>Important: The sample database should not be used for production purposes. </p> <p><pre><code>echo \"Creating sample database\"\nhelm install practicus-sampledb practicusai/practicus-sampledb \\\n  --namespace prt-ns\n</code></pre> In order to connect to this database inside the kubernetes cluster you can use the below address: </p> <ul> <li>prt-svc-sampledb.prt-ns.svc.cluster.local</li> </ul> <p>Please note that if you installed the sample database with the above defaults, the rest of the installation will already have the sample database address and credentials set as the default for easier testing.    </p>"},{"location":"k8s-setup/#connecting-to-the-sample-database-from-your-laptop","title":"Connecting to the sample database from your laptop","text":"<p>If you ever need to connect the sample database from your laptop using a tool such as PgAdmin, you can open a temporary connection tunnel using kubectl. </p> <pre><code># Get sample db pod name \nSAMPLEDB_POD_NAME=$(kubectl -n prt-ns get pod -l \\\n  app=postgresdb -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Sample db pod name is: $SAMPLEDB_POD_NAME\"\n\necho \"Starting temporary connection tunnel\"\nkubectl -n prt-ns port-forward \"$SAMPLEDB_POD_NAME\" 5432:5432\n</code></pre>"},{"location":"k8s-setup/#deploying-management-console","title":"Deploying Management Console","text":""},{"location":"k8s-setup/#helm-chart-valuesyaml","title":"Helm chart values.yaml","text":"<p>Practicus AI helm chart's come with many default values that you can leave as-is, especially for local dev/test configurations. </p> <p>For all other settings, we suggest you to use values.yaml file</p> <pre><code>mkdir ~/practicus \nmkdir ~/practicus/helm\ncd ~/practicus/helm\ntouch values.yaml\n</code></pre> Sample values.yaml file contents for a local test environment<pre><code>migrate:\n  superUserEmail: \"your_email@your_company.com\"\n  superUserPassword: \"first super admin password\"\n\nenterpriseLicense:\n  email: \"your_email@your_company.com\"\n  key: \"__add_your_key_here__\"\n\ndatabase:\n  engine: POSTGRESQL\n  host: host.docker.internal\n  name: console\n  user: console\n  password: console\n\nadvanced:\n  debugMode: true\n  logLevel: DEBUG\n\nnotification:\n  api_auth_token: \"(optional) _your_email_notification_api_key_\"\n</code></pre> Sample values.yaml file contents for a production environment on AWS, GCE, Azure, OpenShift etc.<pre><code>main:\n  # Dns accessible by app\n  host: practicus.your_company.com\n  # try ssl: false to troubleshoot issues\n  ssl: true\n\nmigrate:\n  superUserEmail: \"your_email@your_company.com\"\n  superUserPassword: \"first super admin password\"\n\nenterpriseLicense:\n  email: \"your_email@your_company.com\"\n  key: \"__add_your_key_here__\"\n\ndatabase:\n  engine: POSTGRESQL\n  host: \"ip address or dns of db\"\n  name: \"db_name\"\n  user: \"db_user\"\n  password: \"db_password\"\n\njwt:\n  # API JWT token issuer, can be any value \n  issuer: iss.my_company.com\n\nnotification:\n  api_auth_token: \"(optional) _your_email_notification_api_key_\"\n</code></pre>"},{"location":"k8s-setup/#ingress-for-aws-eks","title":"Ingress for AWS EKS","text":"<p>This step is not required for a local test setup. </p> <p>For AWS, our helm charts automatically configure Application Load Balancer and SSL certificates. </p> <p>You can simply add the below to values.yaml file.</p> Ingress for AWS EKS<pre><code>aws:\n  albIngress: true\n  # AWS Certificate Manager (ACM) certificate ARN for your desired host address\n  certificateArn: \"arn:aws:acm:__aws_region__:__acct_id___:certificate/___cert_id___\"\n\nistio:\n  # In order to use ALB, Istio gateway host must be \"*\"\n  gatewayHosts:\n  - \"*\"\n</code></pre>"},{"location":"k8s-setup/#ingress-and-other-settings-for-various-kubernetes-systems","title":"Ingress and other settings for various Kubernetes systems","text":"<p>Please check the below documentation to configure Istio and Istio gateway depending on your Kubernetes infrastructure. </p> <ul> <li>Azure</li> <li>Google Cloud</li> <li>OpenShift</li> <li>Oracle Cloud</li> <li>IBM Cloud</li> <li>MicroK8s</li> </ul>"},{"location":"k8s-setup/#configuring-management-database","title":"Configuring management database","text":"<p>Since the management console will immediately try to connect to its database, it makes sense to prepare the database first. </p> <p>The below steps will create a temporary pod that will create or update the necessary tables and populate initial data.</p> <pre><code>cd ~/practicus/helm\n\n# Confirm you are using the correct Kubernetes environment\nkubectl config current-context\n\n# Step 1) Create a temporary pod that will create (or update) the database\n\nhelm install prt-migrate-console-db practicusai/practicus-migrate-console-db \\\n  --namespace prt-ns \\\n  --set advanced.imagePullPolicy=Always \\\n  --values ./values.yaml\n\n# Step 2) View the db migration pod status and logs. \n#   Run it multiple times if pulling the container takes some time.  \n\necho \"DB migration pod status\"\necho \"-----------------------\"\nkubectl get pod -n prt-ns | grep prt-pod-migrate-db\necho \"\"\necho \"Pod logs\"\necho \"--------\"\nkubectl logs --follow prt-pod-migrate-db -n prt-ns\n</code></pre> <p>Once the database migration is completed, you will see success log messages such as the below: </p> <pre><code>Running migrations:\n  Applying contenttypes.0001_initial... OK\n  Applying contenttypes.0002_remove_content_type_name... OK\n  Applying auth.0001_initial... OK\n  Applying auth.0002_alter_permission_name_max_length... OK\n  Applying auth.0003_alter_user_email_max_length... OK\n  Applying auth.0004_alter_user_username_opts... OK\n...\n</code></pre> <pre><code># Step 3) (Recommended) After you confirm that the tables are created,\n#   you can safely terminate the database migration pod using helm.\n#   If you do not, the pod will self-terminate after 10 minutes. \n\nhelm uninstall prt-migrate-console-db --namespace prt-ns \n</code></pre> <p>You can repeat the above 1-3 steps as many times as you need, and for each new version of the management console. </p> <p>If there are no updates to the database schema, the pod will not make any changes.   </p>"},{"location":"k8s-setup/#installing-management-console","title":"Installing management console","text":"<p>Practicus AI management console will be the central place for several administrative tasks. </p> Install management console<pre><code>cd ~/practicus/helm\nhelm repo update \n\nhelm install practicus-console practicusai/practicus-console \\\n  --namespace prt-ns \\\n  --values values.yaml\n</code></pre>"},{"location":"k8s-setup/#logging-in-to-management-console","title":"Logging in to management console","text":"<p>You should be able to log in to Practicus AI management console using http://local.practicus.io/console/admin or https://practicus.your_company.com/console/admin</p> <p>Note: local.practicus.io DNS entry points to localhost ip address (127.0.0.1) </p> <p>Your super admin username / password was defined at the top of your values.yaml file. (superUserEmail, superUserPassword)</p>"},{"location":"k8s-setup/#troubleshooting","title":"Troubleshooting","text":"<pre><code># Find the pod name(s)\nkubectl -n prt-ns get pod | grep prt-depl-console-\n\n# View status\nkubectl -n prt-ns describe pod prt-depl-console-...\n\n# View logs\nkubectl -n prt-ns logs --follow prt-depl-console-...\n\n# Analyze using the interactive terminal\nkubectl -n prt-ns exec -it prt-depl-console-... -- /bin/bash  \n</code></pre>"},{"location":"k8s-setup/#upgrading-management-console-to-a-newer-version","title":"Upgrading management console to a newer version","text":"<pre><code>cd ~/practicus/helm\nhelm repo update\n\nhelm upgrade practicus-console practicusai/practicus-console \\\n  --namespace prt-ns \\\n  --values values.yaml\n</code></pre>"},{"location":"k8s-setup/#uninstalling-management-console","title":"Uninstalling management console","text":"<pre><code>helm uninstall practicus-console --namespace=prt-ns\n</code></pre>"},{"location":"k8s-setup/#recommended-start-a-new-cloud-worker-using-practicus-ai-app","title":"(Recommended) Start a new Cloud Worker using Practicus AI App","text":"<ul> <li>Open Practicus AI App</li> <li>Go to settings &gt; click login</li> <li>Enter service address e.g. for local test http://local.practicus.io or https://practicus.your_company.com</li> <li>You can use your super admin user / password</li> <li>Click on either Explore or Cloud at the top bar</li> <li>For Explore: You should see a new \"Practicus AI Service\" (can be renamed later)</li> <li>Click on \"New Worker\", select a size (if on your laptop, select 1 or 2 GB RAM)</li> <li>For Cloud: Select the newly added region (upper right)</li> <li>Click \"Start New\", select a size (if on your laptop, select 1 or 2 GB RAM)</li> </ul> <p>This will start pulling the Cloud Worker image on first use, which can take a while since the Cloud Worker image is ~ 9GB in size. </p> <p>During this time the app will show the Cloud Worker (pod) status as pending. Once the pull is completed the app will notify you. Go to Explore tab, click on \"Worker-x Files\" (x is the counter) and view the local disk content of the pod. This verifies everything is deployed correctly. </p>"},{"location":"k8s-setup/#troubleshooting-cloud-workers","title":"Troubleshooting Cloud Workers","text":"<pre><code># Find the pod name(s)\nkubectl -n prt-ns get pod | grep prt-pod-wn-\n\n# View status\nkubectl -n prt-ns describe pod prt-pod-wn-...\n\n# View logs\nkubectl -n prt-ns logs --follow prt-pod-wn-...\n\n# Analyze using the interactive terminal\nkubectl -n prt-ns exec -it prt-pod-wn-... -- /bin/bash  \n</code></pre> <p>Tip: You can view the status of Cloud Workers for any user and terminate them if needed using the management console. Simply open http://local.practicus.io/console/admin &gt; scroll to Cloud Worker Admin &gt; click on Cloud Worker Consumption Logs. You will see all the active and terminated Cloud Workers. If you click on a log, you will see the Kubernetes pod conditions. </p>"},{"location":"k8s-setup/#openai-gpt-services-additional-settings","title":"OpenAI GPT services additional settings","text":"<p>Some optional services such as OpenAI GPT require additional setup.</p> <p>Sample setup:</p> <ul> <li>Open management console e.g. http://local.practicus.io/console/admin</li> <li>Go to \"Machine Learning Services\" &gt; \"API Configurations\" page</li> <li>Click \"Add API Configuration\"</li> <li>Select OpenAI GPT</li> <li>Enter your API key that you obtained from OpenAI E.g. \"sk-abc123...\" View your key</li> <li>In optional settings section add the below </li> </ul> <pre><code>OpenAI-Organization=your_openai_organization_id\nmodel=gpt-4\nmax_tokens=350\n</code></pre> <ul> <li>OpenAI-Organization You can find this id on OpanAI account page </li> <li>model You can choose between gpt-3.5-turbo, gpt-4 etc. View available models </li> <li>max_tokens This is a cost control measure preventing high OpenAI costs. The system logs tokens used by your users, so you can adjust this number later.</li> </ul>"},{"location":"k8s-setup/#management-console-settings","title":"Management console settings","text":"<p>There are several settings on the management console that you can easily change using the admin console page.</p> <p>These changes are stored in the management database, so we strongly suggest you to regularly back up your database.</p> <ul> <li>Groups: We strongly suggest you to create groups before granting rights. E.g.: power users, data scientists, data engineers, citizen data scientists. </li> <li>Users: You can create users and give fine-grained access to admin console elements. Staff users can log in to admin console. Most users should not need this level access, and only use Practicus AI App.</li> <li>Central Configuration: Please view \"Cluster Definitions\" to change your service name and location. E.g. to \"Practicus AI Service\" located in \"Seattle\". When end users login using the App, this is the information they will see while exploring data sources. This information is cached for future use, so the earlier you change the better. </li> <li>Cloud Worker Admin: It is crucial you visit every page on this section and adjust Cloud Worker (pod) capacity settings. You can adjust which group/user should have access to what kind of capacity.  </li> <li>Connection Admin: Users can only use analytical database connections that they add to the system AND the connections you make visible to certain groups / users. </li> <li>SaaS Admin: This section is only used if you activate self-service SaaS through a 3rd party payment gateway. We suggest only the super admin has access to it, and you make this section invisible to all other admin or staff users.</li> </ul>"},{"location":"k8s-setup/#advanced-settings","title":"Advanced settings","text":"<p>Practicus AI helm charts values.yaml files include many advanced settings and explanations as inline comments. Please navigate and alter these settings, upgrade your deployments and validate the state as you see fit.</p> <p>Please see below a sample values.yaml file where you can adjust replica count of a deployment:  <pre><code>...\ncapacity:\n  # Console API service replica\n  replicas: 1\n...\n</code></pre></p>"},{"location":"k8s-setup/#recommended-install-persistent-volume-support","title":"(Recommended) Install persistent volume support","text":"<p>Practicus AI Cloud Workers support 2 types of persistent volumes, personal and shared between users.</p>"},{"location":"k8s-setup/#personal-drives","title":"Personal drives","text":"<p>If enabled, every Cloud Worker gets a drive mounted under ~/my. With the personal drive, a user can persist their files under ~/my folder, so the files are not lost after a Cloud Worker is terminated. This can have some benefits, e.g. persisting jupyter notebooks on Cloud Workers.</p> <p>By default, the personal drive is shared between Cloud Workers using Kubernetes ReadWriteMany (RWX) mode.</p> <p>Please be aware that if you enable personal drives and force ReadWriteOnce (RWO), a user can only use one Cloud Worker at a time and this is not recommended.</p>"},{"location":"k8s-setup/#shared-drives-between-users","title":"Shared drives between users","text":"<p>If enabled, every Cloud Worker gets the shared folder(s) mounted in user home dir e.g. ~/shared/folder/..  </p> <p>You can control which group or individual user has access to which shared folder and the share name.</p>"},{"location":"k8s-setup/#kubernetes-storageclass","title":"Kubernetes StorageClass","text":"<p>In order to dynamically create persistent volumes and to avoid administrative burden, Practicus AI uses Kubernetes storage classes. Please prefer StorageClass read / write many mode. </p> <p>Please be aware that only some Kubernetes storage types, such as NFS, support read / write many mode. Common storage classes such as AWS EBS only allow one Kubernetes pod to mount a drive at a time (read / write once), which is not suitable for sharing use cases.</p> <p>If your storage class does not allow read / write many, you cannot implement shared drives between users. And for personal drives, implementing read / write once will cause users to be able to use one Cloud Worker at a time, which is not recommended.</p> <p>To summarize, please prefer to use NFS or similar style storage systems for Practicus AI persistent volumes.</p>"},{"location":"k8s-setup/#supported-nfs-implementations","title":"Supported NFS implementations","text":"<p>You can use any NFS system, inside or outside your Kubernetes cluster. You can also use NFS as a service, such as AWS EFS with Practicus AI. </p>"},{"location":"k8s-setup/#sample-nfs-server-for-your-local-computer","title":"Sample NFS server for your local computer","text":"<p>Please find below a simple implementation to install a NFS pod on your computer to test it with Practicus AI.</p> Sample NFS server configuration<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: nfs-service\nspec:\n  selector:\n    role: nfs\n  ports:\n    # Open the ports required by the NFS server\n    # Port 2049 for TCP\n    - name: tcp-2049\n      port: 2049\n      protocol: TCP\n    # Port 111 for UDP\n    - name: udp-111\n      port: 111\n      protocol: UDP\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: nfs-server-pod\n  labels:\n    role: nfs\nspec:\n  containers:\n    - name: nfs-server-container\n      image: cpuguy83/nfs-server\n      securityContext:\n        privileged: true\n      args:\n        - /exports\n</code></pre> <p>You can save the above to nfs-server.yaml and run</p> <pre><code>kubectl apply -f nfs-server.yaml\n\n# To delete \nkubectl delete -f nfs-server.yaml\n</code></pre> <p>After you create the NFS pod named nfs-server-pod, please run the below to get its IP address, e.g. 10.0.0.1. you will need this IP address in the below section.</p> <pre><code>kubectl get pod -o wide\n</code></pre> <p>Please note that after you restart your computer, the NFS server IP address might change, and in this case you would have to re-install (or upgrade) the below helm chart to update the IP address.  </p>"},{"location":"k8s-setup/#using-nfs-inside-kubernetes","title":"Using NFS inside Kubernetes","text":"<p>The below will create a provisioner pod and a storage class named prt-sc-primary. You can create as many provisioners and storage classes. These can point to the same or different NFS systems. </p> <pre><code>helm repo add nfs-subdir-external-provisioner \\\n  https://Kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\nhelm repo update\n\nexport NFS_DNS_NAME=\"add NFS server DNS or IP address here\"\n\nhelm install nfs-subdir-external-provisioner \\\n  nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n  --set nfs.server=\"$NFS_DNS_NAME\" \\\n  --set nfs.path=\"/\" \\\n  --set storageClass.accessModes=\"ReadWriteMany\" \\\n  --set storageClass.pathPattern=\"practicus\" \\\n  --set storageClass.onDelete=\"retain\" \\\n  --set storageClass.name=\"prt-sc-primary\"\n\n# To uninstall\nhelm uninstall nfs-subdir-external-provisioner\n</code></pre> <p>To learn more and customize the helm chart, please visit provisioner GitHub page.</p> <p>By using the above helm chart and granting user access, NFS server will have directories such as /practicus/users/john-acme.com that gets mounted to ~/my for a user with email john@acme.com. Only John will have access to this folder.</p> <p>You can also define several shared folders between users E.g. shared/finance which would map to /practicus/shared/finance on the NFS server and gets mounted to ~/shared/finance on Cloud Workers.  </p> <p>The above path structure is an example and be customized flexibly through the NFS system itself, StorageClass provisioner setup, or simply by using the Practicus AI Management Console.</p> <p>Please view Cloud Worker Admin section to customize user or group based persistent drives.       </p>"},{"location":"k8s-setup/#using-aws-efs","title":"Using AWS EFS","text":"<p>If you are using AWS EFS, you can use the above provisioner, or as an alternative, you can also use a CSI specific for AWS EKS and AWS EFS. Please view the AWS EFS CSI documentation to learn more. Tip: Even if you decide to use the generic NFS provisioner with AWS EKS, you can still review the CSI page to learn more about security group settings, availability zone optimization etc.</p>"},{"location":"k8s-setup/#optional-creating-sample-object-storage","title":"(Optional) Creating sample object storage","text":"<p>Although it is optional, using object storage systems such as Amazon S3 or compatible for Machine Learning is very common. If you are testing Practicus AI and do not have access to S3 or a compatible store, you can simply use MinIO. </p>"},{"location":"k8s-setup/#sample-object-storage-with-minio","title":"Sample Object Storage with MinIO","text":"<p>You can install MinIO inside your Kubernetes cluster. For demo purposes, we will use a simple Docker container. We will also avoid using the default MinIO S3 port 9000, in case you are also using Practicus AI standalone docker deployment (not K8s). This type of test deployment already uses port 9000.   </p> <pre><code>echo \"Creating sample object storage\"\nhelm install practicus-sampleobj practicusai/practicus-sampleobj \\\n  --namespace prt-ns \n</code></pre> <p>After the minio object storage is created, you can connect to minio management console to create buckets, access credentials etc. For these, you need to create a temporary connection tunnel to minio service.   </p> <pre><code>SAMPLEOBJ_POD_NAME=$(kubectl -n prt-ns get pod -l \\\n  app=minio -o jsonpath=\"{.items[0].metadata.name}\")\necho \"Sample object store pod name is: $SAMPLEOBJ_POD_NAME\"\n\necho \"Starting temporary connection tunnel\"\nkubectl -n prt-ns port-forward \"$SAMPLEOBJ_POD_NAME\" 9090:9090\n</code></pre> <ul> <li>Login to MinIO Console using http://127.0.0.1:9090 and credentials:</li> <li>User: minioadmin password: minioadmin</li> <li>Click Buckets &gt; Create bucket and create testbucket</li> <li>Click Identity &gt; Users &gt; Create User &gt; select readwrite policy</li> <li>Click Access Keys &gt; Create &gt; Note your access and secret keys </li> <li>Click Object Browser &gt; testbucket &gt; upload a .csv file</li> </ul> <p>You should now see a .csv file in testbucket and created a user, access/secret keys. </p> <p></p> <p>View MinIO installation document</p> <p>To test MinIO or other S3 compatible storage with the Practicus AI app:</p> <ul> <li>Open App &gt; Explore tab &gt; Click on New Connection &gt; Amazon S3</li> <li>Enter your access / secret keys</li> <li>Enter sample object storage endpoint url http://prt-svc-sampleobj.prt-ns.svc.cluster.local</li> <li>Select the bucket you created: testbucket</li> </ul> <p>You can now connect to this object storage to upload/download objects using the Practicus AI app. You will not need to create a connection tunnel to the minio management console to test with the app. </p>"},{"location":"k8s-setup/#optional-install-kubernetes-dashboard","title":"(Optional) Install Kubernetes dashboard","text":"<p>You can visualize and troubleshoot Kubernetes clusters using the dashboard. Please follow the below steps to install and view Kubernetes dashboard on your local development environment. Production setup and viewing will require some additional changes, which are beyond the scope of this document.  </p> <pre><code>echo \"Installing Kubernetes Dashboard\"\nkubectl apply -f \\\n  https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml\n\necho \"Setting dashboard permissions\"\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: dashboard-admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dashboard-admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: dashboard-admin-user\n  namespace: kubernetes-dashboard\nEOF\n</code></pre> <p>After the dashboard is installed you can open it using the below commands. </p> <p>Please do not forget to run kubectl proxy in a separate terminal window first, so the web interface is accessible from your browser. </p> <pre><code>echo \"Generating a dashboard access token for 90 days and copying to clipboard\"\nkubectl -n kubernetes-dashboard create token dashboard-admin-user \\\n  --duration=2160h &gt; dashboard-token.txt\n\necho \"Generated token:\"\necho \"\"\ncat dashboard-token.txt\necho \"\"\necho \"\"\npbcopy &lt; dashboard-token.txt\nrm dashboard-token.txt\n\necho \"Opening dashboard at:\"\necho \"http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\"\n\necho \"\"\nopen \"http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\"\necho \"Paste the access token at login page.\"\n\necho \"No login page? Make sure you ran kubectl proxy first\"\n</code></pre> <p></p> <p>Please note that the above steps installed Practicus AI elements to prt-ns namespace. You will have to switch the namespace in the dashboard</p>"},{"location":"k8s-setup/#optional-using-a-private-image-registry","title":"(Optional) Using a private image registry","text":"<p>All Practicus AI container images are customizable, and you can use a private image registry to host your custom images.  In this case, your Kubernetes cluster needs to be able to access the private registry. The below is a step-by-step example for AWS ECR private registry. </p> <ul> <li>Create a practicus-private-test private registry repository </li> </ul> <p></p> <ul> <li>Create your Dockerfile.</li> </ul> <p>Important: We strongly recommend you to use virtual environments for your custom python packages and avoid updating global packages.  </p> <pre><code>FROM ghcr.io/practicusai/practicus:24.1.0\n\nRUN echo \"this is a private repo\" &gt; /home/ubuntu/private.txt\n\nRUN echo \"**** Creating Virtual Env ****\" &amp;&amp; \\\n    python3 -m venv /home/ubuntu/.venv/practicus_test --system-site-packages &amp;&amp; \\\n    echo \"**** Installing packages ****\" &amp;&amp; \\\n    /home/ubuntu/.venv/practicus_test/bin/python3 -m pip install some-package &amp;&amp; \\\n    echo \"**** Installing Jupyter Kernel ****\" &amp;&amp; \\\n    python3 -m ipykernel install --user --name practicus_test --display-name \"My virtual environment\"\n</code></pre> <ul> <li>Build and push the image to private repository</li> </ul> <pre><code>aws ecr get-login-password --region us-east-1 | docker login \\\n  --username AWS --password-stdin _your_account_id_.dkr.ecr.us-east-1.amazonaws.com\n\ndocker build -t practicus-private-test:24.1.0 .\n\ndocker tag practicus-private-test:24.1.0 \\\n  _your_account_id_.dkr.ecr.us-east-1.amazonaws.com/practicus-private-test:24.1.0\n\ndocker push _your_account_id_.dkr.ecr.us-east-1.amazonaws.com/practicus-private-test:24.1.0\n</code></pre> <p>After this step, you should see the image in the AWS ECR console. </p> <ul> <li>Create an access token for the repository, and add as a Kubernetes secret</li> </ul> <pre><code>TOKEN=`aws ecr get-login-password --region us-east-1 | cut -d' ' -f6`\nNAMESPACE=prt-ns\n\nkubectl create secret docker-registry practicus-private-test-secret \\\n  --docker-server=_your_account_id_.dkr.ecr.us-east-1.amazonaws.com/practicus-private-test \\\n  --docker-username=AWS \\\n  --docker-password=$TOKEN \\\n  -n $NAMESPACE\n</code></pre> <p>Note: Some private registry tokens have short lifespans. E.g. AWS ECR default is 12 hours. </p> <ul> <li>Open Practicus AI management console, create a new custom image and add the private registry secret.</li> </ul> <p></p> <ul> <li>The end users that you gave permission to the custom image will be able to launch workers from it.</li> </ul> <p></p> <p>Note: You can also define custom images for other workloads such as model hosting, workspaces, etc. </p>"},{"location":"k8s-setup/#optional-forwarding-dns-for-local-installations","title":"(Optional) Forwarding DNS for local installations","text":"<p>If you are deploying Practicus AI using a local DNS such as local.practicus.io (points to 127.0.0.1) traffic inside the Kubernetes cluster might work since each pod would search for the service locally. This might not be a problem for traffic outside Kubernetes since the local cluster such as Docker Desktop would be listening on 127.0.0.1</p> <p>To solve this problem, we can edit Kubernetes coreDNS setup, so that a local DNS queries would forward to the Istio load balancer. </p> <p>Tip: You can use Kubernetes dashboard UI for the below steps 1-3. </p> <p>1) Locate Istio ingress gateway cluster ip address. E.g. 10.106.28.249 by running the below.</p> <pre><code>kubectl get svc istio-ingressgateway -n istio-system\n</code></pre> <p>2) Edit coredns configmap to add the ip address.</p> <pre><code>kubectl edit configmap coredns -n kube-system\n</code></pre> <p>Sample coredns configmap forwarding local.practicus.io to an istio ingress gateway ip address <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: coredns\n... \ndata:\n  Corefile: |\n    .:53 {\n        ...\n    }\n\n    local.practicus.io {\n      hosts {\n        10.106.28.249 local.practicus.io\n      }\n    }\n</code></pre></p> <p>3) Restart coredns for new changes to be active. </p> <pre><code>kubectl rollout restart deployment/coredns -n kube-system\n</code></pre> <p>4) Test. Start a Practicus AI worker, open a jupyter notebook and run the below code. You should get the istio gateway id address.</p> <pre><code>import socket\nprint(socket.gethostbyname(\"local.practicus.io\"))\n</code></pre>"},{"location":"k8s-setup/#troubleshooting-issues","title":"Troubleshooting issues","text":"<p>Please follow the below steps to troubleshoot some common issues with Kubernetes Dashboard, or equivalent kubectl commands if you do not use the dashboard.</p> <ul> <li>Did the prt-depl-console-... pod start? (Green) If not, view its details.</li> <li>If the pod started, but is not accessible using http://local.practicus.io/console/admin view the pod logs. Click on the pod name &gt; View Logs (upper right)</li> <li>If the logs do not show any errors, Istio sidecar proxy might not be running. Click on the pod name, scroll down to containers and verify there are 2 containers running, prt-cnt-console and istio-proxy.</li> <li>Analyze istio to see if there are any proxy issues detected istioctl analyze -n prt-ns</li> </ul>"},{"location":"k8s-setup/#no-enterprise-key","title":"No enterprise key?","text":"<p>This step is mandatory if you do not have your Enterprise license key. </p> <p>By installing Practicus AI app you will be able to test connectivity to your newly created Kubernetes cluster. You will also have access to your Enterprise license key.</p> <ul> <li>Install the app</li> <li>Go to settings &gt; container section</li> <li>Enter your email to activate your enterprise license </li> <li>View Practicus AI local docker setup guide if you need any help</li> </ul> <p>Once your enterprise license is activated, please open ~/.practicus/core.conf file on your computer, locate the license section, and copy license_key info.</p> <p>Sample license_key inside ~/.practicus/core.conf :  <pre><code>[license]\nemail = your_email@your_company.com\nlicense_key = abc12345-1234-1234-12ab-123456789012\nvalid_until = Wed, 01 Jan 2022 00:00:00 GMT\n</code></pre></p>"},{"location":"k8s-setup/#no-connectivity","title":"No Connectivity?","text":"<p>Most connectivity issues will be a result of mis-configured Istio.  To solve these issues we recommend you to create test namespace e.g. istio-test and install the sample Istio app using the below help page.</p> <p>https://istio.io/latest/docs/setup/getting-started/</p> <p>Once you pass this step, deploying Practicus AI will be very similar.    </p>"},{"location":"k8s-setup/#support","title":"Support","text":"<p>Need help? Please visit https://helpdesk.practicus.io/ to open a support ticket.</p> <p>Thank you!</p>"},{"location":"logging/","title":"Logging","text":"<p>You can log regular events or audit logs to additional log systems such as CloudWatch, Splunk, Sumo Logic etc. </p> <p>All you need to do is add the necessary handlers to core.conf file on your local computer (rare)  or on Cloud Worker (common). </p>"},{"location":"logging/#cloudwatch","title":"CloudWatch","text":"<p>For CloudWatch, watchtower Python library is already installed on Cloud Worker.  You can update core.conf like the below to start logging.  </p> <pre><code># 1- add CloudWatch handler key\n[handlers]\nkeys = ... ,  cwHandler\n\n# 2- Define handler, select level i.e. DEBUG, INFO and other parameters such as log format\n[handler_cwHandler]\nclass = watchtower.CloudWatchLogHandler\nlevel = DEBUG\nformatter = simpleFormatter\n\n# 3- Add the new handler to any logger  \n[logger_practicus]\nhandlers = ... , cwHandler\n</code></pre> <p>For more information you can check watchtower documentation </p>"},{"location":"logging/#other-log-systems","title":"Other log systems","text":"<p>For log systems other than CloudWatch, you need to install the necessary python logger libraries first. And then you can add the log handler the same way as to CloudWatch</p>"},{"location":"mlops/","title":"Introduction to MLOps","text":"<p>This section requires a Practicus AI Cloud Worker, S3 compatible object storage, and a Kubernetes Cluster. Please visit the introduction to Cloud Workers section of our tutorial to learn more.</p>"},{"location":"mlops/#what-is-mlops","title":"What is MLOps","text":"<ul> <li>MLOps, short for Machine Learning Operations, is a set of practices that automates the machine learning lifecycle from development to deployment to monitoring and maintenance. It bridges the gap between ML engineers and DevOps teams to create a culture of continuous improvement for ML products.</li> </ul>"},{"location":"mlops/#why-is-mlops-important","title":"Why is MLOps Important?","text":"<ul> <li>MLOps is a valuable tool for organizations that want to get more value from their ML investments. By automating and managing the ML lifecycle, MLOps can help organizations to deploy ML models to production faster, improve the quality and reliability of ML models, reduce the risk of ML failures, and increase the ROI of ML investments.</li> </ul>"},{"location":"mlops/#what-is-practicus-ais-mlops-approach","title":"What is Practicus AI's MLOps approach?","text":"<p>Practicus AI MLOps offer a way to deploy and manage AI models effectively. It does this by providing a unified user experience, open-source Cloud Native technology, different deployment methods, dynamic service mesh, fine-grained access control, global APIs, and the ability to modernize legacy systems.</p> <ol> <li>Unified user experience and federated governance: Practicus AI provides a single user interface for managing and consuming AI models, even if they are deployed in multiple locations using different cloud providers and data sources. This makes it easy for business users and developers to interact with your AI models, regardless of their technical expertise.</li> <li>Open-source Cloud Native technology: Practicus AI is built using open-source Cloud Native technology, so you can avoid vendor lock-in. To learn more about cloud native please visit Cloud Native Computing Foundation website</li> <li>Different deployment methods: Practicus AI offers a variety of deployment methods, so you can choose the one that best suits your needs. AutoML makes it easy to build and deploy models without writing any code. Jupyter Notebook allows you to experiment with models and deploy them to production with just a few clicks. And custom code gives you complete control over the deployment process.</li> <li>Dynamic service mesh: Practicus AI uses Kubernetes deployments and Istio technology to create a dynamic service mesh for your AI models. This makes it easy to scale your models up or down as needed, and to manage multiple model versions simultaneously.</li> <li>Fine-grained access control: Practicus AI provides fine-grained access control tokens that allow you to control who can deploy, consume, and manage your AI models. This helps you to protect your models from unauthorized access.</li> <li>Global APIs: Practicus AI allows you to enable global APIs that allow developers to use a single URL to automatically use the closest cloud region, edge location or on-premise deployment. This makes it easy to deploy and consume your AI models globally, with high availability and low latency.</li> <li>Modernize legacy or proprietary models: Practicus AI can be used to modernize legacy or proprietary AI systems by wrapping them with Practicus AI Open MLOps. This allows you to get the benefits of Practicus AI MLOps without having to make changes to your existing code.</li> </ol>"},{"location":"mlops/#deploying-ai-models-using-the-app","title":"Deploying AI Models using the app","text":"<p>There are multiple ways to deploy AI models. In this section we will learn how to deploy the models we create with Practicus AI AutoML. You can also use the SDK or admin web UI to deploy models.</p> <ul> <li>Open Practicus AI App.</li> <li>Click Start Exploring and see this screen.</li> </ul> <p></p> <ul> <li>In this step, tap on the dataset to load the dataset to build the model.</li> <li>Preview your dataset and make optional advanced adjustments, and Load dataset.</li> <li>Click on the model and choose the type of model according to your problem.(E.g. Classification, Clustering, Anomaly Detection...)</li> <li>Select Objective Columns and adjust Advanced settings.</li> </ul> <p></p> <ul> <li>Click OK.</li> <li>Wait for model setup and see the model dialog.</li> </ul> <p></p> <ul> <li>When you see the Model dialog, select Deploy Model (API)</li> </ul> <p></p> <ul> <li>Select the appropriate one from the Prefixes you created in the Admin Console. (We will examine creating them in the admin console tab)</li> <li>Click Select model prefix or model. (Hint: If you register your model on an existing model, it will register as version 2)</li> <li>See your model and specifications under this prefix.</li> </ul> <p></p> <ul> <li>Back to Dataset.</li> <li>Click predict and select the model you deploy.</li> <li>Click OK.</li> <li>You can see the new Predict columns created.</li> </ul> <p>Now that we have learned how to deploy the model we created, let's learn how to manage these operations from the Practicus AI Admin Console.</p>"},{"location":"mlops/#practicus-ai-admin-console-for-ai-model-hosting-administration","title":"Practicus AI Admin Console for AI Model Hosting Administration","text":"<ul> <li>Open Practicus AI Admin Console.</li> </ul>"},{"location":"mlops/#model-deployments","title":"Model Deployments","text":"<ul> <li>Open Practicus AI Admin Console and click AI Model Hosting.</li> </ul> <ul> <li>Click Model Deployments. (Hint: The reason why it is called Model Deployment is that the models you host are created as Deployment in Kubernetes. This has several advantages, and you can do all operations from the Admin Console without updating the YAML file.)</li> </ul> <ul> <li>In the area where you open Model Deployments, you can make configurations related to the model, set the resource to assign to the models and assign auto scaler.</li> </ul> <ul> <li>At the same time, there may be containers that you have prepared with extra packages in this area, you can assign them, and you can add extra libraries to these containers with pip install by writing a startup script.</li> </ul> <ul> <li> <p>You can assign these Deployments on a group and user basis so that you can easily manage the models. You can also authorize these users and groups to run custom code.</p> </li> <li> <p>After completing and saving the necessary operations, you can open Deployments from Kubernetes Dashboard and see the Deployment you created.</p> </li> </ul> <p></p>"},{"location":"mlops/#model-prefixes","title":"Model Prefixes","text":"<ul> <li>Click Model Prefixes.</li> </ul> <p>Prefixes also represent URLs. You can set OpenAPI documentation settings in this area. You can also upload your pre-built models to Practicus AI and manage production settings and super secrets from there.</p> <p></p> <ul> <li>You can set access authorizations on group and user basis to the prefixes you create from this area. You can also set who can run custom code in this prefix.</li> <li>You can also assign a token to this prefix and access the models with this token.</li> </ul> <p></p>"},{"location":"mlops/#models","title":"Models","text":"<ul> <li>The models you create may not work as they used to over time. You can create new models and save these models as new versions and assign these versions as staging and production.</li> <li>At the same time, you can easily switch between the models you have created and saved as staging and/or production.</li> <li>With Practicus AI's dynamic service mesh approach, you can easily route traffic between models. Let's take a look at how this works.</li> </ul> <ul> <li>Click Models.</li> <li>Select Owner.</li> <li>Select Model Prefix.</li> <li>Select Name for model.</li> </ul> <ul> <li>Perform Deployment assignment of models.</li> <li>You can mark Stage as Staging and/or Production.</li> <li>Perform traffic routing between these versions with Traffic Volume Weight%. </li> <li>If you select Cascade prefix access tokens, you can access this model with the access token you defined for the prefix.</li> </ul>"},{"location":"mlops/#model-versions","title":"Model Versions","text":"<ul> <li>Select Model Versions.</li> <li>Click Add Model Version.</li> <li>Choose your existing model.</li> <li>Enter Model Version.</li> <li>Assign Model Deployment.</li> <li>Set Stage status .</li> <li>Assign Traffic Volume Weight%.</li> <li>You can choose the model as a draft if you want. If you choose, this model will not be marked as latest.</li> <li>Click Save.</li> </ul>"},{"location":"mlops/#external-api-access-tokens","title":"External API Access Tokens","text":"<ul> <li>Click External API Access Tokens.</li> <li>Select Add External API Access Token.</li> <li>Select Owner.</li> <li>Set Expiry date. (Hint: For advanced use cases. Python style dictionary to add extra key value pairs to the JWT token.)</li> <li>Select Global id. (Hint: For advanced use cases.  To build highly available and accessible APIs distributed in different geos, you can define tokens with the same global id and use them across the globe. E.g. a mobile app can access a global API 'api.company.com', and the DNS can  route the traffic to 'us.api.company.com' or 'eu.api.company.com' depending on user location OR service availability.)</li> <li>Select Model Prefix External API Access Tokens or Add another Model Prefix External API Access Token.</li> <li>Select Model External API Access Tokens or Add another Model External API Access Token.</li> <li>Click Save.</li> </ul> <p>We have completed the MLOps operations that can be done from the Practicus AI Admin Console. </p> <p>Now you can access our MLOps video from the link below to try these operations and digest the information you have gained:</p> <p>Practicus AI Open MLOps</p>"},{"location":"mlops/#optional-model-documentation","title":"Optional: Model Documentation","text":"<ul> <li>Business users can easily explore individual systems with the interface and access data sources. Technical users can easily access the documentation of the models with Swagger, OpeAPI or Redoc. </li> </ul>"},{"location":"model-api/","title":"For developers","text":"<p>Welcome to Practicus AI Model API documentation. You can use the Practicus AI App or the SDK to build, distribute and deploy AI/ML models and then consume these models with the app or any other REST API compatible system.</p>"},{"location":"model-api/#model-prefixes","title":"Model prefixes","text":"<p>Practicus AI models are logically grouped with the model prefix concept using the https:// [some.address.com] / [some/model/prefix] / [model-name] / [optional-version] / format. E.g. https://practicus.company.com/models/marketing/churn/v6/ can be a model API address where models/marketing is the prefix, churn is the model, and v6 is the optional version.</p>"},{"location":"model-api/#model-documentation","title":"Model Documentation","text":"<p>Models under a prefix can be viewed by using Practicus AI App. If your admin enabled them, you can also view the documentation by visiting ../prefix/redoc/ or ../prefix/docs/ or E.g. https://practicus.company.com/models/marketing/redoc/</p>"},{"location":"model-api/#model-deployment-concept","title":"Model Deployment concept","text":"<p>Models are physically deployed to Kubernetes deployment(s). These have several characteristics including capacity, auto-scaling, additional security etc.</p>"},{"location":"model-api/#authentication","title":"Authentication","text":"<p>Consuming models inside the Practicus AI app does not require additional authentication. For external use, you will need an access token. An admin can create tokens at the model prefix or individual model level.</p>"},{"location":"model-api/#submitting-data-in-batches","title":"Submitting data in batches","text":"<p>Practicus AI app will automatically split large data into mini-batches to improve performance and avoid memory issues.  You are encouraged to do the same and submit data in a volume compatible with your model deployment capacity. E.g. If your model deployment pods have 1 GB RAM, it is advisable to submit data in 250MB or less batch size.    </p>"},{"location":"model-api/#compression","title":"Compression","text":"<p>Practicus AI app automatically compresses requests and responses for the model API. You can also compress using 'lz4', 'zlib', 'deflate', 'gzip' compression algorithms. All you need to do is to compress, and send the algorithm using 'content-encoding' http header, or by simply naming the attached file with the compression extension. E.g. my_data.lz4</p>"},{"location":"model-api/#model-metadata","title":"Model Metadata","text":"<p>You can learn more about any AI model by simply requesting it's meta data using ?get_meta=true query string. E.g. https://practicus.company.com/models/marketing/churn?get_meta=true</p>"},{"location":"model-api/#sample-python-code","title":"Sample Python code","text":"<pre><code>import requests\nheaders = {\n    'authorization': 'Bearer _your_access_token_',\n    'content-type': 'text/csv'\n}\nr = requests.post('https://practicus.company.com/models/marketing/churn/', headers=headers, data=some_csv_data)\nprint('Prediction result: ', r.text)\n</code></pre>"},{"location":"model-api/#getting-session-and-access-api-tokens","title":"Getting session and access API tokens","text":"<p>Please prefer a 'short-lived' session API access token where you can get one programmatically like the below example. If you do not use Practicus AI SDK, you can request a 'long-lived' API access token from a Practicus AI admin as well.  Please note that session tokens will offer increased security due to their short lifetime.  </p> <pre><code>import practicuscore as prt \ntoken = prt.models.get_session_token('https://practicus.company.com/models/marketing/churn/')\n</code></pre>"},{"location":"model-api/#compression-code","title":"Compression code","text":"<p>With or without streaming, you can submit the data after compressing with lz4', 'zlib', 'deflate', 'gzip' algorithms. If you compress the request, the response will also arrive using the same compression algorithm. </p> <pre><code>import lz4 \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv',\n    'content-encoding': 'lz4'\n}\ndata_csv = df.to_csv(index=False)\ncompressed = lz4.frame.compress(data_csv.encode())\nprint(f\"DataFrame compressed from {len(data_csv)} bytes to {len(compressed)} bytes\")\n\nr = requests.post(api_url, headers=headers, data=compressed)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\ndecompressed = lz4.frame.decompress(r.content)\nprint(f\"Result de-compressed from {len(r.content)} bytes to {len(decompressed)} bytes\")\n\npred_df = pd.read_csv(BytesIO(decompressed))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"model/","title":"Model","text":"<p>Build AI models on past data with one-click using Automated Machine Learning (AutoML) and then make predictions  on unseen data. If you are a data scientist, or working with one, export to Jupyter code and share your experimentation  details in a central database (MLflow). Your team will save significant amount of time by avoiding to start coding from  scratch and by collaborating better</p>"},{"location":"model/#create-a-model","title":"Create a Model","text":"<p>You need to have a Cloud Worker ready for the model creation process and start the model creation process with our data on the Cloud Worker.</p> <p></p> <p>We create a model of regression type by selecting the target column revenue using the ice cream data. You can also limit the model with the advanced options section and select build excel for model.</p> <p></p> <p>When the model creation process is finished, some options appear.</p> <p></p> <p>You can load the model in Excel, register it in MLflow, see the details of the experiment with MLflow, you can see the code with Jupyter Notebook.</p> <p></p> <p></p>"},{"location":"modern-data-pipelines/","title":"Modern Data Pipelines","text":"<p>Modern data pipelines often need to implement 2 key concepts: Atomicity and Idempotency to provide consistent and reliable solutions. In this section we will explain what these concepts are and how they are implemented in Practicus AI.    </p>"},{"location":"modern-data-pipelines/#atomicity","title":"Atomicity","text":"<p>Traditional ETL pipelines often processed data in small batches in order to fit in the limited memory of the available hardware. This sometimes created major issues and race conditions, such as a data pipeline running half-way through and failing, leaving the final destination having only half of the data.</p> <p>Sample Non-Atomic Pipeline: </p> <p>Sample Atomic Pipeline:</p> <p></p>"},{"location":"modern-data-pipelines/#idempotency","title":"Idempotency","text":"<p>Idempotence is the concept of allowing a task to run multiple times, producing the same result. For instance, you could press the on button of an idempotent machine multiple times, and it would not change the result.</p> <p>In the below pipeline, if a task had to run multiple times (often due to a failure) and gives the same result, we can say it implements idempotency principle. This wasn't the case in many traditional ETL pipelines.  </p> <p></p>"},{"location":"modern-data-pipelines/#practicus-ai-approach","title":"Practicus AI approach","text":"<p>The default Practicus AI data processing approach is to break down any data pipeline into atomic and idempotent tasks that include 4 key activities.</p>"},{"location":"modern-data-pipelines/#default-anatomy-of-a-practicus-ai-data-pipeline-task","title":"Default anatomy of a Practicus AI data pipeline task","text":"<p>1) Read data from any data source (mandatory, do it once)</p> <p>2) Apply data transformation steps (optional, can be done many times)</p> <p>3) Join to any data by reading it directly from its data source (optional, can do many times)</p> <p>4) Save final data to any destination (mandatory, do it once)</p>"},{"location":"modern-data-pipelines/#default-anatomy-of-a-practicus-ai-data-pipeline","title":"Default anatomy of a Practicus AI data pipeline","text":"<p>Practicus AI creates a modern DAG (directed acyclic graph) which defines the order, parallelism and dependency of as many atomic and idempotent tasks as you need. </p> <p>Let's take a look at the example below:</p> <p>1) Load table1, make some changes and export to table1_new. Keep this worksheet open in the app.</p> <p>2) Do the same for table_2</p> <p>3) Load table_3 and join to table_1 and table_2. Since data in these tables are exported, table_3 will use data from table1_new and table2_new to join.</p> <p>4) Load table4, do not make any changes</p> <p>5) In table3, join again, this time to table4. Since no changes are made and exported, the original data source of table4 will be used</p> <p>6) Export table3 data pipeline code to go to production. </p> <p></p> <p>Your exported Airflow DAG will look like the below:</p> <p></p> <ul> <li>The code that loads table1 and table2, processes the steps, and exports to table1_new and table2_new will work in parallel.</li> <li>If any of these tasks fail, it will retry (default 3 times)</li> <li>If, for some reason, these tasks run successfully again, the resulting tableX_new will not be different (idempotent) </li> <li>When both of these tasks are completed, table3 code will run: loads table3, executes all steps including joining to table1_new, table2_new and table4 in the requested order</li> <li>Once all of the steps and joins for table3 are completed, it will export to final_data (atomicity) </li> <li>Please note that in a traditional pipeline the table1 task could pass in-memory processed data to table3, eliminating the need to export to an intermediary location (table1_new) first. </li> <li>In comparison, Practicus AI generated pipelines require your changes to be exported first. This is due to scalability requirements of modern big data systems.     </li> <li>If you close a worksheet in the app before exporting the deployment code, the exported code will not include the task of that table in the Airflow DAG. </li> <li>I.e. In the above example, if you close table1 before exporting the deployment code, the DAG would become simpler table2 -&gt; table3.  In this case, we assume the task needed to create table1_new will be part of another data pipeline. The new DAG and code for table3 will simply expect table1_new is available and up to date.    </li> <li>Please note all of the above are simply convenience defaults. You can always make changes to your DAG, moving tasks back and forth after code is exported.</li> </ul>"},{"location":"predict/","title":"Predict with App","text":"<p>Predict the unknown with one-click. Search and use models built with any technology / platform. You can use model APIs hosted anywhere, or use models cold stored on your laptop, S3 or other location,  which will become live within seconds. With short startup time and ease of use, your organization\u2019s ML models will reach new users easier</p> <p>This section of the documentation is work in progress..</p>"},{"location":"predict/#prediction","title":"Prediction","text":"<p>You need to have a Cloud Worker ready for the prediction process and start the prediction process with our data on the Cloud Worker.</p> <p></p> <p>In the predict operation, where we will use the registered model, MLflow is selected as the model location(MLflow, Local Computer, S3).</p> <p></p> <p>You can search and select the ice cream model registered as Model URI.</p> <p></p> <p></p> <p>When the prediction is completed, the prediction column is created on the data.</p> <p></p>"},{"location":"prepare/","title":"Prepare with App","text":"<p>Process, clean and prepare your data without any coding. When clicking is not enough, use 200+ Excel compatible formulas.  Add custom Python code using the built-in editor for more complex requirements. Export the final clean data to a file,  data lake or database directly from the app. To build repeatable data pipelines, export to pure Python code and run anywhere you need.</p> <p>This section of the documentation is work in progress..</p>"},{"location":"prepare/#sort","title":"Sort","text":"<p>You can sort the columns you want in ascending or descending order. </p>"},{"location":"prepare/#filter","title":"Filter","text":"<p>You can filter any column using logical operators.</p> <p></p>"},{"location":"prepare/#rename","title":"Rename","text":"<p>You can change the name of the column you choose.</p> <p></p>"},{"location":"prepare/#formula","title":"Formula","text":"<p>You can run and test more than one formula on the column you want.</p> <p></p> <p>When you want to add the formula, the new column on the worksheet will be ready.</p> <p></p>"},{"location":"prepare/#code","title":"Code","text":"<p>In this section, you can run the code yourself and test the code you wrote on the worksheet. When you apply the code, the change on the worksheet is saved.</p> <p></p>"},{"location":"prepare/#other-features","title":"Other Features","text":"<p>In addition, you can easily perform Show/Hide, Move Left , Move Right, Delete , Split, Convert, Missing, Group By, Categorical, One Hot and Join operations on data and columns.</p>"},{"location":"sdk-start/","title":"Home","text":"<p>Basic data preparation use case</p> <p>1) Export a Pandas DataFrame, NumPy array or a Python list to Excel</p> <pre><code>import practicus\n# export data to an Excel file\npracticus.export_data(my_df, \"my_data.xlsx\")\n</code></pre> <p>2) Open the file in Excel, Google Sheets, LibreOffice or any other Spreadsheet platform to analyze and make changes as usual. </p> <p></p> <p>3) After you are finished updating your data in Excel, you can apply all changes made to create a new data set. </p> <pre><code># import back from Excel, detect all the changes made, and apply to the Data Frame  \nmy_df2 = practicus.apply_changes(my_df, \"my_data.xlsx\") \n\n# practicus auto-generates Python code for you, and applies the updates..\n\n# display the result, which will be the same as what you see in Excel\ndisplay(my_df2)\n</code></pre> <p></p> <p>4) (Optional) Practicus AI will automatically create a data prep (.dp) file containing all detected changes, before generating Python code. You can review this file, remove changes you don't like, or add new ones manually as you wish. Once done, you can apply the updates directly from the .dp file. </p> <p></p> <pre><code># apply changes, but this time directly from the .dp file that you reviewed / updated\nmy_df2 = practicus.apply_changes(my_df, \"my_data.dp\")\n</code></pre> <p>5) (Optional) Rinse and repeat... You can continue the above steps, also working with others in a collaborative environment, to keep generating new versions of Excel files and auto-generated data sets. The detected changes (.dp files) can be updated and archived as needed. Outside of Jupyter notebooks, you can also chain multiple .dp files to create complex data preparation / ML pipelines and later embed these data pipelines to a data engineering platform for production purposes.  Any production grade data integration platform that can run Python code will easily run Practicus AI detected changes at scale.   </p>"},{"location":"sdk-start/#aiml-model-sharing","title":"AI/ML Model Sharing","text":"<p>Beyond data preparation, Practicus AI can also be used to export ML models to Excel, which can be used for different purposes. Below you can find some use cases to export your models to Excel. </p> <ul> <li> <p>Practicus AI exported models can help with ML deployment and testing and increase your chances of getting them to production to be used by masses.   </p> </li> <li> <p>The exported models have zero dependency, meaning they only use core Excel functionality and do not depend on 3rd party libraries, products, services, REST APIs etc. You can attach the exported ML model to an email, and the recipient would be able to predict / infer offline without any issues. </p> </li> <li> <p>You can use the exported Excel file to debug your models, since the model representation will be in a very simple form. </p> </li> <li> <p>Model Explainability can be a key blocker for getting ML models to production. Often times, data analysts, business analysts and other business leaders will not allow moving an ML model to production, simply because they do not understand how the model works. Practicus AI exported models in Excel will be significantly easier to consume and understand.</p> </li> </ul> <p>Basic model sharing use case</p> <p>1) Build your ML model as usual. </p> <pre><code># sample Support Vector Machine model\n...\nmy_svm_model.fit(X, Y)\n</code></pre> <p>2) Export to Excel </p> <pre><code>import practicus    \npracticus.export_model(my_svm_model, output_path=\"iris_svm.xlsx\",\n      columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>3) Open the file in Excel, Google Sheets, LibreOffice or others to make predictions, analyze how the model works and make changes as well.</p> <p></p> <p></p> <p></p> <p>4) (Optional) You can use pre-processing pipelines as well. Necessary calculations prior to model prediction will also be exported to Excel as pre-processing steps.   </p> <pre><code># create a pipeline with StandardScaler and LogisticRegression\nmy_pipeline = make_pipeline(\n   StandardScaler(),\n   LogisticRegression())\n\n# train\nmy_pipeline.fit(X, y)\n\n# Export the pre-processing and model calculation to Excel\npracticus.export_model(my_pipeline, output_path=\"model_with_pre_processing.xlsx\",\n                   columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>5) (Optional) You can also export models with Practicus AI using PMML. If you are using R, KNIME, Alteryx or any other ML platform, you can export your models to a .pmml file first (optionally including pre-processing steps as well) and then use the .pmml file with Practicus AI in Python to export it to Excel. The final Excel file will not have any dependencies to your ML platform. </p> <pre><code>practicus.export_model(\"my_model_developed_on_R.pmml\", output_path=\"R_model.xlsx\",\n                   columns=['petal length', 'petal width'], target_name=\"Species\")\n</code></pre>"},{"location":"setup-guide/","title":"Setup Guide","text":"<p>Welcome! It should take a few minutes to set up everything and be ready to go!</p>"},{"location":"setup-guide/#overview","title":"Overview","text":"<p>You can see a simplified view of Practicus AI setup options below. </p> <ul> <li>Practicus AI App is Forever Free and include common analytics and data preparation features. </li> <li>Cloud Workers with Forever Free Tier are optional but highly recommended. They bring in more functionality such as AutoML. You can choose one or more Cloud Worker system. </li> </ul> <p></p>"},{"location":"setup-guide/#install-the-practicus-ai-app","title":"Install the Practicus AI App","text":"<p>Practicus AI App works on your computer and contains the core functionality of our platform. </p> <p>If you haven't already, please install Practicus AI App for Windows, macOS or Linux.  </p> <p>If you are a Python user and prefer to Install the Practicus AI App as a library, please check our Python Setup Guide section below. </p> <p>If you are a programmer and only interested in installing the lightweight Practicus AI SDK (5MB), please only install practicuscore library using the Python Setup Guide section below.</p>"},{"location":"setup-guide/#optional-choose-a-cloud-worker-system-backend","title":"Optional - Choose a Cloud Worker System (backend)","text":"<p>If you prefer the quickest option, you can use our Software as a Service (SaaS), which will be ready in less than a minute. Click here to start Practicus AI SaaS trial </p>"},{"location":"setup-guide/#what-is-a-cloud-worker","title":"What is a Cloud Worker?","text":"<p>Some Practicus AI features such as AutoML, making ** AI Predictions, Advanced Profiling and production deployment** capabilities require a larger setup, so we moved these from the app to a backend (server) system.  </p> <p>You have multiple Cloud Worker options to choose from, and you can find a quick summary on pros and cons of each option below.</p> <p>Please view the detailed comparison table for the pros and cons for each option. </p>"},{"location":"setup-guide/#software-as-a-service","title":"Software as a Service","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p>"},{"location":"setup-guide/#enterprise-cloud","title":"Enterprise Cloud","text":"<p>You cna think of the Enterprise Cloud as a private Practicus AI SaaS that can run on any cloud, or your private data enter. </p> <p>Configuration is exactly the same as Software as a Service. You only need to change your service address from https://service.practicus.io to your private address that your administrator provided, such as https://practicus.your_company.com</p> <p>Your password will be provided by your administrator. You can also click the Forgot Password button to reset your password. </p> <p></p>"},{"location":"setup-guide/#aws-marketplace","title":"AWS Marketplace","text":"<p>This section explains setting up AWS Cloud Workers. </p> <p></p>"},{"location":"setup-guide/#before-we-begin","title":"Before we begin","text":"<p>Some of our advanced features require cloud capacity. </p> <p>Instead of asking for access to your data, Practicus AI cloud runs 100% in your AWS account, fully isolated. This allows us to offer improved security and absolute privacy.</p> <p>You can activate Practicus AI on your AWS account in a few minutes, and use up to a certain cloud capacity free of charge.  Let's get started.</p>"},{"location":"setup-guide/#1-select-license","title":"1) Select license","text":"<p>Practicus AI cloud offers 3 different licenses. </p>"},{"location":"setup-guide/#free-license","title":"Free License","text":"<p>We offer a forever free cloud tier using 2 vCPUs and 1 GB RAM on AWS with t3.micro cloud instances.</p> <p>Please note that some AWS cloud regions charge roughly 1 cent / hour for t3.micro (see below). If you need to make sure everything your use is absolutely free, please make sure you pick an appropriate AWS cloud region. AWS also offer free tiers for other potential charges like S3 storage and network traffic. To keep everything free, you must experiment responsibly and make sure you do not go beyond these AWS limits. </p> <ul> <li>Please view AWS free tier details to learn more.</li> </ul>"},{"location":"setup-guide/#professional-license","title":"Professional license","text":"<p>When you need larger Practicus AI cloud capacity, you can simply pay-as-you-go (PAYG) hourly without sharing your credit card info, or making any commitments. AWS will charge for the Practicus AI cloud usage hourly, which will show as a separate Practicus AI line item in your monthly AWS cloud invoice. </p> <p>Practicus AI cloud works like electricity, you switch it on when you need it, and only pay for the total number of hours that the lights are on. Our Cloud Workers auto shut down after 90 minutes of inactivity by default, preventing unexpected costs. It works similar to this scenario: you leave your home and forget to turn the lights off. Your lights turn off automatically after 1.5 hours, since there is no motion detected in the house. </p> <p>Please visit practicus.ai/pricing for more info, and example pricing scenarios.   </p>"},{"location":"setup-guide/#enterprise-license","title":"Enterprise license","text":"<p>We offer a bring-your-own-license (BYOL) model with benefits such as unlimited use for multiple users, with a fixed fee. </p> <p>If you have an enterprise license, simply open Practicus AI app and go to settings (preferences in macOS), cloud tab, and enter your email to activate your license.</p> <p>Please feel free to contact us to learn more about our enterprise support and licensing.</p> <p>Use your email to activate the enterprise license in app settings: </p>"},{"location":"setup-guide/#2-ready-your-aws-cloud-account","title":"2) Ready your AWS cloud account","text":"<p>This is a one-time task for all users sharing the same AWS account. </p> <p>Please skip this step if you already have an AWS account. </p> <p>Create an Amazon Web Services (AWS) cloud account for free. </p> <p>Please make sure you have created an admin user as well. You can check our AWS account creation guide below for help.</p>"},{"location":"setup-guide/#3-enable-on-aws-marketplace","title":"3) Enable on AWS marketplace","text":"<p>This is a one-time task for all users sharing the same AWS Account. </p> <p>Learn more about AWS marketplace. </p> <p>We have multiple offerings on AWS marketplace. Please click on each in the below list to view the marketplace page explaining the offering, and then click Continue to Subscribe button to enable (see screenshot below). You need at least one AWS marketplace offering enabled to use Practicus AI cloud. </p> <p>Please note that it can take a few minutes for the offering to be active. Once your subscription is active, please do not create a new EC2 instance using AWS cloud console. The next step will take care of configuration.</p>"},{"location":"setup-guide/#free-professional-pay-as-you-go-payg","title":"Free / Professional pay-as-you-go (PAYG)","text":"<ul> <li>Practicus AI - Most common, offers free tier, will give you all the functionality.</li> <li>Practicus AI with GPUs - Accelerated computing for very large data or if you have limited time. Can be 500+ times faster for some operations. </li> </ul>"},{"location":"setup-guide/#enterprise-license-bring-your-own-license-byol","title":"Enterprise License bring-your-own-license (BYOL)","text":"<ul> <li>Practicus AI - Most common enterprise offer</li> <li>Practicus AI with GPUs - Accelerated computing, enterprise offer</li> </ul> Sample view of our AWS marketplace page <p>Please carefully review software, hardware and total cost / hour on our AWS marketplace page. Sample professional license below: </p> <p></p>"},{"location":"setup-guide/#4-activate-your-aws-user-in-the-app","title":"4) Activate your AWS user in the app","text":"<p>You should now have an AWS user Access key ID and Secret access key ready, and the AWS account for this user has at least one Practicus AI AWS marketplace offer enabled.   </p> <p>Simply open the Practicus AI app, go to settings (preferences in macOS), cloud tab, click the Activate your AWS user button, choose a default cloud region (you can change this later) and enter your cloud credentials:</p> <p></p> <p>Please note that your cloud credentials are not shared with 3rd parties, including Practicus AI. The app only uses the credentials to communicate with AWS cloud.</p> <p>Before you finalize the cloud settings, we will verify your configuration to check if everything is configured correctly.    </p> <p></p> Sample AWS marketplace verification result. You need at least one verified <p>Pro Tip: You can save the cloud configuration info to a file and share with others, so they can open this file with Practicus AI app and automatically configure the cloud setup. Please check the Setup for others section below to learn more.  </p>"},{"location":"setup-guide/#troubleshooting","title":"Troubleshooting","text":"<p>If you could not verify one of the AWS marketplace offers, please check the below as potential reasons:</p> <ul> <li>AWS marketplace activation can take a few minutes. Please make sure you stay on the AWS marketplace page, confirm the activation is completed and go back to the app settings to verify again.  </li> <li>If you use multiple AWS accounts, please make sure you  subscribe using the correct AWS account since it is very easy to mix them up. Simply log-off from your AWS account, click on one of the view buttons inside the app settings to view AWS marketplace page again, login using the correct user, click subscribe, wait for it to be completed, and finally go back to app settings and click verify again.</li> <li>In rare cases, a company admin can disable AWS marketplace usage. If this is the case, please contact your admin, or create a new AWS account. </li> </ul>"},{"location":"setup-guide/#local-container","title":"Local Container","text":"<p>This section explains setting up a local container Cloud Worker.</p>"},{"location":"setup-guide/#1-install-a-container-engine","title":"1) Install a container engine","text":"<p>In order to run a container on your computer you need to first install a container engine.</p> <p>Docker is the most popular option: Install Docker Desktop </p> <p>Although Docker Desktop is free, there has been some licensing changes in the recent years. </p> <p>Podman is a great Docker alternative: Install Podman Desktop</p> <p>Once the installation is completed, simply run Docker or Podman Desktop and confirm the container engine is running.</p> <p>Active Docker Desktop </p> <p></p> <p>Active Podman Desktop </p> <p></p>"},{"location":"setup-guide/#2-pull-download-practicus-ai-container-image","title":"2) Pull (download) Practicus AI container image","text":"<p>Additional Practicus AI software is bundled inside a container image. You need to pull this package on your computer before using it. </p> <ul> <li>Open Practicus AI App settings (preferences in macOS) dialog and navigate to the Container section.</li> <li>If you have a Practicus AI Enterprise license, enter your email to activate and unlock all features. If not, you can use the free tier. Please note that Professional pay-as-you-go license option is not available for local containers. Compare license options.   </li> <li>Choose a container engine, Docker or Podman, and confirm in the app the engine is running.   </li> <li>Click the Pull (download) Practicus AI container image button</li> </ul> <p></p> <ul> <li>A command prompt window (terminal in macOS) will open to start the pull. This one-time download task can take anywhere between 5 - 20 minutes depending on your internet speed. </li> </ul> <p></p> <ul> <li>Once the container pull is completed, go back to the app and click refresh to view active Practicus AI images on your computer. Confirm you successfully pulled the container image.</li> <li>Click New Cloud Worker button to open Cloud Workers tab.</li> <li>Click Save to close settings.</li> </ul> <p></p> <ul> <li>In the Cloud Workers tab, select local container as Cloud Region.</li> <li>Click Launch New button to start a Cloud Worker.</li> </ul> <p></p> <ul> <li>When navigating cloud data sources in the Explore tab, you can switch between local and Cloud Workers by using the drop-down list at the top right.</li> <li>Practicus AI app also attaches (mounts) container_shared folder, so you can easily copy files back and forth between your file system and the container. Simply open Windows Explorer (Finder in macOS), navigate to: your home folder / practicus / container_shared and copy files. Then navigate to Cloud Worker Files in Explore tab, and the files you copied will be visible under container_shared folder. Click Reload button at the top if you recently copied files.  </li> </ul> <p></p> <p>Issues? If you are facing any issues, please check the container troubleshooting section below.</p>"},{"location":"setup-guide/#references","title":"References","text":""},{"location":"setup-guide/#aws-account-creation","title":"AWS Account Creation","text":"<p>Practicus AI Cloud Workers can start and stop with a click using our app, and they run in your Amazon Web Services (AWS) cloud account in a fully isolated fashion. This way we can offer you 100% privacy. </p> <p>Please follow the below steps to create a free AWS account.</p> <ol> <li>Please click here to visit AWS home page and click Create an AWS account</li> <li>Follow the steps to finish account creation. Please check the AWS help page if you need assistance on creating an account. After this step you will have a root account.</li> <li>Login to AWS management console using your root account. </li> <li>Navigate to IAM (Identity and Access Management), click Users on the left menu and click Add users</li> <li>For User name enter admin, click Access key and Password check boxes (see below screenshot)</li> <li>In Set permissions section select Attach existing policies directly and pick AdministratorAccess (see below screenshot)</li> <li>In the last screen carefully save your Access Key ID, Secret access key and Password (see below screenshot)</li> <li>All done! You can continue with the next step, Enabling on AWS marketplace </li> </ol> <p>Notes:</p> <ul> <li>Although the admin AWS user will be sufficient for Practicus AI Cloud Workers to run, as a security best practice we recommend you to create a least privileged AWS user for day to day usage.  Practicus AI app cloud setup can create this user for you. If you rather prefer to create one manually please make sure the user has access for EC2 and S3 operations. </li> <li>If you plan on having multiple AWS users sharing the same AWS account, you can simply add new users to the appropriate practicus AWS IAM user group that our app creates for you. </li> <li>If you have a local AWS profile (i.e. to be used with AWS CLI) Practicus AI setup can use this directly. </li> </ul> AWS account creation screenshots <p></p> <p></p> <p></p>"},{"location":"setup-guide/#aws-marketplace-reference","title":"AWS Marketplace Reference","text":"<p>Similar to our Windows app being available on Microsoft app store, our cloud workers are available on AWS marketplace. This gives our users \"there's an app for that!\" type experience, but for AI in the cloud. </p> <p>Any time you need to do AI in the cloud, you can just click a button in the Practicus AI app, and we will create the cloud capacity of your choice. And also shut it down with a click when you no longer need it, saving on cost.,</p> <p>For our app to be able to start/stop cloud worker you need to enable (subscribe to) the AWS marketplace offering of your choice. </p> <p>If you use the free tier (t3.micro) with 2 vCPUs and 1 GB RAM, there won't be any software license charges. AWS also offers t3.micro free of charge for eligible customers and regions. For larger capacity, AWS will charge you per hour. i.e. you start a cloud worker, use it for 2 hours and never use it again. Your cloud bill will have a line item for the 2 hours you use. Larger capacity is more expensive and the larger the capacity the bigger discount you will get.  </p>"},{"location":"setup-guide/#setup-for-others","title":"Setup for others","text":"<p>You can save cloud setup information to a file and share with others, so they can simply open the file with Practicus AI app to complete the cloud setup. </p> <p>Practicus AI uses .prt files to save worksheet data and steps. Since .prt files directly open with Practicus AI app, we use the same file extension to configure the app as well.</p> <p>You can simply create a text file, give it a name such as cloud_setup.prt and add setup information like the below:</p> <p>Sample cloud_setup.prt file: <pre><code>[add_license]\nemail = user@mycompany.com\n\n[add_cloud_config]\ncloud_region = us-west-1\naccess_key = ...\nsecret_key = ...\n</code></pre></p> <p>[add_license] section can be used if you have an enterprise license. </p> <p>[add_cloud_config] section can be used to add an AWS cloud configuration. You can use the cloud_region key to set the default region (can be changed later), and it is optional.  access_key (Access Key ID) and secret_key (Secret access key) are mandatory, and can be obtained during AWS IAM user creation. </p>"},{"location":"setup-guide/#container-troubleshooting","title":"Container troubleshooting","text":"<p>If you face any issues using local containers on your computer, please follow the below steps.   </p> <p>View Status on Container Engine</p> <p>Open Docker Desktop or Podman Desktop, navigate to containers, and iew it's status. You can delete using these apps, which is the same thing as Terminate in Practicus AI, and create a new one using Launch New button inside Practicus AI App Cloud Workers tab.</p> <p>Starting a Container manually</p> <p>You can use the below command prompt (terminal) command to start a container from an image you downloaded.</p> <pre><code>docker run -p9000:9000 -p5500:5500 -p8585:8585 -p50000-50020:50000-50020 -d \\\n  --mount type=bind,src=$HOME/practicus/container_shared/,dst=/home/ubuntu/container_shared \\\n  --env MAX_PRACTICUS_WORKERS=21 \\\n  --env DEBUG_MODE=False \\\n  --name practicus ghcr.io/practicusai/practicus:22.11.0 \\\n  --env PRACTICUS_ENT_LIC_EMAIL='your-name@your-company.com' \\\n  --env PRACTICUS_ENT_LIC='abcd-1234-abcd-1234-abcd'\n</code></pre> <p>Notes:</p> <ul> <li>Please do not forget to create container_shared folder under your home directory / practicus folder first.   </li> <li>No Enterprise License? Please remove PRACTICUS_ENT_LIC_EMAIL and PRACTICUS_ENT_LIC lines.</li> <li>To find your Enterprise license key (PRACTICUS_ENT_LIC), please view your home directory / .practicus / core.conf file and search for text 'license_key'. Please note that .practicus is a hidden folder.</li> <li>Replace 'docker' with podman or another container engine if you are not using docker. </li> <li>Need less or more workers? change MAX_PRACTICUS_WORKERS and also the port range -p50000-50020:50000-50020. E.g. For 10 workers use  MAX_PRACTICUS_WORKERS=10 -p50000-50009:50000-50009</li> <li>If you use network proxies, add them in the below format <pre><code># ...\n  --env PRACTICUS_ENT_LIC='abcd-1234-abcd-1234-abcd' \\\n  --env HTTP_PROXY='http://192.168.0.1:9090' \\\n  --env HTTPS_PROXY='http://192.168.0.1:9090'  \n</code></pre></li> </ul> <p>Pulling a new container manually</p> <ul> <li>Please visit https://github.com/practicusai/deploy/pkgs/container/practicus and choose a version of Cloud Worker to pull. Use matching yy.mm versions between the App and the container. E.g. If your app is 22.11.2 you can use any 22.11.x Cloud Worker. Ideally use the latest e.g. 22.11.5 instead of 22.11.4. If you use Practicus AI App to pull, it already pulls the latest compatible image.</li> <li>Pull using the command. Prefer to use version tags instead of 'latest'</li> </ul> <pre><code>docker pull ghcr.io/practicusai/practicus:22.11.0\n</code></pre> <p>Hard reset</p> <p>Sometimes previously downloaded images can linger in the cache even if you deleted them. In order to hard reset and delete everything, you can use the below command.</p> <pre><code># Use with caution! This will delete all images and containers\ndcker image prune --force \n</code></pre>"},{"location":"setup-guide/#linux-installation","title":"Linux Installation","text":"<p>Since almost all Linux distros come with Python 3 installed, we do not offer a prepackaged installer. </p> <p>Please check the quick start guide below to see how you can install and run Practicus AI app on Linux. We have extensively tested on Ubuntu, if you encounter any issues please share with us using feedback section.</p>"},{"location":"setup-guide/#python-setup-guide","title":"Python Setup Guide","text":"<p>You can run pip install practicus (Windows/macOS/Linux: Python 3.10+) and then run practicus from the terminal. Or run, python -c \u201cimport practicus; practicus.run()\u201d  (python3 for macOS or Linux).</p> <p>Installing using pip will give you the exact same end-to-end GUI application experience. Similarly, if you download the packaged app you can still code freely when you want to. So, please select any method of installation as you prefer. </p> <p>As for any Python application, we strongly recommend you to use a virtual environment such as venv or conda. Please check the recommended QuickStart scripts on this page to create a virtual env, install Practicus AI and run with peace of mind and in one go.  </p> <p>For server environments or API only usage, you can pip install practicuscore to install the core library by itself without the GUI elements. (Windows/macOS/Linux: Python 3.10+) </p> <p>This is a small library with fewer requirements and no version enforcement for any of its dependencies. It\u2019s designed to run in existing virtual environments without overriding the version requirements of other libraries. Please check the documentation for more details. </p>"},{"location":"setup-guide/#windows-quickstart","title":"Windows QuickStart","text":"<pre><code>:: install \npython -m venv %UserProfile%\\practicus\\venv\n%UserProfile%\\practicus\\venv\\Scripts\\python -m pip install --upgrade practicus\n\n\n:: run\n%UserProfile%\\practicus\\venv\\Scripts\\activate\npracticus\n</code></pre>"},{"location":"setup-guide/#macos-quickstart","title":"macOS QuickStart","text":"<pre><code># install\npython3 -m venv ~/practicus/venv \n~/practicus/venv/bin/python -m pip install --upgrade practicus\n\n\n# run\nsource ~/practicus/venv/bin/activate\npracticus\n</code></pre>"},{"location":"setup-guide/#linux-quickstart","title":"Linux QuickStart","text":"<pre><code># install\npython3 -m venv ~/practicus/venv \n~/practicus/venv/bin/python -m pip install --upgrade practicus\n\n\n# run\nsource ~/practicus/venv/bin/activate\npracticus\n\n# Note: if you get qt.qpa.plugin error please run\nsudo apt-get install '^libxcb.*-dev' libx11-xcb-dev libglu1-mesa-dev libxrender-dev libxi-dev libxkbcommon-dev libxkbcommon-x11-dev\n</code></pre>"},{"location":"setup-guide/#starter-app","title":"Starter app","text":"<p>Instead of running Practicus AI from the command prompt (terminal) you can create shortcuts and run with double click. The below tips assume you installed using the QuickStart tips above.</p> <p>Windows: Navigate to ** %UserProfile%\\practicus\\venv\\Scripts\\ ** folder and locate practicus.exe, which is essentially a starter for practicus Python library. You can right-click and select pin to start. You can also create a shortcut to this .exe and change its name to Practicus AI and its icon by downloading our icon practicus.ico. </p> <p>macOS: You can download Practicus AI Starter app which is a tiny (100KB) app that starts the Python virtual env in ** ~/practicus/venv ** and then starts Practicus AI GUI from practicus Python library. To keep the app in dock please drag &amp; drop the .app file on the dock itself. Right-clicking and choosing \u201ckeep in dock\u201d will not create a shortcut.</p>"},{"location":"share-models/","title":"Sharing AI Models use case","text":"<p>Beyond data preparation, Practicus AI can also be used to export ML models to Excel, which can be used for different purposes. Below you can find some use cases to export your models to Excel. </p> <ul> <li> <p>Practicus AI exported models can help with ML deployment and testing and increase your chances of getting them to production to be used by masses.   </p> </li> <li> <p>The exported models have zero dependency, meaning they only use core Excel functionality and do not depend on 3rd party libraries, products, services, REST APIs etc. You can attach the exported ML model to an email, and the recipient would be able to predict / infer offline without any issues. </p> </li> <li> <p>You can use the exported Excel file to debug your models, since the model representation will be in a very simple form. </p> </li> <li> <p>Model Explainability can be a key blocker for getting ML models to production. Often times, data analysts, business analysts and other business leaders will not allow moving an ML model to production, simply because they do not understand how the model works. Practicus AI exported models in Excel will be significantly easier to consume and understand.</p> </li> </ul> <p>Basic model sharing use case</p> <p>1) Build your ML model as usual. </p> <pre><code># sample Support Vector Machine model\n...\nmy_svm_model.fit(X, Y)\n</code></pre> <p>2) Export to Excel </p> <pre><code>import practicus    \npracticus.export_model(my_svm_model, output_path=\"iris_svm.xlsx\",\n      columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>3) Open the file in Excel, Google Sheets, LibreOffice or others to make predictions, analyze how the model works and make changes as well.</p> <p></p> <p></p> <p></p> <p>4) (Optional) You can use pre-processing pipelines as well. Necessary calculations prior to model prediction will also be exported to Excel as pre-processing steps.   </p> <pre><code># create a pipeline with StandardScaler and LogisticRegression\nmy_pipeline = make_pipeline(\n   StandardScaler(),\n   LogisticRegression())\n\n# train\nmy_pipeline.fit(X, y)\n\n# Export the pre-processing and model calculation to Excel\npracticus.export_model(my_pipeline, output_path=\"model_with_pre_processing.xlsx\",\n                   columns=iris.feature_names, target_name=\"Species\")\n</code></pre> <p>5) (Optional) You can also export models with Practicus AI using PMML. If you are using R, KNIME, Alteryx or any other ML platform, you can export your models to a .pmml file first (optionally including pre-processing steps as well) and then use the .pmml file with Practicus AI in Python to export it to Excel. The final Excel file will not have any dependencies to your ML platform. </p> <pre><code>practicus.export_model(\"my_model_developed_on_R.pmml\", output_path=\"R_model.xlsx\",\n                   columns=['petal length', 'petal width'], target_name=\"Species\")\n</code></pre>"},{"location":"trial-ent-saas/","title":"Enterprise License and limited SaaS Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your Enterprise license will unlock all advanced features.   </p>"},{"location":"trial-ent-saas/#activating-your-enterprise-license","title":"Activating your Enterprise License","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is not included in your trial. </li> <li>Please click here to start Practicus AI SaaS trial </li> </ul> <p>Enterprise Kubernetes</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p>"},{"location":"trial-ent-saas/#optional-using-your-limited-saas-account","title":"Optional - Using your limited SaaS account","text":"<p>Since we are allocating our online resources to production workloads, your SaaS account is limited to online features such as GPT, but not Cloud Workers. </p> <p>For features such as AutoML, please use Cloud Workers on local containers or AWS marketplace.</p>"},{"location":"trial-ent-saas/#sample-data-exploration-view-utilizing-cost-effective-options","title":"Sample Data Exploration view, utilizing cost-effective options","text":""},{"location":"trial-ent-saas/#using-gpt-and-other-online-only-features-with-your-limited-saas","title":"Using GPT and other \"online only\" features with your limited SaaS","text":"<p>When you use the App + local container or AWS marketplace option, the App intelligently decides when an \"online only\" feature such as GPT is required to use the SaaS. </p> <p>By combining local container or AWS marketplace with the limited SaaS, you will be able to trial all features with no limitations, and in a cost-effective way. </p>"},{"location":"trial-ent-saas/#unlimited-saas-option","title":"Unlimited SaaS option","text":"<p>If you prefer to use our SaaS unlimited, please click here to start Practicus AI SaaS trial </p>"},{"location":"trial-ent-saas/#logging-in-to-your-limited-saas-account","title":"Logging in to your limited SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-ent/","title":"Enterprise License Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your Enterprise license will unlock all advanced features.   </p>"},{"location":"trial-ent/#activating-your-enterprise-license","title":"Activating your Enterprise License","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is not included in your trial. </li> <li>Please click here to start Practicus AI SaaS trial </li> </ul> <p>Enterprise Kubernetes</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-saas-ent/","title":"SaaS and Enterprise License Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Both your SaaS account and the enterprise license will unlock all advanced features.   </p>"},{"location":"trial-saas-ent/#logging-in-to-your-saas-account","title":"Logging in to your SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p>"},{"location":"trial-saas-ent/#optional-activating-your-enterprise-license","title":"Optional - Activating your Enterprise License","text":"<p>If you prefer to use local containers on your laptop offline, or your personal AWS account (GPUs available) instead of Practicus AI SaaS, please activate your enterprise license.</p> <p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Pick one or more Cloud Worker (backend system) options. </p> <p>View detailed comparison</p> <p>Container Quick Overview</p> <ul> <li>Runs on your laptop</li> <li>Pros: Free</li> <li>Cons: You need to download Docker Desktop (free)</li> </ul> <p>AWS Marketplace Quick Overview </p> <ul> <li>Runs on your AWS account</li> <li>Pros: No need to download extra software </li> <li>Cons: You pay for AWS cloud usage  </li> </ul> <p>Software as a Service</p> <ul> <li>This option is included in your trial. </li> </ul> <p>Enterprise Cloud</p> <ul> <li>This option is not included in your trial. </li> <li>Please contact us get Enterprise Cloud </li> </ul> <p>View detailed comparison</p> <p>4.a) Container Option: </p> <ul> <li>Open Container tab in settings</li> <li>Enter your email and click Activate</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Enable Container Support and follow the steps to pull (download) Practicus AI container on your laptop</li> </ul> <p>View local container setup guide </p> <p></p> <p>4.b) AWS Marketplace Option: </p> <ul> <li>Open AWS tab in settings</li> <li>Enter your email and click Activate. You can skip this step if you already activated for containers. Simply close and re-open App settings.</li> <li>You will receive your license code as email. Please check your spam folder if you did not receive</li> <li>Click Activate your AWS user button and follow the steps</li> </ul> <p>View AWS cloud marketplace setup guide </p> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"trial-saas/","title":"Software as a Service (SaaS) Trial Activation","text":"<p>Practicus AI App is forever free without any limitations. For advanced features such as AutoML, you can use Cloud Workers.</p> <p>Cloud Workers also offer a free tier, but with some limitations. Your SaaS account will unlock all advanced features.   </p>"},{"location":"trial-saas/#logging-in-to-your-saas-account","title":"Logging in to your SaaS account","text":"<p>1) Install the Practicus AI App</p> <p>2) Open App settings </p> <p></p> <p>3) Login to service</p> <ul> <li>Click Login button  </li> </ul> <p></p> <ul> <li>Enter your email and password and click OK</li> <li>You should have received your password in email, please check your spam folder if you haven't</li> <li>To reset your password, click Forgot Password</li> </ul> <p></p> <p>If you need any, help please view the detailed setup guide or contact support.</p> <p>Thank you!</p>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Practicus AI offers two primary types of tutorials\u2014one designed for technical users who prefer to code and another suited for those who don\u2019t code, using AI Studio\u2019s more intuitive, no-code/low-code capabilities.</p>"},{"location":"tutorials/#technical-tutorial-if-you-code","title":"Technical Tutorial (if you code)","text":"<p>You can start by reading the technical tutorial right here in the documentation\u2014simply open the Technical Tutorial</p>"},{"location":"tutorials/#for-a-hands-on-coding-experience-recommended","title":"For a hands-on coding experience (recommended)","text":"<p>If you\u2019d like a hands-on experience, follow these steps to run the tutorial notebooks in your Practicus AI environment:</p> <ol> <li> <p>Create a Worker:    From Practicus AI Home, create a Worker.</p> </li> <li> <p>Open Jupyter Lab or VS Code:    Open Jupyter Lab or VS Code on the worker you just created.</p> </li> <li> <p>View the <code>samples/notebooks</code> Directory:    Here you\u2019ll find the tutorial notebooks that match exactly to what you read in the technical tutorial.</p> </li> </ol>"},{"location":"tutorials/#ai-studio-tutorial-if-you-do-not-code","title":"AI Studio Tutorial (if you do not code)","text":"<p>If coding isn\u2019t your focus, the AI Studio tutorial is ideal:</p> <ol> <li> <p>Access Practicus AI Studio or Workspaces:    From Practicus AI Home, open a browser-based Workspace or download AI Studio. </p> </li> <li> <p>Follow the tutorial steps    Open AI Studio tutorial and start following the tutorial steps.</p> </li> </ol> <p>Next: Choose one of,</p> <ul> <li>Technical Tutorial If you code</li> <li>AI Studio tutorial If you do not code </li> </ul>"},{"location":"notebooks/01_getting_started/01_introduction/","title":"Introduction","text":"<p>Getting started with Practicus AI is straightforward.</p>"},{"location":"notebooks/01_getting_started/01_introduction/#common-usage-patterns","title":"Common Usage Patterns","text":"<p>The following steps outline a typical workflow for users who write code:</p> <ol> <li>Log in to your chosen region (e.g., <code>https://practicus.your-company.com</code>).</li> <li>Create one or more workers with the desired features and resource capacities.</li> <li>Start an IDE, such as JupyterLab or VS Code, within a worker.</li> <li>Develop models, applications, and process data as usual.</li> <li>Deploy models, applications, or use add-ons (e.g., create Airflow workflows).</li> </ol> <p></p>"},{"location":"notebooks/01_getting_started/01_introduction/#practicus-ai-platform-components","title":"Practicus AI Platform Components","text":"<p>Below are the primary components you will interact with when using Practicus AI.</p>"},{"location":"notebooks/01_getting_started/01_introduction/#practicus-ai-workers","title":"Practicus AI Workers","text":"<p>Practicus AI Workers are dedicated compute environments that run ML, data processing, and other tasks.</p> <p>Key characteristics include:</p> <ul> <li>On-demand: Request as many workers as you need, available within seconds.</li> <li>Interactive: Launch JupyterLab or VS Code for hands-on experimentation.</li> <li>Batch-capable: Run tasks or jobs in non-interactive mode as well.</li> <li>Isolated: Issues in other workers or systems do not affect your worker.</li> <li>Configurable: Each worker is defined by a container image, which can be chosen from the provided options or customized.</li> <li>Flexible Resources: Assign a specific amount of CPU, memory, and GPU resources.</li> <li>Ephemeral: Workers can be replaced easily. Since each restart resets the file system, save important files in <code>~/my</code> or <code>~/shared</code> to preserve them, or push to a source control system such as git.</li> </ul>"},{"location":"notebooks/01_getting_started/01_introduction/#practicus-ai-modelhost","title":"Practicus AI ModelHost","text":"<p>Practicus AI ModelHost deployments run classic ML and LLM models, optimized for CPUs and GPUs.</p> <ul> <li>Shared deployments can host thousands of models, each with up to 100 versions.</li> <li>Isolated deployments allow you to create a dedicated environment for a set of models.</li> </ul>"},{"location":"notebooks/01_getting_started/01_introduction/#practicus-ai-apphost","title":"Practicus AI AppHost","text":"<p>Practicus AI AppHost deployments are used to build visual Gen AI applications or microservices focused on ML workflows.</p>"},{"location":"notebooks/01_getting_started/01_introduction/#practicus-ai-add-ons","title":"Practicus AI Add-ons","text":"<p>Practicus AI Add-ons, such as Airflow or MLflow, extend the platform\u2019s core functionality. They integrate seamlessly, allowing you to manage and orchestrate complex workflows and track experiments.</p>"},{"location":"notebooks/01_getting_started/01_introduction/#practicus-ai-regions","title":"Practicus AI Regions","text":"<p>Practicus AI is a multi-region environment, where each region is a separate deployment and isolated Kubernetes namespace. Regions can differ by geography, cloud vendor, lifecycle stage, department, or security requirements.</p> <p>For example, you might have:</p> <ul> <li>One region in a certain geographic location and another in a different one.</li> <li>Regions across different cloud vendors (e.g., AWS, Azure, on-premises).</li> <li>Separate regions for production, development, or testing.</li> <li>Regions dedicated to different departments or security contexts.</li> </ul>"},{"location":"notebooks/01_getting_started/01_introduction/#practicus-ai-clients","title":"Practicus AI Clients","text":"<p>Practicus AI clients enable you to connect to multiple regions seamlessly, allowing you to develop in one region and deploy in another. Common client options include:</p> <ul> <li>Browser: Access the platform via a standard web interface to launch JupyterLab, VS Code, and manage workloads.</li> <li>AI Studio: A desktop application for Windows, macOS, and Linux that connects to multiple regions for unified management.</li> <li>SDK: Install the SDK (<code>pip install practicuscore</code>) to interact programmatically with any Practicus AI region.</li> <li>CLI: With the SDK installed, use the <code>prtcli</code> command-line tool to manage tasks and resources.</li> </ul>"},{"location":"notebooks/01_getting_started/01_introduction/#example-a-multi-region-setup","title":"Example: A Multi-Region Setup","text":"<p>Below is an example of a deployment where a customer utilizes three regions in two geographies, accessible through various clients.</p> <p></p> <p>Next: Workers</p>"},{"location":"notebooks/01_getting_started/02_workers/","title":"Workers","text":""},{"location":"notebooks/01_getting_started/02_workers/#using-workers","title":"Using Workers","text":"<p>This example illustrates the basic workflow of creating a worker, accessing it through JupyterLab, performing tasks, and then terminating the environment when finished.</p>"},{"location":"notebooks/01_getting_started/02_workers/#steps","title":"Steps","text":"<ol> <li> <p>Create a Worker:    From the Practicus AI interface, request a new worker with the required specifications (e.g., CPU, RAM, GPU). This process typically takes a few seconds.</p> </li> <li> <p>Access JupyterLab or VS Code:    Once the worker is ready, start a JupyterLab session directly on it. Here you can:</p> </li> <li>Develop and run code</li> <li>Explore and process data</li> <li> <p>Train and evaluate models</p> </li> <li> <p>Perform Tasks:    Within the JupyterLab environment, you can execute standard Python notebooks, run scripts, or use a variety of integrated tools and libraries.</p> </li> <li> <p>Terminate the Worker:    After completing your work, simply stop or delete the worker. A new worker can be created at any time, providing a clean environment without leftover state.</p> </li> </ol> <p>This workflow ensures you have an isolated, on-demand environment for efficient development and testing, while preserving the flexibility to scale resources and maintain a clean slate for subsequent tasks.</p>"},{"location":"notebooks/01_getting_started/02_workers/#worker-with-default-settings","title":"Worker with default settings","text":"<p>Let's create a worker with default settings.</p> <pre><code># Import the Practicus AI SDK\nimport practicuscore as prt\n</code></pre> <pre><code># Select a region (assuming a single-region setup)\nregion = prt.get_default_region()\n\n\nworker = region.create_worker()\n</code></pre> <pre><code># Start JupyterLab on the worker and open it in a new tab\nworker.open_notebook()\n\n# To use VS Code instead of JupyterLab, uncomment the line below:\n# worker.open_vscode()\n</code></pre> <pre><code># After using the worker, you can terminate it:\nworker.terminate()\n\n# Alternatively, if you're inside the worker environment, you can 'self-terminate' by running:\n# prt.get_local_worker().terminate()\n</code></pre>"},{"location":"notebooks/01_getting_started/02_workers/#customized-worker","title":"Customized Worker","text":"<p>Now let's create a worker with additional settings.</p> <pre><code># First step is to define what we need\nworker_config = prt.WorkerConfig(\n    # The below image has GenAI related features\n    worker_image=\"practicus-genai\",\n    # Worker sizes are defined by your admin, more on this later.\n    worker_size=\"X-Small\",\n\n    startup_script=\"\"\"\n    echo \"Hello Practicus AI\" &gt; ~/hello.txt\n    \"\"\",\n)\n\n# And create the worker with the custom configuration\nsecond_worker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Verify you have hello.txt in home directory\nsecond_worker.open_notebook()\n</code></pre> <pre><code>second_worker.terminate()\n</code></pre> <p>Previous: Introduction | Next: Modeling Basics</p>"},{"location":"notebooks/02_modeling/01_basics/modeling_basics/","title":"Introduction","text":"<p>In this example, we will walk through the process of building a simple XGBoost model using a small dataset. We\u2019ll begin by training and saving the model, and then demonstrate how to deploy it as a REST API endpoint. Finally, we\u2019ll make some predictions by calling the deployed API.</p> <p>By the end of this example, you will have a clear understanding of how to:</p> <ol> <li>Prepare and train a basic XGBoost model.</li> <li>Save the trained model to a file.</li> <li>Deploy the model as a simple API service.</li> <li>Make predictions by sending requests to the API.</li> </ol> <p>This approach can be extended and adapted to more complex models and larger systems, giving you a foundation for building scalable machine learning services.</p>"},{"location":"notebooks/02_modeling/01_basics/modeling_basics/#sample-xgboost-model","title":"Sample XGBoost model","text":"<p>Let's build a simple XGBoost model</p> <pre><code>import pandas as pd\nfrom xgboost import XGBRegressor\n\n# Load the ice cream dataset that come pre-installed in Workers\ndf = pd.read_csv(\"/home/ubuntu/samples/ice_cream.csv\")\n\n# Separate features and target\nX = df[[\"Temperature\"]]\ny = df[\"Revenue\"]\n\n# Create and train the XGBoost regressor\nmodel = XGBRegressor(n_estimators=50)\nmodel.fit(X, y)\n\n# Save the model using the recommended XGBoost .ubj format\nmodel.save_model(\"model.ubj\")\n\nprint(\"Model saved as model.ubj\")\n</code></pre>"},{"location":"notebooks/02_modeling/01_basics/modeling_basics/#how-to-query-for-deployments-and-prefixes","title":"How to Query for Deployments and Prefixes","text":"<p>If you are unsure about which model deployment (the underlying infrastructure) or which logical address group (prefix) to use, you can run the code below to dynamically query the available options.</p> <p>If you already have the required information, you can define it directly as shown here and skip the query step:</p> <pre><code>deployment_key = \"some-deployment\"\nprefix = \"models\"\n</code></pre> <p>The code below demonstrates how to programmatically identify the first available model deployment system and model prefix. These values are then used to construct a URL for deploying and accessing a model\u2019s REST API.</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\n\n# Identify the first available model deployment system\nif len(region.model_deployment_list) == 0:\n    raise SystemError(\n        \"No model deployment systems are available. \"\n        \"Please contact your system administrator.\"\n    )\nelif len(region.model_deployment_list) &gt; 1:\n    print(\"Multiple model deployment systems found. Using the first one.\")\n\nmodel_deployment = region.model_deployment_list[0]\ndeployment_key = model_deployment.key\n\n# Identify the first available model prefix\nif len(region.model_prefix_list) == 0:\n    raise SystemError(\n        \"No model prefixes are available. \"\n        \"Please contact your system administrator.\"\n    )\nelif len(region.model_prefix_list) &gt; 1:\n    print(\"Multiple model prefixes found. Using the first one.\")\n\nprefix = region.model_prefix_list[0].key\n\nmodel_name = \"my-xgboost-model\"\nmodel_dir = None  # Use the current directory by default\n\n# All Practicus AI model APIs follow this URL convention:\nexpected_api_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Note: Ensure the URL ends with a slash (/) to support correct routing.\n\nprint(\"Expected Model REST API URL:\", expected_api_url)\nprint(\"Using model deployment:\", deployment_key)\n</code></pre>"},{"location":"notebooks/02_modeling/01_basics/modeling_basics/#modelpy","title":"model.py","text":"<p>Review the <code>model.py</code> file to see how the XGBoost model is integrated and consumed within the environment.</p>"},{"location":"notebooks/02_modeling/01_basics/modeling_basics/#deploy-the-model","title":"Deploy the model","text":"<pre><code># This function can be called multiple times to deploy additional versions.\napi_url, api_version_url, api_meta_url = prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=model_dir\n)\n</code></pre> <pre><code>print(\"Which model API URL to use:\")\nprint(\"If you prefer the system admin dynamically route\")\nprint(\"  between model versions (recommended), use the below:\")\nprint(api_url)\nprint(\"If you prefer to use exactly this version, use the below:\")\nprint(api_version_url)\nprint(\"If you prefer to get the metadata of this version, use the below:\")\nprint(api_meta_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code># Now let's consume the Rest API to make the prediction\nimport requests \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df[\"Temperature\"].to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text} - {r.headers}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\ndisplay(pred_df)\n</code></pre>"},{"location":"notebooks/02_modeling/01_basics/modeling_basics/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/02_modeling/01_basics/modeling_basics/#modelpy_1","title":"model.py","text":"<pre><code>import os\nimport pandas as pd\nfrom xgboost import XGBRegressor\n\nmodel = None\n\n\nasync def init(*args, **kwargs):\n    print(\"Initializing model\")\n    global model\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.ubj')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model = XGBRegressor()\n    model.load_model(model_file)\n\n\nasync def predict(df, *args, **kwargs):\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    X = df[[\"Temperature\"]]\n\n    # Generate predictions\n    predictions = model.predict(X)\n\n    # Return predictions as a new DataFrame\n    return pd.DataFrame({\"predictions\": predictions})\n</code></pre> <p>Previous: Workers | Next: Tasks</p>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/","title":"Tasks","text":""},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#running-tasks-on-practicus-ai-workers","title":"Running Tasks on Practicus AI Workers","text":"<p>Before diving into Airflow and orchestrating workflows, it's important to understand the fundamental concept of tasks in the Practicus AI platform and how they are executed. Practicus AI provides a streamlined system to execute user-defined code within isolated, scalable Kubernetes pods, called Practicus AI Workers. Here\u2019s how it works:</p>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#what-is-a-task","title":"What is a Task?","text":"<p>A task in Practicus AI is a unit of work that encapsulate the code: A Python function, bash script, or any logic defined by the user.</p> <p>Tasks are designed to be executed independently, making them ideal building blocks for larger workflows.</p>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#how-practicus-ai-executes-tasks","title":"How Practicus AI Executes Tasks","text":"<p>Practicus AI creates, manages, and terminates isolated worker Kubernetes pods for each task execution. This ensures: - Scalability: Resources are provisioned dynamically based on the workload. - Isolation: Each task runs in its own containerized environment, avoiding interference. - Flexibility: Custom user code can be executed safely with controlled resources.</p> <p>Here\u2019s the lifecycle of a task:</p> <ol> <li>Task Submission:</li> <li>You define a task using the Practicus AI SDK.</li> <li> <p>Tasks can include parameters, dependencies, and required artifacts.</p> </li> <li> <p>Worker Creation:</p> </li> <li>Practicus AI provisions the worker for the task.</li> <li> <p>The worker is initialized with the necessary environment, including the code, parameters, and dependencies.</p> </li> <li> <p>Task Execution:</p> </li> <li>The worker pod runs the customer\u2019s code.</li> <li> <p>Standard output (<code>stdout</code>) and error (<code>stderr</code>) are captured for logging, which can be disabled.</p> </li> <li> <p>Worker Pod Termination:</p> </li> <li>After task execution is complete, the worker pod is automatically terminated to free up resources.</li> <li>You can also disable worker termination to open the notebook after executing the task worker to troubleshoot issues.</li> </ol>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#running-a-task-using-the-practicus-ai-sdk","title":"Running a Task Using the Practicus AI SDK","text":"<p>Practicus AI provides an SDK to simplify task submission and execution. Below is an example workflow for running a task using the SDK:</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code># The below will:\n# - Start a new worker with default settings\n# - Upload the files located in this folder\n# - Run task_1.py\n# - Capture the script output and display\n# - Terminate the worker\n\nprt.run_task(file_name=\"task_1.py\")\n\nprint(\"Completed the task and worker is terminated..\")\n</code></pre> <pre><code># You can also run a shell script.\n\nprt.run_task(file_name=\"task_2.sh\")\n\nprint(\"Completed the task and worker is terminated..\")\n</code></pre> <pre><code># Let's customize the worker that the task runs on\n\n# Sample startup script, worker will not get ready before this completes running.\nstartup_script = \"\"\"\nsudo apt-get update \nsudo apt-get install -y some-package\npip install some-library\n\"\"\"\n\nworker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",\n    worker_size=\"X-Small\",\n    # startup_script=startup_script,\n)\n\n# This task will fail and you can view the error message in logs\nworker, success = prt.run_task(\n    file_name=\"task_with_error.py\",\n    worker_config=worker_config,\n    terminate_on_completion=False,  # will manually terminate\n)\n\nprint(\"Execute next cell to further investigate..\")\n</code></pre> <pre><code># Failed? Let's troubleshoot..\n# Task files are uploaded and run under ~/practicus/task/\n\nif not success:\n    # The below will open a Jupyter notebook in a new tab\n    worker.open_notebook()\n</code></pre> <pre><code>print(f\"Done analyzing, terminating {worker.name}\")\nworker.terminate()\n</code></pre>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#tasks-with-jupyter-notebooks","title":"Tasks with Jupyter notebooks","text":"<ul> <li>You can use Jupyter notebooks for your tasks and other automation needs.</li> <li>Automated notebooks have many features, including:<ul> <li>Dynamic notebook parameters.</li> <li>Routing failed notebooks to designated locations.</li> </ul> </li> <li>To learn more, please check the samples under:<ul> <li>~/samples/notebooks/05_others/automated_notebooks</li> </ul> </li> </ul> <pre><code># As a first step, create \"task_with_notebook.ipynb\" file and add the below\n\nprint(\"This task is running in a notebook\")\n\n# Code as you would normally do..\n\nimport practicuscore as prt\n\nregion = prt.get_default_region()\n\nprint(\"Task is running with login credentials of\", region.email)\n\nprint(\"Let's simulate a failure\")\n\nraise SystemError(\"Simulated error\")\n</code></pre> <pre><code># And then create \"task_with_notebook.py\" and add the below\n\nimport practicuscore as prt \n\nprint(\"Starting to run notebook.\")\n\nprt.notebooks.execute_notebook(\n    \"task_with_notebook\",\n    # By default failed notebooks will not fail caller and just print result.\n    # Since this is a task, let's fail the task too.\n    raise_on_failure=True,\n)\n\nprint(\"Notebook completed running without issues.\")\n</code></pre> <pre><code>worker, success = prt.run_task(\n    # The .py task file here is a trigger for the notebook\n    file_name=\"task_with_notebook.py\",\n    terminate_on_completion=False,\n)\n\nprint(\"Task was successful.\" if success else \"Task failed.\")\n</code></pre> <pre><code>print(f\"Opening Jupyter notebook on {worker.name}.\")\nprint(\"You can view the notebook output in ~/practicus/task/*\")\nprint(\"&gt; Look for task_with_notebook_output.ipynb\")\nworker.open_notebook()\n</code></pre> <pre><code>print(\"Terminating worker.\")\nworker.terminate()\n</code></pre>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#other-topics","title":"Other Topics","text":"<ul> <li>Already logging your task output?<ul> <li>You can disable with setting:</li> <li>prt.run_task(.., capture_task_output=False)</li> </ul> </li> <li>Need a custom virtual environment?<ul> <li>Installed under \"~/.venv\"</li> <li>Create a new container image and set:</li> <li>prt.run_task(.., python_venv_name=\"your_venv\")</li> <li>Run the task using this image</li> </ul> </li> <li>Run automated notebooks from CLI?<ul> <li>You can also trigger a notebook with 'prtcli' command from an .sh file.</li> </ul> </li> </ul>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#task_1py","title":"task_1.py","text":"<pre><code>print(\"Hello from simple task 1\")\n</code></pre>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#task_2sh","title":"task_2.sh","text":"<pre><code>echo \"Hello from simple task 2\"\n</code></pre>"},{"location":"notebooks/03_workflows/01_tasks/01_tasks/#task_with_errorpy","title":"task_with_error.py","text":"<pre><code>import practicuscore as prt\n\n\ndef main():\n    print(\"Starting task..\")\n\n    # Code as usual:\n    # - Process data\n    # - Train models\n    # - Make predictions\n    # - Orchestrate other tasks\n    # - ...\n\n    try:\n        raise NotImplementedError(\"Still baking..\")\n    except Exception as ex:\n        # Psudo detail log\n        with open(\"my_log.txt\", \"wt\") as f:\n            f.write(str(ex))\n        raise ex\n\n    print(\"Finished task..\")\n\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Previous: Modeling Basics | Next: Airflow</p>"},{"location":"notebooks/03_workflows/02_airflow/airflow/","title":"Airflow","text":""},{"location":"notebooks/03_workflows/02_airflow/airflow/#workflows-with-practicus-ai-airflow-add-on-service","title":"Workflows with Practicus AI Airflow Add-on Service","text":"<ul> <li>Practicus AI provides several add-ons, including Airflow for workflow orchestration.</li> <li>This notebook demonstrates how to deploy workflows using Airflow through Practicus AI.</li> </ul>"},{"location":"notebooks/03_workflows/02_airflow/airflow/#recommended-create-tasks-airflow-dag-and-supporting-files","title":"(Recommended) Create tasks, Airflow DAG and supporting files","text":"<p>Plan your tasks and divide them into isolated, manageable units: - Each task runs fully isolated on its own Worker (Kubernetes Pod). - Group related actions into a single task for simplicity.</p> <p>Generating files: - Practicus AI can generate files as a starting point to develop your workflow. - An important part of the workflow is the DAG, which is the order and parallelism of the tasks in your flow. For example: <pre><code>dag_flow = \"my_1st_task &gt;&gt; my_2nd_task\"\n</code></pre> - You can define complex task dependencies here, for example: <pre><code>dag_flow = \"my_1st_task &gt;&gt; [my_2nd_task, my_3rd_task] &gt;&gt; my_4th_task\"\n</code></pre> - This flow means:     - <code>my_1st_task</code> runs first.     - If successful, <code>my_2nd_task</code> and <code>my_3rd_task</code> run in parallel.     - Finally, if both succeed, <code>my_4th_task</code> runs. - For more details, see:     - https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html</p> <pre><code>import practicuscore as prt\n\n# Let's define 4 tasks, 2 of which can run in parallel\ndag_flow = \"my_1st_task &gt;&gt; [my_2nd_task , my_3rd_task] &gt;&gt; my_4th_task\"\n\n# Let's define the default worker configuration for *all* tasks\ndefault_worker_config = prt.WorkerConfig(\n    worker_image=\"practicus\",\n    worker_size=\"X-Small\",\n)\n\n# Let's define a different worker configuration for the second task:\nmy_2nd_task_worker_config = prt.WorkerConfig(\n    worker_image=\"practicus-genai\",\n    worker_size=\"Small\",\n)\n\n# You can add as many custom worker configurations as you need in the below 'list of tuples'\ncustom_worker_configs = [\n    ('my_2nd_task', my_2nd_task_worker_config),\n]\n\n# Choose a DAG key describing the overall workflow\ndag_key = \"my_workflow\"\n\n# Define a schedule. If None, workflow will only run on-demand\n# E.g. @daily\n# You can also use cron schedules, E.g.\n# For 2 AM, every Thursday you can use '0 2 * * THU'\n# For help generating cron schedules: https://crontab.guru\nschedule_interval=None\n\n# For dev/test you can fail fast with 0 retires \n# and increase for production\nretries=0\n\nprt.workflows.generate_files(\n    dag_key=dag_key,\n    dag_flow=dag_flow,\n    # Optional parameters\n    files_path=None,  #  Current dir\n    default_worker_config=default_worker_config,\n    custom_worker_configs=custom_worker_configs,\n    save_credentials=True,\n    overwrite_existing=False,\n    schedule_interval=schedule_interval,  \n    retries=retries,\n)\n</code></pre>"},{"location":"notebooks/03_workflows/02_airflow/airflow/#understanding-the-generated-files","title":"Understanding the generated files","text":"<p>Now let's take a look at the generated files. - <code>my_1st_task.py to my_4th_task.py</code>:     - These are regular python code that you perform the actions. Please note that each run it's on Worker and fully isolated. - <code>default_worker.json</code>:     - The default container image, size, and login credentials that the workers will run.     - Pay close attention to credentials, the workflow might not run with your credentials in production.     - An admin can define the default system-wide credentials for all workflows.     - An end-user can trigger the workflow from Airflow UI and change credentials.     - Or an admin can alter the worker .json files to add specific credentials before deploying to Airflow. - <code>my_2nd_task_worker.json</code>:     - Similar to above but overrides the worker config for the second task.     - You can skip adding credentials, in which case the above logic will be applied. - <code>my_workflow_dag.py</code>:     - The Airflow DAG file that brings everything together.</p>"},{"location":"notebooks/03_workflows/02_airflow/airflow/#recommended-test-your-tasks-before-deploying-to-airflow","title":"(Recommended) Test your tasks before deploying to Airflow","text":"<ul> <li>It is a good idea to test your tasks before deploying to Airflow.</li> <li>You can add the below to any task .py file to simulate a failure. <pre><code>raise SystemError(\"Simulated error\")\n</code></pre></li> <li>To learn more about tasks, please view the tasks sample notebook in parent folder.</li> </ul> <pre><code>successful_task_workers, failed_task_workers = prt.workflows.test_tasks(\n    dag_flow=dag_flow,\n    task_list=None,  # To only test certain tasks\n    files_path=None,  #  Current dir\n    default_worker_config=default_worker_config,\n    custom_worker_configs=custom_worker_configs,\n    # Let's terminate workers of successful tasks\n    terminate_on_success=True,\n    # Let's skip terminating failed task workers to analyze further\n    terminate_on_failed=False,\n)\n</code></pre> <pre><code># Now let's open jupyter notebook to analyze issues\n\nfor worker in successful_task_workers:\n    # This list will be empty unless you changed terminate_on_success to False.\n    print(f\"Opening notebook on successful task worker: {worker.name}\")\n    worker.open_notebook()\n\nfor worker in failed_task_workers:\n    # This list will be empty if all tasks were successful.\n    print(f\"Opening notebook on failed task worker: {worker.name}\")\n    worker.open_notebook()\n\n# Do you prefer VS Code over Jupyter? replace worker.open_notebook() with:\n# url, token = worker.open_vscode()\n</code></pre> <pre><code># Analyze complete? Let's clean-up workers.\n\nfor worker in successful_task_workers:\n    print(f\"Terminating successful task worker: {worker.name}\")\n    worker.terminate()\n\nfor worker in failed_task_workers:\n    print(f\"Terminating failed task worker: {worker.name}\")   \n    worker.terminate()\n</code></pre>"},{"location":"notebooks/03_workflows/02_airflow/airflow/#optional-search-for-an-airflow-practicus-ai-add-on-service","title":"(Optional) Search for an Airflow Practicus AI Add-on Service","text":"<ul> <li>Practicus AI add-on's allow you to use different fully isolated services for different purposes, and you can programmatically search for them.</li> <li>You can skip this step if you already know the Airflow service key.</li> </ul> <pre><code>import practicuscore as prt\n\n# Retrieve default region and available add-ons\nregion = prt.get_default_region()\naddons_df = region.addon_list.to_pandas()\n\nprint(\"Add-on services that I have access to:\")\ndisplay(addons_df)\n\nairflow_services_df = addons_df[addons_df[\"service_type\"] == \"Airflow\"]\n\nprint(\"Airflow services that I have access to:\")\ndisplay(airflow_services_df)\n\nif airflow_services_df.empty:\n    raise RuntimeError(\n        \"You don't have access to any Airflow service. Please request access from your admin.\"\n    )\n\n# Choosing first available Airflow service, please change accordingly\nservice_key = airflow_services_df.iloc[0][\"key\"]\nservice_url = airflow_services_df.iloc[0][\"url\"]\n\nprint(\"Selected Airflow Service:\")\nprint(f\"Service Key : {service_key}\")\nprint(f\"Service URL : {service_url}\")\n</code></pre> <pre><code># Alternative: manually select an Airflow service key\n# service_key = 'my-airflow-service-key'\n</code></pre>"},{"location":"notebooks/03_workflows/02_airflow/airflow/#deploy-to-create-or-update-the-workflow-on-airflow","title":"Deploy to create or update the workflow on Airflow","text":"<ul> <li>You can now deploy the workflow dag, tasks, and all other supporting files using the below SDK method.</li> <li>The code and all artifacts will be stored on the git source control system that your selected Airflow service uses.</li> </ul> <pre><code>prt.workflows.deploy(\n    service_key=service_key, \n    dag_key=dag_key, \n    files_path=None, # Leave None for current directory\n)\n</code></pre>"},{"location":"notebooks/03_workflows/02_airflow/airflow/#other-topics","title":"Other Topics","text":"<p>Running shell scripts    - Your task files don't have to be .py files. Shell scripts (.sh) are also supported.</p> <p>Manually Creating Worker Configuration Files    - Instead of passing <code>default_worker_config</code>, <code>custom_worker_configurations</code>, and <code>save_credentials</code> to <code>prt.workflows.deploy()</code>, you can create the worker configuration <code>.json</code> files manually.    - Example configuration file structure:      <pre><code>{\n  \"worker_image\": \"practicus\",\n  \"worker_size\": \"X-Small\",\n  \"service_url\": \"https://example-service-url\",\n  \"email\": \"user@example.com\",\n  \"refresh_key\": \"your-refresh-key\"\n}\n</code></pre></p> <p>Avoiding Credential Storage    - To avoid saving your worker creation credentials (e.g., refresh key) in the default <code>.json</code> files, ask your Airflow admin to define these as global variables in the Airflow environment.    - This enhances security by avoiding local storage of sensitive information.</p> <p>Manually Passing Credentials    - If necessary, you can manually pass credentials while running a DAG using the Airflow UI:    - Click Run on the DAG and provide the required parameters, such as <code>refresh_key</code> and <code>service_url</code>.</p> <p>Deploying to Another Region    - By default, <code>prt.workflows.deploy()</code> deploys workflows to the current region.    - To deploy a workflow to a different region:      <pre><code>some_region.deploy_workflow()\n</code></pre>      - First, log in to the target region and then deploy using the region-specific method.</p> <p>Geo-Distributed Task Configuration    - Each task in your workflow can run in a different region, enabling a geo-distributed service mesh.    - To configure this, create separate worker configuration <code>.json</code> files with region-specific details:      - <code>service_url</code>      - <code>email</code>      - <code>refresh_key</code></p> <ul> <li>Example:      <pre><code>{\n  \"worker_image\": \"practicus\",\n  \"worker_size\": \"Small\",\n  \"service_url\": \"https://us-region.example.com\",\n  \"email\": \"user-us@example.com\",\n  \"refresh_key\": \"us-region-refresh-key\"\n}\n</code></pre></li> </ul> <p>Customizing DAG</p> <ul> <li>You can customize pretty much everything in the task flow.</li> <li>Let's say you are already logging extensively and do not want to capture stdout/stderr output.</li> <li>You can customize the dag file to add the below:</li> </ul> <pre><code>def generate_dag():\n    # Define a custom task, and make sure task_id matches the task file e.g. my_1st_task.py\n    @task(\n        task_id=\"my_1st_task\"\n    )\n    def my_1st_task_updated(**kwargs):\n        # Access user-provided or default params\n        params = kwargs[\"params\"]\n\n        # Dynamically override parameters at runtime\n        if not params.get(\"capture_task_output\"):\n            print(\"User has not set capture_task_output, will default to False\")\n            params[\"capture_task_output\"] = \"False\"\n\n        # Run the task as usual\n        prt.workflows.run_airflow_task(**kwargs)\n\n    # Comment out the generated task definition\n    #   my_1st_task = task(prt.workflows.run_airflow_task, task_id=\"my_1st_task\")()\n    # And use your custom task function.\n    my_1st_task = my_1st_task_updated()\n\n    # The rest of the DAG code stays the same..\n\n    # The `task_id` must match the task file (e.g., `my_1st_task.py`)\n    # located in the same folder as this DAG file.\n    my_2nd_task = task(prt.workflows.run_airflow_task, task_id=\"my_2nd_task\")()\n\n    # Define how your task will flow\n    my_1st_task &gt;&gt; my_2nd_task\n</code></pre> <p>Previous: Tasks | Next: AI Studio</p>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/","title":"AI Studio","text":""},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#using-practicus-ai-studio-with-airflow","title":"Using Practicus AI Studio with Airflow","text":"<p>You can use Practicus AI Studio for the following tasks for Airflow workflows.</p>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#practicus-ai-studio-functionality-for-airflow","title":"Practicus AI Studio functionality for Airflow","text":"<ul> <li>Explore data sources such as Data Lakes, Data Warehouses and Databases</li> <li>Transform data</li> <li>Join data from different data sources</li> <li>Export the result to any data source</li> <li>Perform these tasks on individual Workers or on distributed Spark cluster</li> <li>Generate data processing steps as Python code</li> <li>Auto-detect dependencies between taks</li> <li>Generate the DAG code </li> <li>Export data connection files separately so you can change them later</li> </ul>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#sample-scenario","title":"Sample scenario","text":"<ul> <li>Load some_table from a Database A<ul> <li>Make changes</li> <li>Save to Dabatase B</li> </ul> </li> <li>Load some_other_table from a Data Lake C<ul> <li>Make changes</li> <li>Save to Data Warehouse D</li> </ul> </li> <li>Load final_table from Database E<ul> <li>Join to some_table</li> <li>Join to some_other_table</li> <li>Make other changes</li> <li>Save to Data Lake F</li> <li>Export everything to Airflow</li> </ul> </li> </ul> <p>Let's take a quick look on the experience.</p> <pre><code>import json\nimport base64\n\n# Path to your notebook\nnotebook_path = \"intro.ipynb\"\n\n# Load the notebook JSON\nwith open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n    notebook = json.load(f)\n\n# Find the image in the attachments\nfor cell in notebook[\"cells\"]:\n    if \"attachments\" in cell:\n        for attachment, data in cell[\"attachments\"].items():\n            for mime, b64_data in data.items():\n                # Decode and save the image\n                with open(attachment, \"wb\") as img_file:\n                    img_file.write(base64.b64decode(b64_data))\n                print(f\"Saved: {attachment}\")\n</code></pre>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#joining-data-sources","title":"Joining data sources","text":"<ul> <li>Left joining final_table with column ID to some_other_table column ID</li> </ul>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#exporting-to-airflow","title":"Exporting to Airflow","text":"<ul> <li>Practicus AI automatically detects the dependency:</li> <li>Operations on some_table and some_other_table can execute in parallel since they do not depend on each other</li> <li>If both are successful, operations on final_table can happen including joins</li> </ul>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#viewing-the-exported-code","title":"Viewing the exported code","text":"<ul> <li>After the code export is completed you can update 4 types of files:</li> <li><code>.py files:</code> Each are tasks that include the data processing steps, SQL etc.</li> <li><code>.._worker.json files:</code> Defines the worker that each task will run on.<ul> <li>Container image to use, worker capacity (CPU, GPU, RAM) ..</li> </ul> </li> <li><code>.._conn.json files:</code> Defines how to read data for each task.<ul> <li>Note: Data source credentials can be stored in the Practicus AI data catalog.</li> </ul> </li> <li><code>.._save_conn.json files:</code> Defines how to write data for each task.<ul> <li>Note: Data source credentials can be stored in the Practicus AI data catalog.</li> </ul> </li> <li><code>.._join_.._conn.json files:</code> Defines how each join operation will work: how to read data and where to join.</li> <li><code>.._dag.py file:</code> The DAG file that brings everything together.</li> </ul> <p>Sample view from the embedded Jupyter notebook inside Practicus AI Studio.</p> <p></p>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#airflow-deployment-options","title":"Airflow deployment options","text":"<p>You have 2 options to deploy to Airflow from Practicus AI Studio.</p>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#self-service","title":"Self-service","text":"<ul> <li>Select the schedule and deploy directly to Airflow add-on service that an admin gave you access to.</li> <li>This will instantly start the Airflow schedule.</li> <li>You can then view your DAGs using Practicus AI and monitor the state of your workflows.</li> <li>You can also manually trigger DAGs.</li> </ul>"},{"location":"notebooks/03_workflows/03_AI_Studio/AI_Studio/#working-with-a-data-engineer-recommended-for-sensitive-data","title":"Working with a Data Engineer (recommended for sensitive data)","text":"<ul> <li>Just export the code and share with a Data Engineer, so they can:</li> <li>Validate your steps (.py files)</li> <li>Update data sources for production databases (conn.json files)</li> <li>Select appropriate Worker capacity (worker.json files)</li> <li>Select appropriate Worker user credentials (worker.json files)</li> <li>Deploy to Airflow </li> <li>Define the necesary monitoring steps with automation (e.g. with Practicus AI observability)</li> </ul> <p>Previous: Airflow | Next: Build</p>"},{"location":"notebooks/04_generative_ai/01_app_building/build/","title":"Build","text":"<pre><code>import practicuscore as prt\n</code></pre> <pre><code>prt.apps.test_app()\n</code></pre> <pre><code>import apis.say_hello\nfrom apis.say_hello import Person, SayHelloRequest, SayHelloResponse\n\nperson = Person(\n    name=\"Alice\", \n    email=\"alice@wonderland.com\"\n)\n\npayload = SayHelloRequest(\n    person=person\n)\nfrom pydantic import BaseModel\n\nprint(issubclass(type(payload), BaseModel))\n\nresponse: SayHelloResponse = prt.apps.call_api(apis.say_hello, payload)\n\nprint(\"Greeting message:\", response.greeting_message)\nprint(\"Email:\", response.for_person.email)\n</code></pre> <pre><code>region = prt.get_region()\n\nmy_app_settings = region.app_deployment_setting_list\nassert len(my_app_settings) &gt; 0, \"I don't have access to any app deployment settings!\"\n\nprint(\"Application deployment settings I have access to:\") \ndisplay(my_app_settings.to_pandas())\n\ndeployment_setting_key = my_app_settings[0].key\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre> <pre><code>my_app_prefixes = region.app_prefix_list\nassert len(my_app_prefixes) &gt; 0, \"I don't have access to any app prefix!\"\n\nprint(\"Application prefixes (groups) I have access to:\") \ndisplay(my_app_prefixes.to_pandas())\n\nprefix = my_app_prefixes[0].prefix\nprint(\"Using first app prefix with key:\", prefix)\n</code></pre> <pre><code>app_name=\"my-first-app\"\n# The below are optional, but recommended fields\nvisible_name = \"My First App\",\ndescription = \"A very useful app..\",\n# Font awesome 6 short icon name, or full class name e.g. fa-solid fa-rocket\n# You can find an icon at https://fontawesome.com/icons \n# Please select \"Free\" icons only and not the \"pro\" ones.\nicon = \"rocket\",  \n\napp_url, api_url = prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,\n    prefix=prefix,\n    app_name=app_name,\n    app_dir=None, # Current dir\n    visible_name=visible_name,\n    description=description,\n    icon=icon,\n)\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#understanding-versions","title":"Understanding versions","text":"<ul> <li>You can use the UI App and API URLs wihtout the version part. </li> <li>E.g. instead of https://practicus.company.com/apps/my-first-app/v3/ you can use https://practicus.company.com/apps/my-first-app/ and the traffic will route to the correct version automatically.</li> </ul>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#automated-traffic-routing","title":"Automated traffic routing","text":"<ul> <li>You can also access different versions like the below, or let the system decide routing.</li> <li>Let the system decide: https://practicus.company.com/apps/my-first-app/</li> <li>The traffic routes to the latest version OR the production version if an admin marked a version as prod.</li> <li>Production: https://practicus.company.com/apps/my-first-app/prod/</li> <li>Staging: https://practicus.company.com/apps/my-first-app/staging/</li> <li>Latest: https://practicus.company.com/apps/my-first-app/latest/</li> <li>Specific version: https://practicus.company.com/apps/my-first-app/v[version]/</li> <li>The above applies to both UI App and API URLs.</li> </ul> <pre><code>import requests \n\n# Get a short-lived session token using Practicus AI SDK.\ntoken = prt.apps.get_session_token(api_url=api_url)\n\n# You can also ask an admin to create a long-lived access token for this app,\n#   or all apps under a certain app prefix, e.g. apps/finance/* \n#   With this option, a developer can use your APIs without Practicus AI SDK.\n\nsay_hello_api_url = f\"{api_url}say-hello/\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n    \"content-type\": 'application/json'}\n\njson=payload.model_dump_json(indent=2)\nprint(f\"Sending below JSON to: {say_hello_api_url}\")\nprint(json)\n\nresp = requests.post(say_hello_api_url, json=json, headers=headers)\n\nif resp.ok:\n    print(\"Response text:\")\n    print(resp.text)\n    response_obj = SayHelloResponse.model_validate_json(resp.text)\n    print(\"Response object:\")\n    print(response_obj)\nelse:\n    print(\"Error:\", resp.status_code, resp.text)\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#deleting-apps-app-versions","title":"Deleting Apps / App Versions","text":"<p>You can delete: - An app, which will delete all of the versions (deployment) of the app. - A particular app version. Note: you cannot delete the latest version.</p> <p>To have delete permissions on an app or one of it's versions, you need one of the following:</p> <ul> <li>Be the owner (creator) of the app. Note: A system admin can change the owner of an app after it is created.</li> <li>Have admin privileges for the app's prefix. E.g. If you are an admin for the prefix \"apps/finance/test\" you can override / delete all apps under this prefix e.g. \"apps/finance/test/some-other-users-app\"</li> <li>Be a system admin (superuser). </li> </ul> <pre><code>print(\"Listing all the apps and their versions I have access to:\")\nregion.app_list.to_pandas()\n</code></pre> <pre><code># Deleting an app and all it's versions\nregion.delete_app(app_id=123)\n# If you don't know the app_id you can use prefix and app_name\nregion.delete_app(prefix=\"apps\", app_name=\"my-first-app\")\n</code></pre> <pre><code># Deleting a particular version of an app\nregion.delete_app_version(app_id=123, version=4)\n# If you don't know the app_id you can use prefix and app_name\nregion.delete_app_version(prefix=\"apps\", app_name=\"my-first-app\", version=4)\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/01_app_building/build/#homepy","title":"Home.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n\n# The below will secure the page by authenticating and authorizing users with Single-Sign-On.\n# Please note that security code is only activate when the app is deployed.\n# Pages are always secure, even without the below, during development and only the owner can access them.\nprt.apps.secure_page(\n    page_title=\"Hello World App\",\n    must_be_admin=False,\n)\n\n# The below is standard Streamlit code..\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello!\")\nst.write(\"This is a text from the code inside the page.\")\n\nst.write(some_function())\n\nif 'counter' not in st.session_state:\n    st.session_state.counter = 0\n\nincrement = st.button('Increment Counter')\nif increment:\n    current = st.session_state.counter\n    new = current + 1\n    st.session_state.counter = new\n    prt.apps.logger.info(f\"Increased counter from {current} to {new}\")\n\nst.write('Counter = ', st.session_state.counter)\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#apissay_hellopy","title":"apis/say_hello.py","text":"<pre><code>from pydantic import BaseModel\n\n\nclass Person(BaseModel):\n    name: str\n    email: str | None = None\n\n\nclass SayHelloRequest(BaseModel):\n    person: Person\n\n\nclass SayHelloResponse(BaseModel):\n    greeting_message: str\n    for_person: Person\n\n\ndef run(payload: SayHelloRequest, **kwargs):\n    return SayHelloResponse(greeting_message=f\"Hello {payload.person.name}\", for_person=payload.person)\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#pages01_first_pagepy","title":"pages/01_First_Page.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n\n# Child pages must also request to be secured.\n# Or else, they will be accessible by everyone after deployment.\n\nprt.apps.secure_page(\n    page_title=\"My first child page\",\n)\n\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello from first page!\")\n\nst.write(some_function())\n\nif 'page_1_counter' not in st.session_state:\n    st.session_state.page_1_counter = 0\n\nincrement = st.button('Increment Counter +2')\nif increment:\n    st.session_state.page_1_counter += 2\n\nst.write('Counter = ', st.session_state.page_1_counter)\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#pages02_second_pagepy","title":"pages/02_Second_Page.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nfrom shared.helper import some_function\n\n# Since this page is not secured, it will be public after deployment.\n# During development, it is still only accessible to the owner, and only from Practicus AI Studio.\n# If the home page is secured, a public child page will only be accessible if directly requested.\n# prt.apps.secure_page(\n#     page_title=\"My second child page\"\n# )\n\nst.title(\"My App on Practicus AI\")\n\nst.write(\"Hello from my second page!\")\nst.write(\"This page is not secured and will be open to public.\")\n\nst.write(some_function())\n\nif 'page_2_counter' not in st.session_state:\n    st.session_state.page_2_counter = 0\n\nincrement = st.button('Increment Counter +4')\nif increment:\n    st.session_state.page_2_counter += 4\n\nst.write('Counter = ', st.session_state.page_2_counter)\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#pages03_mixed_contentpy","title":"pages/03_Mixed_Content.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nprt.apps.secure_page(\n    page_title=\"Mixed content page\",\n)\n\nst.title(\"Mixed content page\")\n\nst.write(\"Everyone will see this part of the page.\")\nst.write(\"If you see nothing below, you are not an admin.\")\n\n\n# Only admins will see this\nif prt.apps.user_is_admin():\n    st.subheader(\"Admin Section\")\n    st.write(\"If you see this part, you are an admin, owner of the app, or in development mode.\")\n\n    # Input fields\n    admin_input1 = st.text_input(\"Admin Input 1\")\n    admin_input2 = st.text_input(\"Admin Input 2\")\n\n    admin_action = st.button('Admin Button')\n    if admin_action:\n        st.write(\"Performing some dummy admin action..\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#pages04_cookiespy","title":"pages/04_Cookies.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\n# Secure the page using the provided SDK\nprt.apps.secure_page(\n    page_title=\"Using Cookies\"\n)\n\nst.title(\"Cookies Management\")\n\n# Inputs for cookie operations\ncookie_name = st.text_input(\"Cookie Name\", placeholder=\"Enter cookie name\")\ncookie_value = st.text_input(\"Cookie Value\", placeholder=\"Enter cookie value\")\nmax_age = st.number_input(\"Max Validity (seconds)\", min_value=None, value=None, step=60, placeholder=\"Leave empty for 30 days\")\npath = st.text_input(\"Cookie path\", placeholder=\"Leave empty for /\")\n\n# Add Cookie\nif st.button(\"Add Cookie\"):\n    if cookie_name and cookie_value:\n        prt.apps.set_cookie(name=cookie_name, value=cookie_value, max_age=max_age, path=path)\n        st.success(f\"Cookie '{cookie_name}' has been set!\")\n    else:\n        st.error(\"Please provide both a cookie name and value.\")\n\n# Get Cookie Value\nif st.button(\"Get Cookie Value\"):\n    if cookie_name:\n        cookie_value = prt.apps.get_cookie(name=cookie_name)\n        if cookie_value:\n            st.success(f\"The value of cookie '{cookie_name}' is: {cookie_value}\")\n        else:\n            st.warning(f\"No cookie found with the name '{cookie_name}'.\")\n    else:\n        st.error(\"Please provide a cookie name to retrieve its value.\")\n\n# Delete Cookie\nif st.button(\"Delete Cookie\"):\n    if cookie_name:\n        prt.apps.delete_cookie(name=cookie_name)\n        st.success(f\"Cookie '{cookie_name}' has been deleted!\")\n    else:\n        st.error(\"Please provide a cookie name to delete.\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#pages05_app_metapy","title":"pages/05_App_Meta.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\nprt.apps.secure_page(\n    page_title=\"Application Metadata\"\n)\n\nst.title(\"Application Metadata\")\n\nif prt.apps.development_mode():\n    st.subheader(\"Development Mode\")\n    st.markdown(\n        \"\"\"\n        You are in **development mode**, and application metadata is only available after deploying the app.\n\n        **Developer Information:**\n        \"\"\"\n    )\n    st.write({\n        \"Email\": prt.apps.get_user_email(),\n        \"Username\": prt.apps.get_username(),\n        \"User ID\": prt.apps.get_user_id(),\n    })\nelse:\n    st.subheader(\"Deployed App Metadata\")\n    col1, col2 = st.columns(2)\n\n    with col1:\n        st.markdown(\"**Application Details**\")\n        st.write({\n            \"Name\": prt.apps.get_app_name(),\n            \"Prefix\": prt.apps.get_app_prefix(),\n            \"Version\": prt.apps.get_app_version(),\n            \"App ID\": prt.apps.get_app_id(),\n        })\n\n    with col2:\n        st.markdown(\"**User Information**\")\n        st.write({\n            \"Email\": prt.apps.get_user_email(),\n            \"Username\": prt.apps.get_username(),\n            \"User ID\": prt.apps.get_user_id(),\n        })\n\nif st.button(\"View User Groups\"):\n    st.write(\n        prt.apps.get_user_groups()\n    )\n    # User groups are cached. If you need reset you can call:\n    # reload = True\n    # prt.apps.get_user_groups(reload)\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#pages06_settingspy","title":"pages/06_Settings.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\n\n# User must have admin privileges to view this page (must_be_admin=True)\nprt.apps.secure_page(\n    page_title=\"Settings Page\",\n    must_be_admin=True,\n)\n\nst.title(\"Settings page\")\n\nst.write(\"If you see this, you are an admin, owner of the app, or in development mode.\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/01_app_building/build/#sharedhelperpy","title":"shared/helper.py","text":"<pre><code>def some_function():\n    return \"And, this text is from a shared function.\"\n</code></pre> <p>Previous: AI Studio | Next: Langchain With Streaming</p>"},{"location":"notebooks/04_generative_ai/02_langchain/langchain_basics/","title":"Langchain pipeline development","text":"<p>By using 'ChatPracticus' it is possible to create llm models which can be used in langchains.</p> <p>ChatPracticus method could take the variables down below: - endpoint_url: the api url of llm modelhost - api_token: the secret key to reach llm modelhost api - model_id: the model id of the model which is intended to use</p> <p>After defining a 'chat' by using 'ChatPracticus', the chat can be invoked with desired prompts.</p> <p>Now, let's write a function which will use 'ChatPracticus' method.</p> <pre><code>from langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_practicus import ChatPracticus\n\ndef test_langchain_practicus(api_url, token, inputs):\n    chat = ChatPracticus(\n        endpoint_url=api_url,\n        api_token=token,\n        model_id=\"current models ignore this\",\n        stream = True\n    )\n\n    response = chat.invoke(input=inputs)\n\n    print(\"\\n\\nReceived response:\\n\", response)\n    print(\"\\n\\nReceived Content:\\n\", response.content)\n</code></pre> <p>Async also works, but not on jupyter,</p> <pre><code>import asyncio\nasyncio.run(llm.ainvoke([sys_input, human_input1, human_input2]))\nprint(response)\n</code></pre> <p>After defining token and api url, we could insert our desired prompts. We could send our messages within 'HumanMessage' while sending our system message by wraping them with 'SystemMessage' methods of 'langchain' library.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>region = prt.get_region()\n\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\nmodel_name = my_model_list[0].name\nprint(\"Using first model name:\", model_name)\n</code></pre> <pre><code>my_app_list = region.app_list\ndisplay(my_app_list.to_pandas())\napp_name = my_app_list[0].name\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\nmodel_prefix = my_model_prefixes[0].key\nprint(\"Using first prefix:\", model_prefix)\n</code></pre> <pre><code>host = 'company.practicus.io' # Example url -&gt; 'company.practicus.io'\napi_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <pre><code>human_input1 = HumanMessage(\"Capital of United Kingdom?\")\nhuman_input2 = HumanMessage(\"And things to do there?\")\nsystem_message = SystemMessage(\"Less 50 words.\")\n\ninputs = [human_input1, human_input2, system_message]\n</code></pre> <pre><code>test_langchain_practicus(api_url, token, ['who is einstein'])\n</code></pre>"},{"location":"notebooks/04_generative_ai/02_langchain/langchain_basics/#received-response","title":"Received response:","text":"<p>content=\"Albert Einstein was a theoretical physicist born on March 14, 1879, in Ulm, in the Kingdom of W\u00fcrttemberg in the German Empire. He is best known for developing the theory of relativity, which revolutionized the understanding of space, time, and energy. His most famous equation, E=mc\u00b2, expresses the equivalence of mass and energy.\\n\\nEinstein's work laid the foundation for much of modern physics and he received the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect, which was pivotal in the development of quantum theory. Beyond his scientific contributions, Einstein was also known for his philosophical views, advocacy for civil rights, and his involvement in political and humanitarian causes. He passed away on April 18, 1955, in Princeton, New Jersey, USA.\" response_metadata={'model_id': 'current models ignore this'} id='run-12ae83b4-ec3e-4f5f-a9e2-3577fd4a2ff9-0' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}</p>"},{"location":"notebooks/04_generative_ai/02_langchain/langchain_basics/#content","title":"Content:","text":"<p>Albert Einstein was a theoretical physicist born on March 14, 1879, in Ulm, in the Kingdom of W\u00fcrttemberg in the German Empire. He is best known for developing the theory of relativity, which revolutionized the understanding of space, time, and energy. His most famous equation, E=mc\u00b2, expresses the equivalence of mass and energy.</p> <p>Einstein's work laid the foundation for much of modern physics and he received the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect, which was pivotal in the development of quantum theory. Beyond his scientific contributions, Einstein was also known for his philosophical views, advocacy for civil rights, and his involvement in political and humanitarian causes. He passed away on April 18, 1955, in Princeton, New Jersey, USA.</p> <p>Previous: Langchain With Streaming | Next: Build</p>"},{"location":"notebooks/04_generative_ai/02_langchain/langchain_with_streaming/","title":"Langchain With Streaming","text":""},{"location":"notebooks/04_generative_ai/02_langchain/langchain_with_streaming/#langchain-with-streaming","title":"Langchain with streaming","text":"<p>In this sample, we aim to give our human and system messages to our model and get a response. Our main steps:</p> <ul> <li>Url and token definitions</li> <li>Transfer of the llm model</li> <li>Json conversion</li> <li>Streaming</li> </ul>"},{"location":"notebooks/04_generative_ai/02_langchain/langchain_with_streaming/#context","title":"Context","text":"<ul> <li>The purpose of PrtLangMessage is to keep the content and role given into it in a dictionary</li> <li>This messages is our context. What u want to ask to chat you need to write this messages and you can give a role.</li> </ul> <pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre> <pre><code>my_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\nmodel_name = my_model_list[0].name\nprint(\"Using first model name:\", model_name)\n</code></pre> <pre><code>my_app_list = region.app_list\ndisplay(my_app_list.to_pandas())\napp_name = my_app_list[0].name\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\nmodel_prefix = my_model_prefixes[0].key\nprint(\"Using first prefix:\", model_prefix)\n</code></pre> <pre><code>my_app_prefix_list = region.app_prefix_list\ndisplay(my_app_prefix_list.to_pandas())\napp_prefix = my_app_prefix_list[0].prefix\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>host = 'company.practicus.io' # Example url -&gt; 'company.practicus.io'\napi_url = f\"https://{host}/{model_prefix}/{model_name}/\"\n#api_url = f\"https://{host}/{app_prefix}/{app_name}/api/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <pre><code>from practicuscore.gen_ai import PrtLangMessage, PrtLangRequest, PrtLangResponse\nimport requests\n</code></pre> <pre><code>human_msg = PrtLangMessage(\n    content=\"Who is einstein? \",\n    role = \"human\"\n)\n\nsystem_msg = PrtLangMessage(\n    content=\"Give me answer less than 100 words.\",\n    role = \"system\"\n)\n</code></pre>"},{"location":"notebooks/04_generative_ai/02_langchain/langchain_with_streaming/#request-llm","title":"Request LLM","text":"<ul> <li>The purpose of PrtLangRequest is to keep the messages, lang_model and streaming mode.</li> <li>If you need to data json you can use 'model_dump_json'. This fucniton will return json.</li> </ul> <pre><code># This class need message and model and if you want to stream, \n# you should change streaming value false to true\npracticus_llm_req = PrtLangRequest( \n    # Our context\n    messages=[human_msg, system_msg], \n    # Select a model, leave empty for default\n    lang_model=\"\", \n    # Streaming mode\n    streaming=True, \n    # If we have a extra parameters at model.py we can add them here \n    llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"} \n)\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'application/json'\n}\n\n# Convert our returned parameter to json\ndata_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True) \n</code></pre> <pre><code>with requests.post(api_url, headers=headers, data=data_js, stream=True) as r: \n    for response_chunk in r.iter_content(1024): \n        print(response_chunk.decode(\"utf-8\"), end = '')\n</code></pre>"},{"location":"notebooks/04_generative_ai/02_langchain/langchain_with_streaming/#sample-streaming-output","title":"Sample streaming output","text":"<p>Albert Einstein was a theoretical physicist born in 1879 in Germany. He is best known for developing the theory of relativity, particularly the equation (E=mc^2), which describes the equivalence of energy (E) and mass (m) with (c) being the speed of light. His work revolutionized the understanding of space, time, and gravity. Einstein received the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect. He is considered one of the most influential scientists of the 20th century.</p> <p>Previous: Build | Next: Langchain Basics</p>"},{"location":"notebooks/04_generative_ai/03_llm_apps/01_api_llm_apphost/build/","title":"Hosting of LLM application on AppHost without using front-end.","text":"<p>In this notebook we will try to host an LLM application which only used by API requests. This application will use an already deployed llm model.</p> <p>If you don't have a deployed llm model or you're unfamiliar with deployment of llm model you can check-out our llm deployment tuttorial.</p> <pre><code>import practicuscore as prt\nimport requests \nimport json\n</code></pre>"},{"location":"notebooks/04_generative_ai/03_llm_apps/01_api_llm_apphost/build/#api-python-script","title":"API Python script","text":"<p>First of all we need to create a Python scripts which will be invoked by using requests. For this instance we should create an 'apis' folder which will contain the Python scripts. Then we can create our scripts within the folder.</p> <p>You can check-out sample api script simple_api.py</p>"},{"location":"notebooks/04_generative_ai/03_llm_apps/01_api_llm_apphost/build/#define-params-from-region","title":"Define params from region","text":"<pre><code>host = 'preview.practicus.io' # Example url -&gt; 'company.practicus.com'\nassert host, \"Please enter your host url\" \n\nlang_model= 'LLAMA-3-70b'\nassert lang_model, \"Please select a model\"\n\napp_name = 'api-chatbot'\nassert app_name, \"Please enter application name\"\n</code></pre> <pre><code>region = prt.get_region()\n\n# Let's list our models and select one of them.\nmy_model_list = region.model_list\ndisplay(my_model_list.to_pandas())\n\n# We will select second model\nmodel_name = my_model_list[1].name\nprint(\"Using second model name:\", model_name)\n</code></pre> <pre><code># Let's list our model prefixes and select one of them.\nmy_model_prefixes = region.model_prefix_list\ndisplay(my_model_prefixes.to_pandas())\n\n# We will select first prefix\nmodel_prefix = my_model_prefixes[0].key\nprint(\"Using first prefix:\", model_prefix)\n</code></pre> <pre><code># Let's list our app deployments and select one of them.\nmy_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\n\n# We will select the first deployment\ndeployment_setting_key = my_app_settings[0].key\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre> <pre><code># Let's list our app prefixes and select one of them.\nmy_app_prefix_list = region.app_prefix_list\ndisplay(my_app_prefix_list.to_pandas())\n\n# We will select first prefix\napp_prefix = my_app_prefix_list[0].prefix\nprint(\"Using first app prefix\", app_prefix)\n</code></pre>"},{"location":"notebooks/04_generative_ai/03_llm_apps/01_api_llm_apphost/build/#testing-scripts","title":"Testing scripts","text":"<p>We can call our api scripts within this notebook and test them</p> <pre><code>api_url = f\"https://{host}/{model_prefix}/{model_name}/\"\ntoken = prt.models.get_session_token(api_url=api_url)\n</code></pre> <pre><code>from apis.simple_api import Messages, ModelRequest, run\n\n# Let's test our message class\nmessages = Messages(\n    content=\"Who is einstein?\", \n    role=\"human\"\n)\n\nmessages\n</code></pre> <pre><code># Let's test our Model request class\nmodelreq = ModelRequest(\n    messages = messages,\n    lang_model = lang_model,\n    streaming = False,\n    api_token = token,\n    end_point = api_url\n)\n\ndict(modelreq)\n</code></pre> <pre><code># Let's test our prediction function\nresponse = run(modelreq)\n\ndict(response)\n</code></pre>"},{"location":"notebooks/04_generative_ai/03_llm_apps/01_api_llm_apphost/build/#deployment-of-api","title":"Deployment of API","text":"<p>After testing our python scripts, and make sure they work, we can deploy them by using our prefix and SDK</p> <pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,\n    prefix=app_prefix,\n    app_name=app_name,\n    app_dir=None # Current dir\n)\n</code></pre>"},{"location":"notebooks/04_generative_ai/03_llm_apps/01_api_llm_apphost/build/#prediction-by-using-api","title":"Prediction by using API","text":"<p>After the deployment process we can consume the api url by using the code cell down below</p> <pre><code>api_url = f\"https://{host}/{app_prefix}/{app_name}/api/simple_api/\"\ntoken = prt.apps.get_session_token(api_url=api_url)\n</code></pre> <pre><code>headers = {\n    \"Authorization\": f\"Bearer {token}\",\n    \"content-type\": 'application/json'}\n\n\ndata_js = modelreq.model_dump_json(indent=2)\n\nresp = requests.post(api_url, json=data_js, headers=headers)\n\nif resp.ok:\n    print(f\"Response text:\", resp.text)\nelse:\n    print(resp.status_code, resp.text)\n</code></pre> <p>After deployment, comprehensive documentation for the API service is automatically generated. You can review this documentation and easily share it with your colleagues and other team members for seamless collaboration and onboarding.</p> <pre><code>documentation_url = f\"https://{host}/{app_prefix}/{app_name}/api/redoc/\"\nprint(f\"Your documentation url:{documentation_url}\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/03_llm_apps/01_api_llm_apphost/build/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/03_llm_apps/01_api_llm_apphost/build/#apissimple_apipy","title":"apis/simple_api.py","text":"<pre><code>from pydantic import BaseModel\nfrom practicuscore.gen_ai import PrtLangRequest, PrtLangMessage\nfrom requests import get\nimport json\n\n''' We are defining classes for taken inputs from api call. Using classes allows you to enforce type safety. \nThis means you can be sure that the data your functions receive has the correct types and structure, \nreducing the likelihood of runtime errors. But you don't have to use clases while creating api scripts.'''\n\n# Holds a message's content and an optional role for model to consume prompts.\nclass Messages(PrtLangMessage):\n    content: str\n    role: str | None = None\n\n# Stores details for a language model request, including the message, model type, and API information.\nclass ModelRequest(BaseModel):\n    messages: Messages\n    lang_model: str | None = 'None'\n    streaming: bool | None = False\n    end_point: str\n    api_token: str\n\n# We need to define a 'run' function to process incoming data to API\ndef run(payload: ModelRequest, **kwargs):\n\n    # Set up authorization headers using the API token from the payload\n    headers = {'authorization': f'Bearer {payload.api_token}'}\n\n    # Create a language model request object with message, model, and streaming options\n    practicus_llm_req = PrtLangRequest(\n        messages=[payload.messages],\n        lang_model=payload.lang_model,\n        streaming=payload.streaming,\n        llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"} # (Optional) Additional parameters for the language model could be added here\n        )\n\n    # Convert the request object to a JSON string, excluding unset fields\n    data_js = json.loads(practicus_llm_req.model_dump_json(indent=2, exclude_unset=True))\n\n    # Send the HTTP GET request to the specified endpoint with the headers and JSON data\n    r = get(payload.end_point, headers=headers, json=data_js)\n\n    # Parse the JSON response text into a Python dictionary\n    parsed = json.loads(r.text)\n\n    # Return the parsed response dictionary\n    return parsed\n</code></pre> <p>Previous: Langchain Basics | Next: Sdk Streamlit Hosting</p>"},{"location":"notebooks/04_generative_ai/03_llm_apps/02_sdk_llm_apphost/non-stream/sdk_streamlit_hosting/","title":"Hosting of LLM which is built by using SDK","text":"<p>If you're unfamiliar with building LangChain by using our SDK you can check-out our basic langchain development tuttorial.</p> <p>First of all we need build our streamlit application. You can view our sample and edit it:</p> <p>View streamlit app python file</p> <p>Ater creating/editing our streamlit application, we can test it by using our SDK:</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre> <p>After testing our application we can set our configurations and start the deployment process.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>region = prt.get_region()\n</code></pre> <pre><code>my_app_prefixes = region.app_prefix_list\ndisplay(my_app_prefixes.to_pandas())\napp_prefix = my_app_prefixes[0].prefix #Select your app with my_app_prefixes[index]\n</code></pre> <pre><code>my_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\ndeployment_setting_key = my_app_settings[1].key\n</code></pre> <pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key, # Deployment Key, ask admin for deployment key\n    prefix=app_prefix, # Apphost deployment extension\n    app_name='test', \n    app_dir=None # Directory of files that will be deployed ('None' for currnet directory)\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/04_generative_ai/03_llm_apps/02_sdk_llm_apphost/non-stream/sdk_streamlit_hosting/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/03_llm_apps/02_sdk_llm_apphost/non-stream/sdk_streamlit_hosting/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code># The below is official Streamlit + Langchain demo.\n\nimport streamlit as st\nimport practicuscore as prt\n\nfrom langchain_practicus import ChatPracticus\n\nprt.apps.secure_page(\n    page_title=\"\ud83e\udd9c\ud83d\udd17 Quickstart App\" # Give page title\n)\n\nst.title(\"\ud83e\udd9c\ud83d\udd17 Quickstart App v1\") # Give app title\n\n\n# This function use our 'api_token' and 'endpoint_url' and return the response.\ndef generate_response(input_text, endpoint, api):\n\n    model = ChatPracticus(\n        endpoint_url=endpoint, # Give model url\n        # Give api token , ask your admin for api\n        api_token=api,\n        model_id=\"model\",\n        verify_ssl=True,\n    )    \n\n    st.info(model.invoke(input_text).content) # We are give the input to model and get content\n\n\nwith st.form(\"my_form\"): # Define our question\n    endpoint = st.text_input('Enter your end point url:')\n    api = st.text_input('Enter your api token:')\n    text = st.text_area(\n        \"Enter text:\",\n        \"Who is Einstein ?\",\n    )\n    submitted = st.form_submit_button(\"Submit\") # Define the button\n\n    if submitted:\n        generate_response(text, endpoint, api) # Return the response\n</code></pre> <p>Previous: Sdk Streamlit Hosting | Next: Langflow Streamlit Hosting</p>"},{"location":"notebooks/04_generative_ai/03_llm_apps/02_sdk_llm_apphost/stream/sdk_streamlit_hosting/","title":"Hosting LLM APIs and Apps","text":"<p>If you're unfamiliar with building LangChain by using our SDK you can check-out our basic langchain development tuttorial.</p> <p>First of all we need build our streamlit application. You can view our sample and edit it:</p> <p>View streamlit app python file</p> <p>Ater creating/editing our streamlit application, we can test it by using our SDK:</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre> <p>After testing our application we can set our configurations and start the deployment process.</p>"},{"location":"notebooks/04_generative_ai/03_llm_apps/02_sdk_llm_apphost/stream/sdk_streamlit_hosting/#define-params-from-region","title":"Define params from region","text":"<pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre> <pre><code>my_app_list = region.app_list\ndisplay(my_app_list.to_pandas())\napp_name = my_app_list[0].name\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_app_prefix_list = region.app_prefix_list\ndisplay(my_app_prefix_list.to_pandas())\napp_prefix = my_app_prefix_list[0].prefix\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>my_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\ndeployment_setting_key = my_app_settings[0].key\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre> <pre><code>prt.apps.deploy(\n    deployment_setting_key=deployment_setting_key, # Deployment Key, ask admin for deployment key\n    prefix=app_prefix, # Apphost deployment extension\n    app_name=app_name, \n    app_dir=None # Directory of files that will be deployed ('None' for currnet directory)\n)\n</code></pre>"},{"location":"notebooks/04_generative_ai/03_llm_apps/02_sdk_llm_apphost/stream/sdk_streamlit_hosting/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/03_llm_apps/02_sdk_llm_apphost/stream/sdk_streamlit_hosting/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code># The below is official Streamlit + Langchain demo.\n\nimport streamlit as st\nimport practicuscore as prt\nfrom practicuscore.gen_ai import PrtLangMessage, PrtLangRequest\nimport requests\n\n\nprt.apps.secure_page(\n    page_title=\"\ud83e\udd9c\ud83d\udd17 Quickstart App\" # Give page title\n)\n\n\nst.title(\"\ud83e\udd9c\ud83d\udd17 Quickstart App v2\") # Give app title\n\n\n# This function use our 'api_token' and 'endpoint_url' and return the response.\ndef generate_response(messages, model):\n\n    api_url = \"Enter your model api\"\n    token =\"Enter your model token\"\n\n    practicus_llm_req = PrtLangRequest( # This class need message and model and if u want to stream u should change streaming value false to true\n        messages=messages, # Our contest\n        lang_model= model, #\"gpt-4o\", # Select model\n        streaming=True, # Streaming mode\n        llm_kwargs={\"kw1\": 123, \"kw2\": \"k2\"} # If we have a extra parameters at model.py we can add them here \n    )\n\n    headers = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'application/json'\n    }\n\n    data_js = practicus_llm_req.model_dump_json(indent=2, exclude_unset=True) # Convert our returned parameter to json\n\n    with requests.post(api_url, headers=headers, data=data_js, stream=True) as r: \n        for word in r.iter_content(1024):\n            yield word.decode(\"utf-8\")\n\ndef reset_chat():\n    st.session_state[\"messages\"] = []\n\n\n# Save chat history\nif \"messages\" not in st.session_state:\n    st.session_state[\"messages\"] = []\n\n# Input chat\nuser_message = st.chat_input(\"Write message\")\nif user_message:\n    # Add user message to history\n    st.session_state.messages.append({\"role\": \"user\", \"content\": user_message})\n\n    human_msg = PrtLangMessage(\n        content=user_message,\n        role = \"human\"\n    )\n\n    # Show messages\n    for msg in st.session_state.messages:\n        st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n\n    with st.chat_message(\"assistant\"):\n        response = st.write_stream(generate_response([human_msg], \"gpt-4o\"))\n\n    # Show answer\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n# Create a container to hold the button at the bottom\nwith st.container():\n    st.write(\"\")  # Add some empty space to push the button to the bottom\n    if st.button(\"Clear history\"):\n        reset_chat()\n        st.success(\"History has been cleaned\")\n</code></pre> <p>Previous: Build | Next: Sdk Streamlit Hosting</p>"},{"location":"notebooks/04_generative_ai/03_llm_apps/03_langflow_llm_apphost/langflow_streamlit_hosting/","title":"Flow hosting of Langflow by using Streamlit","text":"<p>First of all we need to create a \"Basic Prompting (Hello, World)\" flow at langflow and export the json of it.</p>"},{"location":"notebooks/04_generative_ai/03_llm_apps/03_langflow_llm_apphost/langflow_streamlit_hosting/#define-params-from-region","title":"Define params from region","text":"<pre><code>import practicuscore as prt\nregion = prt.get_region()\n</code></pre> <pre><code>my_app_prefix_list = region.app_prefix_list\ndisplay(my_app_prefix_list.to_pandas())\napp_prefix = my_app_prefix_list[0].prefix\nprint(\"Using first app prefix\", app_prefix)\n</code></pre> <pre><code>my_app_list = region.app_list\ndisplay(my_app_list.to_pandas())\napp_name = my_app_list[0].name\nprint(\"Using first app name:\", app_name)\n</code></pre> <pre><code>my_app_settings = region.app_deployment_setting_list\ndisplay(my_app_settings.to_pandas())\ndeployment_setting_key = my_app_settings[1].key\nprint(\"Using first setting with key:\", deployment_setting_key)\n</code></pre> <p>After exporting the json and save it within current directory of this tuttorial, you should test it if it's working.</p> <pre><code>from langflow.load import run_flow_from_json\n\nresult = run_flow_from_json(\n    flow=\"Flow.json\",\n    input_value=\"What is the capital of Australia?\"\n)\n\nrun_output = result[0]\nresult_data = run_output.outputs[0]\nmessage_obj = result_data.results['message']\nmessage_text = message_obj.data['text']\n\nmessage_text\n</code></pre> <pre><code># When you finish test, stop this cell. If you dont stop cell always be open.\nprt.apps.test_app()\n</code></pre> <p>Now we can creat our own stream-lit app and use api url of our flow within stream-lit's front-end. You can check-out our streamlit_app.py:</p> <p>View streamlit_app.py</p> <p>After creating/editing stream_app.py we could test it by hosting it as test by using our SDK:</p> <pre><code>import practicuscore as prt\n\nprt.apps.deploy(\n    deployment_setting_key=deployment_setting_key,\n    prefix=app_prefix,\n    app_name=\"chatbot\",\n    app_dir=None # Current dir\n)\n</code></pre> <p>After the deployment process completed we could enter UI url (e.g. https://dev.practicus.io/apps/langflow-json-test/v1/) to show-case our app.</p>"},{"location":"notebooks/04_generative_ai/03_llm_apps/03_langflow_llm_apphost/langflow_streamlit_hosting/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/03_llm_apps/03_langflow_llm_apphost/langflow_streamlit_hosting/#streamlit_apppy","title":"streamlit_app.py","text":"<pre><code>import practicuscore as prt\nimport streamlit as st\nfrom langflow.load import run_flow_from_json\n\n# The below will secure the page by authenticating and authorizing users with Single-Sign-On.\n# Please note that security code is only activate when the app is deployed.\n# Pages are always secure, even without the below, during development and only the owner can access them.\nprt.apps.secure_page(\n    page_title=\"Hello World App\",\n    must_be_admin=False,\n)\n\n\ndef main():\n    # The below is standard Streamlit code..\n    st.title(\"My App on Practicus AI\")\n\n    st.markdown(\"##### Welcome to the front-end of your flow\")\n\n    # Initialize session state to store chat messages if not already initialized.\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display all messages stored in session state in the chat interface.\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    # When the user inputs a message, add it to the chat history and display it.\n    if prompt := st.chat_input(\"I'm your flow, how may I help you?\"):\n        # Add user message to chat history\n        st.session_state.messages.append(\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        )\n        # Display user message in chat message container\n        with st.chat_message(\n                \"user\"\n        ):\n            st.write(prompt)\n        # Display assistant response in chat message container\n        with st.chat_message(\n                \"assistant\"\n        ):\n            message_placeholder = st.empty()\n            with st.spinner(text=\"Thinking...\"):\n                assistant_response = generate_response(prompt)\n                message_placeholder.write(assistant_response)\n        # Add assistant response to chat history\n        st.session_state.messages.append(\n            {\n                \"role\": \"assistant\",\n                \"content\": assistant_response\n            }\n        )\n\n\ndef run_flow(message, flow_json):\n    result = run_flow_from_json(flow=flow_json,\n                                input_value=message,\n                                fallback_to_env_vars=True)  # False by default\n    return result\n\n\n# Function to generate a response from the flow based on the user's input.\ndef generate_response(prompt):\n    # Log the user's question.\n    # logging.info(f\"question: {prompt}\")\n\n    # Run the flow to get the response.\n    response = run_flow(message=prompt, flow_json='Flow.json')\n\n    run_output = response[0]\n    result_data = run_output.outputs[0]\n    message_obj = result_data.results['message']\n    message_text = message_obj.data['text']\n\n    try:\n        # Log and return the assistant's response.\n        #logging.info(f\"answer: {message_obj}\")\n        return message_text\n    except Exception as exc:\n        # Log any errors and return a fallback message.\n        #logging.error(f\"error: {exc}\")\n        return \"Sorry, there was a problem finding an answer for you.\"\n\n\n# Run the main function to start the Streamlit app.\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Previous: Sdk Streamlit Hosting | Next: Milvus</p>"},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/","title":"Milvus","text":""},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/#milvus-vector-database-sample","title":"Milvus vector database sample","text":"<p><code>hello_milvus.ipynb</code> demonstrates the basic operations of PyMilvus, a Python SDK of Milvus. Before running, make sure that you have a running Milvus instance.</p> <ol> <li>connect to Milvus</li> <li>create collection</li> <li>insert data</li> <li>create index</li> <li>search, query, and hybrid search on entities</li> <li>delete entities by PK</li> <li>drop collection</li> </ol> <pre><code>import numpy as np\nimport time\n\nfrom pymilvus import (\n    connections,\n    utility,\n    FieldSchema, CollectionSchema, DataType,\n    Collection,\n)\n\nfmt = \"\\n=== {:30} ===\\n\"\nsearch_latency_fmt = \"search latency = {:.4f}s\"\nnum_entities, dim = 3000, 8\n</code></pre>"},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/#1-connect-to-milvus","title":"1. connect to Milvus","text":"<p>Add a new connection alias <code>default</code> for Milvus server in <code>localhost:19530</code>. </p> <p>Actually the <code>default</code> alias is a buildin in PyMilvus. If the address of Milvus is the same as <code>localhost:19530</code>, you can omit all parameters and call the method as: <code>connections.connect()</code>.</p> <p>Note: the <code>using</code> parameter of the following methods is default to \"default\".</p> <pre><code>connections.connect(\"default\", host=\"practicus-milvus.prt-ns-milvus.svc.cluster.local\", port=\"19530\")\n\nhas = utility.has_collection(\"hello_milvus\")\nprint(f\"Does collection hello_milvus exist in Milvus: {has}\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/#2-create-collection","title":"2. create collection","text":"<p>We're going to create a collection with 3 fields.</p> field name field type other attributes field description 1 \"pk\" VARCHAR is_primary=True, auto_id=False \"primary field\" 2 \"random\" Double \"a double field\" 3 \"embeddings\" FloatVector dim=8 \"float vector with dim 8\" <pre><code>fields = [\n    FieldSchema(name=\"pk\", dtype=DataType.VARCHAR, is_primary=True, auto_id=False, max_length=100),\n    FieldSchema(name=\"random\", dtype=DataType.DOUBLE),\n    FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n]\n\nschema = CollectionSchema(fields, \"hello_milvus is the simplest demo to introduce the APIs\")\n\nhello_milvus = Collection(\"hello_milvus\", schema, consistency_level=\"Strong\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/#3-insert-data","title":"3. insert data","text":"<p>We are going to insert 3000 rows of data into <code>hello_milvus</code>. Data to be inserted must be organized in fields.</p> <p>The insert() method returns: - either automatically generated primary keys by Milvus if auto_id=True in the schema; - or the existing primary key field from the entities if auto_id=False in the schema.</p> <pre><code>rng = np.random.default_rng(seed=19530)\nentities = [\n    # provide the pk field because `auto_id` is set to False\n    [str(i) for i in range(num_entities)],\n    rng.random(num_entities).tolist(),  # field random, only supports list\n    rng.random((num_entities, dim)),    # field embeddings, supports numpy.ndarray and list\n]\n\ninsert_result = hello_milvus.insert(entities)\n\nprint(f\"Number of entities in Milvus: {hello_milvus.num_entities}\")  # check the num_entites\n</code></pre>"},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/#4-create-index","title":"4. create index","text":"<p>We are going to create an IVF_FLAT index for hello_milvus collection.</p> <p>create_index() can only be applied to <code>FloatVector</code> and <code>BinaryVector</code> fields.</p> <pre><code>index = {\n    \"index_type\": \"IVF_FLAT\",\n    \"metric_type\": \"L2\",\n    \"params\": {\"nlist\": 128},\n}\n\nhello_milvus.create_index(\"embeddings\", index)\n</code></pre>"},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/#5-search-query-and-hybrid-search","title":"5. search, query, and hybrid search","text":"<p>After data were inserted into Milvus and indexed, you can perform: - search based on vector similarity - query based on scalar filtering(boolean, int, etc.) - hybrid search based on vector similarity and scalar filtering.</p> <p>Before conducting a search or a query, you need to load the data in <code>hello_milvus</code> into memory.</p> <pre><code>hello_milvus.load()\n</code></pre> <p>Search based on vector similarity</p> <pre><code>vectors_to_search = entities[-1][-2:]\nsearch_params = {\n    \"metric_type\": \"L2\",\n    \"params\": {\"nprobe\": 10},\n}\n\nstart_time = time.time()\nresult = hello_milvus.search(vectors_to_search, \"embeddings\", search_params, limit=3, output_fields=[\"random\"])\nend_time = time.time()\n\nfor hits in result:\n    for hit in hits:\n        print(f\"hit: {hit}, random field: {hit.entity.get('random')}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre> <p>Query based on scalar filtering(boolean, int, etc.)</p> <p>Start quering with <code>random &gt; 0.5</code></p> <pre><code>start_time = time.time()\nresult = hello_milvus.query(expr=\"random &gt; 0.5\", output_fields=[\"random\", \"embeddings\"])\nend_time = time.time()\n\nprint(f\"query result:\\n-{result[0]}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre> <p>Hybrid search</p> <p>Start hybrid searching with <code>random &gt; 0.5</code></p> <pre><code>start_time = time.time()\nresult = hello_milvus.search(vectors_to_search, \"embeddings\", search_params, limit=3, expr=\"random &gt; 0.5\", output_fields=[\"random\"])\nend_time = time.time()\n\nfor hits in result:\n    for hit in hits:\n        print(f\"hit: {hit}, random field: {hit.entity.get('random')}\")\nprint(search_latency_fmt.format(end_time - start_time))\n</code></pre>"},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/#6-delete-entities-by-pk","title":"6. delete entities by PK","text":"<p>You can delete entities by their PK values using boolean expressions.</p> <pre><code>ids = insert_result.primary_keys\nexpr = f'pk in [\"{ids[0]}\", \"{ids[1]}\"]'\n\nresult = hello_milvus.query(expr=expr, output_fields=[\"random\", \"embeddings\"])\nprint(f\"query before delete by expr=`{expr}` -&gt; result: \\n-{result[0]}\\n-{result[1]}\\n\")\n\nhello_milvus.delete(expr)\n\nresult = hello_milvus.query(expr=expr, output_fields=[\"random\", \"embeddings\"])\nprint(f\"query after delete by expr=`{expr}` -&gt; result: {result}\\n\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/04_vector_databases/milvus/#7-drop-collection","title":"7. drop collection","text":"<p>Finally, drop the hello_milvus collection</p> <pre><code>utility.drop_collection(\"hello_milvus\")\n</code></pre> <pre><code>\n</code></pre> <p>Previous: Langflow Streamlit Hosting | Next: Prep</p>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/","title":"Sample LLM Deployment with Practicus AI","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#deploying-an-llm-large-language-model-with-practicus-ai-involves-a-series-of-straightforward-steps-designed-to-get-your-model-up-and-running-swiftly","title":"Deploying an LLM (Large Language Model) with Practicus AI involves a series of straightforward steps designed to get your model up and running swiftly.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-1-request-model-access","title":"Step 1: Request Model Access","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#first-you-need-to-obtain-access-to-download-the-model-visit-metas-llama-downloads-page-and-request-access","title":"First, you need to obtain access to download the model. Visit Meta's LLaMA Downloads page and request access.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#httpsaimetacomresourcesmodels-and-librariesllama-downloads","title":"https://ai.meta.com/resources/models-and-libraries/llama-downloads/","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#once-approved-you-will-receive-an-email-with-a-download-url-active-for-24-hours-resembling-the-following-format","title":"Once approved, you will receive an email with a download URL, active for 24 hours, resembling the following format:","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#httpsdownloadllamametanetpolicyeyj","title":"https://download.llamameta.net/*?Policy=eyJ...","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-2-clone-the-llama-repo","title":"Step 2: Clone the LLaMA Repo","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#cd-sharedpracticuscodellama","title":"cd ~/shared/practicus/codellama","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#git-clone-httpsgithubcomfacebookresearchcodellamagit","title":"git clone https://github.com/facebookresearch/codellama.git","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-3-install-dependencies-and-download-the-model","title":"Step 3: Install Dependencies and Download the Model","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#before-downloading-the-model-ensure-wget-is-installed-in-your-jupyter-terminal","title":"Before downloading the model, ensure wget is installed in your Jupyter terminal:","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#sudo-apt-get-update","title":"sudo apt-get update","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#sudo-apt-get-install-wget","title":"sudo apt-get install wget","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#now-proceed-to-download-the-specific-model-version-youre-interested-in-for-instance-the-7b-instruct-model","title":"Now, proceed to download the specific model version you're interested in, for instance, the 7b-Instruct model:","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#cd-sharedpracticuscodellamacodellama","title":"cd ~/shared/practicus/codellama/codellama","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#bash-downloadsh","title":"bash download.sh","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-4-prepare-the-model-directory","title":"Step 4: Prepare the Model Directory","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#mkdir-sharedpracticuscodellamacache-echo-exists","title":"mkdir ~/shared/practicus/codellama/cache || echo \"exists\"","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#mv-sharedpracticuscodellamacodellamacodellama-7b-instruct-sharedpracticuscodellamacache","title":"mv ~/shared/practicus/codellama/codellama/CodeLlama-7b-Instruct ~/shared/practicus/codellama/cache","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-5-install-the-llama-python-library","title":"Step 5: Install the LLaMA Python Library","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#navigate-to-the-llama-library-directory-to-install-it-this-might-also-involve-simply-copying-the-llama-folder-to-your-code-execution-directory-or-automatically-downloading-it-from-cached-files-in-the-model-host","title":"Navigate to the LLaMA library directory to install it. This might also involve simply copying the LLaMA folder to your code execution directory or automatically downloading it from cached files in the model host:","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#cd-sharedpracticuscodellamacodellama_1","title":"cd ~/shared/practicus/codellama/codellama","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#python3-m-pip-install","title":"python3 -m pip install .","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-6-copy-llama-python-files-to-cache","title":"Step 6: Copy LLaMA Python Files to Cache","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#since-the-llama-library-might-not-be-installable-via-pip-youll-need-to-manually-copy-it-to-a-cache-directory-this-step-ensures-the-library-is-accessible-to-the-model-host","title":"Since the LLaMA library might not be installable via pip, you'll need to manually copy it to a cache directory. This step ensures the library is accessible to the model host:","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#cp-r-sharedpracticuscodellamacodellamallama-sharedpracticuscodellamacachecodellama-01","title":"cp -r ~/shared/practicus/codellama/codellama/llama ~/shared/practicus/codellama/cache/codellama-01","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-7-upload-cache-to-object-storage","title":"Step 7: Upload Cache to Object Storage","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#using-the-upload_downloadipynb-notebook-upload-the-prepared-cache-directory-to-your-object-storage-this-action-facilitates-easy-access-to-the-model-and-its-dependencies","title":"Using the upload_download.ipynb notebook, upload the prepared cache directory to your object storage. This action facilitates easy access to the model and its dependencies.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-8-create-new-model-versions-with-deployipynb","title":"Step 8: Create New Model Versions with deploy.ipynb","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#utilize-the-deployipynb-notebook-to-create-new-versions-of-your-model-this-notebook-guides-you-through-the-process-of-deploying-your-model-to-the-practicus-ai-environment","title":"Utilize the deploy.ipynb notebook to create new versions of your model. This notebook guides you through the process of deploying your model to the Practicus AI environment.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#step-9-use-consumeipynb-to-query-your-model","title":"Step 9: Use consume.ipynb to Query Your Model","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#finally-with-your-model-deployed-use-the-consumeipynb-notebook-to-send-queries-to-your-model-this-notebook-is-your-interface-for-interacting-with-the-llm-allowing-you-to-test-its-capabilities-and-ensure-its-functioning-as-expected","title":"Finally, with your model deployed, use the consume.ipynb notebook to send queries to your model. This notebook is your interface for interacting with the LLM, allowing you to test its capabilities and ensure it's functioning as expected.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#by-following-these-steps-youll-have-your-llm-deployed-and-ready-for-use-with-practicus-ai-this-guide-is-crafted-to-ensure-a-seamless-and-straightforward-deployment-process-enabling-you-to-focus-on-leveraging-the-models-capabilities-for-your-projects","title":"By following these steps, you'll have your LLM deployed and ready for use with Practicus AI. This guide is crafted to ensure a seamless and straightforward deployment process, enabling you to focus on leveraging the model's capabilities for your projects.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/codellama-01/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/01_Prep/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n\n    # Assuming llama library is copied into cache dir, in addition to torch .pth files\n    llama_cache = \"/var/practicus/cache\"\n    if llama_cache not in sys.path:\n        sys.path.insert(0, llama_cache)\n\n    try:\n        from llama import Llama\n    except Exception as e:\n        raise ModuleNotFoundError(\"llama library not found. Have you included it in the object storage cache?\") from e\n\n    try:\n        generator = Llama.build(\n            ckpt_dir=f\"{llama_cache}/CodeLlama-7b-Instruct/\",\n            tokenizer_path=f\"{llama_cache}/CodeLlama-7b-Instruct/tokenizer.model\",\n            max_seq_len=512,\n            max_batch_size=4,\n            model_parallel_size=1\n        )\n    except:\n        building_generator = False\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\n\ndef _predict(http_request=None, model_meta=None, payload_dict=None, *args, **kwargs):\n    start = datetime.now()\n\n    # instructions = [[\n    #     {\"role\": \"system\", \"content\": payload_dict[\"system_context\"]},\n    #     {\"role\": \"user\", \"content\": payload_dict[\"user_prompt\"]}\n    # ]]\n\n    instructions = [[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\"role\": \"user\", \"content\": \"Capital of Turkey\"}\n    ]]\n\n    results = generator.chat_completion(\n        instructions,\n        max_gen_len=None,\n        temperature=0.2,\n        top_p=0.95,\n    )\n\n    answer = \"\"\n    for result in results:\n        answer += f\"{result['generation']['content']}\\n\"\n\n    print(\"thread answer:\", answer)\n    total_time = (datetime.now() - start).total_seconds()\n    print(\"thread asnwer in:\", total_time)    \n\n    global answers \n    answers += f\"start:{start} end: {datetime.now()} time: {total_time} answer: {answer}\\n\"\n\n\nasync def predict(http_request, model_meta=None, payload_dict=None, *args, **kwargs):\n    await init(model_meta)\n\n    import threading \n\n    threads = []\n\n    count = int(payload_dict[\"count\"])\n    thread_start = datetime.now()\n    for _ in range(count):\n        thread = threading.Thread(target=_predict)\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    print(\"Total finished in:\", (datetime.now() - thread_start).total_seconds())    \n\n    return {\n        \"answer\": f\"Time:{(datetime.now() - thread_start).total_seconds()}\\nanswers:{answers}\"\n    }\n</code></pre> <p>Previous: Milvus | Next: Upload Download</p>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/","title":"Upload and Download to Object Storage","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#in-this-guide-well-walk-through-the-process-of-uploading-and-downloading-model-related-files-to-and-from-an-object-storage-solution-using-aws-s3-as-an-example-this-functionality-is-essential-for-deploying-and-managing-models-in-the-practicus-ai-environment-allowing-you-to-efficiently-handle-model-files-configurations-and-other-necessary-assets-we-will-use-the-boto3-library-for-interacting-with-aws-services-specifically-s3-for-object-storage","title":"In this guide, we'll walk through the process of uploading and downloading model-related files to and from an object storage solution using AWS S3 as an example. This functionality is essential for deploying and managing models in the Practicus AI environment, allowing you to efficiently handle model files, configurations, and other necessary assets. We will use the boto3 library for interacting with AWS services, specifically S3 for object storage.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#step-1-import-required-libraries","title":"Step 1: Import Required Libraries","text":"<pre><code>import os\nimport boto3\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#the-os-module-is-used-for-operating-system-dependent-functionality-like-reading-or-writing-files-whereas-boto3-is-the-amazon-web-services-aws-sdk-for-python-allowing-you-to-interact-with-aws-services-including-s3","title":"The os module is used for operating system-dependent functionality like reading or writing files, whereas boto3 is the Amazon Web Services (AWS) SDK for Python, allowing you to interact with AWS services including S3.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#step-2-uploading-files-to-object-storage","title":"Step 2: Uploading Files to Object Storage","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#define-a-function-upload_files-that-recursively-uploads-all-files-from-a-specified-folder-to-your-s3-bucket-this-function-is-particularly-useful-for-batch-uploading-model-files-and-associated-configurations","title":"Define a function upload_files that recursively uploads all files from a specified folder to your S3 bucket. This function is particularly useful for batch uploading model files and associated configurations.","text":"<pre><code>def upload_files(folder_path, bucket_name, s3_client):\n    for subdir, dirs, files in os.walk(folder_path):\n        for file in files:\n            try:\n                relative_path = os.path.join(subdir, file)\n                s3_client.upload_file(relative_path, bucket_name, relative_path)\n                print(f\"Successfully uploaded {relative_path}\")\n            except Exception as ex:\n                print(f\"Failed to upload {relative_path}\\n{ex}\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#step-3-downloading-files-from-object-storage","title":"Step 3: Downloading Files from Object Storage","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#create-a-function-download_items_with_prefix-that-downloads-files-from-your-s3-bucket-that-match-a-specified-prefix-into-a-local-directory-this-is-useful-for-fetching-model-versions-or-specific-configurations","title":"Create a function download_items_with_prefix that downloads files from your S3 bucket that match a specified prefix into a local directory. This is useful for fetching model versions or specific configurations.","text":"<pre><code>def download_items_with_prefix(bucket_name, prefix, local_folder, s3_client):\n    if not os.path.exists(local_folder):\n        os.makedirs(local_folder)\n\n    paginator = s3_client.get_paginator('list_objects_v2')\n    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n        for obj in page.get('Contents', []):\n            try:\n                file_name = obj['Key']\n                local_file_path = os.path.join(local_folder, file_name)\n                local_file_dir = os.path.dirname(local_file_path)\n                if not os.path.exists(local_file_dir):\n                    os.makedirs(local_file_dir)\n                s3_client.download_file(bucket_name, file_name, local_file_path)\n                print(f\"Successfully downloaded {file_name} to {local_file_path}\")\n            except Exception as ex:\n                print(f\"Failed to download {file_name} to {local_file_path}\\n{ex}\")\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#step-4-configure-s3-client-and-execute-functions","title":"Step 4: Configure S3 Client and Execute Functions","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#before-executing-the-upload-and-download-functions-configure-your-s3-client-with-your-aws-credentials-ensure-your-aws-access-key-id-and-aws-secret-access-key-are-securely-stored-and-not-hard-coded-or-exposed-in-your-scripts","title":"Before executing the upload and download functions, configure your S3 client with your AWS credentials. Ensure your AWS Access Key ID and AWS Secret Access Key are securely stored and not hard-coded or exposed in your scripts.","text":"<pre><code>bucket_name = 'your-bucket-name'\naws_access_key_id = 'your-access-key-id'\naws_secret_access_key = 'your-secret-access-key'\n\ns3_client = boto3.client(\n    's3',\n    aws_access_key_id=aws_access_key_id,\n    aws_secret_access_key=aws_secret_access_key\n)\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#now-call-the-upload_files-function-to-upload-your-model-directory-to-s3-and-use-the-download_items_with_prefix-function-to-download-specific-files-or-configurations-needed-for-your-model-deployment","title":"Now, call the upload_files function to upload your model directory to S3, and use the download_items_with_prefix function to download specific files or configurations needed for your model deployment.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/codellama-01/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/02_upload_download/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n\n    # Assuming llama library is copied into cache dir, in addition to torch .pth files\n    llama_cache = \"/var/practicus/cache\"\n    if llama_cache not in sys.path:\n        sys.path.insert(0, llama_cache)\n\n    try:\n        from llama import Llama\n    except Exception as e:\n        raise ModuleNotFoundError(\"llama library not found. Have you included it in the object storage cache?\") from e\n\n    try:\n        generator = Llama.build(\n            ckpt_dir=f\"{llama_cache}/CodeLlama-7b-Instruct/\",\n            tokenizer_path=f\"{llama_cache}/CodeLlama-7b-Instruct/tokenizer.model\",\n            max_seq_len=512,\n            max_batch_size=4,\n            model_parallel_size=1\n        )\n    except:\n        building_generator = False\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\n\ndef _predict(http_request=None, model_meta=None, payload_dict=None, *args, **kwargs):\n    start = datetime.now()\n\n    # instructions = [[\n    #     {\"role\": \"system\", \"content\": payload_dict[\"system_context\"]},\n    #     {\"role\": \"user\", \"content\": payload_dict[\"user_prompt\"]}\n    # ]]\n\n    instructions = [[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\"role\": \"user\", \"content\": \"Capital of Turkey\"}\n    ]]\n\n    results = generator.chat_completion(\n        instructions,\n        max_gen_len=None,\n        temperature=0.2,\n        top_p=0.95,\n    )\n\n    answer = \"\"\n    for result in results:\n        answer += f\"{result['generation']['content']}\\n\"\n\n    print(\"thread answer:\", answer)\n    total_time = (datetime.now() - start).total_seconds()\n    print(\"thread asnwer in:\", total_time)    \n\n    global answers \n    answers += f\"start:{start} end: {datetime.now()} time: {total_time} answer: {answer}\\n\"\n\n\nasync def predict(http_request, model_meta=None, payload_dict=None, *args, **kwargs):\n    await init(model_meta)\n\n    import threading \n\n    threads = []\n\n    count = int(payload_dict[\"count\"])\n    thread_start = datetime.now()\n    for _ in range(count):\n        thread = threading.Thread(target=_predict)\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    print(\"Total finished in:\", (datetime.now() - thread_start).total_seconds())    \n\n    return {\n        \"answer\": f\"Time:{(datetime.now() - thread_start).total_seconds()}\\nanswers:{answers}\"\n    }\n</code></pre> <p>Previous: Prep | Next: Model</p>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/","title":"Preparation of Model File","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#the-modelpy-file-is-a-critical-component-when-deploying-a-large-language-model-llm-in-environments-like-practicus-ai-it-encapsulates-the-logic-for-initializing-the-model-making-predictions-and-cleaning-up-resources-below-ill-provide-a-detailed-explanation-of-a-modelpy-script-designed-for-deploying-an-llm-ensuring-no-step-is-overlooked","title":"The model.py file is a critical component when deploying a Large Language Model (LLM) in environments like Practicus AI. It encapsulates the logic for initializing the model, making predictions, and cleaning up resources. Below, I'll provide a detailed explanation of a model.py script designed for deploying an LLM, ensuring no step is overlooked.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#import-statements","title":"Import Statements","text":"<pre><code>import sys \nfrom datetime import datetime\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#sys-used-for-interacting-with-the-interpreter-including-adding-paths-for-python-to-search-for-modules","title":"sys: Used for interacting with the interpreter, including adding paths for Python to search for modules.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#datetime-facilitates-recording-timestamps-useful-for-performance-monitoring","title":"datetime: Facilitates recording timestamps, useful for performance monitoring.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#global-variables","title":"Global Variables","text":"<pre><code>generator = None\nanswers = \"\"\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#generator-holds-the-model-instance-initialized-as-none-and-later-assigned-the-llm-object","title":"generator: Holds the model instance. Initialized as None and later assigned the LLM object.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#answers-accumulates-responses-from-the-model-initialized-as-an-empty-string-and-appended-with-each-prediction","title":"answers: Accumulates responses from the model. Initialized as an empty string and appended with each prediction.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#initialization-function","title":"Initialization Function","text":"<pre><code>async def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n    llama_cache = \"/var/practicus/cache\"\n    if llama_cache not in sys.path:\n        sys.path.insert(0, llama_cache)\n\n    try:\n        from llama import Llama\n    except Exception as e:\n        raise ModuleNotFoundError(\"llama library not found. Have you included it in the object storage cache?\") from e\n\n    try:\n        generator = Llama.build(\n            ckpt_dir=f\"{llama_cache}/CodeLlama-7b-Instruct/\",\n            tokenizer_path=f\"{llama_cache}/CodeLlama-7b-Instruct/tokenizer.model\",\n            max_seq_len=512,\n            max_batch_size=4,\n            model_parallel_size=1\n        )\n    except Exception as e:\n        print(f\"Failed to build generator: {e}\")\n        raise\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#this-function-prepares-the-environment-for-the-model-it-checks-if-the-generator-is-already-created-to-avoid-redundant-loads-if-not-it-attempts-to-import-the-llama-library-and-build-the-model-with-specified-parameters-errors-in-this-process-trigger-exceptions-alerting-you-to-potential-issues-with-library-presence-or-model-construction","title":"This function prepares the environment for the model. It checks if the generator is already created to avoid redundant loads. If not, it attempts to import the LLaMA library and build the model with specified parameters. Errors in this process trigger exceptions, alerting you to potential issues with library presence or model construction.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#cleanup-function","title":"Cleanup Function","text":"<pre><code>async def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n    global generator\n    generator = None\n    from torch import cuda\n    cuda.empty_cache()\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#this-function-is-designed-to-free-up-resources-once-theyre-no-longer-needed-setting-generator-back-to-none-and-clearing-the-gpu-memory-cache-to-prevent-memory-leaks-crucial-for-maintaining-performance","title":"This function is designed to free up resources once they're no longer needed, setting generator back to None and clearing the GPU memory cache to prevent memory leaks, crucial for maintaining performance.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#prediction-wrapper-function","title":"Prediction Wrapper Function","text":"<pre><code>async def predict(http_request, model_meta=None, payload_dict=None, *args, **kwargs):\n    await init(model_meta)\n\n    import threading \n    threads = []\n    count = int(payload_dict[\"count\"])\n    thread_start = datetime.now()\n    for _ in range(count):\n        thread = threading.Thread(target=_predict)\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    print(\"Total finished in:\", (datetime.now() - thread_start).total_seconds())    \n    return {\n        \"answer\": f\"Time:{(datetime.now() - thread_start).total_seconds()}\\nanswers:{answers}\"\n    }\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#this-asynchronous-function-is-the-main-entry-for-prediction-requests-it-ensures-the-model-is-initialized-then-creates-and-manages-multiple-threads-for-concurrent-prediction-tasks-each-thread-calls-the-_predict-function-to-generate-a-response-the-total-execution-time-is-calculated-providing-insights-into-the-performance","title":"This asynchronous function is the main entry for prediction requests. It ensures the model is initialized, then creates and manages multiple threads for concurrent prediction tasks. Each thread calls the _predict function to generate a response. The total execution time is calculated, providing insights into the performance.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#prediction-execution-function","title":"Prediction Execution Function","text":"<pre><code>def _predict(http_request=None, model_meta=None, payload_dict=None, *args, **kwargs):\n    start = datetime.now()\n    instructions = [[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\"role\": \"user\", \"content\": \"Capital of Turkey\"}\n    ]]\n    results = generator.chat_completion(\n        instructions,\n        max_gen_len=None,\n        temperature=0.2,\n        top_p=0.95,\n    )\n    answer = \"\"\n    for result in results:\n        answer += f\"{result['generation']['content']}\\n\"\n    print(\"thread answer:\", answer)\n    total_time = (datetime.now() - start).total_seconds()\n    print(\"thread answer in:\", total_time)    \n    global answers \n    answers += f\"start:{start} end: {datetime.now()} time: {total_time} answer: {answer}\\n\"\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#_predict-is-called-by-each-thread-to-perform-the-actual-prediction-task-it-formats-the-instructions-for-the-model-executes-the-prediction-and-logs-the-response-and-execution-time-the-global-answers-string-is-appended-with-each-prediction-result-for-final-aggregation","title":"_predict is called by each thread to perform the actual prediction task. It formats the instructions for the model, executes the prediction, and logs the response and execution time. The global answers string is appended with each prediction result for final aggregation.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#summary","title":"Summary","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#this-modelpy-script-outlines-a-robust-framework-for-deploying-and-interacting-with-a-llm-in-a-scalable-asynchronous-manner-it-highlights-essential-practices-like-dynamic-library-loading-concurrent-processing-with-threads-resource-management-and-detailed-logging-for-performance-monitoring-this-setup-is-adaptable-to-various-models-and-can-be-tailored-to-fit-specific-requirements-of-different-llm-deployments","title":"This model.py script outlines a robust framework for deploying and interacting with a LLM in a scalable, asynchronous manner. It highlights essential practices like dynamic library loading, concurrent processing with threads, resource management, and detailed logging for performance monitoring. This setup is adaptable to various models and can be tailored to fit specific requirements of different LLM deployments.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/codellama-01/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/03_model/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n\n    # Assuming llama library is copied into cache dir, in addition to torch .pth files\n    llama_cache = \"/var/practicus/cache\"\n    if llama_cache not in sys.path:\n        sys.path.insert(0, llama_cache)\n\n    try:\n        from llama import Llama\n    except Exception as e:\n        raise ModuleNotFoundError(\"llama library not found. Have you included it in the object storage cache?\") from e\n\n    try:\n        generator = Llama.build(\n            ckpt_dir=f\"{llama_cache}/CodeLlama-7b-Instruct/\",\n            tokenizer_path=f\"{llama_cache}/CodeLlama-7b-Instruct/tokenizer.model\",\n            max_seq_len=512,\n            max_batch_size=4,\n            model_parallel_size=1\n        )\n    except:\n        building_generator = False\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\n\ndef _predict(http_request=None, model_meta=None, payload_dict=None, *args, **kwargs):\n    start = datetime.now()\n\n    # instructions = [[\n    #     {\"role\": \"system\", \"content\": payload_dict[\"system_context\"]},\n    #     {\"role\": \"user\", \"content\": payload_dict[\"user_prompt\"]}\n    # ]]\n\n    instructions = [[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\"role\": \"user\", \"content\": \"Capital of Turkey\"}\n    ]]\n\n    results = generator.chat_completion(\n        instructions,\n        max_gen_len=None,\n        temperature=0.2,\n        top_p=0.95,\n    )\n\n    answer = \"\"\n    for result in results:\n        answer += f\"{result['generation']['content']}\\n\"\n\n    print(\"thread answer:\", answer)\n    total_time = (datetime.now() - start).total_seconds()\n    print(\"thread asnwer in:\", total_time)    \n\n    global answers \n    answers += f\"start:{start} end: {datetime.now()} time: {total_time} answer: {answer}\\n\"\n\n\nasync def predict(http_request, model_meta=None, payload_dict=None, *args, **kwargs):\n    await init(model_meta)\n\n    import threading \n\n    threads = []\n\n    count = int(payload_dict[\"count\"])\n    thread_start = datetime.now()\n    for _ in range(count):\n        thread = threading.Thread(target=_predict)\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    print(\"Total finished in:\", (datetime.now() - thread_start).total_seconds())    \n\n    return {\n        \"answer\": f\"Time:{(datetime.now() - thread_start).total_seconds()}\\nanswers:{answers}\"\n    }\n</code></pre> <p>Previous: Upload Download | Next: Model Json</p>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/","title":"Model.json","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#the-provided-modeljson-snippet-exemplifies-how-configuration-files-are-used-to-specify-operational-parameters-for-deploying-and-running-large-language-models-llms-within-an-ecosystem-like-practicus-ai-this-json-configuration-plays-a-critical-role-in-streamlining-the-deployment-process-enhancing-model-management-and-ensuring-the-model-operates-efficiently-within-its-environment-heres-an-explanation-of-why-this-modeljson-content-is-significant","title":"The provided model.json snippet exemplifies how configuration files are used to specify operational parameters for deploying and running Large Language Models (LLMs) within an ecosystem like Practicus AI. This JSON configuration plays a critical role in streamlining the deployment process, enhancing model management, and ensuring the model operates efficiently within its environment. Here's an explanation of why this model.json content is significant:","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#specifying-resource-locations","title":"Specifying Resource Locations","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#download_files_from-cachecodellama-01","title":"\"download_files_from\": \"cache/codellama-01/\":","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#this-key-value-pair-indicates-the-directory-or-path-from-which-the-necessary-model-files-should-be-downloaded-in-the-context-of-deploying-an-llm-these-files-could-include-the-model-weights-tokenizer-files-and-any-other-dependencies-required-for-the-model-to-run-this-parameter-ensures-that-the-deployment-system-knows-where-to-fetch-the-models-components-which-is-crucial-for-initializing-the-model-in-the-target-environment","title":"This key-value pair indicates the directory or path from which the necessary model files should be downloaded. In the context of deploying an LLM, these files could include the model weights, tokenizer files, and any other dependencies required for the model to run. This parameter ensures that the deployment system knows where to fetch the model's components, which is crucial for initializing the model in the target environment.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#customizable-download-target","title":"Customizable Download Target","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#_comment-you-can-also-define-download_files_to-otherwise-varpracticuscache-is-used-this-comment-within-the-json-highlights-an-optional-parameter-that-could-be-specified-in-a-similar-json-configuration-file-if-the-download_files_to-parameter-is-provided-it-would-dictate-the-destination-directory-on-the-local-system-where-the-downloaded-files-should-be-stored-in-the-absence-of-this-parameter-a-default-location-varpracticuscache-is-used-this-flexibility-allows-for-adaptability-to-different-deployment-environments-and-configurations-ensuring-that-the-files-are-stored-in-a-location-that-is-accessible-and-appropriate-for-the-models-operation","title":"\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\": This comment within the JSON highlights an optional parameter that could be specified in a similar JSON configuration file. If the download_files_to parameter is provided, it would dictate the destination directory on the local system where the downloaded files should be stored. In the absence of this parameter, a default location (/var/practicus/cache) is used. This flexibility allows for adaptability to different deployment environments and configurations, ensuring that the files are stored in a location that is accessible and appropriate for the model's operation.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#modeljson_1","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/codellama-01/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/04_model_json/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n\n    # Assuming llama library is copied into cache dir, in addition to torch .pth files\n    llama_cache = \"/var/practicus/cache\"\n    if llama_cache not in sys.path:\n        sys.path.insert(0, llama_cache)\n\n    try:\n        from llama import Llama\n    except Exception as e:\n        raise ModuleNotFoundError(\"llama library not found. Have you included it in the object storage cache?\") from e\n\n    try:\n        generator = Llama.build(\n            ckpt_dir=f\"{llama_cache}/CodeLlama-7b-Instruct/\",\n            tokenizer_path=f\"{llama_cache}/CodeLlama-7b-Instruct/tokenizer.model\",\n            max_seq_len=512,\n            max_batch_size=4,\n            model_parallel_size=1\n        )\n    except:\n        building_generator = False\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\n\ndef _predict(http_request=None, model_meta=None, payload_dict=None, *args, **kwargs):\n    start = datetime.now()\n\n    # instructions = [[\n    #     {\"role\": \"system\", \"content\": payload_dict[\"system_context\"]},\n    #     {\"role\": \"user\", \"content\": payload_dict[\"user_prompt\"]}\n    # ]]\n\n    instructions = [[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\"role\": \"user\", \"content\": \"Capital of Turkey\"}\n    ]]\n\n    results = generator.chat_completion(\n        instructions,\n        max_gen_len=None,\n        temperature=0.2,\n        top_p=0.95,\n    )\n\n    answer = \"\"\n    for result in results:\n        answer += f\"{result['generation']['content']}\\n\"\n\n    print(\"thread answer:\", answer)\n    total_time = (datetime.now() - start).total_seconds()\n    print(\"thread asnwer in:\", total_time)    \n\n    global answers \n    answers += f\"start:{start} end: {datetime.now()} time: {total_time} answer: {answer}\\n\"\n\n\nasync def predict(http_request, model_meta=None, payload_dict=None, *args, **kwargs):\n    await init(model_meta)\n\n    import threading \n\n    threads = []\n\n    count = int(payload_dict[\"count\"])\n    thread_start = datetime.now()\n    for _ in range(count):\n        thread = threading.Thread(target=_predict)\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    print(\"Total finished in:\", (datetime.now() - thread_start).total_seconds())    \n\n    return {\n        \"answer\": f\"Time:{(datetime.now() - thread_start).total_seconds()}\\nanswers:{answers}\"\n    }\n</code></pre> <p>Previous: Model | Next: Deploy</p>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/","title":"Deploy","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#deploying-a-model-with-practicus-ai-involves-a-sequence-of-steps-designed-to-securely-and-efficiently-transition-a-model-from-development-to-a-production-ready-state-heres-a-step-by-step-explanation-aimed-at-providing-clarity-and-guidance","title":"Deploying a model with Practicus AI involves a sequence of steps designed to securely and efficiently transition a model from development to a production-ready state. Here's a step-by-step explanation, aimed at providing clarity and guidance:","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#step-1-importing-necessary-modules","title":"Step 1: Importing Necessary Modules","text":"<pre><code>import practicuscore as prt\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#step-2-setting-up-deployment-parameters","title":"Step 2: Setting Up Deployment Parameters","text":"<pre><code>deployment_key = \"deployment-gpu\"\nprefix = \"models/practicus\"\nmodel_name = \"codellama\"\nmodel_dir = None  # Current dir\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#deployment-key-identifies-the-deployment-configuration-such-as-compute-resources-eg-gpu-or-cpu-preferences-this-key-ensures-your-model-is-deployed-with-the-appropriate-infrastructure-for-its-needs","title":"Deployment Key: Identifies the deployment configuration, such as compute resources (e.g., GPU or CPU preferences). This key ensures your model is deployed with the appropriate infrastructure for its needs.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#prefix-a-logical-grouping-or-namespace-for-your-models-this-helps-in-organizing-and-managing-models-within-practicus-ai","title":"Prefix: A logical grouping or namespace for your models. This helps in organizing and managing models within Practicus AI.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#model-name-a-unique-identifier-for-your-model-within-practicus-ai-this-name-is-used-to-reference-and-manage-the-model-post-deployment","title":"Model Name: A unique identifier for your model within Practicus AI. This name is used to reference and manage the model post-deployment.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#model-directory-the-directory-containing-your-models-files-if-none-it-defaults-to-the-current-directory-from-which-the-deployment-script-is-run","title":"Model Directory: The directory containing your model's files. If None, it defaults to the current directory from which the deployment script is run.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#secure-password-entry-getpassgetpass-securely-prompts-for-the-password-ensuring-its-not-visible-or-stored-in-plaintext-within-the-script","title":"Secure Password Entry: getpass.getpass() securely prompts for the password, ensuring it's not visible or stored in plaintext within the script.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#authentication-token-dpget_auth_token-requests-an-authentication-token-from-practicus-ai-using-your-email-and-password-this-token-is-crucial-for-subsequent-api-calls-to-be-authenticated","title":"Authentication Token: dp.get_auth_token() requests an authentication token from Practicus AI, using your email and password. This token is crucial for subsequent API calls to be authenticated.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#step-3-deploying-the-model","title":"Step 3: Deploying the Model","text":"<pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix, \n    model_name=model_name, \n    model_dir=model_dir\n)\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#model-deployment-a-call-to-deploy-initiates-the-deployment-process-it-requires-the-host-url-your-email-the-obtained-auth_token-and-other-previously-defined-parameters","title":"Model Deployment: A call to deploy() initiates the deployment process. It requires the host URL, your email, the obtained auth_token, and other previously defined parameters.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#feedback-upon-successful-deployment-youll-receive-a-confirmation-if-authentication-fails-or-other-issues-arise-youll-be-prompted-with-an-error-message-to-help-diagnose-and-resolve-the-issue","title":"Feedback: Upon successful deployment, you'll receive a confirmation. If authentication fails or other issues arise, you'll be prompted with an error message to help diagnose and resolve the issue.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#summary","title":"Summary","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#this-process-encapsulates-a-secure-and-structured-approach-to-model-deployment-in-practicus-ai-leveraging-the-datapipeline-for-effective-model-management-by-following-these-steps-you-ensure-that-your-model-is-deployed-to-the-right-environment-with-the-appropriate-configurations-ready-for-inference-at-scale-this-systematic-approach-not-only-simplifies-the-deployment-process-but-also-emphasizes-security-and-organization-critical-factors-for-successful-ai-project-implementations","title":"This process encapsulates a secure and structured approach to model deployment in Practicus AI, leveraging the DataPipeline for effective model management. By following these steps, you ensure that your model is deployed to the right environment with the appropriate configurations, ready for inference at scale. This systematic approach not only simplifies the deployment process but also emphasizes security and organization, critical factors for successful AI project implementations.","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/codellama-01/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/05_deploy/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n\n    # Assuming llama library is copied into cache dir, in addition to torch .pth files\n    llama_cache = \"/var/practicus/cache\"\n    if llama_cache not in sys.path:\n        sys.path.insert(0, llama_cache)\n\n    try:\n        from llama import Llama\n    except Exception as e:\n        raise ModuleNotFoundError(\"llama library not found. Have you included it in the object storage cache?\") from e\n\n    try:\n        generator = Llama.build(\n            ckpt_dir=f\"{llama_cache}/CodeLlama-7b-Instruct/\",\n            tokenizer_path=f\"{llama_cache}/CodeLlama-7b-Instruct/tokenizer.model\",\n            max_seq_len=512,\n            max_batch_size=4,\n            model_parallel_size=1\n        )\n    except:\n        building_generator = False\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\n\ndef _predict(http_request=None, model_meta=None, payload_dict=None, *args, **kwargs):\n    start = datetime.now()\n\n    # instructions = [[\n    #     {\"role\": \"system\", \"content\": payload_dict[\"system_context\"]},\n    #     {\"role\": \"user\", \"content\": payload_dict[\"user_prompt\"]}\n    # ]]\n\n    instructions = [[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\"role\": \"user\", \"content\": \"Capital of Turkey\"}\n    ]]\n\n    results = generator.chat_completion(\n        instructions,\n        max_gen_len=None,\n        temperature=0.2,\n        top_p=0.95,\n    )\n\n    answer = \"\"\n    for result in results:\n        answer += f\"{result['generation']['content']}\\n\"\n\n    print(\"thread answer:\", answer)\n    total_time = (datetime.now() - start).total_seconds()\n    print(\"thread asnwer in:\", total_time)    \n\n    global answers \n    answers += f\"start:{start} end: {datetime.now()} time: {total_time} answer: {answer}\\n\"\n\n\nasync def predict(http_request, model_meta=None, payload_dict=None, *args, **kwargs):\n    await init(model_meta)\n\n    import threading \n\n    threads = []\n\n    count = int(payload_dict[\"count\"])\n    thread_start = datetime.now()\n    for _ in range(count):\n        thread = threading.Thread(target=_predict)\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    print(\"Total finished in:\", (datetime.now() - thread_start).total_seconds())    \n\n    return {\n        \"answer\": f\"Time:{(datetime.now() - thread_start).total_seconds()}\\nanswers:{answers}\"\n    }\n</code></pre> <p>Previous: Model Json | Next: Consume Parallel</p>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/06_consume_parallel/","title":"Consume Parallel","text":"<pre><code>model_name = \"codellama-workshop\"\ntoken = \"...\"\n\nfrom requests import get\napi_url = f'https://practicus.company.com/models/practicus/{model_name}/'\nheaders = {'authorization': f'Bearer {token}'}\nr = get(api_url + '?get_meta=true', headers=headers)\n\nprint('Model details: ', r.text)\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n\n# r = get(api_url, headers=headers, files={'data.csv': open('data.csv', 'rb')})\n# print('Prediction result: ', r.text)\n</code></pre> <pre><code>def query():\n    from datetime import datetime\n    from requests import get\n    import json\n\n    start = datetime.now()\n    print(\"thread start: \", start)\n\n    data = {\n        'system_context': \"\",\n        'user_prompt': 'Capital of Tanzania'\n    }\n    r = get(api_url, headers=headers, json=data)\n\n    if r.status_code != 200:\n        print(f\"Error code {r.status_code}\")\n\n    print('Prediction time (sec): ', (datetime.now() - start).total_seconds())\n    print('Prediction result:')\n    try:\n        parsed = json.loads(r.text)\n        print(json.dumps(parsed, indent=1))\n    except:\n        print(r.text)\n\n    print(\"Headers: \", r.headers)\n</code></pre> <pre><code>import threading \nfrom datetime import datetime \n\nthreads = []\n\nthread_start = datetime.now()\nfor _ in range(5):\n    thread = threading.Thread(target=query)\n    thread.start()\n    threads.append(thread)\n\nfor thread in threads:\n    thread.join()\n\nprint(\"Total finished in:\", (datetime.now() - thread_start).total_seconds())\n</code></pre> <pre><code>from datetime import datetime\nfrom requests import get\nimport json\n\nstart = datetime.now()\n\ndata = {\n    'system_context': 'You answer generic questions',\n    'user_prompt': 'tell me what you know about Praticus AI'\n    }\n\nr = get(api_url, headers=headers, json=data)\n\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n\nprint('Prediction time (sec): ', (datetime.now() - start).total_seconds())\nprint('Prediction result:')\ntry:\n    parsed = json.loads(r.text)\n    print(json.dumps(parsed, indent=1))\nexcept:\n    print(r.text)\n\nprint(\"Headers: \", r.headers)\n</code></pre> <pre><code>reset_cache_url = \"https://practicus.company.com/models/codellama-01/v2/?reset_cache=True\"\n\nr = get(reset_cache_url, headers=headers, json=data)\n\nif r.status_code != 200:\n    print(f\"Error code {r.status_code}\")\n\nprint(r.text)\n\nprint(\"Headers: \", r.headers)\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/06_consume_parallel/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/04_generative_ai/05_deploying_llm/06_consume_parallel/#modeljson","title":"model.json","text":"<pre><code>{\n\"download_files_from\": \"cache/codellama-01/\",\n\"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/04_generative_ai/05_deploying_llm/06_consume_parallel/#modelpy","title":"model.py","text":"<pre><code>import sys\nfrom datetime import datetime\n\ngenerator = None\nanswers = \"\"\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global generator\n    if generator is not None:\n        print(\"generator exists, using\")\n        return\n\n    print(\"generator is none, building\")\n\n    # Assuming llama library is copied into cache dir, in addition to torch .pth files\n    llama_cache = \"/var/practicus/cache\"\n    if llama_cache not in sys.path:\n        sys.path.insert(0, llama_cache)\n\n    try:\n        from llama import Llama\n    except Exception as e:\n        raise ModuleNotFoundError(\"llama library not found. Have you included it in the object storage cache?\") from e\n\n    try:\n        generator = Llama.build(\n            ckpt_dir=f\"{llama_cache}/CodeLlama-7b-Instruct/\",\n            tokenizer_path=f\"{llama_cache}/CodeLlama-7b-Instruct/tokenizer.model\",\n            max_seq_len=512,\n            max_batch_size=4,\n            model_parallel_size=1\n        )\n    except:\n        building_generator = False\n        raise\n\n\nasync def cleanup(model_meta=None, *args, **kwargs):\n    print(\"Cleaning up memory\")\n\n    global generator\n    generator = None\n\n    from torch import cuda\n    cuda.empty_cache()\n\n\ndef _predict(http_request=None, model_meta=None, payload_dict=None, *args, **kwargs):\n    start = datetime.now()\n\n    # instructions = [[\n    #     {\"role\": \"system\", \"content\": payload_dict[\"system_context\"]},\n    #     {\"role\": \"user\", \"content\": payload_dict[\"user_prompt\"]}\n    # ]]\n\n    instructions = [[\n        {\"role\": \"system\", \"content\": \"\"},\n        {\"role\": \"user\", \"content\": \"Capital of Turkey\"}\n    ]]\n\n    results = generator.chat_completion(\n        instructions,\n        max_gen_len=None,\n        temperature=0.2,\n        top_p=0.95,\n    )\n\n    answer = \"\"\n    for result in results:\n        answer += f\"{result['generation']['content']}\\n\"\n\n    print(\"thread answer:\", answer)\n    total_time = (datetime.now() - start).total_seconds()\n    print(\"thread asnwer in:\", total_time)    \n\n    global answers \n    answers += f\"start:{start} end: {datetime.now()} time: {total_time} answer: {answer}\\n\"\n\n\nasync def predict(http_request, model_meta=None, payload_dict=None, *args, **kwargs):\n    await init(model_meta)\n\n    import threading \n\n    threads = []\n\n    count = int(payload_dict[\"count\"])\n    thread_start = datetime.now()\n    for _ in range(count):\n        thread = threading.Thread(target=_predict)\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    print(\"Total finished in:\", (datetime.now() - thread_start).total_seconds())    \n\n    return {\n        \"answer\": f\"Time:{(datetime.now() - thread_start).total_seconds()}\\nanswers:{answers}\"\n    }\n</code></pre> <p>Previous: Deploy | Next: Start Cluster</p>"},{"location":"notebooks/05_distributed_computing/01_spark/01_interactive/01_start_cluster/","title":"Starting an interactive Spark Cluster","text":"<ul> <li>This notebook demonstrates how to create, and connect to a Practicus AI Spark cluster, and execute simple Spark operations. </li> </ul>"},{"location":"notebooks/05_distributed_computing/01_spark/01_interactive/01_start_cluster/#note-on-shared-drives","title":"Note on shared drives","text":"<ul> <li>Practicus AI distributed clusters require a shared drive accsible by multiple workers, such as Practicus AI <code>my/shared</code> olders.</li> <li>If you do not have access to my/shared folders, please check the auto-scaled sample notebook which does not need such drives, but are limited in functionality.</li> </ul> <pre><code>import practicuscore as prt\n\n# Let's define the distributed features\ndistributed_config = prt.distributed.JobConfig(\n    job_type = prt.distributed.JobType.spark,\n    worker_count = 2,\n)\n\n# Let's define worker features of the cluster \nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker \n# will also create the cluster.\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code># Since this is an interactive Spark cluster, \n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"notebooks/05_distributed_computing/01_spark/01_interactive/01_start_cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the newxt notebook in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator \n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Consume Parallel | Next: Use Cluster</p>"},{"location":"notebooks/05_distributed_computing/01_spark/01_interactive/02_use_cluster/","title":"Using the interactive Spark Cluster Client","text":"<ul> <li>This notebook demonstrates how to connect to the Practicus AI Spark cluster we created, and execute simple Spark operations.</li> <li>Please run this notebook on the <code>Spark Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt \n\n# Let's get a Spark session\nspark = prt.distributed.get_client()\n</code></pre> <pre><code># And execute some code\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Perform a transformation\ndf_filtered = df.filter(df.Age &gt; 30)\n\n# Show results\ndf_filtered.show()\n</code></pre> <pre><code># Let's end the session\nspark.stop()\n</code></pre>"},{"location":"notebooks/05_distributed_computing/01_spark/01_interactive/02_use_cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <pre><code>\n</code></pre> <p>Previous: Start Cluster | Next: Batch Job</p>"},{"location":"notebooks/05_distributed_computing/01_spark/02_batch_job/batch_job/","title":"Executing batch jobs in Spark Cluster","text":"<p>In this notebook we will: - Create a Spark cluster - Submit a job python file - Terminate the cluster after job is completed.</p>"},{"location":"notebooks/05_distributed_computing/01_spark/02_batch_job/batch_job/#before-you-begin","title":"Before you begin","text":"<ul> <li>Create \"spark\" under your \"~/my\" folder</li> <li>And copy job.py under this folder</li> </ul> <pre><code>import practicuscore as prt\n\njob_dir = \"~/my/spark\"\n\ndistributed_config = prt.distributed.JobConfig(\n    job_type = prt.distributed.JobType.spark,\n    job_dir = job_dir,\n    py_file = \"job.py\",\n    worker_count = 2,\n)\n\nworker_config = prt.WorkerConfig(\n    worker_size=\"X-Small\",\n    distributed_config=distributed_config,\n    log_level=\"DEBUG\",\n)\n\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n</code></pre> <pre><code>prt.distributed.live_view(\n    job_dir=job_dir,\n    job_id=coordinator_worker.job_id,\n)\n</code></pre>"},{"location":"notebooks/05_distributed_computing/01_spark/02_batch_job/batch_job/#wrapping-up","title":"Wrapping up","text":"<ul> <li>Once the job is completed, you can view the resuls in <code>~/my/spark/result.csv/</code></li> <li>Please note that result.csv is a folder that contains <code>parts of the processed file</code> by each worker (Spark executors)</li> <li>Also note that you do not need to terminate the cluster since it has a 'py_file' to execute, which defaults <code>terminate_on_completion</code> parameter to True.</li> <li>You can change terminate_on_completion to False to keep the cluster running after the job is completed to troubleshoot issues.</li> <li>You can view other <code>prt.distributed.JobConfig</code> properties to customize the cluster</li> </ul> <pre><code>\n</code></pre>"},{"location":"notebooks/05_distributed_computing/01_spark/02_batch_job/batch_job/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/05_distributed_computing/01_spark/02_batch_job/batch_job/#jobpy","title":"job.py","text":"<pre><code>import practicuscore as prt \n\n# Let's get a Spark session\nprint(\"Getting Spark session\")\nspark = prt.distributed.get_client()\n\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\nprint(\"Creating DataFrame\")\ndf = spark.createDataFrame(data, columns)\n\nprint(\"Calculating\")\ndf_filtered = df.filter(df.Age &gt; 30)\n\nprint(\"Writing to csv\")\ndf_filtered.write.csv(\"/home/ubuntu/my/spark/result.csv\", header=True, mode=\"overwrite\")\n</code></pre> <p>Previous: Use Cluster | Next: Start Cluster</p>"},{"location":"notebooks/05_distributed_computing/01_spark/03_auto_scaled/01_start_cluster/","title":"Starting an auto-scaled Spark Cluster","text":"<ul> <li>This notebook demonstrates how to create, and connect to a Practicus AI Spark auto-scaled cluster, and execute simple Spark operations. </li> </ul>"},{"location":"notebooks/05_distributed_computing/01_spark/03_auto_scaled/01_start_cluster/#important-note-on-worker-container-image","title":"Important note on worker container image","text":"<ul> <li>Unlike standard Spark cluster, auto-scaled Spark cluster executors have a separate type of container image ghcr.io/practicusai/practicus-spark</li> <li>This means packages accessible to coordinator worker might not be accessible to the executors.</li> <li>To install packages please install to both the coordinator and the executor images and create custom container images.</li> <li>While creating the spark client, you can then pass arguments to specify which executor image to use.</li> </ul>"},{"location":"notebooks/05_distributed_computing/01_spark/03_auto_scaled/01_start_cluster/#important-note-on-privileged-access","title":"Important note on privileged access","text":"<ul> <li>For auto-scaled Spark to work, <code>you will need additional privileges</code> on the Kubernetes cluster.</li> <li>Please ask your admin to grant you access to worker size definitions with privileged access before you continue with this notebook.</li> </ul>"},{"location":"notebooks/05_distributed_computing/01_spark/03_auto_scaled/01_start_cluster/#finding-an-auto-scaled-privileged-worker-size","title":"Finding an auto-scaled (privileged) worker size","text":"<p>Let's find a worker size with auto-scaled (privileged) capabilities</p> <pre><code>import practicuscore as prt\n\nregion = prt.get_default_region()\n\nauto_dist_worker_size = None\n\nfor worker_size in region.worker_size_list:\n    if worker_size.auto_distributed:\n        auto_dist_worker_size = worker_size.name \n        break\n\nassert auto_dist_worker_size, \"You do not have access to any auto-distributed (privileged) worker sizes\"\n\nprint(\"Located an auto-distributed (privileged) worker size:\", auto_dist_worker_size)\n</code></pre> <pre><code># Let's define the distributed features\ndistributed_config = prt.distributed.JobConfig(\n    job_type = prt.distributed.JobType.spark,\n    # ** The below changes the default cluster behavior **\n    auto_distributed=True,\n    # Set the initial size. \n    # These are 'additional` executors to coordinator, \n    # E.g. the below will create a cluster of 2 workers.\n    initial_count=1,\n    # Optional: set a maximum to auto-scale to, if needed.\n    # E.g. with the below, the cluster can scale up to 5 workers\n    max_count=4,\n)\n\n# Let's define worker features of the cluster \nworker_config = prt.WorkerConfig(\n    # Please make sure to use a worker size with\n    #   privileged access.\n    worker_size=auto_dist_worker_size,\n    distributed_config=distributed_config,\n)\n\n# Creating the coordinator (master) worker:\ncoordinator_worker = prt.create_worker(\n    worker_config=worker_config,\n)\n# - The above will NOT create the executors instantly.\n# - You will only create one worker.\n# - Additional executors will be created when needed.\n</code></pre> <pre><code># Since this is an interactive Spark cluster, \n#  let's login to execute some code.\n\nnotebook_url = coordinator_worker.open_notebook()\n\nprint(\"Page did not open? You can open this url manually:\", notebook_url)\n</code></pre>"},{"location":"notebooks/05_distributed_computing/01_spark/03_auto_scaled/01_start_cluster/#please-continue-experimenting-on-the-new-browser-tab","title":"Please continue experimenting on the new browser tab","text":"<p>by opening the next notebook in this directory</p> <pre><code># Done experimenting? Let's terminate the coordinator \n#  which will also terminate the cluster.\ncoordinator_worker.terminate()\n</code></pre> <p>Previous: Batch Job | Next: Use Cluster</p>"},{"location":"notebooks/05_distributed_computing/01_spark/03_auto_scaled/02_use_cluster/","title":"Using the interactive Spark Cluster Client","text":"<ul> <li>This notebook demonstrates how to connect to the Practicus AI Spark cluster we created, and execute simple Spark operations.</li> <li>Please run this notebook on the <code>Spark Coordinator (master)</code>.</li> </ul> <pre><code>import practicuscore as prt \n\n# Let's get a Spark session\nspark = prt.distributed.get_client()\n</code></pre>"},{"location":"notebooks/05_distributed_computing/01_spark/03_auto_scaled/02_use_cluster/#behind-the-scenes","title":"Behind the scenes","text":"<ul> <li>After the above code, new Spark executors will start running.</li> <li>This is specific to auto-scaled Spark only and not the dfault behavior.</li> </ul> <pre><code># And execute some code\ndata = [(\"Alice\", 29), (\"Bob\", 34), (\"Cathy\", 23)]\ncolumns = [\"Name\", \"Age\"]\n\n# Create a DataFrame\ndf = spark.createDataFrame(data, columns)\n\n# Perform a transformation\ndf_filtered = df.filter(df.Age &gt; 30)\n\n# Show results\ndf_filtered.show()\n</code></pre> <pre><code># Explicitly delete spark session\nprt.engines.delete_spark_session()\n# Unlike the standard Spark cluster, the below won't work for auto-scaled.\n# spark.stop()\n</code></pre>"},{"location":"notebooks/05_distributed_computing/01_spark/03_auto_scaled/02_use_cluster/#terminating-the-cluster","title":"Terminating the cluster","text":"<ul> <li>You can go back to the other worker where you created the cluster to run:</li> </ul> <p><pre><code>coordinator_worker.terminate()\n</code></pre> - Or, terminate \"self\" and children workers with the below:</p> <pre><code>prt.get_local_worker().terminate()\n</code></pre> <p>Previous: Start Cluster | Next: Code Quality</p>"},{"location":"notebooks/06_other_topics/addons/","title":"Addons","text":""},{"location":"notebooks/06_other_topics/addons/#practicus-ai-add-ons","title":"Practicus AI Add-ons","text":"<p>Practicus AI Add-ons are external services such as Airflow, MlFlow, observability, analytics etc. that are either installed part of Practicus AI, or embedded as a serice managed by a 3rd party. You can programmatically view, access and view them on your browser.</p> <pre><code>import practicuscore as prt\n\n# Add-ons are bundled under a Practicus AI region\nregion = prt.current_region()\n\n# You can also conenct to a remote region instead of the default one\n# region = prt.regions.get_region(..)\n\n# Let's make sure you have at least one add-on\nif len(region.addon_list) == 0:\n    raise NotImplementedError(\"You do not have any add-ons installed..\")\n</code></pre> <pre><code># Let's iterate over add-ons\nfor addon in region.addon_list:\n    print(\"Add-on key:\", addon.key)\n    print(\"  Url:\", addon.url)\n</code></pre> <pre><code># Converting the addon_list into string will give you a csv \nprint(region.addon_list)\n</code></pre> <pre><code># You can also get add-ons as a pandas DataFrame for convenience\nregion.addon_list.to_pandas()\n</code></pre> <pre><code># Simply accessing the addon_list will print a formatted table string \nregion.addon_list\n</code></pre> <pre><code>first_addon = region.addon_list[0]\n# Accessing an add-on object will print it's details\nfirst_addon\n</code></pre> <pre><code># You can open the add-on url on an external browser tab\nfirst_addon.open()\n</code></pre> <pre><code># If you have the add-on key, you can directly open using region object\naddon_key = first_addon.key  # e.g. my-airflow-service\nregion.open_addon(addon_key)\n</code></pre> <pre><code># You can search for an add-on using the region object and add-on key\naddon_key = first_addon.key\nfound_addon = region.get_addon(addon_key)\nassert found_addon, f\"Addon {addon_key} not found\"\nfound_addon.open()\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/addons/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/addons/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Customizing Templates | Next: Connections</p>"},{"location":"notebooks/06_other_topics/advanced_gpu/","title":"Advanced GPU Resource Management with Practicus AI","text":"<p>This document provides an overview of how to configure partial NVIDIA GPUs (MIG), along with high-level steps for AMD and Intel GPU support, and how Practicus AI platform facilitates GPU management.</p>"},{"location":"notebooks/06_other_topics/advanced_gpu/#topics-covered-in-this-document","title":"Topics covered in this document","text":"<ul> <li>Partial NVIDIA GPUs (MIG)</li> <li>AMD GPUs</li> <li>Intel GPUs</li> </ul>"},{"location":"notebooks/06_other_topics/advanced_gpu/#1-partial-nvidia-gpus-mig","title":"1. Partial NVIDIA GPUs (MIG)","text":""},{"location":"notebooks/06_other_topics/advanced_gpu/#how-mig-works","title":"How MIG Works","text":"<ul> <li>NVIDIA's <code>Multi-Instance GPU (MIG)</code> feature allows you to split a single physical GPU (e.g., NVIDIA A100m, H100, H200) into multiple independent GPU instances.</li> <li>Each MIG instance provides dedicated memory and compute resources, ideal for running multiple workloads on the same GPU.</li> </ul>"},{"location":"notebooks/06_other_topics/advanced_gpu/#setup-steps","title":"Setup Steps","text":"<ol> <li>Enable MIG Mode on the GPU:</li> <li>Log into the GPU node and enable MIG using <code>nvidia-smi</code>:      <pre><code>sudo nvidia-smi -i 0 --mig-enable\nsudo reboot\n</code></pre></li> <li> <p>After reboot, confirm MIG mode is enabled:      <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Create MIG Instances:</p> </li> <li>Use <code>nvidia-smi</code> to create MIG profiles. For example, split a GPU into 7 instances:      <pre><code>sudo nvidia-smi mig -i 0 -cgi 0,1,2,3,4,5,6\nsudo nvidia-smi mig -i 0 -cci\n</code></pre></li> <li> <p>Check the configuration:      <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Expose MIG Resources in Kubernetes:</p> </li> <li>Deploy the NVIDIA Device Plugin:      <pre><code>kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/main/deployments/k8s-device-plugin-daemonset.yaml\n</code></pre></li> <li> <p>Verify available resources:      <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre></p> </li> <li> <p>To learn more please visit:</p> <ul> <li>https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html</li> </ul> </li> </ol>"},{"location":"notebooks/06_other_topics/advanced_gpu/#2-custom-gpu-configuration-in-practicus-ai","title":"2. Custom GPU Configuration in Practicus AI","text":"<p>Practicus AI simplifies advanced GPU management through the intuitive management UI:</p> <p>Open Practicus AI Management Console: - Access the platform's web console for infrustructure management.</p> <p>Select Worker Sizes: - Choose from predefined worker sizes or create a new one to include GPU capacity :    - <code>Number of GPUs</code>    - <code>Amount of Video RAM (VRAM)</code></p> <p>Enter GPU Type Selector: - Specify the custom GPU type you need:     - Specify for Nvidia MIG (e.g. <code>nvidia.com/mig-1g.5gb</code>) you defined in the above step.     - Specify for other vendors (e.g., <code>amd.com/gpu</code> or <code>intel.com/gpu</code>).     - Leave empty for the default, which will use entire NVDIA GPUs without fractions.</p> <p>Deploy workloads as usual: - Deploy end user workers, model hosts, app hosts etc. as usual with the worker size you defined above. - The platform will dynamically manage the resources with the selected GPU configuration.</p> <p>Example Configuration</p> <ul> <li>If you set GPU count to 2, with a GPU type selector of <code>nvidia.com/mig-1g.5gb</code> running on a single <code>NVIDIA H100 GPU</code>, the end user could get two separate GPU instances, each with 1/7th of the GPU's compute and memory resources (1 compute slice and 5 GB of memory per instance).</li> <li>This configuration allows the same physical GPU to handle multiple workloads independently, providing dedicated resources for each workload without interference. </li> <li>This setup is ideal for lightweight GPU workloads, such as inference or smaller-scale training tasks, that do not require the full power of an entire GPU.</li> </ul> <p>Important Note - Please note that the VRAM setting in the Practicus AI Management Console does not dictate how much VRAM a user gets. It is only used to measure usage and ensure a user is kept within their designated daily/weekly/monthly usage limits.  - To actually enforce VRAM limits, you must use NVIDIA MIG profiles (e.g., <code>nvidia.com/mig-1g.5gb</code>) or equivalent to <code>apply resource constraints at the hardware level.</code></p>"},{"location":"notebooks/06_other_topics/advanced_gpu/#3-high-level-steps-for-amd-gpus","title":"3. High-Level Steps for AMD GPUs","text":"<p>AMD GPUs (e.g., using ROCm) require setup similar to NVIDIA but with their own tools and configurations:</p> <ol> <li>Install AMD ROCm Drivers:</li> <li> <p>Install ROCm drivers on the nodes with AMD GPUs.</p> </li> <li> <p>Deploy AMD Device Plugin:</p> </li> <li>Use the AMD ROCm Kubernetes device plugin to expose AMD GPU resources:      <pre><code>kubectl apply -f https://github.com/RadeonOpenCompute/k8s-device-plugin\n</code></pre></li> </ol> <p>The rest is the same as NVIDIA MIG, define a new worker size and use the GPU Type Selector <code>amd.com/gpu</code></p>"},{"location":"notebooks/06_other_topics/advanced_gpu/#4-high-level-steps-for-intel-gpus","title":"4. High-Level Steps for Intel GPUs","text":"<p>Intel GPUs can be managed using the Intel GPU Device Plugin:</p> <ol> <li>Install Intel GPU Drivers:</li> <li> <p>Install Intel drivers and libraries for iGPU or discrete GPU support.</p> </li> <li> <p>Deploy Intel Device Plugin:</p> </li> <li>Use the Intel GPU plugin to expose GPU resources:      <pre><code>kubectl apply -f https://github.com/intel/intel-device-plugins-for-kubernetes\n</code></pre> The rest is the same as NVIDIA MIG, define a new worker size and use the GPU Type Selector <code>intel.com/gpu</code></li> </ol> <pre><code>\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/advanced_gpu/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/advanced_gpu/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Streamlined Model Deployment | Next: Processes</p>"},{"location":"notebooks/06_other_topics/connections/","title":"Connections","text":""},{"location":"notebooks/06_other_topics/connections/#practicus-ai-connections","title":"Practicus AI Connections","text":""},{"location":"notebooks/06_other_topics/connections/#data-upload-to-s3-compatible-storage-with-the-practicus-sdk","title":"Data Upload to S3-Compatible Storage with the Practicus SDK","text":"<p>This notebook demonstrates how to securely and efficiently upload local data  from a specified folder to an S3-compatible storage solution\u2014such as Amazon S3,  MinIO, or other similar services\u2014using the Practicus SDK. As a data scientist,  you can integrate this step into your data pipelines to simplify dataset management.</p> <p>By providing your AWS credentials or compatible credentials, as well as optional  parameters for regions, endpoints, and prefixes, you can:</p> <ul> <li>Automatically upload large datasets to remote object storage</li> <li>Maintain versioned data repositories for improved reproducibility</li> <li>Effortlessly share data and results with collaborators or production services</li> </ul> <p>Simply fill in the required parameters below and run the notebook to transfer  files from this worker or your local machine to the target object storage.</p> <pre><code>import practicuscore as prt\n\n_aws_access_key_id = None   # AWS Access Key ID or compatible service key\n_aws_secret_access_key = None  # AWS Secret Access Key or compatible service secret\n_bucket = None  # The name of your target bucket, e.g. \"my-data-bucket\"\n\n# Ensure that essential parameters are provided\nassert _aws_access_key_id and _aws_secret_access_key and _bucket\n\n_aws_session_token = None  # (Optional) AWS session token for temporary credentials\n_aws_region = None         # (Optional) Your AWS region. If unknown, you may leave it as None.\n_endpoint_url = None       # (Optional) Endpoint URL for S3-compatible services (e.g., MinIO API URL)\n\n_prefix = None  # (Optional) Prefix for organizing objects within the bucket. \n                 # Use None or \"\" for root-level placement, or specify something \n                 # like \"folder\" or \"folder/subfolder\" for nested directories.\n\n_folder_path = None  # The local path containing files to upload.\n                     # Example: \"/home/ubuntu/your/folder/path/\"\n\n_source_path_to_cut = None  # (Optional) A prefix within the local folder path \n                            # that you want to remove from the uploaded object keys.\n                            # Leave as None to default to the entire folder path.\n\n# Ensure the folder path is provided\nassert _folder_path\n\n_upload_conf = prt.connections.UploadS3Conf(\n    bucket=_bucket,\n    prefix=_prefix,\n    folder_path=_folder_path,\n    source_path_to_cut=_source_path_to_cut,\n    aws_access_key_id=_aws_access_key_id,\n    aws_secret_access_key=_aws_secret_access_key\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/connections/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/connections/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Addons</p>"},{"location":"notebooks/06_other_topics/customizing_templates/","title":"Practicus AI Code Generation Templates","text":"<p>Practicus AI workers use Jinja2 templates in ~/practicus/templates folder to streamline and automate code generation for key components such as Python libraries, Airflow DAGs and Jupyter notebooks. This approach ensures consistency, scalability, and customization across your projects. Below is an overview of how these templates work and how you can use them effectively.</p>"},{"location":"notebooks/06_other_topics/customizing_templates/#1-overview-of-jinja2-templates","title":"1. Overview of Jinja2 Templates","text":"<p>Jinja2 is a powerful templating engine for Python, enabling dynamic creation of files by inserting variables and logic into predefined structures. In our platform, templates are configured to: - Standardize code structure and style. - Dynamically generate project-specific code. - Simplify the onboarding process for new workflows. - To learn more please visit:     - https://jinja.palletsprojects.com/en/stable/intro/</p>"},{"location":"notebooks/06_other_topics/customizing_templates/#2-how-to-customize-templates","title":"2. How to Customize Templates","text":"<ul> <li>Start a worker and view templates in ~/practicus/templates folder</li> <li>Customize templates by editing the Jinja2 syntax with your specific project details.</li> <li>Templates can be rendered to test programmatically, see below a sample.</li> <li>Once your testing is completed, you can create a new Practicus AI worker container image and override existing template files.</li> </ul>"},{"location":"notebooks/06_other_topics/customizing_templates/#3-best-practices","title":"3. Best Practices","text":"<ul> <li>Test Before Use: Validate generated code in a test environment.</li> <li>Document Changes: If you modify templates, document the changes for future reference.</li> <li>Leverage Variables and Logic: Use Jinja2's advanced features like loops and conditionals for maximum flexibility.</li> </ul> <pre><code># Jinja templates in action..\n\nfrom jinja2 import Template\n\ntemplate_content = \"\"\"\ndef {{ function_name }}(x):\n    return x ** 2\n\"\"\"\n\ntemplate = Template(template_content)\nrendered_code = template.render(function_name=\"square\")\nprint(rendered_code)\n\n# prints\n# def square(x):\n#    return x ** 2\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/customizing_templates/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/customizing_templates/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Model Tokens | Next: Addons</p>"},{"location":"notebooks/06_other_topics/data_catalog/","title":"Data Catalog","text":""},{"location":"notebooks/06_other_topics/data_catalog/#practicus-ai-data-catalog","title":"Practicus AI Data Catalog","text":"<p>Practicus AI provides a Data Catalog where you, or an admininstrator can save data source connection information. </p> <p>Data sources can be Data Lakes, Object Storage (E.g. S3), Data Warehouses (E.g. Snowflake), Databases (e.g. Oracle) ...</p> <p>Data catalog info does not include details like the actual SQL queries to run, S3 keys to read etc., but just the info such as host address, port (if needed), user name, password etc. You can think of them as a \"connection string\" in most programming languages. </p> <pre><code>import practicuscore as prt\n\n# Connections are saved under a Practicus AI region\nregion = prt.current_region()\n\n# You can also conenct to a remote region instead of the default one\n# region = prt.regions.get_region(..)\n</code></pre> <pre><code># Let's get connections that we have access to \n# If a connection is missing, please ask your admin to be granted access,\n# OR, create new connections using the Practicus AI App or SDK\nconnections = region.connection_list\n\nif len(connections) == 0:\n    raise ConnectionError(\n        \"You or an admin has not defined any conenctions yet. \"\n        \"This notebook will not be meanignful..\")\n</code></pre> <pre><code># Let's view our connections as a Pandas DF for convenience\nconnections.to_pandas()\n</code></pre> <pre><code># Lets view the first connection\nfirst_connection = connections[0]\nfirst_connection\n</code></pre> <pre><code># Is the data source read-only? \nif first_connection.can_write:\n    print(\"You can read from, and write to this data source.\")\nelse:\n    print(\"Data source is read-only. You cannot write to this data-source.\")\n    # Note: read-only data sources are created by Practicus AI admins \n    # and shared with users or user groups using Management Console.\n</code></pre> <pre><code># You can search a connection using it's uuid\nprint(\"Searching with connection uuid:\", first_connection.uuid)\nfound_connection = region.get_connection(first_connection.uuid)\nprint(\"Found:\", found_connection)\n</code></pre> <pre><code># You can also search using the connection name.\n# Please note that connection names can be updated later,\n#   and they are not unique in the Data Catalog.\n# Please prefer to search using a connection uuid for production deployments.\nprint(\"Searching with connection name:\", first_connection.name)\nfound_connection = region.get_connection(first_connection.name)\nprint(\"Found:\", found_connection)\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#deep-dive-into-connections","title":"Deep dive into connections","text":"<p>There are multiple ways to load data into a Practicus AI process. </p> <p>Lets' start with the simplest, just using a dictionary, and then we will discuss other options including the Data Catalog.</p>"},{"location":"notebooks/06_other_topics/data_catalog/#loading-data-without-the-data-catalog","title":"Loading data without the data catalog","text":"<p>This is the simplest option and does not use a central data catalog to store connections. If you have the database credentials, you can read from that database.</p> <pre><code># Let's get a worker to use, one that you are already working on, or a remote one.\ntry:\n    worker = region.get_local_worker()\nexcept: \n    workers = region.worker_list\n    if len(workers) == 0:\n        raise ConnectionError(\n            \"Please run this code on a Practicus AI worker, or have at least one active worker\") \n    worker = workers[0]\n</code></pre> <pre><code># Let's load using the sample SQLLite DB that comes pre-installed with Practicus AI\nsql_query = \"\"\"\n  select artists.Name, albums.Title \n  from artists, albums \n  where artists.ArtistId = albums.ArtistId \n  limit 1000\n\"\"\"\n\n# Let's configure a connection\nconn_conf_dict = {\n    \"connection_type\": \"SQLITE\",\n    \"file_path\": \"/home/ubuntu/samples/chinook.db\",\n    \"sql_query\": sql_query,\n}\n\nproc = worker.load(conn_conf_dict)\nproc.show_head()\nproc.kill()\n</code></pre> <pre><code># Connection configuration can be a json\nimport json\n\nconn_conf_json = json.dumps(conn_conf_dict)\n\nproc = worker.load(conn_conf_json)\nproc.show_head()\nproc.kill()\n</code></pre> <pre><code># Connection configuration can be path to a json file\nwith open(\"my_conn_conf.json\", \"wt\") as f:\n    f.write(conn_conf_json)\n\nproc = worker.load(\"my_conn_conf.json\")\nproc.show_head()\nproc.kill()\n\nimport os \nos.remove(\"my_conn_conf.json\")\n</code></pre> <pre><code># You can use the appropriate conn conf class, \n#   which can offer some benefits such as intellisense in Jupyter or other IDE.\n# The below will use an Oracle Connection Configuration Class\n\nfrom practicuscore.api_base import OracleConnConf, PRTValidator\n\noracle_conn_conf = OracleConnConf(\n    db_host=\"my.orcle.db.address\",\n    service_name=\"my_service\",\n    sid=\"my_sid\",\n    user=\"alice\",\n    password=\"in-wonderland\",\n    sql_query=\"select * from my_table\",\n\n    # Wrong port !!\n    db_port=100_000,\n)\n\n# We deliberately entered the wrong Oracle port. Let's validate, and fail\nfield_name, issue = PRTValidator.validate(oracle_conn_conf)\nif issue:\n    print(f\"'{field_name}' field has an issue: {issue}\")\n# Will print:\n# 'db_port' field has an issue: Port must be between 1 and 65,535\n\n# With the right Oracle db connection info, you would be able to load\n# proc = worker.load(oracle_conn_conf)\n# df = proc.get_df_copy()\n</code></pre> <pre><code># Practicus AI conn conf objects are easy to convert to a dictionary\nprint(\"Oracle conn dict:\", oracle_conn_conf.model_dump())\n</code></pre> <pre><code># Or to json\noracle_conn_conf_json = oracle_conn_conf.to_json()\nprint(\"Oracle conn json:\", oracle_conn_conf_json)\n</code></pre> <pre><code># And, vice versa. from a dict or json back to class the instance\n# This can be very conventient, e.g. save to a file, including the SQL Query, \n# and reuse later, e.g. scheduled every night in Airflow.\nreloaded_oracle_conn_conf = OracleConnConf.from_json(oracle_conn_conf_json)\ntype(reloaded_oracle_conn_conf)\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#loading-using-the-data-catalog","title":"Loading using the Data Catalog","text":"<p>When you read a connection from Practicus AI Data Catalog, you also download it's \"base\" connection configuration class.</p> <p>But instead of the database credentials like user name, password etc. you will load a \"reference\" (uuid) to the Data Catalog. </p> <p>Practicus AI workers will load data intelligently;  - if there are database credentials in the conn conf, these will be used. - Or else, the worker will sue your credentials to \"fetch\" connection credentials from the Data Catalog, and by using the reference.</p> <pre><code># Accessing the conn_conf will print the json\nconn_conf_object = first_connection.conn_conf\nconn_conf_object\n</code></pre> <p>In most cases, accesing the \"conn_conf\" of a connection that you load from the Data Catalog will just have:</p> <ul> <li>Connection type, e.g. Oracle</li> <li>And the unique reference uuid</li> </ul> <p>For relational DBs, you can just supply a SQL query and you're good to go. Practicus AI will take care of the rest.</p> <pre><code># The conn_conf is actually a child class of ConnConf\ntype(conn_conf_object)\n# e.g. OracleConnconf\n</code></pre> <pre><code># Let's make sure we use a connection type that can run a SQL statement, \n#   which will be a child class of Relational DB class RelationalConnConf. \nfrom practicuscore.api_base import RelationalConnConf\n\nif not isinstance(conn_conf_object, RelationalConnConf):\n    raise ConnectionError(\"The rest of the notebook needs a conn type that can run SQL\")\n</code></pre> <pre><code># With the below code, you can see that the conn conf class has many advanced properties\n# dir(conn_conf_object)\n# We just need to use sql_query property\nconn_conf_object.sql_query = \"Select * from Table\"\n</code></pre> <pre><code># In additon to a dict, json or json file, we can also use a conn conf object to read data\n# proc = worker.load(conn_conf_object)\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#summary","title":"Summary","text":"<p>Let's summarize some of the common options to load data.</p> <pre><code>region = prt.current_region()\n\nprint(\"My connections:\", region.connection_list)\n\npostgres_conn = region.get_connection(\"My Team/My Postgres\")\nif postgres_conn:\n    postgres_conn.sql_query = \"Select * from Table\"\n    proc = worker.load(postgres_conn)\n\nredshift_conn = region.get_connection(\"Some Department/Some Project/Some Redshift\")\nif redshift_conn:\n    conn_dict = redshift_conn.model_dump()\n    conn_dict[\"sql_query\"] = \"Select * from Table\"\n    proc = worker.load(redshift_conn)\n\nconn_with_credentials = {\n    \"connection_type\": \"SNOWFLAKE\",\n    \"db_name\": \"my.snowflake.com\",\n    # add warehouse etc. \n    \"user\": \"bob\",\n    \"password\": \"super-secret\",\n    \"sql_query\": \"Select * from Table\"\n}\n# proc = worker.load(conn_with_credentials)\n\n# And lastly, which can include all the DB credentials + SQL\n#  or a reference to the data catalog + SQL\n# proc = worker.load(\"path/to/my_conn.json\")\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/data_catalog/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/data_catalog/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Model Foundation | Next: Model Tokens</p>"},{"location":"notebooks/06_other_topics/distributed_spark/","title":"Distributed Spark","text":""},{"location":"notebooks/06_other_topics/distributed_spark/#distributed-spark-with-practicus-ai","title":"Distributed Spark with Practicus AI","text":"<p>Practicus AI can create ephemeral Spark clusters. These are disposable, instant use Spark environments with no upper limit on scalability. You can start, use and dispose a Spark cluster within seconds.</p> <p>A Spark cluster is made up of a Spark Master and one or more Spark Executors. Spark master always run on the primary Practicus AI worker that you start below. Practicus AI worker will then create Spark executors as needed (auto-scales). </p> <p>Spark Executors are not Practicus AI Workers, and therefore will have limited installation on them. For additional packages, please create new Practicus AISpark executor docker images and ask Practicus AI Worker to use these images to use for Spark executors.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>maximum_spark_executors = 1\n# initial executor count is optional since executors auto-scale\n# you can set a larger value to speed up autoscaling.\ninitial_spark_executors = 1\n\nworker_config = {\n    \"worker_size\": \"X-Small\",\n    \"worker_image\": \"practicus\",\n\n    # Assign distributed worker settings\n    \"distributed_conf_dict\": {\n        \"max_count\": maximum_spark_executors,\n        \"initial_count\": initial_spark_executors\n    },\n\n    # To enforce image pull policy to Always, make the below True\n    # Only use in dev/test where you cannot easily upgrade image versions\n    \"k8s_pull_new_image\": False,\n    # To skip SSL verification\n    # Only use in dev/test and for self signed SSL certificates\n    \"bypass_ssl_verification\": False\n}\n</code></pre> <pre><code># Let's create a Practicus AI worker and let it know we will scale on-demand\nworker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Let's read from Object stroage. \n# Please note that for distributed Spark you can only read from data sources \n# that the Spark executor also have access to.\nconnection = {\n    \"connection_type\": \"S3\",\n    \"endpoint_url\": \"http://prt-svc-sampleobj.prt-ns.svc.cluster.local\",\n    \"aws_access_key_id\": \"...\",\n    \"aws_secret_access_key\": \"...\",\n}\n\nspark = prt.engines.get_spark_session(connection)\n\ndf = spark.read.csv(\"s3a://sample/boston.csv\")\ndf.head()\n</code></pre> <pre><code># Termianting the worker will terminate Spark executors as well\nworker.terminate()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/distributed_spark/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/distributed_spark/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Security Tokens | Next: Sharing Workers</p>"},{"location":"notebooks/06_other_topics/model_foundation/","title":"Model Foundation","text":""},{"location":"notebooks/06_other_topics/model_foundation/#practicus-ai-model-foundation","title":"Practicus AI Model Foundation","text":"<p>Below you can read about a simplified version of the Practicus AI model hosting foundation. </p>"},{"location":"notebooks/06_other_topics/model_foundation/#model-prefixes","title":"Model Prefixes","text":"<p>Practicus AI models are logically grouped with the model prefix concept using the https:// [some.address.com] / [some/model/prefix] / [model-name] / [optional-version] / format. E.g. https://practicus.company.com/models/marketing/churn/v6/ can be a model API address where models/marketing is the prefix, churn is the model, and v6 is the optional version.</p>"},{"location":"notebooks/06_other_topics/model_foundation/#model-deployments","title":"Model Deployments","text":"<p>Models are physically deployed to Kubernetes, featuring characteristics such as capacity, auto-scaling, and enhanced security.</p> <p>You can use the Practicus AI App, the SDK or CLI to navigate model prefixes, deployments, models and model versions.</p> <pre><code>import practicuscore as prt\n\n# Add-ons are bundled under a Practicus AI region\nregion = prt.current_region()\n\n# You can also conenct to a remote region instead of the default one\n# region = prt.regions.get_region(..)\n</code></pre> <pre><code># Let's list the model prefixes that the admin defined \n#   AND I have access to\nregion.model_prefix_list.to_pandas()\n</code></pre> <pre><code># Let's list the model deployments that the admin defined \n#   AND I have access to\nregion.model_deployment_list.to_pandas()\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#re-creating-a-model-deployment","title":"Re-creating a model deployment","text":"<ul> <li>You can re-create (first delete, and then create) a model deployment using existing model deployment configuration.</li> <li>This can be a useful operation during dev/test, but not a very ideal operation in a production setting since all models on this model deployment will not be available while their host is being re-created.</li> <li>You must have admin privileges on the model deployment to perform this operation.</li> </ul> <pre><code>model_deployment_key = \"development-model-deployment\"\nregion.recreate_model_deployment(model_deployment_key)\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/model_foundation/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_foundation/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Workers | Next: Data Catalog</p>"},{"location":"notebooks/06_other_topics/model_tokens/","title":"Model Tokens","text":""},{"location":"notebooks/06_other_topics/model_tokens/#locating-and-getting-model-access-tokens","title":"Locating and getting model access tokens","text":"<ul> <li>In this notebook we will show how to find model prefixes, models and get short lived session tokens.</li> </ul>"},{"location":"notebooks/06_other_topics/model_tokens/#anatomy-of-a-model-url","title":"Anatomy of a model url","text":"<ul> <li>Practicus services follow this pattern:<ul> <li>[ primary service url ] / [ model prefix ] / [ model name ] / &lt; optional version &gt; /</li> </ul> </li> <li>Sample model addresses:<ul> <li>https://service.practicus.io/models/practicus/diamond-price/</li> <li>https://service.practicus.io/models/practicus/diamond-price/v3/</li> </ul> </li> <li>Please note that Practicus AI model urls always end with a \"/\" </li> </ul> <pre><code>import practicuscore as prt \n\nregion = prt.regions.get_default_region()\n</code></pre> <pre><code># Let's get model prefixes dataframe\n# We can also use the list form with: region.model_prefix_list \nmodel_prefix_df = region.model_prefix_list.to_pandas()\n\nprint(\"Current model prefixes:\")\nprint(\"Note: we will use the 'prefix' column in the API urls, and not the 'key'.\")\n\ndisplay(model_prefix_df)\n</code></pre> <pre><code>print(\"Current models:\")\nprint(\"Note: we will use the 'name' column in the API urls, and not 'model_id'\")\n\ndf = region.model_list.to_pandas()\ndisplay(df)\n</code></pre> <pre><code># You can use regular pandas filters\n# E.g. let's search for models with a particular model prefix, \n# and remove all models that are not deployed (hs no version)\n\nmodel_prefix = 'models/practicus'\nfiltered_df = df[(df['prefix'] == model_prefix) &amp; (df['versions'].notna())]\ndisplay(filtered_df)\n</code></pre> <pre><code>model = \"diamond-price\"\n\napi_url = f\"{region.url}/{model_prefix}/{model}/\"\n\nprint(\"Geting Model API session token for:\", api_url)\ntoken = prt.models.get_session_token(api_url)\n\nprint(\"Model access token with a short life:\")\nprint(token)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#getting-model-api-session-token-using-rest-api-calls","title":"Getting Model API session token using REST API calls","text":"<p>If your end users do not have access to Practicus AI SDK, they can simply make the below REST API calls with any programming language to get a Model API session token.</p> <pre><code># \"No Practicus SDK\" sample to get a session token\n\nimport requests\n\nconsole_api_url = \"http://local.practicus.io/console/api/\"\n\n# Option 1 - Use password auth every time you need tokens\nprint(\"[Not Recommended] Getting console API access token using password.\")\nemail = \"admin@admin.com\"\npassword = \"admin\"\n\ndata = {\"email\": email, \"password\": password}\nconsole_login_api_url = f\"{console_api_url}auth/\"\nr = requests.post(console_login_api_url, headers=headers, json=data)\nif not r.ok:\n    raise ConnectionError(r.status_code)\nbody = r.json()\nrefresh_token = body[\"refresh\"]  # Keep refresh tokens safe!\nconsole_access_token = body[\"access\"] \n\n# Option 2 - Get a refresh token once, and only use that until it expires in ~3 months\nprint(\"[Recommended] Getting console API access token using refresh token\")\nconsole_access_api_url = f\"{console_api_url}auth/refresh/\"\nheaders = {\"authorization\": f\"Bearer {refresh_token}\"}\ndata = {\"refresh\": refresh_token}\nr = requests.post(console_access_api_url, headers=headers, json=data)\nif not r.ok:\n    raise ConnectionError(r.status_code)\nbody = r.json()\nconsole_access_token = body[\"access\"]\nheaders = {\"authorization\": f\"Bearer {console_access_token}\"}\n\n# Console API access tokens expire in ~30 minutes\nprint(\"Console API access token:\", console_access_token)\n\n# Locating model id\nprint(\"Getting model id.\")\nprint(\"Note: you can also view model id using Open API documentation (E.g. https://../models/redoc/), or using Practicus AI App.\")\nr = requests.get(model_api_url + \"?get_meta=true\", headers=headers, data=data)\nif not r.ok:\n    raise ConnectionError(r.status_code)\nmodel_id = int(r.headers[\"x-prt-model-id\"])\nprint(\"Model id:\", model_id)\n\n# Getting model access token, expires in ~4 hours\nprint(\"Getting a model API session token using the console API access token\") \nconsole_model_token_api_url = f\"{console_api_url}modelhost/model-auth/\"\ndata = {\"model_id\": model_id}\nr = requests.get(console_model_token_api_url, headers=headers, data=data)\nif not r.ok:\n    raise ConnectionError(r.status_code)\nbody = r.json()\nmodel_api_token = body[\"token\"]\nprint(\"Model API session token:\", model_api_token) \n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/model_tokens/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/model_tokens/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Data Catalog | Next: Customizing Templates</p>"},{"location":"notebooks/06_other_topics/open_notebook/","title":"Open Notebook","text":""},{"location":"notebooks/06_other_topics/open_notebook/#practicus-ai-worker-notebooks","title":"Practicus AI Worker Notebooks","text":"<p>You can start a Practicus AI worker and open Jupyter Notebook using the App, SDK or CLI.</p> <pre><code>import practicuscore as prt \n\nworker_config = {\n    \"worker_size\": \"X-Small\",\n    \"worker_image\": \"practicus\",\n}\nworker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Open the Jupyter notebook on a new browser tab\nprint(f\"Opening jupyter notebook of {worker.name}\")\nnotebook_login_url = worker.open_notebook()\n</code></pre> <pre><code># You can change the UI theme color to light\n# Note: this will only work the first time you open the notebook on a given worker\nnotebook_login_url = worker.open_notebook(dark_mode=False)\n</code></pre> <pre><code># You can get the login url and skip opening the notebook in a new browser tab\nnotebook_login_url = worker.open_notebook(get_url_only=True)\nprint(notebook_login_url)\n</code></pre> <pre><code>worker.terminate()\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/open_notebook/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_notebook/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Open Vscode | Next: Workers</p>"},{"location":"notebooks/06_other_topics/open_vscode/","title":"Open Vscode","text":""},{"location":"notebooks/06_other_topics/open_vscode/#practicus-ai-worker-visual-studio-code","title":"Practicus AI Worker Visual Studio Code","text":"<p>You can start a Practicus AI worker and open VS Code using the App, SDK or CLI.</p> <pre><code>import practicuscore as prt \n\nworker_config = {\n    \"worker_size\": \"X-Small\",\n    \"worker_image\": \"practicus\",\n}\nworker = prt.create_worker(worker_config)\n</code></pre> <pre><code># Open VS Code on a new browser tab\nprint(f\"Opening VS Code of {worker.name}\")\nvscode_login_url, vscode_password = worker.open_vscode()\nprint(\"Password:\", vscode_password)\n</code></pre> <pre><code># You can change the UI theme color to light\n# Note: this will only work the first time you open VS Code on a given worker\nvscode_login_url, vscode_password = worker.open_vscode(dark_mode=False)\nprint(\"Password:\", vscode_password)\n</code></pre> <pre><code># You can get the login url and skip opening VS Code in a new browser tab\nvscode_login_url, vscode_password = worker.open_vscode(get_url_only=True)\nprint(vscode_login_url, \"Password:\", vscode_password)\n</code></pre> <pre><code>worker.terminate()\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/open_vscode/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/open_vscode/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Workspaces | Next: Open Notebook</p>"},{"location":"notebooks/06_other_topics/processes/","title":"Processes","text":""},{"location":"notebooks/06_other_topics/processes/#practicus-ai-processes","title":"Practicus AI Processes","text":"<p>Practicus AI Processes live in Practicus AI Workers and you can create, work with and kill them as the below.</p> <pre><code>import practicuscore as prt\n</code></pre> <pre><code>worker = prt.get_local_worker()\n</code></pre> <pre><code># We can also create a remote worker by running the below, either from your computer,\n# or from an existing Practicus AI worker.\nworker_config = {\n    \"worker_size\": \"Small\",\n    \"worker_image\": \"practicus\",\n}\n# worker = prt.create_worker(worker_config) \n</code></pre> <pre><code># Processes start by loading some data.  \nworker_file_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/ice_cream.csv\"\n}\n\nproc_1 = worker.load(worker_file_conn)\nproc_1.show_head()\n</code></pre> <pre><code># Let's create another process\nsqlite_conn = {\n    \"connection_type\": \"SQLITE\",\n    \"file_path\": \"/home/ubuntu/samples/chinook.db\",\n    \"sql_query\": \"select * from artists\"\n}\n\nproc_2 = worker.load(sqlite_conn)\nproc_2.show_head()\n</code></pre> <pre><code># Let's iterate over processes\nfor proc in worker.proc_list:\n    print(f\"Process id: {proc.proc_id} connection: {proc.conn_info}\")\n</code></pre> <pre><code># Converting the proc_list into string will give you a csv \nprint(worker.proc_list)\n</code></pre> <pre><code># You can also get current processes as a pandas DataFrame for convenience\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># Simply accessing the proc_list will also print a formatted table string \nworker.proc_list\n</code></pre> <pre><code>first_proc = worker.proc_list[0]\n# Accessing a process object will print it's details\nfirst_proc\n</code></pre> <pre><code># You can ask a process to kill itself and free resources\nproc_1.kill()\n</code></pre> <pre><code># It's parent worker will be updated\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># You can also ask a worker to kill one of it's processes\nremaining_proc_id = worker.proc_list[0].proc_id\nworker.kill_proc(remaining_proc_id)\n</code></pre> <pre><code># Will return empty\nworker.proc_list.to_pandas()\n</code></pre> <pre><code># You can also ask a worker to kill all its processes to free up resources faster\nworker.kill_all_procs()\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/processes/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/processes/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Advanced Gpu | Next: Security Tokens</p>"},{"location":"notebooks/06_other_topics/security_tokens/","title":"Security Tokens","text":""},{"location":"notebooks/06_other_topics/security_tokens/#access-and-refresh-jwt-tokens","title":"Access and Refresh JWT tokens","text":"<ul> <li>In this example we will show how to login to a Practicus AI region and get access and or refresh tokens.</li> <li>Access tokens are short lived, refresh tokens are long.</li> <li>Refresh tokens allow you the ability to store your login credentials without actually storing your password</li> <li>JWT tokens are human readable, you can visit jwt.io and view what is inside the token.<ul> <li>Is this secure? Yes, jwt.io does not store tokens and decryption happens with javascript on your browser.</li> <li>Who can create JWT tokens? Practicus AI tokens are asymmetric, one can read what is inside a token but cannot create a new one without the secret key. Only your system admin has access to secrets.</li> <li>Can I use a token created for one Practicus AI region for another region? By default, no. If your admin deployed the regions in \"federated\" mode, yes.</li> </ul> </li> </ul> <pre><code>import practicuscore as prt \nimport getpass\n</code></pre> <pre><code># Method 1) If you are already logged in, or if you are running this code on a Practicus AI Worker.\nregion = prt.regions.get_default_region()\n\n# Get tokens for the current region.\nrefresh_token, access_token = region.get_refresh_and_access_token()\n\n# Will print long strings like eyJhbG...\nprint(\"Refresh token:\", refresh_token)\nprint(\"Access token:\", access_token)\n</code></pre> <pre><code># Method 2) You are logging in using the SDK on your laptop,\n#   Or, you are running this code on a worker in a region, but logging in to another region.\n\n# Optionally, you can log-out first\n# prt.auth.logout(all_regions=True)\n\npracticus_url = \"https://practicus.your-company.com\"\n# Tip: region.url shows the current Practicus AI service URL that you are logged-in to.\n\nemail = \"your-email@your-company.com\"\n\nprint(f\"Please enter the password for {email} to login {practicus_url}\")\npassword = getpass.getpass()\n\nsome_practicus_region = prt.auth.login(\n    url=practicus_url,\n    email=email,\n    password=password,\n    # Optional parameters:\n    # Instead of using a password, you can login using a refresh token or access token\n    #   refresh_token = ... will keep logged in for many days\n    #   access_token = ... will keep logged in for some minutes\n    # By default, your login token is stored for future use under ~/.practicus/core.conf, to disable:\n    #   save_config = False\n    # By default, your password is not saved under ~/.practicus/core.conf, to enable:\n    #   save_password = True\n)\n\n# Now you can get as many refresh/access tokens as you need.\nrefresh_token, access_token = some_practicus_region.get_refresh_and_access_token()\n\nprint(\"Refresh token:\", refresh_token)\nprint(\"Access token:\", access_token)\n</code></pre> <pre><code># If you just need an access token.\naccess_token = region.get_access_token()\n\nprint(\"Access token:\", access_token)\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/security_tokens/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/security_tokens/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Processes | Next: Distributed Spark</p>"},{"location":"notebooks/06_other_topics/sharing_workers/","title":"Sharing Workers","text":"<p>To allow another user to access a Practicus AI worker you\u2019ve created, you can run the code below and then share the connection details. This provides a quick way to collaborate on the same environment, including data and code.</p> <p>Important Note: Any user you share a worker with will have access to the contents of your <code>~/my</code> and <code>~/shared</code> directories.</p> <pre><code>import practicuscore as prt \n\n# To share the worker you are currently using,\n#  first get a reference to 'self'\nworker = prt.get_local_worker()\n\n# To start and share a worker, create a worker as usual, e.g.\n# worker = prt.create_worker(worker_config)\n\n# The rest of the code will be the same\n</code></pre> <pre><code># To share using Jupyter Lab\nurl = worker.open_notebook(get_url_only=True)\n\nprint(\"Jupyter Lab login url:\", url)\n</code></pre> <pre><code># To share using VS Code\nurl, token = worker.open_vscode(get_url_only=True)\n\nprint(\"VS Code login url:\", url)\nprint(\"VS Code token    :\", token)\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/sharing_workers/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/sharing_workers/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Distributed Spark | Next: Workspaces</p>"},{"location":"notebooks/06_other_topics/workers/","title":"Workers","text":""},{"location":"notebooks/06_other_topics/workers/#practicus-ai-workers","title":"Practicus AI Workers","text":"<p>Practicus AI Workers live in a Practicus AI region and you can create, work with and terminate them as the below.</p>"},{"location":"notebooks/06_other_topics/workers/#simple-use-of-workers","title":"Simple use of workers","text":"<pre><code>import practicuscore as prt\n\n# Workers are created under a Practicus AI region\nregion = prt.current_region()\n\n# You can also connect to a remote region instead of the default one\n# region = prt.get_region(..)\n</code></pre> <pre><code># Let's view our region\nregion\n</code></pre> <pre><code># The below is the easiest way to start working with a worker \nworker = region.get_or_create_worker()\n</code></pre> <pre><code># Let's view worker details\nworker\n</code></pre> <pre><code># Let's load a sample file on the Worker\nsome_data_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/ice_cream.csv\"\n}\nwith worker.load(some_data_conn) as proc:\n    df = proc.get_df_copy()\n    # Exiting the 'with' will free up the process\n</code></pre> <pre><code># The rest is business as usual..\ndf\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#behind-the-scenes","title":"Behind the scenes..","text":"<pre><code># Let's break down what the below code actually does\n# worker = region.get_or_create_worker()\n\nif prt.running_on_a_worker():\n    print(\"Since I am already running on a worker, let's use this one!\")\n    worker = prt.get_local_worker()\nelse:\n    print(\"I am running on my laptop, let's get a worker.\")\n    print(\"Do I already have running workers?\")\n    if len(region.worker_list) &gt; 0:\n        print(\"Yes, I do have workers, let's use one of them.\")\n        worker = region.worker_list[0]\n        # The real code is actually a bit smarter,\n        # it will give the worker with the max memory\n    else:\n        print(\"I don't have a worker. Let's create a new one!\")\n        worker = prt.create_worker()\n\nprint(\"My worker details:\")\nworker\n</code></pre> <pre><code># Always terminate the workers once you are done\n\nif prt.running_on_a_worker():\n    print(\"The worker is terminating itself..\")\n    print(\"Please make sure you are not losing any active workon this worker.\")\n\nworker.terminate()\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#customizing-workers","title":"Customizing workers","text":"<pre><code># We can choose the size, container image and also configure other parameters\nworker_config = {\n    \"worker_size\": \"X-Small\",\n    \"worker_image\": \"practicus\",\n    # # Assign distributed worker settings\n    # \"distributed_conf_dict\": {\n    #     \"max_count\": 0,\n    #     \"initial_count\": 0,\n    #     \"memory_mb\": 1024\n    # },\n    # # To enforce image pull policy to Always, make the below True\n    # # Only use in dev/test where you cannot easily upgrade image versions\n    # \"k8s_pull_new_image\": False,\n    # # To skip SSL verification\n    # # Only use in dev/test and for self signed SSL certificates\n    # \"bypass_ssl_verification\": False\n}\nworker = region.create_worker(worker_config)\n</code></pre> <pre><code># If you do not know which worker sizes you have access to:\nregion.worker_size_list.to_pandas()\n</code></pre> <pre><code>\n</code></pre> <pre><code># If you do not know which worker container images you have access to:\nregion.worker_image_list.to_pandas()\n</code></pre> <pre><code># Let's iterate over workers\nfor worker in region.worker_list:\n    print(\"Worker details:\", worker)\n</code></pre> <pre><code># We can also realod workers.\n# This is useful if you create workers from multiple systems\nregion.reload_worker_list()\n</code></pre> <pre><code># Converting the worker_list into string will give you a csv \nprint(region.worker_list)\n</code></pre> <pre><code># You can also get workers as a pandas DataFrame for convenience\nregion.worker_list.to_pandas()\n</code></pre> <pre><code># Simply accessing the worker_list will print a formatted table string \nregion.worker_list\n</code></pre> <pre><code>first_worker = region.worker_list[0]\n# Accessing an worker object will print it's details\nfirst_worker\n</code></pre> <pre><code>worker_name = first_worker.name\nprint(\"Searching for:\", worker_name)\n# You can locate a worker using it's name\nworker = region.get_worker(worker_name)\nprint(\"Located:\", worker)\n# Or using worker index only, e.g. 123 for Worker-123\n# region.get_worker(123)\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#worker-logs","title":"Worker Logs","text":"<pre><code># You can view worker logs like the below \nworker.view_logs()\n\n# To get logs as a string \nworker_logs = worker.get_logs()\n\n# E.g. to search for an error string\nprint(\"some error\" in worker_logs)\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#cleaning-up","title":"Cleaning up","text":"<pre><code># There are multiple ways to terminate a worker.\nworker.terminate()\n# Or, \n# region.terminate_worker(worker_name)\n# Worker number works too\n# region.terminate_worker(123)\n</code></pre> <pre><code># The below will terminate all of your workers\nregion.terminate_all_workers()\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/workers/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/workers/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Open Notebook | Next: Model Foundation</p>"},{"location":"notebooks/06_other_topics/workspaces/","title":"Workspaces","text":""},{"location":"notebooks/06_other_topics/workspaces/#practicus-ai-workspaces","title":"Practicus AI Workspaces","text":"<p>Practicus AI Workspaces is a web based remote desktop environment with Practicus AI Studio and many other tools pre-installed and pre-configured. </p> <p></p> <p>They live in a Practicus AI region and you can create, work with and terminate them with he below commands.</p>"},{"location":"notebooks/06_other_topics/workspaces/#simple-use-of-workspaces","title":"Simple use of Workspaces","text":"<pre><code>import practicuscore as prt\n\n# Workers are created under a Practicus AI region\nregion = prt.current_region()\n\n# You can also connect to a remote region instead of the default one\n# region = prt.get_region(..)\n</code></pre> <pre><code># Let's view our region\nregion\n</code></pre> <pre><code># The below is the easiest way to start working with a worker \nworkspace = region.create_workspace()\n</code></pre> <pre><code># Let's view workspace details\nworkspace\n</code></pre> <pre><code># A Workspace is actually a \"worker\" too, so you can list all workers and workspaces with the same command\n# Please note the \"service_type\" column\nregion.worker_list.to_pandas()\n\n# To use an existing wokspace\n# workspace = region.worker_list[0]\n</code></pre> <pre><code># Lets get the login credentials for our newly created workspace\nusername, token = workspace.get_workspace_credentials()\n\nprint(\"Opening Workspace in your browser\")\nprint(f\"Pelase login with username: {username} and password: {token}\")\n\nlogin_url = workspace.open_workspace()\n\n# You can only get the URL wihtout opening in your browser with get_url_only=True\n# login_url = workspace.open_workspace(get_url_only=True)\n</code></pre> <pre><code># You can terminate a workspace using the below command\nworkspace.terminate()\n</code></pre> <pre><code># If you terminate all of your workers in a region, you will also terminate all your workspaces\n# region.terminate_all_workers()\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#shared-folders","title":"Shared folders","text":"<p>Ideally, an admin will define \"my\" and \"shared\" folders for your Workspaces.</p> <p>Your \"my\" folder will be shared between all of your workspaces, and the shared folders between select users and their workspaces allowing better collaboration.</p>"},{"location":"notebooks/06_other_topics/workspaces/#sharing-folders-between-workers-and-workspaces","title":"Sharing folders between Workers and Workspaces","text":"<p>An admin can also define shared folders that are accessible by both Workspaces and Workers. In this case, please be careful about potential write permission issues.</p> <p>E.g. You can write a file from Workspace (practicus user with id 911) and you won't be able to overwrite that file from a Worker (ubuntu user with id 1000) without changing permissions first. To resolve this unique issue:</p> <p>On a Workspace, to change ownership of a file you can run: <pre><code>sudo chown practicus:practicus some_file.txt\n</code></pre></p> <p>On a Worker, to change ownership of a file you can run: <pre><code>sudo chown ubuntu:ubuntu some_file.txt\n</code></pre></p>"},{"location":"notebooks/06_other_topics/workspaces/#advanced-usage","title":"Advanced Usage","text":"<pre><code># You can provide additional parameters for workspace creation\nimport json \nimport base64 \n\nadditional_params = {\n    # If you don't select a timezone, UTC (Greenwich Mean Time) default will be used\n    \"timezone\": \"America/Los_Angeles\",\n    # You can also select custom password. This is ** not recommended ** due to weak security.\n    \"password\": \"super_secret\",\n}\n\n# Additional params need to be base64 encoded\nadditional_params_str = json.dumps(additional_params)\nadditional_params_b64 = str(base64.b64encode(bytes(additional_params_str, encoding=\"utf-8\")), \"utf-8\")\n</code></pre> <pre><code># Create a new workspace with additional_params\nworkspace = region.create_workspace(\n    worker_config = {\n        \"additional_params\": additional_params_b64\n    }\n)\n</code></pre> <pre><code>username, token = workspace.get_workspace_credentials()\n\nprint(\"Opening Workspace in your browser\")\nprint(f\"Pelase login with username: {username} and password: {token}\")\n\nlogin_url = workspace.open_workspace()\n\n# You should now see that the computer clock is using US Pacific time zone \n</code></pre> <pre><code>workspace.terminate()\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/workspaces/#01_code_qualitybad_codepy","title":"01_code_quality/bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#07_sdk_preprocessingsnippetsimpute_missing_knnpy","title":"07_sdk_preprocessing/snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#07_sdk_preprocessingsnippetsnormalizepy","title":"07_sdk_preprocessing/snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#07_sdk_preprocessingsnippetssuppress_outlierspy","title":"07_sdk_preprocessing/snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#08_additional_modelingbank_marketingsnippetslabel_encoderpy","title":"08_additional_modeling/bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#08_additional_modelingbank_marketingsnippetsone_hotpy","title":"08_additional_modeling/bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#08_additional_modelingmodel_trackingmodel_driftsmodelpy","title":"08_additional_modeling/model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#08_additional_modelingsparkmlmodeljson","title":"08_additional_modeling/sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#08_additional_modelingsparkmlmodelpy","title":"08_additional_modeling/sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#08_additional_modelingstreamlined_model_deploymentmodelpy","title":"08_additional_modeling/streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#08_additional_modelingxgboostmodelpy","title":"08_additional_modeling/xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/workspaces/#08_additional_modelingxgboostmodel_custom_dfpy","title":"08_additional_modeling/xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Sharing Workers | Next: Open Vscode</p>"},{"location":"notebooks/06_other_topics/01_code_quality/code_quality/","title":"Automated Code Quality","text":"<p>You can check for code quality issues, fix and format your files automatically.</p> <pre><code>import practicuscore as prt\n\n# Let's check for code quality issues in this folder\nsuccess = prt.quality.check()\n</code></pre> <pre><code># Some issues like 'unused imports' can be fixed automatically\nif not success:\n    prt.quality.check(fix=True)\n</code></pre> <pre><code># Let's also format code to improve quality and readability\nprt.quality.format()\n</code></pre> <pre><code># Still errors? open bad_code.py and delete the wrong code\n# Final check, this should pass\nprt.quality.check()\n</code></pre> <pre><code># We can add a \"No QA\" tag to ignore checking a certain type of issue\n# E.g. to ignore an unused imports for a line of code\nimport pandas  # noqa: F401\n# To ignore all QA checks (not recommended)\nimport numpy  # noqa\n</code></pre> <pre><code>\n</code></pre>"},{"location":"notebooks/06_other_topics/01_code_quality/code_quality/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/01_code_quality/code_quality/#bad_codepy","title":"bad_code.py","text":"<pre><code>import pandas \n\n# this is an error\nprint(undefined_var)\n\n\n\n\nprint(\"Too many blank lines, which is code formatting issue.\")\n</code></pre> <p>Previous: Use Cluster | Next: Sample Notebook</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/","title":"Run Sample Notebook","text":""},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#automated-tests-using-notebooks","title":"Automated tests using notebooks","text":"<p>Practicus AI allows you to execute notebooks in autoamted fashion, which can be used for various purposes including testing.</p> <pre><code>import practicuscore as prt\n</code></pre> <p>```python editable=true slideshow={\"slide_type\": \"\"}</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#this-will-run-just-fine-and-save-the-resulting-output","title":"This will run just fine, and save the resulting output","text":"<p>prt.notebooks.execute_notebook(\"sample_notebook\") <pre><code>```python editable=true slideshow={\"slide_type\": \"\"}\n# This will FAIL since some_param cannot be 0\nprt.notebooks.execute_notebook(\n    \"sample_notebook\",\n    parameters={\n        \"some_param\": 0 \n    }\n)\n</code></pre></p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#advanced-features","title":"Advanced features","text":"<p>```python editable=true slideshow={\"slide_type\": \"\"} tags=[\"parameters\"]</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#advanced-notebook-automation-parameters","title":"Advanced Notebook automation parameters","text":"<p>default_output_folder=\"~/tests\"  # If none, writes notebook output to same folder as notebook default_failed_output_folder=\"~/tests_failed\"  # If not none, collects failed notebook results  <pre><code>```python editable=true slideshow={\"slide_type\": \"\"}\n# By calling configure you can save notebook results to central location\n# Please note that you can do this to a shared/ folder daily where all of our members have access to\nprt.notebooks.configure(\n    default_output_folder=default_output_folder,\n    default_failed_output_folder=default_failed_output_folder,\n    add_time_stamp_to_output=True,\n)\n</code></pre></p> <p>```python editable=true slideshow={\"slide_type\": \"\"}</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#this-will-work","title":"This will work","text":"<p>prt.notebooks.execute_notebook(\"sample_notebook\") <pre><code>```python editable=true slideshow={\"slide_type\": \"\"}\n# This will fail but does not stop the execution of the notebook\nprt.notebooks.execute_notebook(\n    \"sample_notebook\",\n    parameters={\n        \"some_param\": 0 \n    }\n)\n</code></pre></p> <p>```python editable=true slideshow={\"slide_type\": \"\"}</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#calling-validate_history-will-raise-an-exception-if-any-of-the-previous-notebooks-failed","title":"Calling validate_history() will raise an exception IF any of the previous notebooks failed","text":""},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#this-is-useful-to-have-a-primary-orchestration-notebook-that-executes-other-child-notebooks","title":"This is useful to have a primary \"orchestration\" notebook that executes other child notebooks,","text":""},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#and-then-finally-fails-itself-if-there-was-a-mistake","title":"And then finally fails itself if there was a mistake.","text":""},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#you-can-then-report-the-result-essentially-creating-a-final-report","title":"You can then report the result, essentially creating a final report.","text":"<p>prt.notebooks.validate_history()</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#you-can-view-the-passed-and-failed-execution-results-in","title":"You can view the passed and failed execution results in","text":""},{"location":"notebooks/06_other_topics/02_automated_notebooks/run_sample_notebook/#tests-and-tests_failed-with-a-time-stamp-optional","title":"~/tests and ~/tests_failed with a time stamp (optional)","text":"<pre><code> ### Executing notebooks from terminal\n\nYou can run the below command to execute a notebook from the terminal or an .sh script.\n\n```shell\nprtcli execute-notebook -p notebook=my-notebook.ipynb \n</code></pre> <p>Previous: Sample Notebook | Next: Test Core Features</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/sample_notebook/","title":"Sample Notebook","text":""},{"location":"notebooks/06_other_topics/02_automated_notebooks/sample_notebook/#sample-notebook","title":"Sample Notebook","text":"<p>This is a sample notebook that you will be executing from another notebook, passing dynamic parameters.</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/sample_notebook/#defining-parameters","title":"Defining parameters","text":"<ul> <li>For parameters to work, you must add a tag named \"parameters\" to any cell, and then define your variables in it.</li> <li>To add a tag, please do the below</li> </ul>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/sample_notebook/#jupyter-lab","title":"Jupyter Lab","text":"<p>Select a cell, move to the upper right on section of Jupyter Lab and click property inspector, and then add a cell tag named \"parameters\"</p> <p></p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/sample_notebook/#visual-studio-code","title":"Visual Studio Code","text":"<p>Select a cell, click the ... upper right of the cell and add a cell tag named \"parameters\"</p> <p></p> <p>```python editable=true slideshow={\"slide_type\": \"\"}</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/sample_notebook/#notebook-parameters-these-can-change-later","title":"Notebook parameters - these can change later","text":"<p>some_param = 1 some_other_param = 2 <pre><code>```python editable=true slideshow={\"slide_type\": \"\"}\nprint(\"Starting to run sample notebook\")\n</code></pre></p> <p>```python editable=true slideshow={\"slide_type\": \"\"}</p>"},{"location":"notebooks/06_other_topics/02_automated_notebooks/sample_notebook/#this-will-work-now-but-will-fail-later","title":"This will work now, but will fail later.","text":"<p>if some_param &lt;= 0:     raise ValueError(\"some_param must be &gt; 0\") <pre><code>```python editable=true slideshow={\"slide_type\": \"\"}\nprint(\"Finished running sample notebook\")\n</code></pre></p> <p>Previous: Code Quality | Next: Run Sample Notebook</p>"},{"location":"notebooks/06_other_topics/03_automated_tests/test_core_features/","title":"Test Core Features","text":""},{"location":"notebooks/06_other_topics/03_automated_tests/test_core_features/#testing-core-features","title":"Testing Core Features","text":"<p>This notebook utilizes other notebooks on the worker to demonstrate their usage, and allows automated testing.</p> <p>```python editable=true slideshow={\"slide_type\": \"\"} tags=[\"parameters\"]</p>"},{"location":"notebooks/06_other_topics/03_automated_tests/test_core_features/#notebook-parameters","title":"Notebook parameters","text":"<p>some_param = None</p>"},{"location":"notebooks/06_other_topics/03_automated_tests/test_core_features/#where-to-write-test-results","title":"Where to write test results","text":"<p>default_output_folder=\"~/tests\" default_failed_output_folder=\"~/tests_failed\" <pre><code>```python editable=true slideshow={\"slide_type\": \"\"}\n# Validate key parameters are set\n# e.g. assert some_param\n</code></pre></p> <p>```python editable=true slideshow={\"slide_type\": \"\"} import practicuscore as prt</p> <p>prt.notebooks.configure(     default_output_folder=default_output_folder,     default_failed_output_folder=default_failed_output_folder, ) <pre><code>```python editable=true slideshow={\"slide_type\": \"\"}\nprt.notebooks.execute_notebook(\n    \"~/samples/notebooks/01_getting_started/01_insurance.ipynb\",\n)\n</code></pre></p> <p>```python editable=true slideshow={\"slide_type\": \"\"}</p>"},{"location":"notebooks/06_other_topics/03_automated_tests/test_core_features/#add-other-notebook-tests-here","title":"Add other notebook tests here","text":"<pre><code>```python\n# Fail this notebook by raising an Exception\n#   if ANY of the prior notebook executions failed.\nprt.notebooks.validate_history()\n</code></pre> <p>Previous: Run Sample Notebook | Next: Git Integration</p>"},{"location":"notebooks/06_other_topics/04_setting_up_git/git_integration/","title":"Git Integration","text":""},{"location":"notebooks/06_other_topics/04_setting_up_git/git_integration/#git-source-control-integration","title":"Git source control integration","text":"<p>You can use git commands from the terminal, or with the native Git extension from the Jupyter notebook.</p>"},{"location":"notebooks/06_other_topics/04_setting_up_git/git_integration/#setting-up-using-terminal-recommended","title":"Setting up using terminal (recommended)","text":"<ul> <li>Using the main menu open a terminal. File &gt; New &gt; Terminal</li> <li>Run the below commands</li> </ul> <pre><code># 1) Navigate to the directory to clone \nmkdir ~/projects\ncd ~/projects\n\n# 2) Clone a git repo \ngit clone https://github.com/username/repository.git\n\n# 3) Enter your username and password\n# Note: Many git systems such as GitHub only allow \"personal access tokens\" as password\n# Please check below this notebook to learn how to create a personal access token\n\n# 4) Add your email and username information. \n# This will prevent entering it each time you want to commit.\ngit config --global user.email \"alice@wonderland.com\"\ngit config --global user.name \"Alice\"\n\n# 5) (Optional) Save your credentials\ngit config --global credential.helper cache\n\n# If you already cloned a git repo and need to save credentials again\ncd ~/projects/name_of_the_repository\ngit pull\n# Enter credentials, and repeat after step 4)\n</code></pre> <p>After cloning a git repo and saving credentials, you can use both the terminal and the git UI elements inside Jupyter Notebook</p>"},{"location":"notebooks/06_other_topics/04_setting_up_git/git_integration/#setting-up-using-the-jupyter-notebook","title":"Setting up using the Jupyter Notebook","text":"<ul> <li>You can open the Git section and click clone a repository.</li> <li>Please note that with this method you might have to enter your password each time you want sync with your git repo.</li> </ul>"},{"location":"notebooks/06_other_topics/04_setting_up_git/git_integration/#creating-git-personal-access-tokens","title":"Creating Git Personal Access Tokens","text":"<ul> <li>Github<ul> <li>https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens</li> </ul> </li> <li>Bitbucket<ul> <li>https://support.atlassian.com/bitbucket-cloud/docs/create-a-repository-access-token/</li> </ul> </li> <li>GitLab<ul> <li>https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html</li> </ul> </li> </ul> <p>Previous: Test Core Features | Next: Modern Python</p>"},{"location":"notebooks/06_other_topics/04_virtual_environments/01_modern_python/","title":"Modern Python","text":""},{"location":"notebooks/06_other_topics/04_virtual_environments/01_modern_python/#creating-new-virtual-environments","title":"Creating new virtual environments","text":"<p>Practicus AI workers allow you to create new Python Virtual environments using the python default venv module. Pelase follow the below steps to create and use new virtual environments.</p>"},{"location":"notebooks/06_other_topics/04_virtual_environments/01_modern_python/#option-1-re-use-base-python-packages","title":"Option 1) Re-use base python packages","text":"<ul> <li>With this option you can save disk space with fewer package installations </li> <li>If you need to change a python package version of base image you can simply pip install the different version.</li> </ul> <pre><code># Create new venv \npython3 -m venv $HOME/.venv/new_venv --system-site-packages --symlinks\n# Activate\nsource $HOME/.venv/new_venv/bin/activate\n# Add to Jupyter \npython3 -m ipykernel install --user --name new_venv --display-name \"My new Python\"\n# Install packages, these will 'overide' parent python package versions\npython3 -m pip install some_package\n</code></pre>"},{"location":"notebooks/06_other_topics/04_virtual_environments/01_modern_python/#option-2-fresh-install","title":"Option 2) Fresh install","text":"<ul> <li>With the fresh install you will have to install all packages, including Practicus AI</li> </ul> <pre><code># Create new venv \npython3 -m venv $HOME/.venv/new_venv\n# Activate\nsource $HOME/.venv/new_venv/bin/activate\n# Install Jupyter Kernel\npython3 -m pip install ipykernel\n# Add to Jupyter \npython3 -m ipykernel install --user --name new_venv --display-name \"My new Python\"\n# Install packages\npython3 -m pip install practicuscore\n</code></pre>"},{"location":"notebooks/06_other_topics/04_virtual_environments/01_modern_python/#creating-new-notebooks","title":"Creating new Notebooks","text":"<p>On Jupyter click File &gt; New &gt; Notebook. And your new virtual environment should show up.</p> <p>Previous: Git Integration | Next: Legacy Python</p>"},{"location":"notebooks/06_other_topics/04_virtual_environments/02_legacy_python/","title":"Legacy Python","text":""},{"location":"notebooks/06_other_topics/04_virtual_environments/02_legacy_python/#creating-new-virtual-environments-for-older-python-versions","title":"Creating new virtual environments for older Python versions","text":"<p>Practicus AI workers allow you to create new Python Virtual environments using the python default venv module. If you would liek to use a different Python version, please follow the below steps.</p>"},{"location":"notebooks/06_other_topics/04_virtual_environments/02_legacy_python/#important-note","title":"Important note","text":"<p>Please note that Prtacticus AI drops support for Python versions that are currently not supported by the Python ecosystem. For this scenartios, you can use Practicus AI workers and notebooks, but not Practicus AI SDK. </p>"},{"location":"notebooks/06_other_topics/04_virtual_environments/02_legacy_python/#install-and-initialize-conda","title":"Install and initialize conda","text":"<ul> <li>Open Practicus AI juyter notebook, then navigate to: File &gt; New &gt; Terminal</li> <li>Run the below <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\nbash miniconda.sh -b -p $HOME/miniconda3\nrm miniconda.sh\n\nexport PATH=$HOME/miniconda3/bin:$PATH\nconda init\nconda config --set auto_activate_base false\nsource ~/.bashrc\n</code></pre></li> </ul>"},{"location":"notebooks/06_other_topics/04_virtual_environments/02_legacy_python/#install-a-specific-python-version","title":"Install a specific Python version","text":"<ul> <li>E.g. Python 3.7 <pre><code>conda create -n py37 python=3.7 -y\nconda activate py37\n</code></pre></li> </ul>"},{"location":"notebooks/06_other_topics/04_virtual_environments/02_legacy_python/#add-legacy-python-to-jupyter","title":"Add legacy Python to Jupyter","text":"<pre><code>python3 -m pip install ipykernel\npython3 -m ipykernel install --user --name=py37 --display-name \"My Legacy Python\"\n</code></pre>"},{"location":"notebooks/06_other_topics/04_virtual_environments/02_legacy_python/#install-packages","title":"Install packages","text":"<pre><code># Pelase note that you might end up installing an old practicuscore version if your Python version is too old\npython3 -m pip install practicuscore\nconda deactivate\n</code></pre>"},{"location":"notebooks/06_other_topics/04_virtual_environments/02_legacy_python/#creating-new-notebooks","title":"Creating new Notebooks","text":"<ul> <li>On Jupyter, click File &gt; New &gt; Notebook.</li> <li>And your new virtual environment should show up as an option.</li> </ul> <p>Previous: Modern Python | Next: Dynamic Size Color</p>"},{"location":"notebooks/06_other_topics/05_data_exploration/eda/","title":"Eda","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.style as plt_styl\n\nimport warnings\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 30)\npd.set_option('display.width', 150)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\nwarnings.simplefilter(action = \"ignore\")\n</code></pre> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n\nproc = worker.load(data_set_conn, engine='AUTO') \n\ndf = proc.get_df_copy()\ndisplay(df)\n</code></pre> <pre><code>df.info()\n</code></pre> <pre><code>def grab_col_names(dataframe, cat_th=10, car_th=25, show_date=False):\n    date_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"datetime64[ns]\"]\n    cat_cols = dataframe.select_dtypes([\"object\", \"category\"]).columns.tolist()\n    num_but_cat = [col for col in dataframe.select_dtypes([\"float\", \"integer\"]).columns if dataframe[col].nunique() &lt; cat_th]\n    cat_but_car = [col for col in dataframe.select_dtypes([\"object\", \"category\"]).columns if dataframe[col].nunique() &gt; car_th]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    num_cols = dataframe.select_dtypes([\"float\", \"integer\"]).columns\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'date_cols: {len(date_cols)}')\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n\n    if show_date == True:\n        return date_cols, cat_cols, cat_but_car, num_cols, num_but_cat\n    else:\n        return cat_cols, cat_but_car, num_cols, num_but_cat\n</code></pre> <pre><code>grab_col_names(df)\n</code></pre> <pre><code>cat_cols, cat_but_car, num_cols, num_but_cat = grab_col_names(df)\n</code></pre> <pre><code>df.head()\n</code></pre> <pre><code>df[(df[\"region\"]==3)]\n</code></pre> <pre><code>print(cat_cols)\n</code></pre> <pre><code>def cat_analyzer(dataframe, variable, target = None):\n    print(variable)\n    if target == None:\n        print(pd.DataFrame({\n            \"COUNT\": dataframe[variable].value_counts(),\n            \"RATIO\": dataframe[variable].value_counts() / len(dataframe)}), end=\"\\n\\n\\n\")\n    else:\n        temp = dataframe[dataframe[target].isnull() == False]\n        print(pd.DataFrame({\n            \"COUNT\":dataframe[variable].value_counts(),\n            \"RATIO\":dataframe[variable].value_counts() / len(dataframe),\n            \"TARGET_COUNT\":dataframe.groupby(variable)[target].count(),\n            \"TARGET_MEAN\":temp.groupby(variable)[target].mean(),\n            \"TARGET_MEDIAN\":temp.groupby(variable)[target].median(),\n            \"TARGET_STD\":temp.groupby(variable)[target].std()}), end=\"\\n\\n\\n\")\n</code></pre> <pre><code>cat_analyzer(df, 'region') \n</code></pre> <pre><code>df[num_cols].hist(figsize = (25,20), bins=15);\n</code></pre> <pre><code>df[num_cols].describe([0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.95, 0.99]).T.drop(['count'], axis=1)\n</code></pre> <pre><code>def outliers_threshold(dataframe, column):\n    q1 = dataframe[column].quantile(0.05)\n    q3 = dataframe[column].quantile(0.95)\n    inter_quartile_range = q3 - q1\n    low = q1 - 1.5 * inter_quartile_range\n    up = q3 + 1.5 * inter_quartile_range\n    return low, up\n\ndef grab_outlier(dataframe, column, index=False):\n    low, up = outliers_threshold(dataframe, column)\n    if dataframe[(dataframe[column] &lt; low) |\n                 (dataframe[column] &gt; up)].shape[0] &lt; 10:\n        print(dataframe[(dataframe[column] &lt; low) | (dataframe[column] &gt; up)][[column]])\n    else:\n        print(dataframe[(dataframe[column] &lt; low) |\n                 (dataframe[column] &gt; up)][[column]])\n    if index:\n        outlier_index = dataframe[(dataframe[column] &lt; low) |\n                                  (dataframe[column] &gt; up)].index.tolist()\n        return outlier_index\n\ndef replace_with_thresholds(dataframe, col_name):\n    low_limit, up_limit = outliers_threshold(dataframe, col_name)\n    if low_limit &gt; 0:\n        dataframe.loc[(dataframe[col_name] &lt; low_limit), col_name] = low_limit\n        dataframe.loc[(dataframe[col_name] &gt; up_limit), col_name] = up_limit\n    else:\n        dataframe.loc[(dataframe[col_name] &gt; up_limit), col_name] = up_limit\n</code></pre> <pre><code>df[df['age'] &lt;= 64]['age'].plot(kind='box')\n</code></pre> <pre><code>for col in num_cols:\n        print('********************************************************************* {} *****************************************************************************'.format(col.upper()))\n        grab_outlier(df, col, True)\n        replace_with_thresholds(df, col)\n        print('****************************************************************************************************************************************************************', end='\\n\\n\\n\\n\\n')\n</code></pre> <pre><code>import matplotlib.pyplot as plt\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>cat_cols\n</code></pre> <pre><code>df.head()\n</code></pre> <pre><code>plt.figure(figsize=(30,20))\ncorr_matrix = df.select_dtypes(include=['int64', 'int32', 'float64']).corr()\nsns.heatmap(corr_matrix, annot=True, cmap='Reds')\nplt.title('Correlation Heatmap')\n</code></pre> <pre><code>def cat_summary(dataframe, x_col, plot=False, rotation=45):\n    display(pd.DataFrame({x_col: dataframe[x_col].value_counts(),\n                          \"Ratio\": 100 * dataframe[x_col].value_counts() / len(dataframe)}))\n\n    if plot:\n        count = dataframe.groupby(x_col).size().sum()\n        dataframe_grouped = dataframe.groupby(x_col).size().reset_index(name='counts').sort_values('counts', ascending=False)\n        num_bars = len(dataframe_grouped[x_col].unique())\n        colors = plt.cm.Set3(np.linspace(0, 1, num_bars))\n        fig, ax = plt.subplots(figsize=(8, 5))\n\n        x_pos = range(len(dataframe_grouped[x_col]))\n\n        ax.bar(x_pos, dataframe_grouped['counts'], color=colors)\n        ax.set_xlabel(x_col)\n        ax.set_ylabel('Count')\n        ax.set_title(f'Distribution by {x_col}')\n\n        ax.set_xticks(x_pos)\n        ax.set_xticklabels(dataframe_grouped[x_col], rotation=rotation)\n\n        for i, value in enumerate(dataframe_grouped['counts']):\n            ax.annotate('{:.1%}'.format(value / count), (i, value), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n\n        plt.show()\n</code></pre> <pre><code>for col in cat_cols:\n    cat_summary(df, col, plot=True)\n</code></pre> <pre><code>proc.kill()\n</code></pre> <p>Previous: Multiple Layer Analyze | Next: Insurance</p>"},{"location":"notebooks/06_other_topics/05_data_exploration/02_01_Plot/Dynamic_Size_Color/Dynamic_Size_Color/","title":"Dynamic Size &amp; Color","text":"<p>In this notebook we are going to give you a brief tutorial on how to use color and size feature of glyphs (a.k.a graphs and plots) dynamically by assigning features of dataset to color and size parameters.</p> <ul> <li>How to create figure</li> <li>How to create and edit circle plots </li> <li>How to add dynamic explanations over glyphs</li> </ul> <pre><code>from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import LinearColorMapper, ColumnDataSource, ColorBar, HoverTool\nfrom bokeh.transform import transform\nimport practicuscore as prt\nimport pandas as pd\nimport numpy as np\n</code></pre> <p>Here's a breakdown of each bokeh function we've imported:</p> <ol> <li>bokeh.plotting:<ul> <li>figure: This module provides a high-level interface for creating Bokeh plots. It includes functions for creating and customizing figures, such as setting titles, axis labels, plot size, and other visual properties.</li> <li>show: This function displays a Bokeh plot in the current environment, such as a browser or a Jupyter notebook.</li> <li>output_notebook: This function configures Bokeh to display plots directly within Jupyter notebooks.</li> </ul> </li> <li>bokeh.models:<ul> <li>ColumnDataSource: This module contains a collection of classes and functions representing various components of a Bokeh plot, such as glyphs (shapes representing data points), axes, grids, annotations, and tools. ColumnDataSource is a fundamental data structure in Bokeh that holds the data to be plotted and allows for efficient updating and sharing of data between different plot elements.</li> <li>HoverTool: This module provides a tool for adding interactive hover tooltips to Bokeh plots. It allows users to display additional information about data points when the mouse cursor hovers over them.</li> <li>LinearColorMapper: A mapper that maps numerical data to colors in a linear manner. It's often used to color glyphs based on a continuous range of data values.</li> <li>ColorBar: A color bar that provides a visual representation of the mapping between data values and colors, often used with LinearColorMapper</li> </ul> </li> <li>bokeh.transform:<ul> <li>transform: This function is used to apply a transformation to the data in a column of a ColumnDataSource. It takes two arguments, <ul> <li>column_name: The name of the column in the ColumnDataSource to transform.</li> <li>transform_expression: A JavaScript expression defining the transformation to apply to the data. This expression can involve mathematical operations, functions, or other JavaScript constructs.</li> </ul> </li> </ul> </li> </ol> <pre><code>worker = prt.get_local_worker()\n</code></pre> <p>One of the most illustrative datasets for demonstrating dynamic size and color is the Titanic dataset.</p> <p>The Titanic dataset is a popular dataset used in machine learning and data analysis. It contains information about passengers aboard the RMS Titanic, including whether they survived or not. Within this data set we will use columns of pclass, fare, age and survived. Let's describe what these columns means for better understanding.</p> <ul> <li>Pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).</li> <li>Fare: Passenger fare.</li> <li>Age: Passenger's age in years.</li> <li>Survived: Indicates whether the passenger survived or not (0 = No, 1 = Yes).</li> </ul> <p>Let's load it into one of our worker environments.</p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"sample_size\": 1180,\n    \"file_path\": \"/home/ubuntu/samples/titanic.csv\"\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine='AUTO') \n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>To display Bokeh plots inline in a classic Jupyter notebook, use the output_notebook() function from bokeh.io.</p> <pre><code>output_notebook()\n</code></pre> <p>We need to create a data structure that holds the data to be plotted to use Bokeh more efficiently. For this we will use ColumnDataSource() function of Bokeh.</p> <pre><code>source = ColumnDataSource(df)\n</code></pre> <p>We need to create a mapper of colors to use color feature as dynamically. We will use LinearColorMapper() built-in function of Bokeh to create our mapper.</p> <pre><code>color_mapper = LinearColorMapper( \n    palette='Sunset11', \n    low=df['survived'].min(), \n    high=df['survived'].max()\n)\n</code></pre> <p>Here's a breakdown of the parameters:</p> <ul> <li>palette: This parameter specifies the color palette to use for mapping the data values. In this case, it's set to 'Sunset11', which is refers to a predefined color palette named 'Sunset11' of Bokeh. This palette consists of 11 distinct colors arranged in a gradient from light to dark or from one color to another. You can look at Bokeh's documentation to see more options.</li> <li>low: This parameter sets the lowest data value in the range of values to be mapped to colors. It's typically set to the minimum value of the data being mapped. In this case, it's set to df['survived'].min(), indicating that the lowest value in the 'survived' column of the DataFrame (df) will be mapped to the lowest color in the palette.</li> <li>high: This parameter sets the highest data value in the range of values to be mapped to colors. It's typically set to the maximum value of the data being mapped. Here, it's set to df['survived'].max(), indicating that the highest value in the 'survived' column of the DataFrame (df) will be mapped to the highest color in the palette.</li> </ul> <p>Let's create our figure to do some visualisatoin.</p> <pre><code>p = figure(title=\"Analysis Over Survivors\", x_axis_label='age', y_axis_label='fare', width=600, height=400)\n</code></pre> <p>Here's an explanation of each parameter in the figure function call:</p> <ul> <li>title: Sets the title of the plot. In this case, it's set to \"Analysis Over Survivors\". The title is displayed at the top of the plot.</li> <li>x_axis_label: Specifies the label for the x-axis. It provides information about the data represented on the x-axis. In this case, it's set to 'age', indicating that the x-axis represents age of travellers.</li> <li>y_axis_label: Specifies the label for the y-axis. Similar to x_axis_label, it provides information about the data represented on the y-axis. Here, it's set to 'fare', indicating that the y-axis represents paid fares of travellers.</li> <li>width: Sets the width of the plot in pixels. In this case, it's set to 400 pixels, determining the horizontal size of the plot.</li> <li>height: Sets the height of the plot in pixels. Here, it's set to 300 pixels, determining the vertical size of the plot.</li> </ul> <p>Let's continue with our first plotting of circle (circle plotting)!</p> <pre><code>circle = p.circle(x='age', y='fare', radius='pclass', color=transform('survived', color_mapper), alpha=0.5, source=source)\nshow(p)\n</code></pre> <p>Here's an explanation of each parameter:</p> <ul> <li>p: This is the figure object where the circles will be added. It seems like p is previously defined as a figure with certain settings like title, axis labels, etc.</li> <li>circle: This variable stores the result of the p.circle function call, representing the circles added to the plot.</li> <li>x: This parameter specifies the x-coordinates of the circles. It's mapped to the 'age' column in the data source (source), indicating the position of each circle along the x-axis.</li> <li>y: This parameter specifies the y-coordinates of the circles. It's mapped to the 'fare' column in the data source (source), indicating the position of each circle along the y-axis.</li> <li>radius: This parameter specifies the radius of the circles. It's mapped to the 'pclass' column in the data source (source), indicating the radius of each circle.</li> <li>color: This parameter specifies the color of the circles. Here, it's set to transform('survived', color_mapper).</li> <li>transform('survived', color_mapper): This function applies the color mapping defined by the LinearColorMapper object (color_mapper) to the 'survived' column in the data source (source). The color of each circle will be determined by the value in the 'survived' column, mapped to colors based on the color_mapper.</li> <li>alpha: This parameter sets the transparency of the circles. It's set to 0.5, making the circles partially transparent.</li> <li>source: This parameter specifies the data source from which the circles will pull their data. It's set to source, which is likely a ColumnDataSource object containing the data needed to plot the  circles.</li> </ul> <p>At this point we should add a bar which describes the meaning of colors. We could do this by using ColorBar() feature of Bokeh.</p> <pre><code>color_bar = ColorBar(color_mapper=color_mapper, padding=3,\n                         ticker=p.xaxis.ticker, formatter=p.xaxis.formatter)\n\np.add_layout(color_bar, 'right')\nshow(p)\n</code></pre> <p>Here's an explanation of each parameter:</p> <ul> <li>color_mapper: This parameter specifies the LinearColorMapper object (color_mapper) that defines the mapping between data values and colors. The color bar will use this mapper to display the range of colors corresponding to the range of data values.</li> <li>padding: This parameter sets the padding (in pixels) between the color bar and other elements of the plot. It's set to 3 pixels in this case, providing some space around the color bar.</li> <li>ticker: This parameter specifies the ticker to use for labeling the color bar axis. It's set to p.xaxis.ticker, which likely means that the same ticker used for the x-axis of the plot (p) will be used for the color bar axis.</li> <li>formatter: This parameter specifies the formatter to use for formatting the tick labels on the color bar axis. It's set to p.xaxis.formatter, meaning that the same formatter used for the x-axis of the plot (p) will be used for the color bar axis.</li> <li>p.add_layout: This method adds a layout element to the plot (p). Here, we're adding the color bar to the plot.</li> <li>color_bar: This is the ColorBar object that we created earlier, representing the color bar to be added to the plot.</li> <li>'right': This parameter specifies the location where the color bar will be added relative to the plot. Here, it's set to 'right', indicating that the color bar will be placed to the right of the plot.</li> </ul> <p>We still missing something, it would be a cool feature if we could see the values of data points. Actually, we could use HoverTool() bokeh to do that!</p> <pre><code>tips = [\n    ('Fare', '@fare'),\n    ('Age', '@age'),\n    ('Survived', '@survived'),\n    ('Pclass', '@pclass')\n]\n\np.add_tools(HoverTool(tooltips=tips)) \nshow(p)\n</code></pre> <p>Right now, when we hover over plots we could see the values of data points.</p> <p>Let's break down the code we have used:</p> <ul> <li>tips: This is a list of tuples, where each tuple contains two elements. The first element of each tuple represents the label for the tooltip, and the second element represents the data field from the data source (source) to be displayed in the tooltip. For example, Fare is the label for the tooltip, and @fare instructs Bokeh to display the value of the fare column from the data source (source) when hovering over a data point</li> <li>p.add_tools: This method adds tools to the plot (p). Here, we're adding the HoverTool to enable hover tooltips.</li> <li>HoverTool: This is a tool provided by Bokeh for adding hover functionality to plots. It displays additional information when the mouse cursor hovers over a data point.</li> <li>tooltips=tips: This parameter of the HoverTool constructor specifies the tooltips to be displayed when hovering over data points. We pass the tips list, which contains the tooltip labels and data fields.</li> </ul> <p>That was the end. You can always checkout our other notebooks about plotting or documentation of bokeh to see more!</p> <pre><code>proc.kill()\n</code></pre> <p>Previous: Legacy Python | Next: Multiple Layer Analyze</p>"},{"location":"notebooks/06_other_topics/05_data_exploration/02_01_Plot/Multiple_Layer_Analyze/Multiple_Layer_Analyze/","title":"Analyze Over Multiple Layer","text":"<p>In this notebook we are going to give you a brief tuttorial of how to use mutiple graphics on one figure by using bokeh.</p> <p>We are going to cover these topics: - How to create and edit a figure - How to process multiple glyphs (graphs) over a figure - How to add dynamic explanations over glyphs - How to create Bar and Line plot</p> <p>Let's begin by importing our libraries</p> <pre><code>from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import ColumnDataSource, HoverTool\nimport practicuscore as prt\nimport pandas as pd\nimport numpy as np\n</code></pre> <p>Here's a breakdown of each bokeh function we've imported:</p> <ol> <li>bokeh.plotting<ul> <li>figure: Creates a new Bokeh plot with customizable options such as plot size, title, axis labels, etc.</li> <li>show: Displays a Bokeh plot in the current environment, such as a browser or a Jupyter notebook.</li> <li>output_notebook: Configures Bokeh to display plots directly in Jupyter notebooks.</li> </ul> </li> <li>bokeh.models<ul> <li>ColumnDataSource: A data structure that holds the data to be plotted and facilitates efficient updating and sharing of data between different plot elements.</li> <li>HoverTool: A tool that provides interactive hover tooltips, displaying additional information about data points when the mouse cursor hovers over them</li> </ul> </li> </ol> <pre><code>worker = prt.get_local_worker()\n</code></pre> <p>One of the most illustrative datasets for demonstrating multiple layer anlyze is the Iris dataset.</p> <p>The Iris dataset is a popular dataset in machine learning and statistics, often used for classification tasks. It consists of 150 samples of iris flowers, each belonging to one of three species: Setosa, Versicolor, or Virginica. Within this data set we will use both Bar and Circle graphic style from Plot. The dataset comprises four features, each representing measurements of the length and width of both the petals and sepals of flowers.</p> <p>Let's load it into one of our worker environments.</p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"sample_size\": 150,\n    \"file_path\": \"/home/ubuntu/samples/iris.csv\"\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine='AUTO') \n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Before starting data analyzing with bokeh, we need to do some preprocess on Iris dataset</p> <pre><code>means = df.groupby('species').mean().reset_index()\n</code></pre> <pre><code>from sklearn.preprocessing import LabelEncoder\n\n# LabelEncoder nesnesini olu\u015ftur\nlabel_encoder = LabelEncoder()\n\n# species s\u00fctununu label encode et\nmeans['species_encoded'] = label_encoder.fit_transform(means['species'])\n</code></pre> <pre><code>means\n</code></pre> <p>To display Bokeh plots inline in a classic Jupyter notebook, use the output_notebook() function from bokeh.io.</p> <pre><code>output_notebook()\n</code></pre> <p>We need to create a data structure that holds the data to be plotted to use Bokeh more efficiently. For this we will use ColumnDataSource() function of Bokeh.</p> <pre><code>source = ColumnDataSource(means)\n</code></pre> <p>Let's create our figure to do some visualisatoin.</p> <pre><code>p = figure(title=\"Analysis Over Species\", x_axis_label='Species', y_axis_label='Features', width=400, height=300)\n</code></pre> <p>Here's an explanation of each parameter in the figure function call:</p> <ul> <li>title: Sets the title of the plot. In this case, it's set to \"Analysis Over Species\". The title is displayed at the top of the plot.</li> <li>x_axis_label: Specifies the label for the x-axis. It provides information about the data represented on the x-axis. In this case, it's set to 'Species', indicating that the x-axis represents different species.</li> <li>y_axis_label: Specifies the label for the y-axis. Similar to x_axis_label, it provides information about the data represented on the y-axis. Here, it's set to 'Features', indicating that the y-axis represents various features.</li> <li>width: Sets the width of the plot in pixels. In this case, it's set to 400 pixels, determining the horizontal size of the plot.</li> <li>height: Sets the height of the plot in pixels. Here, it's set to 300 pixels, determining the vertical size of the plot.</li> </ul> <p>Let's continue with our first plotting of vbar (vertical bar plotting)!</p> <pre><code>first_layer = p.vbar(x = 'species_encoded', top = 'sepal_length', width=0.9, line_color='green', fill_color='lime', fill_alpha=0.5, legend_label=\"Sepal Length\", source=source)\nshow(p)\n</code></pre> <p>Here's an explanation of each vbar parameters:</p> <ul> <li>p: This is the figure object where the plot will be added.</li> <li>vbar: This is the glyph function used to create vertical bar glyphs (rectangles) on the plot.</li> <li>x: This parameter specifies the x-coordinates of the bars.</li> <li>top: This parameter specifies the top edge of each bar.</li> <li>width: This parameter determines the width of the bars. Here, it's set to 0.9, indicating that the bars will have a width of 0.9 units along the x-axis.</li> <li>line_color: This parameter sets the color of the outline of the bars. It's set to 'green', giving the bars a green outline.</li> <li>fill_color: This parameter sets the fill color of the bars. It's set to 'lime', giving the bars a lime green color.</li> <li>fill_alpha: This parameter sets the transparency of the fill color. It's set to 0.5, making the bars partially transparent.</li> <li>legend_label: This parameter sets the label for the legend entry corresponding to this glyph. It's set to \"Sepal Length\", which will be displayed in the legend.</li> <li>source: This parameter specifies the data source from which the glyph will pull its data. It's set to source, which is defined previously by using ColumnDataSource().</li> </ul> <p>Overall, this line of code creates a vertical bar plot of sepal lengths for different species, with customization for appearance and legend labeling, and it adds this plot as a layer to the existing figure p. You can check out bokeh documentation of colors for more coloring options.</p> <p>Let's add another vbar, a second layer, which will show case petal_length.</p> <pre><code>second_layer = p.vbar(x = 'species_encoded', top = 'petal_length', width=0.9, line_color='blue', fill_color='lightskyblue', fill_alpha=0.5, legend_label=\"Petal Length\", source=source)\nshow(p)\n</code></pre> <p>After adding our second layer it started to be too crowded for such a small figure frame, let's expand it and we could also use some more explanatory labels for axis.</p> <pre><code>p.width = 800\np.height = 600\np.xaxis.axis_label = 'Flower Species'\np.yaxis.axis_label = 'Flower Features'\n\nshow(p)\n</code></pre> <p>Let's add some plots about our flowers width, but if we add more bars it will make the figure too confusing to read. Therefore, let's add line plots to visualize width features of our flowers.</p> <pre><code>third_layer = p.line(x='species_encoded', y='sepal_width', line_width=4, line_color='darkolivegreen', legend_label=\"Sepal Width\", source=source)\nshow(p)\n</code></pre> <p>Here's an explanation of each line parameters:</p> <ul> <li>p: This is the figure object where the line plot will be added.</li> <li>line: This is the glyph function used to create line glyphs on the plot.</li> <li>x: This parameter specifies the x-coordinates of the line.</li> <li>y: This parameter specifies the y-coordinates of the line.</li> <li>line_width: This parameter determines the width of the line. It's set to 4, indicating that the line will be drawn with a width of 4 units.</li> <li>line_color: This parameter sets the color of the line. It's set to 'darkolivegreen', giving the line a dark olive green color.</li> <li>legend_label: This parameter sets the label for the legend entry corresponding to this glyph. It's set to \"Sepal Width\", which will be displayed in the legend.</li> <li>source: This parameter specifies the data source from which the glyph will pull its data.  It's set to source, which is defined previously by using ColumnDataSource().</li> </ul> <p>Overall, this line of code creates a line plot of sepal widths for different species, with customization for appearance and legend labeling, and it adds this plot as a layer to the existing figure p.</p> <p>Let's add our fourth and last plot!</p> <pre><code>fourth_layer = p.line(x='species_encoded', y='petal_width', line_width=4, line_color='darkblue', legend_label=\"Petal Width\", source=source)\nshow(p)\n</code></pre> <p>After adding all layer we still missing something, it would be a cool feature if we could see the values of data points. Actually, we could use HoverTool() bokeh to do that!</p> <pre><code>tips = [\n    ('Sepal Length', '@sepal_length'),\n    ('Petal Length', '@petal_length'),\n    ('Sepal Width', '@sepal_width'),\n    ('Petal Width', '@petal_width')\n]\n\np.add_tools(HoverTool(tooltips=tips)) \nshow(p)\n</code></pre> <p>Right now, when we hover over plots we could see the values of data points.</p> <p>Let's break down the code we have used:</p> <ul> <li>tips: This is a list of tuples, where each tuple contains two elements. The first element of each tuple represents the label for the tooltip, and the second element represents the data field from the data source (source) to be displayed in the tooltip. For example, Sepal Length is the label for the tooltip, and @sepal_length instructs Bokeh to display the value of the sepal_length column from the data source (source) when hovering over a data point</li> <li>p.add_tools: This method adds tools to the plot (p). Here, we're adding the HoverTool to enable hover tooltips.</li> <li>HoverTool: This is a tool provided by Bokeh for adding hover functionality to plots. It displays additional information when the mouse cursor hovers over a data point.</li> <li>tooltips=tips: This parameter of the HoverTool constructor specifies the tooltips to be displayed when hovering over data points. We pass the tips list, which contains the tooltip labels and data fields.</li> </ul> <p>Last but not least, we could remove layers by using their visible parameter:</p> <pre><code>first_layer.visible = False\nshow(p)\n</code></pre> <pre><code>first_layer.visible = True\nshow(p)\n</code></pre> <p>That was the end. You can always checkout our other notebooks about plotting or documentation of bokeh to see more!</p> <pre><code>proc.kill()\n</code></pre> <p>Previous: Dynamic Size Color | Next: Eda</p>"},{"location":"notebooks/06_other_topics/06_data_processing/01_insurance/","title":"Insurance","text":""},{"location":"notebooks/06_other_topics/06_data_processing/01_insurance/#insurance-sample-sdk-usage-with-local-worker","title":"Insurance Sample SDK Usage with Local Worker","text":""},{"location":"notebooks/06_other_topics/06_data_processing/01_insurance/#scenario-process-on-the-interface-deploy-and-access-the-process","title":"Scenario: Process on the interface, deploy and access the process","text":"<ol> <li> <p>Open insurance.csv</p> </li> <li> <p>Encoding categorical variables</p> </li> <li> <p>Delete the originals of the columns you encoded</p> </li> <li> <p>Navigating the Deploy button, choosing Jupyter Notebook Option:</p> </li> <li> <p>Clicking Advanced and selecting view code and include security token options:</p> </li> <li> <p>Seeing your connection and worker json created and your code ready:</p> </li> </ol> <p>Connect to your active worker with get local worker</p> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n</code></pre> <ul> <li>To access the dataset you need to work with connection configuration dictionary</li> <li>Also, you can choose a different Engine than Advance in the Deploy phase</li> </ul> <pre><code># configuration of connection\n\ndata_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"ws_uuid\": \"d9b92183-8832-4fd1-8187-ac741ff6aab0\",\n    \"ws_name\": \"insurance\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn) \n</code></pre> <p>Data preparation with Practicus AI SDK</p> <pre><code>proc.categorical_map(column_name='sex', column_suffix='category') \n</code></pre> <pre><code>proc.categorical_map(column_name='smoker', column_suffix='category') \n</code></pre> <pre><code>proc.categorical_map(column_name='region', column_suffix='category') \n</code></pre> <pre><code>proc.delete_columns(['region', 'smoker', 'sex']) \n</code></pre> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Finish the process</p> <pre><code>proc.kill()\n</code></pre>"},{"location":"notebooks/06_other_topics/06_data_processing/01_insurance/#you-can-also-prepare-this-code-directly-in-pipeline","title":"You can also prepare this code directly in pipeline:","text":""},{"location":"notebooks/06_other_topics/06_data_processing/01_insurance/#if-you-make-the-process-functional-and-do-it-with-with-you-dont-need-to-kill-the-worker-when-the-process-is-finished-the-worker-is-automatically-killed-when-this-process-is-finished","title":"If you make the process functional and do it with with, you don't need to kill the worker when the process is finished. The worker is automatically killed when this process is finished","text":"<pre><code># Running the below will terminate the worker \n# with prt.get_local_worker() as worker:\n\n# E.g. the below would run the task, and then kill the process first, and then the worker.\n# with prt.get_local_worker() as worker:\n#     with worker.load(data_set_conn) as proc:\n#         proc.categorical_map(column_name='sex', column_suffix='category'), \n#         proc.categorical_map(column_name='smoker', column_suffix='category'),\n#         proc.categorical_map(column_name='region', column_suffix='category'),\n#         proc.delete_columns(['region', 'smoker', 'sex']) \n#         proc.wait_until_done()\n#         proc.show_logs()\n</code></pre> <p>Previous: Eda | Next: Insurance With Remote Worker</p>"},{"location":"notebooks/06_other_topics/06_data_processing/02_insurance_with_remote_worker/","title":"Insurance With Remote Worker","text":""},{"location":"notebooks/06_other_topics/06_data_processing/02_insurance_with_remote_worker/#insurance-sample-sdk-usage-with-remote-worker","title":"Insurance Sample SDK Usage with Remote Worker","text":""},{"location":"notebooks/06_other_topics/06_data_processing/02_insurance_with_remote_worker/#scenario-process-on-the-notebook","title":"Scenario: Process on the Notebook","text":"<ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Create json for worker and connection.(Json generation code coming soon)</p> </li> <li>The Worker json file must contain the following configurations:</li> <li> <p>Service url, worker_size, worker_image, email, and refresh_token</p> </li> <li> <p>The Connection json file must contain the following configurations:</p> </li> <li> <p>Connection_type, ws_uuid, ws_name, and file_path</p> </li> <li> <p>Encoding categorical variables</p> </li> <li> <p>Delete the originals of the columns you encoded</p> </li> <li> <p>Run the process and kill processing when finished</p> </li> </ol> <p>Create a new worker with practicuscore method of \"create_worker\" and use this new worker for your operations</p> <pre><code>worker_conf = {\n    \"worker_size\": \"Medium\",\n    \"worker_image\": \"practicus\",\n    #\"service_url\": \"\",\n    #\"email\": \"\",\n    #\"refresh_token\": \"**entry_your_token**\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.create_worker(worker_conf)\n</code></pre> <ul> <li>To access the dataset you need to work with connection configuration dictionary</li> <li>Also, you can choose a different Engine than Advance in the Deploy phase</li> </ul> <pre><code># configuration of connection\n\ndata_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>proc = worker.load(data_set_conn, engine='AUTO') \n</code></pre> <p>Data prep with Practicus ai SDK</p> <pre><code>proc.categorical_map(column_name='sex', column_suffix='category') \n</code></pre> <pre><code>proc.categorical_map(column_name='smoker', column_suffix='category') \n</code></pre> <pre><code>proc.categorical_map(column_name='region', column_suffix='category') \n</code></pre> <pre><code>proc.delete_columns(['region', 'smoker', 'sex']) \n</code></pre> <pre><code>proc.wait_until_done(timeout_min=600)\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre> <p>Finish the process</p> <pre><code>proc.kill()\n</code></pre>"},{"location":"notebooks/06_other_topics/06_data_processing/02_insurance_with_remote_worker/#you-can-also-prepare-this-code-directly-in-pipeline","title":"You can also prepare this code directly in pipeline:","text":""},{"location":"notebooks/06_other_topics/06_data_processing/02_insurance_with_remote_worker/#if-you-make-the-process-functional-and-do-it-with-with-you-dont-need-to-kill-the-worker-when-the-process-is-finished-the-worker-is-automatically-killed-when-this-process-is-finished","title":"If you make the process functional and do it with with, you don't need to kill the worker when the process is finished. The worker is automatically killed when this process is finished","text":"<pre><code>with prt.create_worker(worker_conf) as worker: \n    with worker.load(data_set_conn) as proc:\n        proc.categorical_map(column_name='sex', column_suffix='category'), \n        proc.categorical_map(column_name='smoker', column_suffix='category'),\n        proc.categorical_map(column_name='region', column_suffix='category'),\n        proc.delete_columns(['region', 'smoker', 'sex']) \n        proc.wait_until_done(timeout_min=600)\n        proc.show_logs()\n</code></pre> <p>Previous: Insurance | Next: Spark Custom Config</p>"},{"location":"notebooks/06_other_topics/06_data_processing/deploy_workflow/","title":"Deploy Workflow","text":"<pre><code>import practicuscore as prt\n\nservice_key = \"airflow-dev\"\ndag_key = \"california\"\nfiles_path = \"/home/ubuntu/practicus/generated/california\" \n# workflow_files_dir_path = None  # None for current directory\n\nprt.workflows.deploy(\n    service_key=service_key, \n    dag_key=dag_key, \n    files_path=files_path\n)\n</code></pre> <p>Previous: Polars | Next: Spark Object Storage</p>"},{"location":"notebooks/06_other_topics/06_data_processing/polars/","title":"Polars","text":""},{"location":"notebooks/06_other_topics/06_data_processing/polars/#introduction-to-polars","title":"Introduction to Polars","text":"<p>Polars is a high-performance DataFrame library designed for efficient and fast data manipulation. Built in Rust and leveraging Apache Arrow, Polars provides a modern, user-friendly API for working with structured data. It offers several advantages over traditional libraries like Pandas and Dask:</p>"},{"location":"notebooks/06_other_topics/06_data_processing/polars/#why-use-polars","title":"Why Use Polars?","text":""},{"location":"notebooks/06_other_topics/06_data_processing/polars/#key-benefits","title":"Key Benefits:","text":"<ol> <li>Speed: Written in Rust and optimized for performance, Polars is significantly faster for many operations compared to Pandas.</li> <li>Memory Efficiency: Polars uses Arrow memory structures, which are compact and designed for zero-copy interprocess communication.</li> <li>Parallelism: Automatically leverages multiple CPU cores for computations.</li> <li>Lazy Evaluation: Allows defining a series of operations that are only computed when needed, improving efficiency for complex workflows.</li> <li>Interoperability: Easy to switch between Polars and Pandas, allowing incremental adoption.</li> </ol>"},{"location":"notebooks/06_other_topics/06_data_processing/polars/#example-basic-polars-operations-with-diamondcsv","title":"Example: Basic Polars Operations with <code>diamond.csv</code>","text":"<p>In this notebook, we will: 1. Load the <code>diamonds.csv</code> dataset using Polars. 2. Explore the dataset with basic info commands. 3. Perform simple data manipulations. 4. Showcase interoperation between Polars and Pandas.</p> <pre><code># Import Polars\nimport polars as pl\n\n# Read the dataset using Polars\ndf = pl.read_csv(\"../../diamond.csv\")\n\n# Basic exploration\nprint(\"Shape of the dataset:\", df.shape)\nprint(\"First few rows of the dataset:\")\nprint(df.head())\n</code></pre> <pre><code># Summary statistics\nprint(\"Summary statistics:\")\nprint(df.describe())\n</code></pre> <pre><code># Filter rows where carat is greater than 2\nfiltered_df = df.filter(pl.col(\"Carat Weight\") &gt; 2)\nprint(\"Filtered rows where Carat Weight &gt; 2:\")\nprint(filtered_df)\n</code></pre> <pre><code># Convert to Pandas DataFrame\npandas_df = df.to_pandas()\nprint(\"Converted to Pandas DataFrame:\")\ndisplay(pandas_df.head())\n</code></pre> <pre><code># Convert back to Polars DataFrame\npolars_df = pl.from_pandas(pandas_df)\nprint(\"Converted back to Polars DataFrame:\")\nprint(polars_df.head())\n</code></pre> <pre><code>\n</code></pre> <p>Previous: Spark Custom Config | Next: Deploy Workflow</p>"},{"location":"notebooks/06_other_topics/06_data_processing/spark_custom_config/","title":"Spark Custom Config","text":"<pre><code>s3_access = \"\"\ns3_secret = \"\"\n\n# For AWS S3 \ns3_endpoint = \"s3.amazonaws.com\"\n# For others, e.g. Minio\n# s3_endpoint = \"http://prt-svc-sampleobj.prt-ns.svc.cluster.local\"\n</code></pre> <pre><code>extra_spark_conf = {\n    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n    \"spark.hadoop.fs.s3a.access.key\" : s3_access,\n    \"spark.hadoop.fs.s3a.secret.key\" : s3_secret,\n    \"spark.hadoop.fs.s3a.endpoint\": s3_endpoint\n}\n\nimport practicuscore as prt \n\nspark = prt.engines.get_spark_session(extra_spark_conf=extra_spark_conf)\n\ndf = spark.read.csv(\"s3a://sample-bucket/boston.csv\")\ndf.head()\n</code></pre> <p>Previous: Insurance With Remote Worker | Next: Polars</p>"},{"location":"notebooks/06_other_topics/06_data_processing/spark_object_storage/","title":"Spark Object Storage","text":"<pre><code># For AWS S3\nconnection = {\n    \"connection_type\": \"S3\",\n    \"aws_region\": \"us-east-1\",\n    \"aws_access_key_id\": \"...\",\n    \"aws_secret_access_key\": \"...\",\n    # Optional\n    # \"aws_session_token\", \"...\"\n}\n</code></pre> <pre><code># For others, e.g. Minio\nconnection = {\n    \"connection_type\": \"S3\",\n    \"endpoint_url\": \"http://prt-svc-sampleobj.prt-ns.svc.cluster.local\",\n    \"aws_access_key_id\": \"...\",\n    \"aws_secret_access_key\": \"...\",\n}\n</code></pre> <pre><code>import practicuscore as prt \n\n# Create a Spark session\nspark = prt.engines.get_spark_session(connection)\n\n# If you are using distributed Spark, you should now have the Spark cluster up &amp; running. \n</code></pre> <pre><code>df = spark.read.csv(\"s3a://sample/boston.csv\")\ndf.head()\n</code></pre> <pre><code># Optional: delete Spark Session \nprt.engines.delete_spark_session(spark)\n\n# If you are using distributed Spark, you should now have the Spark cluster terminated.\n# You can also terminate your worker, which will automatically terminate the child Spark Cluster. \n</code></pre> <p>Previous: Deploy Workflow | Next: Preprocess Tut</p>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/","title":"Data Processing","text":""},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#scenario-pre-process-steps-by-using-sdk","title":"Scenario: Pre-process steps by using SDK","text":"<p>In this notebook, we'll showcase how to apply pre-process steps by using our SDK, practicuscore.</p> <ol> <li> <p>Loading the \"income\" dataset</p> </li> <li> <p>Profiling the dataset</p> </li> <li> <p>Applying pre-process steps:</p> <ul> <li>Suppressing outliers by using snippets</li> <li>Applying one hot encoding</li> <li>Applying label encoding</li> <li>Re-naming columns</li> <li>Deleting columns</li> <li>Applying standardization by using snippets</li> </ul> </li> </ol>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#step-1-loading-the-dataset","title":"Step-1: Loading the Dataset","text":"<pre><code>dataset_conn ={\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/income.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(dataset_conn)\n\nproc.show_head()\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#step-2-profiling-the-dataset","title":"Step-2: Profiling the dataset","text":"<p>\"ydata_profiling\" Python library is a powerful tool for data analysts and data scientists to analyze data sets quickly and effectively. </p> <p>The ProfileReport method of this library performs a thorough inspection of a data frame and generates a detailed profile report. This report provides comprehensive summaries of the dataset's overall statistics, missing values, distributions, correlations and other important information. With the report, users can quickly identify potential problems and patterns in the data set, which greatly speeds up and simplifies the data cleaning and pre-processing phases.</p> <pre><code>from ydata_profiling import ProfileReport\n</code></pre> <pre><code>df_raw = proc.get_df_copy()\n</code></pre> <pre><code>ProfileReport(df_raw)\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#step-3-pre-process","title":"Step-3: Pre-process","text":""},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#31-handling-with-missing-values","title":"3.1: Handling with missing values","text":"<p>The 'handle_missing' method of the SDK can be utilized to fill or drop missing values on target columns.</p> <ul> <li>technique: The method which used in handling missing value. It could take the values down below:<ul> <li>'delete': drops the rows with missing values</li> <li>'custom': filling the missing values with a custom value</li> <li>'minumum': filling the missing values with minunmum value of column</li> <li>'maximum': filling the missing values with maximum value of column</li> <li>'average': filling the missing values with average value of column</li> </ul> </li> <li>column_list: List of targeted columns (columns with missing values)</li> <li>custom_value: The value which will be used in filling columns, if not using 'custom' method leave it to be 'None'</li> </ul> <pre><code>proc.handle_missing(technique='minumum', column_list=['workclass'], custom_value='None') \nproc.handle_missing(technique='custom', column_list=['native-country'], custom_value='unknown')\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#32-suppressing-of-outliers-by-using-snippets","title":"3.2: Suppressing of outliers by using snippets","text":"<p>Snippets are built-in python functions prepared by Practicus AI but, also you can build your own snippets for your company (for more information please visit https://docs.practicus.ai/tutorial)</p> <p>To utilize snippets effectively, ensure that you create and open a folder named 'snippets' within your working directory. Then, place the snippet files into this designated folder.</p> <p>Every snippets has parameters which are optional or mandatory to run. You can checkout the parameters within the snippet code.</p> <p>E.g. the paramaters within 'suppress_outliers' can be listed as: - outlier_float_col_list: list[str] | None (List of numeric columns to check for outliers. If left empty, applies to all numeric columns.), - q1_percentile: float = 0.25 (Custom percentile for Q1, takes 0.25 as default value), - q3_percentile: float = 0.75 (Custom percentile for Q3, takes 0.75 as default value), - result_col_suffix: str | None = \"no_outlier\" (suffix for the new column where the suppressed data will be stored, takes \"no_outlier\" as default), - result_col_prefix: str | None = None (Prefix for the new column where the suppressed data will be stored.),</p> <pre><code>proc.run_snippet( \n    'suppress_outliers', \n    outlier_float_col_list=[\"capital-gain\", \"capital-loss\"],\n    q1_percentile = 0.05,\n    q3_percentile = 0.95\n)\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#33-one-hot-encoding","title":"3.3: One-hot Encoding","text":"<p>The 'one_hot' method of the SDK can be utilized to apply one-hot encoding to the selected column.</p> <ul> <li>column_name: The name of the current column to be one-hot encoded.</li> <li>column_prefix: A prefix to use for the new one-hot encoded columns.</li> </ul> <pre><code>cat_col_list = [\n    'workclass', \n    'education', \n    'marital-status', \n    'occupation', \n    'relationship', \n    'native-country'\n]\n</code></pre> <pre><code>for col in cat_col_list:\n    proc.one_hot(column_name=col, column_prefix=col)\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#34-label-encoding","title":"3.4: Label Encoding","text":"<p>The 'categorical_map' method of the SDK can be utilized to apply label encoding to the selected column.</p> <ul> <li>column_name: The name of the current column to be one-hot encoded.</li> <li>column_prefix: A prefix to use for the new one-hot encoded columns.</li> </ul> <pre><code>proc.categorical_map(column_name='sex', column_suffix='cat')\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#35-re-naming-columns","title":"3.5: Re-naming Columns","text":"<p>The 'rename_column' method of the SDK can be utilized to rename columns.</p> <pre><code>proc.rename_column('hours-per-week', 'hours_per_week')\n</code></pre> <pre><code>proc.rename_column('capital-loss', 'capital_loss')\n</code></pre> <pre><code>proc.rename_column('capital-gain', 'capital_gain')\n</code></pre> <pre><code>proc.rename_column('education-num', 'education_num')\n</code></pre> <pre><code>proc.rename_column('income &gt;50K', 'income_50K') \n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#36-deleting-columns","title":"3.6: Deleting Columns","text":"<p>The 'delete_columns' method of the SDK can be utilized to delete columns.</p> <pre><code>proc.delete_columns(['sex', \n                     'workclass', \n                     'education', \n                     'marital-status', \n                     'occupation', \n                     'relationship', \n                     'native-country'])\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#37-standardization-of-numerical-columns","title":"3.7: Standardization of numerical columns","text":"<p>The 'normalize.py' snippet of the SDK can be utilized to apply standardization to numeric columns.</p> <pre><code>proc.run_snippet( \n    'normalize', \n    numeric_col_list=['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week'],    \n    normalization_option=\"Min-Max Normalization\",  \n    result=None\n) \n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#38-logging-the-pre-process","title":"3.8: Logging the pre-process","text":"<p>The 'wait_until_done' and 'show_logs' methods of the SDK can be utilized to check and log the pre-process steps.</p> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#39-exporting-the-data-set-into-pandas-dataframe","title":"3.9: Exporting the data set into pandas dataframe","text":"<p>The 'get_df_copy' methods of the SDK can be utilized to export the dataset as pandas dataframe to continue working with dataset on diffrent aspect of Data Science.</p> <pre><code>df_processed = proc.get_df_copy()\n</code></pre> <pre><code>df_processed.head()\n</code></pre> <pre><code>ProfileReport(df_processed)\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#snippetsimpute_missing_knnpy","title":"snippets/impute_missing_knn.py","text":"<pre><code>from enum import Enum\n\n\nclass WeightsEnum(str, Enum):\n    uniform = \"uniform\"\n    distance = \"distance\"\n\n\ndef impute_missing_knn(df, missing_val_col: list[str] | None, n_neighbors: int = 5, weights: WeightsEnum = WeightsEnum.uniform):\n    \"\"\"\n    Replaces each missing value using K-Nearest Neighbors technique\n    :param missing_val_col: Columns to impute missing values. Leave empty for all columns\n    :param n_neighbors: Number of neighboring samples to use for imputation.\n    :param weights: Weight function used in prediction\n    \"\"\"\n    import pandas as pd\n    import numpy as np\n    from sklearn.impute import KNNImputer\n\n    knn_imp = KNNImputer(n_neighbors=n_neighbors, weights=str(weights))\n\n    numeric_df = df.select_dtypes(include=[np.number])\n\n    if missing_val_col:\n        non_numeric_columns = set(missing_val_col) - set(numeric_df.columns)\n        if non_numeric_columns:\n            raise ValueError(f\"Please only select numeric columns to impute, or do not select any columns. Non-numeric columns: {non_numeric_columns}\")\n\n        imputed_data = knn_imp.fit_transform(numeric_df[missing_val_col])\n        imputed_df = pd.DataFrame(imputed_data, columns=missing_val_col, index=numeric_df.index)\n    else:\n        imputed_data = knn_imp.fit_transform(numeric_df)\n        imputed_df = pd.DataFrame(imputed_data, columns=numeric_df.columns, index=numeric_df.index)\n\n    df.update(imputed_df)\n    return df\n\n\nimpute_missing_knn.worker_required = True\nimpute_missing_knn.supported_engines = ['pandas']\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#snippetsnormalizepy","title":"snippets/normalize.py","text":"<pre><code>from enum import Enum\n\n\nclass NormalizationOptions(str, Enum):\n    Z_SCORE = \"Z-Score Normalization\"\n    MIN_MAX = \"Min-Max Normalization\"\n    ROBUST = \"Robust Normalization\"\n\n\ndef normalize(df, numeric_col_list: list[str] | None = None, normalization_option: NormalizationOptions = NormalizationOptions.Z_SCORE, result: list[str] | None = None):\n    \"\"\"\n    Normalizes certain columns in the DataFrame with the selected normalization method.\n\n    :param numeric_col_list: Names of the numeric columns to normalize. If None, all numeric columns are considered.\n    :param normalization_option: Specifies the method for normalization: Z-Score (standardizes data), Min-Max (scales data to a fixed range, typically [0, 1]), or Robust (reduces the impact of outliers).\n    :param result: Column names to write normalization results. If None, the original column names appended with \"_normalized\" will be used.\n    \"\"\"\n    import numpy as np\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n    # If no specific columns provided, use all numeric columns\n    if numeric_col_list is None:\n        numeric_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    # Process according to the selected normalization method\n    if normalization_option == NormalizationOptions.Z_SCORE:\n        scaler = StandardScaler()\n    elif normalization_option == NormalizationOptions.MIN_MAX:\n        scaler = MinMaxScaler()\n    elif normalization_option == NormalizationOptions.ROBUST:\n        scaler = RobustScaler()\n    else:\n        raise ValueError(\"Unsupported normalization option selected.\")\n\n    # Normalize specified columns and assign results either to new columns or overwrite them\n    for col in numeric_col_list:\n        normalized_col_name = col + \"_normalized\" if result is None else result.pop(0) if result else f\"{col}_normalized\"\n        df[normalized_col_name] = scaler.fit_transform(df[[col]])\n\n    return df\n\n\nnormalize.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/07_sdk_preprocessing/preprocess_tut/#snippetssuppress_outlierspy","title":"snippets/suppress_outliers.py","text":"<pre><code>def suppress_outliers(\n        df, outlier_float_col_list: list[str] | None, q1_percentile: float = 0.25, q3_percentile: float = 0.75,\n        result_col_suffix: str | None = \"no_outlier\", result_col_prefix: str | None = None):\n    \"\"\"\n    Suppresses outliers in specified numeric columns of the dataframe based on custom percentile values for Q1 and Q3.\n    Adds new columns with the selected suffix or prefix. If no suffix or prefix is provided, overwrites the existing column.\n    :param outlier_float_col_list: List of numeric columns to check for outliers. If left empty, applies to all numeric columns.\n    :param q1_percentile: Custom percentile for Q1 (e.g., 0.25 for 25th percentile).\n    :param q3_percentile: Custom percentile for Q3 (e.g., 0.75 for 75th percentile).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import numpy as np\n\n    # If no specific columns provided, use all numeric columns\n    if not outlier_float_col_list:\n        outlier_float_col_list = df.select_dtypes(include=[np.number]).columns.tolist()\n\n    if len(outlier_float_col_list) == 0:\n        raise ValueError(\"No numeric column provided or located.\")\n\n    # Process each specified column\n    for col in outlier_float_col_list:\n        q1 = df[col].quantile(q1_percentile)\n        q3 = df[col].quantile(q3_percentile)\n        iqr = q3 - q1\n\n        lower_bound = q1 - 1.5 * iqr\n        upper_bound = q3 + 1.5 * iqr\n\n        if result_col_suffix:\n            new_col_name = f'{col}_{result_col_suffix}'\n        elif result_col_prefix:\n            new_col_name = f'{result_col_prefix}_{col}'\n        else:\n            new_col_name = col\n\n        # Create a new column (or override), with suppressed values\n        df[new_col_name] = np.where(\n            df[col] &lt; lower_bound, lower_bound, np.where(df[col] &gt; upper_bound, upper_bound, df[col])\n        )\n\n    return df\n</code></pre> <p>Previous: Spark Object Storage | Next: AutoML</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/","title":"AutoML","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#sample-notebook-predicting-insurance-charges-with-automated-machine-learning-automl","title":"Sample Notebook: Predicting Insurance Charges with Automated Machine Learning (AutoML)","text":"<p>In the insurance sector, accurately forecasting the costs associated with policyholders is crucial for pricing policies competitively while ensuring profitability. For insurance companies, the ability to predict these costs helps in tailoring individual policies, identifying key drivers of insurance costs, and ultimately enhancing customer satisfaction by offering policies that reflect a customer's specific risk profile and needs.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#objective","title":"Objective","text":"<p>The primary goal of this notebook is to showcase how Practicus AI users can leverage AutoML to swiftly and proficiently develop a predictive model, minimizing the need for extensive manual modeling work. By engaging with this notebook, you'll acquire knowledge on how to:</p> <ul> <li> <p>Load the dataset specific to insurance costs</p> </li> <li> <p>Using AutoML to train and tune a predictive model tailored for insurance charges prediction.</p> </li> <li> <p>Assess the model's performance accurately</p> </li> </ul> <p>Let's embark on this journey!</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#step-1-setting-up-the-environment","title":"Step 1: Setting Up the Environment","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#first-we-need-to-set-up-our-python-environment-with-the-necessary-libraries-pycaret-an-automl-library-simplifies-the-machine-learning-workflow-enabling-us-to-efficiently-develop-predictive-models","title":"First, we need to set up our Python environment with the necessary libraries. PyCaret, an AutoML library, simplifies the machine learning workflow, enabling us to efficiently develop predictive models.","text":"<pre><code># Standard libraries for data manipulation and numerical operations\nimport pandas as pd\nimport numpy as np\nfrom pycaret.regression import *  # Importing PyCaret's regression module\n# Extras\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#step-2-loading-the-dataset","title":"Step 2: Loading the Dataset","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#the-dataset-consists-of-1337-observations-and-7-variables-with-charges-being-the-target-variable-we-aim-to-predict-this-dataset-is-a-common-benchmark-in-insurance-cost-predictions","title":"The dataset consists of 1,337 observations and 7 variables, with 'charges' being the target variable we aim to predict. This dataset is a common benchmark in insurance cost predictions.","text":"<pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n\nproc = worker.load(data_set_conn, engine='AUTO') \n\ndf = proc.get_df_copy()\ndisplay(df)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#step-3-initializing-the-automl-experiment","title":"Step 3: Initializing the AutoML Experiment","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#pycarets-regression-module-is-utilized-here-for-predicting-a-continuous-target-variable-ie-insurance-costs-we-begin-by-initializing-our-automl-experiment","title":"PyCaret's regression module is utilized here for predicting a continuous target variable, i.e., insurance costs. We begin by initializing our AutoML experiment.","text":"<pre><code>from pycaret.regression import RegressionExperiment, load_model, predict_model\n\nexp = RegressionExperiment()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#this-step-sets-up-our-environment-within-pycaret-allowing-for-automated-feature-engineering-model-selection-and-more","title":"This step sets up our environment within PyCaret, allowing for automated feature engineering, model selection, and more.","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#step-4-configuring-the-experiment","title":"Step 4: Configuring the Experiment","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#well-configure-our-experiment-with-a-specific-name-making-it-easier-to-manage-and-reference","title":"We'll configure our experiment with a specific name, making it easier to manage and reference.","text":"<pre><code># You need to configure using the service unique key, you can find your key on the \"Practicus AI Admin Console\" \nservice_key = 'mlflow-primary'  \n# Optionally, you can provide experime name to create a new experiement while configuring\nexperiment_name = 'insuranceCharges'\n\nprt.experiments.configure(service_key=service_key, experiment_name=experiment_name)\n# No experiment service selected, will use MlFlow inside the Worker. To configure manually:\n# configure_experiment(experiment_name=experiment_name, service_name='Experiment service name')\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#step-5-preparing-data-with-pycarets-setup","title":"Step 5: Preparing Data with PyCaret's Setup","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#a-critical-step-where-we-specify-our-experiments-details-such-as-the-target-variable-session-id-for-reproducibility-and-whether-to-log-the-experiment-for-tracking-purposes","title":"A critical step where we specify our experiment's details, such as the target variable, session ID for reproducibility, and whether to log the experiment for tracking purposes.","text":"<pre><code>setup_params = {'normalize': True, 'normalize_method': 'minmax'}\n</code></pre> <pre><code>exp.setup(data=df, target='charges', session_id=42, \n          log_experiment=True, feature_selection=True, experiment_name=experiment_name, **setup_params)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#step-6-model-selection-and-tuning","title":"Step 6: Model Selection and Tuning","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#this-command-leverages-automl-to-compare-different-models-automatically-selecting-the-one-that-performs-best-according-to-a-default-or-specified-metric-its-a-quick-way-to-identify-a-strong-baseline-model-without-manual-experimentation","title":"This command leverages AutoML to compare different models automatically, selecting the one that performs best according to a default or specified metric. It's a quick way to identify a strong baseline model without manual experimentation.","text":"<pre><code>best_model = exp.compare_models()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#once-a-baseline-model-is-selected-this-step-fine-tunes-its-hyperparameters-to-improve-performance-the-use-of-tune-sklearn-and-hyperopt-indicates-an-advanced-search-across-the-hyperparameter-space-for-optimal-settings-which-can-significantly-enhance-model-accuracy","title":"Once a baseline model is selected, this step fine-tunes its hyperparameters to improve performance. The use of tune-sklearn and hyperopt indicates an advanced search across the hyperparameter space for optimal settings, which can significantly enhance model accuracy.","text":"<pre><code>tune_params = {}\n</code></pre> <pre><code>tuned_model = exp.tune_model(best_model, **tune_params)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#step-7-finalizing-the-model","title":"Step 7: Finalizing the Model","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#after-tuning-the-model-is-finalized-meaning-its-retrained-on-the-entire-dataset-including-the-validation-set-this-step-ensures-the-model-is-as-generalized-as-possible-before-deployment","title":"After tuning, the model is finalized, meaning it's retrained on the entire dataset, including the validation set. This step ensures the model is as generalized as possible before deployment.","text":"<pre><code>final_model = exp.finalize_model(tuned_model)\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#step-8-predictions-and-saving-the-model","title":"Step 8: Predictions and Saving the Model","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#we-predict-insurance-costs-using-our-final-model-and-save-it-for-future-use-ensuring-operational-scalability","title":"We predict insurance costs using our final model and save it for future use, ensuring operational scalability.","text":"<pre><code>predictions = exp.predict_model(final_model, data=df)\ndisplay(predictions)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#the-last-step-involves-saving-the-trained-model-for-future-use-such-as-deployment-in-a-production-environment-or-further-evaluation-it-ensures-the-models-availability-beyond-the-current-session-facilitating-operationalization-and-scalability","title":"The last step involves saving the trained model for future use, such as deployment in a production environment or further evaluation. It ensures the model's availability beyond the current session, facilitating operationalization and scalability.","text":"<pre><code>exp.save_model(final_model, 'model')\n</code></pre> <pre><code>loaded_model = load_model('model')\n\npredictions = predict_model(loaded_model, data=df)\ndisplay(predictions)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#summary","title":"Summary","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#by-following-these-steps-insurance-companies-can-develop-a-predictive-model-for-insurance-costs-using-practicus-ais-automl-capabilities-this-approach-reduces-the-need-for-extensive-manual-modeling-enabling-insurers-to-efficiently-adapt-to-changing-market-conditions-and-customer-profiles","title":"By following these steps, insurance companies can develop a predictive model for insurance costs using Practicus AI's AutoML capabilities. This approach reduces the need for extensive manual modeling, enabling insurers to efficiently adapt to changing market conditions and customer profiles.","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#bank_marketingsnippetslabel_encoderpy","title":"bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#bank_marketingsnippetsone_hotpy","title":"bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#model_trackingmodel_driftsmodelpy","title":"model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#sparkmlmodeljson","title":"sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#sparkmlmodelpy","title":"sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#streamlined_model_deploymentmodelpy","title":"streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#xgboostmodelpy","title":"xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/01_AutoML/#xgboostmodel_custom_dfpy","title":"xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Preprocess Tut | Next: Deploy Model</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/","title":"Deploy Model","text":"<p>To deploy your model along with the <code>model.json</code> file on Practicus AI you can checkout the sample down below:</p> <pre><code>import practicuscore as prt\n\ndeployment_key = \"...\" # The deployment name of k8s. Can be find at \"Practicus AI Admin Console\"\nprefix = \"...\" # The prefix that assigned to your deployment\nmodel_name = \"...\" # Your model name\nmodel_dir = None  # None for current directory\n# _model_dir = \"/home/ubuntu/models/ice_cream_revenue/...\"\n\nprt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=model_dir\n)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#bank_marketingsnippetslabel_encoderpy","title":"bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#bank_marketingsnippetsone_hotpy","title":"bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#model_trackingmodel_driftsmodelpy","title":"model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#sparkmlmodeljson","title":"sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#sparkmlmodelpy","title":"sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#streamlined_model_deploymentmodelpy","title":"streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#xgboostmodelpy","title":"xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/02_deploy_model/#xgboostmodel_custom_dfpy","title":"xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: AutoML | Next: Shap Analysis</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/","title":"Shap Analysis","text":"<pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport shap\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.tree import plot_tree\n</code></pre> <pre><code># load the csv file as a data frame\ndf = pd.read_csv('samples/iris.csv')\n</code></pre> <pre><code>label_encoder = LabelEncoder()\ndf['species'] = label_encoder.fit_transform(df['species'])\n</code></pre> <pre><code># Separate Features and Target Variables\nX = df.drop(columns='species')\ny = df['species']\n</code></pre> <pre><code># Create Train &amp; Test Data\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,\n                                                    stratify =y,\n                                                    random_state = 13)\n\n# Build the model\nrf_clf = RandomForestClassifier(max_features=2, n_estimators =100 ,bootstrap = True)\n\nrf_clf.fit(X_train, y_train)\n</code></pre> <pre><code>y_pred = rf_clf.predict(X_test)\nprint(classification_report(y_pred, y_test))\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#iris-data-set-feature-importance","title":"Iris Data Set Feature Importance","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#overview","title":"Overview","text":"<p>The following visualization presents a ranked bar chart depicting the relative importance of each feature used by our machine learning model to predict Iris flower species. In this analysis, we observe the features derived from the dimensions of the flower's petals and sepals: <code>petal_width</code>, <code>petal_length</code>, <code>sepal_length</code>, and <code>sepal_width</code>.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#interpretation","title":"Interpretation","text":"<ul> <li>Petal Width (petal_width): This feature has the highest relative importance score, indicating its strong predictive power in distinguishing between Iris species. The model heavily relies on petal width, suggesting that this attribute significantly influences the model's decision-making process.</li> <li>Petal Length (petal_length): Following petal width, petal length also shows substantial influence on the model's predictions. Its prominence implies that the length of the petal is another defining characteristic in species classification.</li> <li>Sepal Length (sepal_length) &amp; Sepal Width (sepal_width): These features hold less significance compared to petal measurements. Their lower importance scores may reflect their reduced discriminative ability in the context of this specific model and dataset.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#implications-for-model-refinement","title":"Implications for Model Refinement","text":"<p>We leverage this feature importance chart to guide feature selection and model simplification efforts. In high-dimensional datasets, reducing model complexity and computational load by pruning less significant features is crucial. Additionally, the chart enhances model interpretability by highlighting which features predominantly drive predictions, providing insights into potential dependencies within the dataset.</p> <p>For instance, if petal measurements are more influential than sepal measurements, it could indicate that petals play a more decisive role in identifying the Iris species. </p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#conclusion","title":"Conclusion","text":"<p>This graphical analysis is instrumental for data-driven decision-making, ensuring that our model is both efficient and interpretable. By focusing on the most informative features, we can streamline the model while maintaining or even enhancing its accuracy.</p> <pre><code>importances = rf_clf.feature_importances_\nindices = np.argsort(importances)\nfeatures = df.columns\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='y', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n</code></pre> <pre><code># compute SHAP values\nexplainer = shap.TreeExplainer(rf_clf)\nshap_values = explainer.shap_values(X)\n\nclass_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#random-forest-component-analysis-decision-tree-visualization","title":"Random Forest Component Analysis: Decision Tree Visualization","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#overview_1","title":"Overview","text":"<p>This visualization illustrates a single decision tree from a Random Forest classifier trained on the Iris dataset. It's a visual representation of how the algorithm makes decisions and classifies different Iris species: setosa, versicolor, and virginica.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#specific-observations","title":"Specific Observations","text":"<ul> <li>The first split is made with the condition <code>petal_width &lt;= 0.75</code>, perfectly separating Iris setosa from the other species with a Gini score of 0.0 and 25 samples at the node.</li> <li>Subsequent splits focus on distinguishing Iris versicolor and Iris virginica, proceeding until reaching nodes with lower Gini scores.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#importance-for-model-interpretation","title":"Importance for Model Interpretation","text":"<ul> <li>Such visualizations are crucial for understanding the decision-making process of the model and determining which features are most influential during the classification task.</li> <li>The explainable nature of decision trees allows us to clearly communicate how the model works and when specific features become significant, enhancing the transparency and reliability of the AI model.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#implications-for-stakeholders","title":"Implications for Stakeholders","text":"<ul> <li>The clear delineation of decision paths provides stakeholders with insight into the model's reasoning, facilitating trust in the predictions made by the AI system.</li> <li>It underscores the model's dependence on petal measurements, potentially informing feature engineering and data collection priorities for future modeling efforts.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#conclusion_1","title":"Conclusion","text":"<p>This decision tree is a testament to the power of Random Forest in handling complex classification tasks. By breaking down the decision process step by step, we gain a granular understanding of feature importance and model behavior, laying a foundation for informed model refinement and application.</p> <pre><code>from sklearn.tree import plot_tree\n\n\nfeature_names = list(X_train.columns)\n\n# Select one of the trees from your random forest\ntree_to_plot = rf_clf.estimators_[0]\n\n# Plot the selected tree\nfig = plt.figure(figsize=(25,20))\n_ = plot_tree(tree_to_plot,\n              feature_names=feature_names,\n              class_names=class_names,\n              filled=True)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#shap-value-summary-feature-importance-for-iris-classification","title":"SHAP Value Summary: Feature Importance for Iris Classification","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#overview_2","title":"Overview","text":"<p>This graph utilizes SHAP (SHapley Additive exPlanations) values to provide a summary of feature importance within our Iris species classification model. The length of the bars represents the average impact of each feature on the model's predictions, across all instances in the dataset.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#detailed-feature-contributions","title":"Detailed Feature Contributions","text":"<ul> <li>Petal Width (petal_width): Stands out as the feature with the most significant positive impact on model output, particularly for Iris-virginica predictions. The prominence of this bar suggests that petal width is a critical factor in the classification.</li> <li>Petal Length (petal_length): Exhibits a notable positive influence as well, especially for Iris-setosa. Its impact underlines the importance of petal length in distinguishing this particular species.</li> <li>Sepal Measurements (sepal_length and sepal_width): While having a lesser effect compared to petal features, these still contribute to the model's decision-making process, with sepal_length showing some influence on Iris-versicolor classification.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#model-insight-and-adjustments","title":"Model Insight and Adjustments","text":"<ul> <li>This visualization is a powerful tool for identifying potential biases or over-reliance on specific features. The predominance of petal-related features may suggest the need for balance through feature engineering or model hyperparameter tuning.</li> <li>The insight provided by this summary plot is critical for enhancing model fairness, balance, and ultimately, the trustworthiness of its predictions.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#conclusion_2","title":"Conclusion","text":"<p>The SHAP summary plot offers an in-depth understanding of how each feature influences the classification model. It is essential for developers and stakeholders to make data-driven decisions regarding feature selection and to grasp the relative importance of attributes within the dataset.</p> <pre><code>shap.summary_plot(shap_values, X.values, plot_type=\"bar\", class_names= class_names, feature_names = X.columns)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#shap-summary-plot-for-iris-dataset-model-interpretation","title":"SHAP Summary Plot for Iris Dataset Model Interpretation","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#overview_3","title":"Overview","text":"<p>This SHAP (SHapley Additive exPlanations) summary plot provides a visual representation of the feature impact within our classification model for the Iris dataset. It quantifies the marginal contribution of each feature to the prediction made by the model, offering insights into the decision-making process.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#interpretation-of-shap-values","title":"Interpretation of SHAP Values","text":"<ul> <li>Positive and Negative Impacts: The distribution of SHAP values on the x-axis reveals how each feature affects the model output for individual observations. Red points indicate higher feature values, while blue points represent lower values, demonstrating the directional impact of features on the model output.</li> <li>Feature Contributions: For instance, <code>petal_width</code> is shown to have a predominantly positive impact on the model's predictions\u2014most red points lie to the right of the zero line, suggesting that higher petal width values tend to increase the likelihood of a particular Iris species prediction.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#data-point-analysis","title":"Data Point Analysis","text":"<ul> <li>Each point on the graph corresponds to a unique observation in the dataset. The spread of these points allows us to discern how the model differentiates between the samples, particularly noting the variability of effects across the range of feature values.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#utility-of-shap-summary","title":"Utility of SHAP Summary","text":"<ul> <li>Model Interaction: The SHAP values elucidate interactions between features and their directional influence on predictions, offering a reliable method to transparently showcase how feature variations influence the model's decisions.</li> <li>Insights for Model Improvement: This analysis is instrumental in enhancing our understanding of the model and explaining the heterogeneity and complexity present in the predictions. It aids in identifying areas for model improvement, guiding feature engineering, and ensuring robust prediction performance.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#conclusion_3","title":"Conclusion","text":"<p>By employing the SHAP summary plot, we provide a granular view of feature influences, enhancing interpretability and trust in our model. It serves as a valuable tool for stakeholders alike, enabling data-driven decision-making and promoting a thorough comprehension of the model's predictive dynamics.</p> <pre><code>shap.summary_plot(shap_values[1], X.values, feature_names = X.columns)\n</code></pre> <pre><code>shap.summary_plot(shap_values[0], X.values, feature_names = X.columns)\n</code></pre> <pre><code>shap.summary_plot(shap_values[2], X.values, feature_names = X.columns)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#shap-dependence-plot-sepal-lengths-influence-on-iris-classification","title":"SHAP Dependence Plot: Sepal Length's Influence on Iris Classification","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#overview_4","title":"Overview","text":"<p>This SHAP dependence plot showcases the relationship between <code>sepal_length</code> and the model output, while also highlighting the impact of another feature, <code>sepal_width</code>, using a color gradient. We analyze how variations in sepal dimensions influence the classification predictions made by the model.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#key-observations","title":"Key Observations","text":"<ul> <li>Sepal Length Impact: As <code>sepal_length</code> increases, we observe a general trend of decreasing SHAP values, indicating a potentially negative influence on the model's confidence in predicting a particular Iris species.</li> <li>Interaction Effect: The color coding represents the <code>sepal_width</code> values, with warmer colors (red) indicating larger sepal widths. Notably, data points with larger <code>sepal_width</code> often correspond to higher SHAP values, suggesting that a wider sepal might counteract the negative impact of longer sepal length.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#insights-for-feature-interaction","title":"Insights for Feature Interaction","text":"<ul> <li>This plot allows us to discern not only the individual effects of features but also how they might interact with each other. For instance, while longer sepals (<code>sepal_length</code>) tend to decrease prediction confidence, this effect might be moderated by the width of the sepals (<code>sepal_width</code>).</li> <li>The variability in SHAP values across different <code>sepal_length</code> measurements, especially when colored by <code>sepal_width</code>, provides an understanding of how feature combinations affect model predictions.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#implications-for-model-refinement_1","title":"Implications for Model Refinement","text":"<ul> <li>Such insights are valuable for stakeholders when considering how to optimize features and adjust the model. </li> <li>Recognizing the influence of feature interactions is crucial for developing more robust and accurate classification models and can lead to more nuanced data preprocessing and feature engineering strategies.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#conclusion_4","title":"Conclusion","text":"<p>The dependence plot is a vital interpretability tool, allowing stakeholders to grasp the complex dynamics of feature interactions within the model. This understanding is imperative for fine-tuning the model to enhance predictive performance and ensure that it generalizes well to new data.</p> <pre><code>shap.dependence_plot(0, shap_values[0], X.values, feature_names=X.columns)\n</code></pre> <pre><code>shap.dependence_plot(1, shap_values[0], X.values, feature_names=X.columns)\n</code></pre> <pre><code>shap.dependence_plot(2, shap_values[0], X.values, feature_names=X.columns)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#shap-waterfall-plot-analysis-for-individual-prediction","title":"SHAP Waterfall Plot Analysis for Individual Prediction","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#overview_5","title":"Overview","text":"<p>The displayed SHAP waterfall plot is an interpretative tool used to break down the contribution of each feature to a specific prediction made by our machine learning model. It details the individual and cumulative impact of features on a single prediction.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#feature-contributions-explained","title":"Feature Contributions Explained","text":"<ul> <li>Petal Measurements (petal_width and petal_length): These features exhibit a strong positive effect on the model's output. </li> <li>Sepal Measurements (sepal_length and sepal_width): While <code>sepal_length</code> shows a small positive contribution, <code>sepal_width</code> has a slight negative influence. The limited impact of <code>sepal_width</code> in this instance may suggest it plays a lesser role in the classification for this specific prediction.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#addressing-feature-impact","title":"Addressing Feature Impact","text":"<ul> <li>The substantial influence of petal measurements raises concerns about the model's reliance on a narrow set of features, which could lead to overfitting. This phenomenon occurs when a model learns patterns specific to the training data, impacting its ability to generalize to unseen data.</li> <li>To mitigate over-reliance and enhance generalization, regularization techniques may be employed, or the model could be adjusted to give more weight to other features in the dataset.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#conclusion_5","title":"Conclusion","text":"<p>Understanding which features the model prioritizes and the potential outcomes of this prioritization is central to data scientists' efforts to enhance the model's reliability and applicability. These analyses are crucial for maintaining a balanced performance of the model, ensuring that it remains robust across different scenarios.</p> <pre><code>row = 8\nshap.waterfall_plot(shap.Explanation(values=shap_values[0][row], \n                                        base_values=explainer.expected_value[0], data=X_test.iloc[row],  \n                                        feature_names=X_test.columns.tolist()))\n</code></pre> <pre><code>row = 42\nshap.waterfall_plot(shap.Explanation(values=shap_values[0][row], \n                                        base_values=explainer.expected_value[0], data=X_test.iloc[row],  \n                                        feature_names=X_test.columns.tolist()))\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#shap-force-plot","title":"SHAP Force Plot","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#this-shap-force-plot-illustrates-the-impact-of-each-feature-on-our-models-classification-prediction-the-base-value-represents-our-average-reference-prediction-while-the-fx-100-value-is-the-definitive-prediction-made-by-our-model-for-this-instance-red-bars-petal-length-45-petal-width-13-and-sepal-length-57-indicate-factors-that-increase-the-models-prediction-these-three-features-have-elevated-the-prediction-with-petal-length-having-the-most-significant-impact-sepal-width-28-presents-a-slight-negative-effect-causing-a-minimal-decrease-in-the-models-prediction-overall-in-light-of-these-values-our-model-robustly-classifies-the-given-data-point-into-a-specific-category","title":"This SHAP Force Plot  illustrates the impact of each feature on our model's classification prediction. The \"base value\" represents our average reference prediction, while the \"f(x) = 1.00\" value is the definitive prediction made by our model for this instance. Red bars (petal length: 4.5, petal width: 1.3, and sepal length: 5.7) indicate factors that increase the model's prediction. These three features have elevated the prediction, with petal length having the most significant impact. Sepal width (2.8) presents a slight negative effect, causing a minimal decrease in the model's prediction. Overall, in light of these values, our model robustly classifies the given data point into a specific category.","text":"<pre><code>shap.plots.force(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0, :], matplotlib = True)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#shap-decision-plot","title":"SHAP Decision Plot","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#the-shap-decision-plot-visualizes-the-impact-of-individual-features-on-the-output-of-a-machine-learning-model-this-particular-plot-is-generated-using-the-shapdecision_plot-function-with-parameters-corresponding-to-the-expected-value-of-the-model-shap-values-for-a-set-of-predictions-and-feature-names-from-the-test-dataset","title":"The SHAP decision plot visualizes the impact of individual features on the output of a machine learning model. This particular plot is generated using the shap.decision_plot function with parameters corresponding to the expected value of the model, SHAP values for a set of predictions, and feature names from the test dataset:","text":"<p>X-axis: The model output value after accounting for the impact of each feature. Lines: Represent the shift in the model output due to the impact of the corresponding feature from the base value. Petal length: Shows a consistently negative impact on the model output. Petal width: Generally contributes towards an increase in the model output. Sepal length and sepal width: Exhibit variable impacts on the model output. This plot is instrumental in pinpointing the most influential features for a prediction and understanding their collective impact on the final model output.</p> <pre><code>#For class 0\nshap.decision_plot(explainer.expected_value[0], shap_values[0], X_test.columns)\n</code></pre> <pre><code># For class 1\nshap.decision_plot(explainer.expected_value[1], shap_values[1], X_test.columns)\n</code></pre> <pre><code># For class 2\nshap.decision_plot(explainer.expected_value[2], shap_values[2], X_test.columns)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#bank_marketingsnippetslabel_encoderpy","title":"bank_marketing/snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#bank_marketingsnippetsone_hotpy","title":"bank_marketing/snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#model_trackingmodel_driftsmodelpy","title":"model_tracking/model_drifts/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#sparkmlmodeljson","title":"sparkml/model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#sparkmlmodelpy","title":"sparkml/model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#streamlined_model_deploymentmodelpy","title":"streamlined_model_deployment/model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#xgboostmodelpy","title":"xgboost/model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/03_shap_analysis/#xgboostmodel_custom_dfpy","title":"xgboost/model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Deploy Model | Next: SparkML Ice Cream</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/bank_marketing/bank_marketing/","title":"Bank Marketing","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/bank_marketing/bank_marketing/#bank-marketing","title":"Bank marketing","text":"<p>A banking company wants to develop a model to predict the customers who will subscribe to time deposits and also wants to reach customers who are likely to subscribe to time deposits by using the call center resource correctly.</p> <p>In the data set to be studied, variables such as demographic information, balance information and previous campaign information of the customers will be used to predict whether they will subscribe to time deposits.</p> <p>Using the App - You can open the dataset in the Practicus AI  by loading all data - Then in the analysis phase, you can start with profiling the data - Then Graph &gt; Boxplot &gt; Age - Groupby &gt; Age, Job,  Balance Mean &amp; Median - Analyze &gt; Graph &gt; Plot -&gt; Job -&gt; Balance Mean Add Layer, Balance Median Add Layer</p> <p>Using Notebok - You can find detailed preprocessing steps in the notebook made with SDK - The main idea here is that the model is built by filtering the -1s in the pdays variable, that is, they are not included in the model. - In addition, the poutcome variable should be deleted to prevent Data Leakage. - Then the model Feature Selection was selected as 95% and the Setup params were set to fix_imbalance: True.</p> <pre><code>import practicuscore as prt\n\nworker = prt.get_local_worker()\n</code></pre> <pre><code>data_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/bank_marketing.csv\"\n}\n\nproc = worker.load(data_conn) \nproc.show_head() \n</code></pre> <pre><code>proc.delete_columns(['poutcome']) \n</code></pre> <pre><code>proc.run_snippet( \n    'one_hot', \n    text_col_list=['marital', 'default', 'housing', 'loan', ],  \n    max_categories=25,  \n    dummy_option='Drop First Dummy',  \n    result_col_suffix=[],  \n    result_col_prefix=[],  \n) \n</code></pre> <pre><code>proc.delete_columns(['default']) \n</code></pre> <pre><code>proc.delete_columns(['housing', 'loan']) \n</code></pre> <pre><code>proc.delete_columns(['marital']) \n</code></pre> <pre><code>proc.run_snippet( \n    'label_encoder', \n    text_col_list=['job', 'education', 'contact', 'month', 'deposit', ],  \n) \n</code></pre> <pre><code>filter_expression = ''' \ncol[pdays] != -1 \n''' \nproc.filter(filter_expression) \n</code></pre> <pre><code>proc.wait_until_done()\nproc.show_logs()\n</code></pre> <pre><code>df = proc.get_df_copy()\ndisplay(df)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/bank_marketing/bank_marketing/#buildign-a-model-using-automl","title":"Buildign a model using AutoML","text":"<p>The below code is generated. You can update the code to fit your needs, or re-create it by building a model with Practicus AI app first and then view it's jupter notebook oncethe model building is completed.</p> <pre><code>from pycaret.classification import ClassificationExperiment, load_model, predict_model\n\nexp = ClassificationExperiment()\n</code></pre> <pre><code>experiment_name = 'Bank-marketing'\nexperiment_tracking_service = None\nassert experiment_tracking_service, \"Pelase select an experiment tracking service, or skip this cell\"\nprt.experiments.configure(service_name=experiment_tracking_service, experiment_name=experiment_name)\n</code></pre> <pre><code>setup_params = {'fix_imbalance': True}\n</code></pre> <pre><code>exp.setup(data=df, target='deposit', session_id=7272, \n          log_experiment=True, experiment_name=experiment_name, **setup_params)\n</code></pre> <pre><code>best_model = exp.compare_models()\n</code></pre> <pre><code>tune_params = {}\n</code></pre> <pre><code>tuned_model = exp.tune_model(best_model, **tune_params)\n</code></pre> <pre><code>final_model = exp.finalize_model(tuned_model)\n</code></pre> <pre><code>predictions = exp.predict_model(final_model, data=df)\ndisplay(predictions)\n</code></pre> <pre><code>exp.save_model(final_model, 'model')\n</code></pre> <pre><code>loaded_model = load_model('model')\n\npredictions = predict_model(loaded_model, data=df)\ndisplay(predictions)\n</code></pre> <pre><code>deployment_key = None\nassert deployment_key, \"Please select a deployment key\"\nprefix = 'models'\nmodel_name = 'my-bank-marketing-model'\nmodel_dir = None  # Current dir\n</code></pre> <pre><code># Deploy to current Practicus AI region\nprt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=model_dir\n)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/bank_marketing/bank_marketing/#prediction-by-using-model-api","title":"Prediction by using model API","text":"<pre><code>region = prt.current_region()\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code>import requests \nimport pandas as pd\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/bank_marketing/bank_marketing/#prediction-by-using-sdk","title":"Prediction by using SDK","text":"<pre><code>proc.predict( \n    api_url='https://dev.practicus.io/models/my-bank-marketing-model/', \n    column_names=['age', 'job', 'education', 'balance', 'contact', 'day', 'month',\n        'duration', 'campaign', 'pdays', 'previous','marital_married', \n        'marital_single', 'default_yes', 'housing_yes','loan_yes'], \n    new_column_name='predicted_deposit' \n) \n</code></pre> <pre><code>df_predicted = proc.get_df_copy()\n</code></pre> <pre><code>df_predicted['predicted_deposit'].head()\n</code></pre> <pre><code>proc.kill()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/bank_marketing/bank_marketing/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/bank_marketing/bank_marketing/#snippetslabel_encoderpy","title":"snippets/label_encoder.py","text":"<pre><code>def label_encoder(df, text_col_list: list[str] | None = None):\n    \"\"\"\n    Applies label encoding to specified categorical columns or all categorical columns in the dataframe if none are specified.\n    :param text_col_list: Optional list of column names to apply label encoding. If None, applies to all categorical columns.\n    \"\"\"\n    from sklearn.preprocessing import LabelEncoder\n\n    le = LabelEncoder()\n\n    # If text_col_list is provided, use it; otherwise, select all categorical columns\n    if text_col_list is not None:\n        categorical_cols = text_col_list\n    else:\n        categorical_cols = [col for col in df.columns if col.dtype == 'O']\n\n    # Apply Label Encoding to each specified (or detected) categorical column\n    for col in categorical_cols:\n        # Check if the column exists in the DataFrame to avoid KeyError\n        if col in df.columns:\n            df[col] = le.fit_transform(df[col])\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n\n    return df\n\n\nlabel_encoder.worker_required = True\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/bank_marketing/bank_marketing/#snippetsone_hotpy","title":"snippets/one_hot.py","text":"<pre><code>from enum import Enum\n\n\nclass DummyOption(str, Enum):\n    DROP_FIRST = \"Drop First Dummy\"\n    KEEP_ALL = \"Keep All Dummies\"\n\n\ndef one_hot(df, text_col_list: list[str] | None,\n            max_categories: int = 25, dummy_option: DummyOption = DummyOption.KEEP_ALL,\n            result_col_suffix: list[str] | None = None, result_col_prefix: list[str] | None = None):\n    \"\"\"\n    Applies one-hot encoding to specified columns in the DataFrame. If no columns are specified,\n    one-hot encoding is applied to all categorical columns that have a number of unique categories\n    less than or equal to the specified max_categories. It provides an option to either drop the\n    first dummy column to avoid multicollinearity or keep all dummy columns.\n\n    :param text_col_list: List of column names to apply one-hot encoding. If None, applies to all\n                          suitable categorical columns.\n    :param max_categories: Maximum number of unique categories in a column to be included for encoding.\n    :param dummy_option: Specifies whether to drop the first dummy column (DROP_FIRST) or keep all\n                         (KEEP_ALL).\n    :param result_col_suffix: Suffix for the new column where the suppressed data will be stored.\n    :param result_col_prefix: Prefix for the new column where the suppressed data will be stored.\n    \"\"\"\n    import pandas as pd\n\n    if text_col_list is None:\n        text_col_list = [col for col in df.columns if df[col].dtype == 'object' and df[col].nunique() &lt;= max_categories]\n\n    for col in text_col_list:\n        dummies = pd.get_dummies(df[col], prefix=(result_col_prefix if result_col_prefix else col),\n                                 drop_first=(dummy_option == DummyOption.DROP_FIRST))\n        dummies = dummies.rename(columns=lambda x: f'{x}_{result_col_suffix}' if result_col_suffix else x)\n\n        df = pd.concat([df, dummies], axis=1)\n\n    return df\n</code></pre> <p>Previous: XGBoost | Next: Streamlined Model Deployment</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/experiment_tracking/01_experiment_tracking_logging/","title":"Experiment Tracking Logging","text":"<pre><code>import practicuscore as prt\nimport os\nimport mlflow\n</code></pre> <pre><code>service_name = \"My Mlflow Service\"\n# You need to configure using the service unique key, you can find your key on the \"Practicus AI Admin Console\" \nservice_key =  \"\"\n# Optionally, you can provide experime name to create a new experiement while configuring\nexperiment_name = None\n</code></pre> <pre><code>prt.experiments.configure(service_name=service_name, service_key=service_key, experiment_name=experiment_name)\n</code></pre> <pre><code># Set experiment name, if you haven't already while configuring the service\nmlflow.set_experiment(\"my experiment\")\n\n# Prefer unique run names, or leave empty to auto generate unique names\nrun_name = \"My ML experiment run 123\"\n\n# Start an MLflow run and log params, metrics and artifacts\nwith mlflow.start_run(run_name=run_name):\n    # Log parameters\n    mlflow.log_param(\"param1\", 5)\n    mlflow.log_param(\"param2\", \"test\")\n\n    # Log metrics\n    mlflow.log_metric(\"metric1\", 0.85)\n\n    # Create an artifact (e.g., a text file)\n    artifact_path = \"artifacts\"\n    if not os.path.exists(artifact_path):\n        os.makedirs(artifact_path)\n    file_path = os.path.join(artifact_path, \"output.txt\")\n\n    with open(file_path, \"w\") as f:\n        f.write(\"This is a test artifact.\")\n\n    # Log the artifact\n    mlflow.log_artifacts(artifact_path)\n\n    # Optional: Print the run ID\n    print(\"Run ID:\", mlflow.active_run().info.run_id)\n\n# Explicitly close the active MLflow run, if you are not using the above with keyword\n# mlflow.end_run()\n</code></pre> <p>Previous: SparkML Ice Cream | Next: Experiment Tracking Model Training</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/experiment_tracking/02_experiment_tracking_model_training/","title":"Experiment Tracking Model Training","text":"<pre><code>import practicuscore as prt\nimport os\nimport mlflow\nimport xgboost as xgb\nimport cloudpickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n</code></pre> <pre><code>service_name = \"My Mlflow Service\"\n# You need to configure using the service unique key, you can find your key on the \"Practicus AI Admin Console\" \nservice_key = ''  \n# Optionally, you can provide experime name to create a new experiement while configuring\nexperiment_name = None\n</code></pre> <pre><code>prt.experiments.configure(service_name=service_name, service_key=service_key, experiment_name=experiment_name)\n</code></pre> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/ice_cream.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn) \n\ndata = proc.get_df_copy()\ndata.head()\n</code></pre> <pre><code># Set experiment name, if you haven't already while configuring the service\nmlflow.set_experiment(\"XGBoost Experiment\")\n\n# Loading the dataset\nX = data.Temperature\ny = data.Revenue\n</code></pre> <pre><code># Test and Train split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# XGBoost parameters\nparams = {\n    'max_depth': 3,\n    'eta': 0.1,\n    'objective': 'reg:squarederror',\n}\n</code></pre> <pre><code># Creation of DMatrix\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n</code></pre> <pre><code># Training of model by using mlflow\nwith mlflow.start_run():\n    mlflow.log_params(params)\n    model = xgb.train(params, dtrain, num_boost_round=200)\n    # Prediction process\n    predictions = model.predict(dtest)\n    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n    mlflow.log_metric(\"rmse\", rmse)\n    # Saving the model in MLFlow\n    artifact_path = \"model\"\n    if not os.path.exists(artifact_path):\n        os.makedirs(artifact_path)\n    model_path = os.path.join(artifact_path, \"xgboost_model.pkl\")\n    with open(model_path, \"wb\") as f:\n        cloudpickle.dump(model, f)\n    # Saving the serialised model in MLflow\n    mlflow.log_artifacts(artifact_path)\n    mlflow.log_artifacts(artifact_path)\n    # Pringting out the run id\n    print(\"Run ID:\", mlflow.active_run().info.run_id)\n</code></pre> <pre><code># Ending MLFlow\nmlflow.end_run()\n</code></pre> <p>Previous: Experiment Tracking Logging | Next: Model Drift</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/","title":"Model Observability and Monitoring","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#scenario-model-drift","title":"Scenario: Model Drift","text":"<p>In this notebook, we'll deploy a model on an insurance dataset to make two predictions. Introducing intentional drifts in the BMI and Age columns, we aim to observe their impact on the model's predictions.</p> <ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Train and deploy a model on insurance dataset</p> </li> <li> <p>Making precitions with deployed model</p> </li> <li> <p>Multiplying the BMI and Age columns to create Drifts on features and predictions</p> </li> <li> <p>Observing the model drift plots</p> </li> </ol>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#model-development","title":"Model Development","text":"<p>Loading and preparing the dataset</p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code># Loading the dataset to worker\n\nimport practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn) \n\nproc.show_head()\n</code></pre> <pre><code># Pre-process\n\nproc.categorical_map(column_name='sex', column_suffix='category') \nproc.categorical_map(column_name='smoker', column_suffix='category') \nproc.categorical_map(column_name='region', column_suffix='category') \nproc.delete_columns(['region', 'smoker', 'sex']) \n</code></pre> <pre><code># Taking the dataset into csv\n\ndf = proc.get_df_copy()\ndf.head()\n</code></pre> <p>Model Training</p> <pre><code>X = df.drop('charges', axis=1)\ny = df['charges']\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <pre><code>import xgboost as xgb\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('model', xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100))\n])\npipeline.fit(X_train, y_train)\n</code></pre> <pre><code># Exporting the model\n\nimport cloudpickle\n\nwith open('model.pkl', 'wb') as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#model-deployment","title":"Model Deployment","text":"<pre><code># Please ask for a deployment key from your admin.\ndeployment_key = \"\"\nassert deployment_key, \"Please select a deployment_key\"\nprefix = \"models\"\nmodel_name = \"custom-insurance-test\"\nmodel_dir= None  # Current dir \n</code></pre> <pre><code>prt.models.deploy(\n    deployment_key=deployment_key, \n    prefix=prefix, \n    model_name=model_name, \n    model_dir=model_dir\n)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#prediction","title":"Prediction","text":"<pre><code># Loading the dataset to worker\n\nimport practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn) \n\nproc.categorical_map(column_name='sex', column_suffix='category') \nproc.categorical_map(column_name='smoker', column_suffix='category') \nproc.categorical_map(column_name='region', column_suffix='category') \nproc.delete_columns(['region', 'smoker', 'sex']) \n\ndf = proc.get_df_copy()\ndf.head()\n</code></pre> <pre><code># Let's construct the REST API url.\n# Please replace the below url with your current Practicus AI address\n# e.g. http://practicus.company.com\npracticus_url = \"\"  \nassert practicus_url, \"Please select practicus_url\"\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"https://{practicus_url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code>import requests\nimport pandas as pd\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#after-you-make-the-first-prediction-please-wait-for-5-minutes-to-see-a-clearer-picture-on-the-drift-plot","title":"After you make the first prediction, please wait for 5 minutes to see a clearer picture on the drift plot","text":"<p>When we look at Model Drifts Dashboard at Grafana we will see plots smilar to down below:</p> <p>Prediction column: </p> <p>Features: </p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#hand-made-model-drift","title":"Hand-Made Model Drift","text":"<pre><code>df['age'] = df['age'] * 2\n</code></pre> <pre><code>df['bmi'] = df['bmi'] * 3\n</code></pre> <pre><code>display(df)\n</code></pre> <pre><code>headers = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#after-you-make-the-second-prediction-please-wait-for-2-minutes-to-see-a-clearer-picture-on-the-drift-plot","title":"After you make the second prediction, please wait for 2 minutes to see a clearer picture on the drift plot","text":"<p>When we look at Model Drifts Dashboard at Grafana we will see plots smilar to down below:</p> <p>Prediction column: </p> <p>Features: </p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/model_tracking/model_drifts/Model_Drift/#modelpy","title":"model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nfrom starlette.exceptions import HTTPException\nimport joblib \n\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)  \n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Experiment Tracking Model Training | Next: XGBoost</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/","title":"SparkML Ice Cream","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#end-to-end-sparkml-model-development-and-deployment","title":"End-to-end SparkML Model development and deployment","text":"<p>This sample notebook outlines the process of deploying a SparkML model on the Practicus AI platform and making predictions from the deployed model using various methods.</p> <pre><code>import pandas as pd\n\n# Step 1: Read CSV with Pandas\ndata = pd.read_csv(\"/home/ubuntu/samples/ice_cream.csv\")\n</code></pre> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.linalg import Vectors\n\n# Step 2: Convert to Spark DataFrame with Explicit Schema\nspark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n\n# Define schema for Spark DataFrame\nschema = StructType([\n    StructField(\"label\", DoubleType(), True),\n    StructField(\"features\", DoubleType(), True)\n])\n\nspark_data = spark.createDataFrame(\n    data.apply(lambda row: (float(row['Revenue']), Vectors.dense(float(row['Temperature']))), axis=1),\n    schema=[\"label\", \"features\"]\n)\n</code></pre> <pre><code>from pyspark.ml.regression import LinearRegression\n\n# Step 3: Train Linear Regression Model\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\nmodel = lr.fit(spark_data)\n</code></pre> <pre><code># Step 4: Make Predictions\npredictions = model.transform(spark_data)\n\npredictions.select(\"features\", \"label\", \"prediction\").show()\n</code></pre> <pre><code># Step 5: Save Model\nmodel_name = \"ice_cream_sparkml_model\"\nmodel.save(model_name)\n</code></pre> <pre><code># Optional: Stop Spark session when done\nspark.stop()\n</code></pre> <pre><code># Prediction, you can run this in another notebook \n\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\ndef predict(df: pd.DataFrame) -&gt; pd.DataFrame:\n    spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    model = LinearRegressionModel.load(model_name)\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre> <pre><code>import pandas as pd\n\ndata = pd.read_csv(\"/home/ubuntu/samples/ice_cream.csv\")\n</code></pre> <pre><code>prediction = predict(data)\n\nprediction\n</code></pre> <pre><code># Optional: Stop Spark session when done\nspark.stop()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#deploying-the-sparkml-model","title":"Deploying the SparkML Model","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#step-1-upload-to-object-storage","title":"Step 1) Upload to object storage","text":"<ul> <li>Before you start deploying the model, please upload SparkML model files to the object storage that your Practicus AI model deployment is using.</li> <li>Why? Unlike scikit-learn, XGBoost etc., Spark ML model files are not a file such as model.pkl, but a folder. </li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#sample-object-stroage-cache-folder","title":"Sample object stroage cache folder","text":"<ul> <li>E.g. s3://my-models-bucket/cache/ice_cream_sparkml_model/ice_cream_sparkml_model/ [ add your model folders here ] </li> <li>Why use same folder name twice? ice_cream_sparkml_model/ice_cream_sparkml_model </li> <li>Practicus AI will download all cache files defined in model.json.</li> <li>If you select cache/ice_cream_sparkml_model and don't use the second folder, all of your model files will be downloaded under /var/practicus/cache/</li> <li>If you are caching one model only this would work fine, but if you deploy multiple models their files can override each other.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#step-2-verify-you-practicus-ai-model-host-is-sparkml-compatible","title":"Step 2) Verify you Practicus AI model host is SparkML compatible","text":"<ul> <li>Please make sure you are using a Practicus AI model deployment image with SparkML support.</li> <li>You can use the default SparkML image: ghcr.io/practicusai/practicus-modelhost-sparkml:{add-version-here}</li> <li>Not sure how? Please consult your admin and ask for the deployment key with SparkML.</li> </ul>"},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#step-3-deploy-the-model","title":"Step 3) Deploy the model","text":"<ul> <li>Please follow the below steps to deploy the model as usual</li> </ul> <pre><code>import practicuscore as prt \n\n# Deploying model as an API\n# Please review model.py and model.json files\n\ndeployment_key = \"sparkml\"  # must point to a SparkML compatible model host\nprefix = \"models\"\nmodel_name = \"ice-cream-sparkml\"\nmodel_dir = None  # Current dir\n\nprt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=model_dir    \n)\n</code></pre> <pre><code>region = prt.current_region()\n\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code>token = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <pre><code># Caution: Due to initial Spark session creation process, first prediction can be quite slow.\n\nimport pandas as pd\nimport requests \n\ndf = pd.read_csv(\"/home/ubuntu/samples/ice_cream.csv\")\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#modeljson","title":"model.json","text":"<pre><code>{\n    \"download_files_from\": \"cache/ice_cream_sparkml_model/\",\n    \"_comment\": \"you can also define download_files_to otherwise, /var/practicus/cache is used\"\n}\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/sparkml/sparkml_ice_cream/#modelpy","title":"model.py","text":"<pre><code>import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.sql.types import StructType, StructField, DoubleType\nfrom pyspark.ml.regression import LinearRegressionModel\n\nspark = None\nmodel = None\n\n# Make sure you downloaded the SparkML model files to the correct cache folder\nMODEL_PATH = \"/var/practicus/cache/ice_cream_sparkml_model\"\n\n\nasync def init(*args, **kwargs):\n    global spark, model\n    if spark is None:\n        spark = SparkSession.builder.appName(\"IceCreamRevenuePrediction\").getOrCreate()\n    if model is None:\n        model = LinearRegressionModel.load(MODEL_PATH)\n\n\nasync def predict(df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    # Define schema for Spark DataFrame\n    schema = StructType([\n        StructField(\"features\", DoubleType(), True)\n    ])\n\n    # Convert input Pandas DataFrame to Spark DataFrame\n    spark_data = spark.createDataFrame(\n        df.apply(lambda row: (Vectors.dense(float(row['Temperature'])),), axis=1),\n        schema=[\"features\"]\n    )\n\n    # Make predictions using the Spark model\n    predictions = model.transform(spark_data)\n\n    # Select the relevant columns and convert to Pandas DataFrame\n    predictions_pd = predictions.select(\"features\", \"prediction\").toPandas()\n\n    # Extract the Temperature and predicted Revenue for readability\n    predictions_pd[\"Temperature\"] = predictions_pd[\"features\"].apply(lambda x: x[0])\n    predictions_pd = predictions_pd.rename(columns={\"prediction\": \"predicted_Revenue\"})\n    predictions_pd = predictions_pd[[\"predicted_Revenue\"]]\n\n    return predictions_pd\n</code></pre> <p>Previous: Shap Analysis | Next: Experiment Tracking Logging</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/streamlined_model_deployment/streamlined_model_deployment/","title":"Model Observability and Monitoring","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/streamlined_model_deployment/streamlined_model_deployment/#scenario-model-drift","title":"Scenario: Model Drift","text":"<p>In this notebook, we'll deploy a model on an income dataset. Our main goal within this notebook is deploying pre-procesess into the \"model.pkl\".</p> <ol> <li> <p>Open Jupyter Notebook</p> </li> <li> <p>Preparing pre-process function</p> </li> <li> <p>Preparing train pipeline and deploy a model on income dataset</p> </li> <li> <p>Making precitions with deployed model without making any pre-process</p> </li> </ol>"},{"location":"notebooks/06_other_topics/08_additional_modeling/streamlined_model_deployment/streamlined_model_deployment/#creating-preprocess-and-the-pipeline","title":"Creating Preprocess and the Pipeline","text":"<p>Loading the data set</p> <pre><code>dataset_conn ={\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/income.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(dataset_conn)\n\nproc.show_head()\n</code></pre> <pre><code>df = proc.get_df_copy()\n\ndf.head()\n</code></pre> <p>Creating pre-process function for new features</p> <pre><code>from sklearn.preprocessing import FunctionTransformer\n\ndef add_features(df):    \n\n    for column in df.select_dtypes(include='object'):  # Selecting columns which has type as object\n        mode_value = df[column].mode()[0]  # Find mode\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):  # Selecting columns which has type as in64\n        mean_value = df[column].mean()  # Find median\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):  # Selecting columns which has type as float64\n        mean_value = df[column].mean()  # Find Median\n        df[column] = df[column].fillna(mean_value)\n\n    return df\n\nadd_features_transformer = FunctionTransformer(add_features, validate=False)\n</code></pre> <pre><code>df.columns\n</code></pre> <pre><code>df.dtypes\n</code></pre> <p>Defining categorical and numerical features</p> <pre><code>numeric_features = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\ncategorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n</code></pre> <p>Creating preprocessor object for the pipeline to apply scaling and one hot encoding</p> <pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ])\n</code></pre> <p>Creating the pipeline</p> <pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline(steps=[\n    ('add_features', add_features_transformer),\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier())\n])\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/streamlined_model_deployment/streamlined_model_deployment/#model-training","title":"Model Training","text":"<p>Train test split</p> <pre><code>X = df.drop(['income &gt;50K'], axis=1)\ny = df['income &gt;50K']\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42);\n</code></pre> <p>Fitting the model</p> <pre><code>pipeline.fit(X_train, y_train)\n</code></pre> <pre><code>score = pipeline.score(X_test, y_test)\nprint(f'Accurcy Score of the model: {score}')\n</code></pre> <p>Importing the model by using cloudpickle</p> <pre><code>import cloudpickle\n\nwith open('model.pkl', 'wb') as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre> <pre><code>import joblib\nimport pandas as pd\n\n# Load the saved model\nmodel = joblib.load('model.pkl')\n\n# Making predictions\npredictions = model.predict(X)\n\n# Converting predictions to a DataFrame\npredictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/streamlined_model_deployment/streamlined_model_deployment/#deployment-of-the-model","title":"Deployment of the model","text":"<pre><code># Please ask for a deployment key from your admin.\ndeployment_key = \"\"\nassert deployment_key, \"Please select a deployment_key\"\nprefix = \"models\"\nmodel_name = \"custom-income-test\"\nmodel_dir= None  # Current dir \n</code></pre> <pre><code>prt.models.deploy(\n    deployment_key=deployment_key, \n    prefix=prefix, \n    model_name=model_name, \n    model_dir=model_dir\n)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/streamlined_model_deployment/streamlined_model_deployment/#prediction","title":"Prediction","text":"<pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(dataset_conn) \n\nproc.show_head()\n</code></pre> <pre><code># Let's construct the REST API url.\n# Please replace the below url with your current Practicus AI address\n# e.g. http://practicus.company.com\npracticus_url = \"\"  \nassert practicus_url, \"Please select practicus_url\"\n# *All* Practicus AI model APIs follow the below url convention\napi_url = f\"https://{practicus_url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre> <p>Making predictions without making any pre-process on the income dataset</p> <pre><code>import requests\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre> <pre><code>pred_df.value_counts()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/streamlined_model_deployment/streamlined_model_deployment/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/streamlined_model_deployment/streamlined_model_deployment/#modelpy","title":"model.py","text":"<pre><code>import os\nfrom typing import Optional\nimport pandas as pd\nimport numpy as np\nfrom starlette.exceptions import HTTPException\nimport joblib\n\nmodel_pipeline = None\n\ndef add_features(df):\n    for column in df.select_dtypes(include='object'):\n        mode_value = df[column].mode()[0]\n        df[column] = df[column].fillna(mode_value)\n\n    for column in df.select_dtypes(include='int64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n\n    for column in df.select_dtypes(include='float64'):\n        mean_value = df[column].mean()\n        df[column] = df[column].fillna(mean_value)\n    return df\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise HTTPException(status_code=404, detail=f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: Optional[pd.DataFrame] = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise HTTPException(status_code=500, detail=\"No dataframe received\")\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['income &gt;50K'])\n\n    return predictions_df\n</code></pre> <p>Previous: Bank Marketing | Next: Advanced Gpu</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/","title":"XGBoost","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#end-to-end-custom-xgboost-model-development-and-deployment","title":"End-to-end custom XGBoost Model development and deployment","text":"<p>This sample notebook outlines the process of deploying a custom XGBoost model on the Practicus AI platform and making predictions from the deployed model using various methods.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#data-preparation","title":"Data Preparation","text":"<p>We will be using Practicus AI to prepare data, but you can also do it manually using just Pandas. The rest of the model building and deployment steps would not change. </p> <pre><code>data_set_conn = {\n    \"connection_type\": \"WORKER_FILE\",\n    \"file_path\": \"/home/ubuntu/samples/insurance.csv\"\n}\n</code></pre> <pre><code>import practicuscore as prt\n\nregion = prt.current_region()\nworker = region.get_or_create_worker()\nproc = worker.load(data_set_conn) \n\nproc.show_head()\n</code></pre> <pre><code>proc.categorical_map(column_name='sex', column_suffix='category') \nproc.categorical_map(column_name='smoker', column_suffix='category') \nproc.categorical_map(column_name='region', column_suffix='category') \nproc.delete_columns(['region', 'smoker', 'sex']) \n</code></pre> <pre><code>df = proc.get_df_copy()\ndf.head()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#building-the-model","title":"Building the model","text":"<p>Let's build a model with XGBoost</p> <pre><code>X = df.drop('charges', axis=1)\ny = df['charges']\n</code></pre> <pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre> <pre><code>import xgboost as xgb\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('model', xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100))\n])\npipeline.fit(X_train, y_train)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#saving-your-model","title":"Saving your model","text":"<ul> <li>After training your model, you can save it in <code>.pkl</code> format.</li> <li>Although not mandatory, please prefer to use cloudpickle so your custom code part of the model (e.g. preprocessing steps) is more portable.</li> </ul> <pre><code>import cloudpickle\n\nwith open('model.pkl', 'wb') as f:\n    cloudpickle.dump(pipeline, f)\n</code></pre> <pre><code>import joblib \nimport pandas as pd\n\n# Load the saved model\nmodel = joblib.load('model.pkl')\n\n# Making predictions\npredictions = model.predict(X)\n\npred_df = pd.DataFrame(predictions, columns=['Predictions'])\npred_df.head()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#deploying-your-model-as-an-api-endpoint","title":"Deploying your model as an API endpoint","text":"<pre><code># Let's locate the Kubernetes model deployment to deploy our model\nif len(region.model_deployment_list) == 0:\n    raise SystemError(\"You do not have any model deployment systems. \"\n                      \"Please contact your system admin.\")\nelif len(region.model_deployment_list) &gt; 1:\n    print(\"You have more than one model deployment systems. \"\n          \"Will use the first one\")\nmodel_deployment = region.model_deployment_list[0]\ndeployment_key = model_deployment.key\nprefix = 'models'\nmodel_name = 'my-xgboost-model'\nmodel_dir = None  # Current dir\n\nprint(f\"Will deploy '{prefix}/{model_name}' to '{deployment_key}' kubernetes deployment\")\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#create-modelpy-that-predicts-using-modelpkl","title":"Create model.py that predicts using model.pkl","text":"<p>View sample model.py code</p> <pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=model_dir\n)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#making-predictions-using-the-model-api","title":"Making predictions using the model API","text":"<pre><code># *All* Practicus AI model APIs follow the below url convention\napi_url = f\"{region.url}/{prefix}/{model_name}/\"\n# Important: For effective traffic routing, always terminate the url with / at the end.\nprint(\"Model REST API Url:\", api_url)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#getting-a-session-token-for-the-model-api","title":"Getting a session token for the model API","text":"<ul> <li>You can programmatically get a short-lived (~4 hours) model session token (recommended)</li> <li>You can also get an access token that your admin provids with a custom life span, e.g. months.</li> </ul> <pre><code># We will be using using the SDK to get a session token.\n# To learn how to get a token without the SDK, please view 05_others/tokens sample notebook\ntoken = prt.models.get_session_token(api_url)\nprint(\"API session token:\", token)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#posting-data","title":"Posting data","text":"<ul> <li>There are multiple ways to post your data and construct the DataFrame in your model.py code.</li> <li>If you add content-type header, Practicus AI model hosting system will automatically convert your csv data into a Pandas DataFrame, and pass on to model.py predict() method.</li> <li>If you would like to construct the Dataframe yourself, skip passing content-type header and construct using Starlette request object View sample code</li> </ul> <pre><code>import requests \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#compressing-api-traffic","title":"Compressing API traffic","text":"<ul> <li>Practicus AI currently supports 'lz4' (recommended), 'zlib', 'deflate' and 'gzip' compression algorithms.</li> <li>Compressing to and from the API endpoint can increase performance for large datasets, low network bandwidth.</li> </ul> <pre><code>import lz4 \n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv',\n    'content-encoding': 'lz4'\n}\ndata_csv = X.to_csv(index=False)\ncompressed = lz4.frame.compress(data_csv.encode())\nprint(f\"Request compressed from {len(data_csv)} bytes to {len(compressed)} bytes\")\n\nr = requests.post(api_url, headers=headers, data=compressed)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\ndecompressed = lz4.frame.decompress(r.content)\nprint(f\"Response de-compressed from {len(r.content)} bytes to {len(decompressed)} bytes\")\n\npred_df = pd.read_csv(BytesIO(decompressed))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#detecting-drift","title":"Detecting drift","text":"<p>If enabled, Practicus AI allows you to detect feature and prediction drift and visualize in an observability platform such as Grafana. </p> <pre><code># Let's create an artificial drift for BMI feature, whcih will also affect charges\ndf[\"bmi\"] = df[\"bmi\"] * 1.2\n\nheaders = {\n    'authorization': f'Bearer {token}',\n    'content-type': 'text/csv'\n}\ndata_csv = df.to_csv(index=False)\n\nr = requests.post(api_url, headers=headers, data=data_csv)\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nfrom io import BytesIO\npred_df = pd.read_csv(BytesIO(r.content))\n\nprint(\"Prediction Result:\")\npred_df.head()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#recommneded-adding-model-metadata-to-your-api","title":"(Recommneded) Adding model metadata to your API","text":"<ul> <li>You can create and upload model.json file that defines the input and output schema of your model and potentially other metadata too.</li> <li>This will explain how to consume your model efficiently and make it accessible to more users.</li> <li>Practicus AI uses MlFlow model input/output standard to define the schema</li> <li>You can build the model.json automatically, or let Practicus AI build it for you using the dataframe.</li> </ul> <pre><code>model_config = prt.models.create_model_config(\n    df=df,\n    target=\"charges\",\n    model_name=\"My XG Boost Model\",\n    problem_type=\"Regression\",\n    version_name=\"2024-02-15\",\n    final_model=\"xgboost\",\n    score=123\n)\nmodel_config.save(\"model.json\")\n# You also can directly instantiate ModelConfig class to provide more metadata elements\n# model_config = prt.models.ModelConfig(...)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#adding-a-new-api-version","title":"Adding a new API version","text":"<ul> <li>You can add as many model version as you need.</li> <li>Your admin can then route traffic as needed, includign for A/B testing.</li> </ul> <pre><code>prt.models.deploy(\n    deployment_key=deployment_key,\n    prefix=prefix,\n    model_name=model_name,\n    model_dir=model_dir\n)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#reading-model-metadata","title":"Reading model metadata","text":"<p>You can add the query string '?get_meta=true' to any model to get the metadata.</p> <pre><code>headers = {'authorization': f'Bearer {token}'}\nr = requests.get(api_url + '?get_meta=true', headers=headers)\n\nif not r.ok:\n    raise ConnectionError(f\"{r.status_code} - {r.text}\")\n\nimport json\nfrom pprint import pprint\n\nmodel_config_dict = json.loads(r.text)\nprint(\"Model metadata:\")\npprint(model_config_dict)\nschema_dict = json.loads(model_config_dict[\"model_signature_json\"])\nprint(\"Model input/output schema:\")\npprint(schema_dict)\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#optional-consuming-custom-model-apis-from-practicus-ai-app","title":"(Optional) Consuming custom model APIs from Practicus AI App","text":"<p>Practicus AI App users can consume any REST API model. </p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#end-user-api-view","title":"End user API view","text":"<p>If you add metadata to your models, App users will be able to view your model details in the UI.</p> <p></p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#feature-matching","title":"Feature matching","text":"<p>If your model has an input/output schema, the App will try to match them to current dataset.</p> <p></p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#viewing-the-prediction-result","title":"Viewing the prediction result","text":"<p>Note: Please consider adding categorical mapping to your models as a pre-processing step for improved user experince.</p>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#recommended-clean-up","title":"(Recommended) Clean-up","text":"<p>Explicitly calling kill() on your processes will free un-used resources on your worker faster.</p> <pre><code>proc.kill()\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#supplementary-files","title":"Supplementary Files","text":""},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#modelpy","title":"model.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, df: pd.DataFrame | None = None, *args, **kwargs) -&gt; pd.DataFrame:\n    if df is None:\n        raise ValueError(\"No dataframe received\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n        # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre>"},{"location":"notebooks/06_other_topics/08_additional_modeling/xgboost/xgboost/#model_custom_dfpy","title":"model_custom_df.py","text":"<pre><code>import os\nimport pandas as pd\nimport joblib\n\nmodel_pipeline = None\n\n\nasync def init(model_meta=None, *args, **kwargs):\n    global model_pipeline\n\n    current_dir = os.path.dirname(__file__)\n    model_file = os.path.join(current_dir, 'model.pkl')\n    if not os.path.exists(model_file):\n        raise FileNotFoundError(f\"Could not locate model file: {model_file}\")\n\n    model_pipeline = joblib.load(model_file)\n\n\nasync def predict(http_request, *args, **kwargs) -&gt; pd.DataFrame:\n    # Add the code that creates a dataframe using Starlette Request object http_request\n    # E.g. read bytes using http_request.stream(), decode and pass to Pandas.\n    raise NotImplemented(\"DataFrame generation code not implemented\")\n\n    if 'charges' in df.columns:\n        # Dropping 'charges' since it is the target\n        df = df.drop('charges', axis=1)\n\n    # Making predictions\n    predictions = model_pipeline.predict(df)\n\n    # Converting predictions to a DataFrame\n    predictions_df = pd.DataFrame(predictions, columns=['Predictions'])\n\n    return predictions_df\n</code></pre> <p>Previous: Model Drift | Next: Bank Marketing</p>"},{"location":"tutorial/Experiment-Service/","title":"Introduction to Advanced Model, Experiment, and Artifact Management in Practicus AI with MLFlow","text":"<p>The MLFlow integration of the Practicus AI platform provides a solution that significantly simplifies and optimizes the management process of your machine learning projects. </p> <p>This guide explains how to use model creation, management of experiments and other features MLFlow offers.</p>"},{"location":"tutorial/Experiment-Service/#access-to-mlflow-interface","title":"Access to MLFlow Interface:","text":"<p>Within Practicus AI, to access MLFlow:</p> <ul> <li>Open Explore tab</li> <li>Select the MLFlow service defined for you in the working region</li> <li>You can see the models and experiments saved in MLFlow here</li> </ul> <p></p>"},{"location":"tutorial/Experiment-Service/#saving-a-created-model-in-mlflow","title":"Saving a created model in MLFLow","text":"<ul> <li>Click on Explore tab</li> <li>Make sure your Worker is already selected upper left</li> <li>Click Worker Files to load content </li> <li>Expand samples folder and click on ice_cream.csv</li> <li>Click Load </li> <li>Click on the Model button</li> <li>Click Advance</li> <li>Choose Log Exp. Service as MLFlow Primary</li> </ul> <ul> <li>Click OK</li> <li>When the model is created, a plugin will arrive and set the incoming plugin like this</li> </ul> <ul> <li>Click OK</li> </ul>"},{"location":"tutorial/Experiment-Service/#models-experiments-and-artifacts","title":"Models, Experiments and Artifacts","text":"<ul> <li>Open the opened MLFLow service in the browser from the tab above</li> <li>Find the session you created and open the session</li> <li>Here you can see the prt format file, the json containing the model's metadata and the pickle</li> <li>Click Parameters</li> </ul> <ul> <li>Back to Session</li> <li>Find the session you created and click on the '+' sign under table</li> </ul> <ul> <li>Select the first model under session here</li> </ul> <ul> <li>Click on Metrics and see the error metrics saved in MLFlow:</li> </ul> <ul> <li>Scroll to the bottom of the page and access the model artifacts</li> </ul> <p>Scroll to the bottom of the page and access the model artifacts</p> <p></p>"},{"location":"tutorial/Experiment-Service/#sending-an-experiment-from-notebook-to-mlflow","title":"Sending an Experiment from Notebook to MLFlow","text":"<ul> <li>Back to Notebook opened after the model</li> </ul> <ul> <li>Run step by step and create exp in step 3</li> </ul> <ul> <li>Update setup params and run the cell</li> </ul> <ul> <li>Now you can setup experiments</li> </ul> <ul> <li> <p>Run the rest of the steps</p> </li> <li> <p>Save setup </p> </li> </ul> <p></p> <ul> <li>Open MLFlow in the browser</li> </ul> <p></p> <ul> <li>Click on the new Experiment and open it</li> <li>See the changes here</li> </ul> <p></p>"},{"location":"tutorial/Workflow/","title":"Introduction to Workflow Automation","text":"<p>The Workflow tab is a feature in the Practicus AI platform that allows users to automate their workflow. </p> <p>This feature offers a wide range of functionality from code generation to execution of steps on the desired engine.</p> <p>In the scenario we will consider, we will make simple preview steps in the interface and examine their transformation into workflows</p> <p>Hint: From this tab we will only cover the use of Workflow Automation. For other options, you can check the Airflow tab.</p>"},{"location":"tutorial/Workflow/#1-access-to-workflow-tab","title":"1. Access to Workflow Tab","text":"<ul> <li>Open Explore tab</li> <li>Select Cloud Worker Files and open the file below </li> <li>Explore &gt; samples &gt; select and open any csv file</li> <li>You will see 3 different options for Deploy.</li> </ul>"},{"location":"tutorial/Workflow/#2-workflow-creation","title":"2. Workflow Creation","text":"<ul> <li>Open Explore tab</li> <li>Select Cloud Worker Files and open the file below </li> <li>Explore &gt; samples &gt; insurance.csv</li> <li>Click filter and set charges &lt; Less than 40000 and add then click OK</li> <li>Then select sex and click one hot from the prepare tab.</li> <li>Then select sex and click delete from the prepare tab.</li> <li>Click Steps and see following steps:</li> </ul> <ul> <li>In a moment we will translate these steps into workflows</li> <li>Click Deploy and Select Workflow Automation</li> </ul> <p>Hint: This step allows the created workflow to be automated and run on the specified engine. The \"Deploy\" option turns your workflow into a practical application and allows it to run automatically with the schedule and settings you specify.</p> <p>You can set Deployment name, Schedule interval, Cloud Region, and Workflow Service</p> <ul> <li> <p>Click Advanced and select Data processing engine as SPARK </p> </li> <li> <p>Click OK</p> </li> </ul> <p></p>"},{"location":"tutorial/Workflow/#3-generated-codes","title":"3. Generated codes","text":"<p>After deploying the workflow, the generated scripts are opened. In fact, these codes generated by Practicus IA are located in the user's git repository. </p> <p></p> <p></p> <ul> <li>Here you can see that 2 .py files are generated. </li> <li>One of them contains the Airflow DAG and the other one contains our preprocess steps. </li> <li>After generating the code, users can play with the code and redeploy it.</li> </ul>"},{"location":"tutorial/Workflow/#4-airflow-ui-on-app-and-browser","title":"4. Airflow UI on App and Browser","text":"<ul> <li>Open Explore and choose Workflow Service</li> <li>See Airflow UI on app </li> <li>Then click your DAG </li> <li>Then click browser </li> <li>Then click your DAG and trigger</li> <li>Firstly you need to set configuration  </li> <li>Then trigger your DAG and see status your dag</li> </ul>"},{"location":"tutorial/chatgpt/","title":"Analyzing and preparing data with GPT","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"tutorial/chatgpt/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; insurance.csv</li> </ul>"},{"location":"tutorial/chatgpt/#using-gpt-to-generate-code","title":"Using GPT to generate code","text":"<p>To generate code, you should provide GPT with a prompt that describes the code you want to generate. The prompt should be written in English and include the desired functionality of the code snippet. ChatGPT will then generate a code snippet that matches the description.</p> <p></p> <ul> <li>When you select GPT, you will see the below dialog.</li> <li>Explain to the GPT what you want to do.</li> </ul> <p></p> <ul> <li>If you want to see the code generated by GPT, you can click Advanced. In addition, you can set other options.</li> </ul> <p></p> <ul> <li>Click Apply GPT and see the final dataset.</li> </ul> <p>Let's create a slightly more advanced query with GPT.</p> <ul> <li>Click Test GPT then click Advanced</li> </ul> <p></p> <ul> <li>Please make specific comments in advanced queries so that GPT generates the code properly.</li> </ul> <p></p> <ul> <li>Click Apply GPT and see the final dataset with new variables.</li> </ul> <p></p>"},{"location":"tutorial/chatgpt/#optionalhow-can-i-generate-the-optimal-gpt-query","title":"(Optional)How can I generate the optimal GPT query?","text":"<p>Prompt engineering is the process of crafting effective input prompts to elicit the desired output from large language models (LLMs). LLMs are trained on massive datasets of text and code, and can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way. However, they need to be prompted correctly in order to produce the desired output.</p> <p>Here is a brief overview of the steps involved in prompt engineering:</p> <p>1) Identify the desired output.</p> <p>2) Craft a prompt. </p> <ul> <li>The prompt should be written in plain language that is easy to understand. It should also be as specific as possible, so that the LLMs knows what you want it to generate.</li> </ul> <p>3) Test the prompt.</p> <ul> <li>Once you have crafted a prompt, you need to test it to make sure that it works.</li> </ul> <p>4) Refine the prompt. </p> <ul> <li>If the LLMs does not generate the desired output, you need to refine the prompt. This may involve making the prompt more specific, or providing more examples.</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/code/","title":"Extensibility with Python Coding","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/collaboration/","title":"Collaboration","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/data-analysis-intro/","title":"Introduction to Data Analysis","text":"<p>This section only requires Practicus AI app and can work offline.</p>"},{"location":"tutorial/data-analysis-intro/#working-with-local-files","title":"Working with local files","text":"<ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on Local Files</li> <li>Navigate to the samples directory and open ice_cream.csv : </li> <li>home &gt; practicus &gt; samples &gt; data &gt; ice_cream.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p>This simple dataset shows how much revenue an ice cream shop generates, based on the outside temperature (Celsius) </p>"},{"location":"tutorial/data-analysis-intro/#visualizing-data","title":"Visualizing Data","text":"<ul> <li>Click on Analyze and select Graph</li> </ul> <ul> <li>Select Temperature for X axis and Revenue for Y, click ok</li> </ul> <ul> <li>You will see a new tab opens up and a graph is plotted</li> </ul> <ul> <li>Move your mouse over the blue line, and you will see the coordinates changing at the top right</li> <li>Click on zoom, move your mouse to any spot, left-click and hold to draw a rectangle. You will zoom into that area</li> <li>Click on Pan, left-click and hold a spot and move around</li> </ul> <p>Now let's use a more exciting data set:</p> <ul> <li>Go back to the Explore tab and load the file below:</li> <li>home &gt; practicus &gt; samples &gt; data &gt; insurance.csv</li> </ul> <p></p> <p>This dataset is about the insurance charges of U.S. individuals based on demographics such as, age, sex, bmi .. </p> <ul> <li>Click on the charges column name to select it </li> <li>You see a mini-histogram on the upper right with basic quartile information</li> <li>Min, 25%, 50% (median), Avg (Average / mean), 75%, and Max</li> <li>Move your mouse over a distribution (shown as blue lines) on the histogram, and a small pop-up will show you the data range, how many samples are there, and the total percent of the samples in that distribution. </li> </ul> <p></p> <p>Now let's open a larger histogram for a closer look:</p> <ul> <li>Click on Analyze &gt; Graph</li> <li>Select Histogram for style </li> <li>Select charges, click ok</li> </ul> <p></p> <p>You will see the below histogram </p> <p></p>"},{"location":"tutorial/data-analysis-intro/#visualizing-outliers","title":"Visualizing Outliers","text":"<p>Now let's analyze to see the outliers in our data set.</p> <ul> <li>Click Analyze &gt; Graph</li> <li>Select boxplot style, charges and click ok</li> </ul> <p></p> <p>You will see the boxplot graph visualizing outliers. </p> <p></p> <p>The above tells us that some individuals pay significantly more insurance  charges compared to the rest. E.g. $60,000 which is more than 5x the median (50%). </p> <p>Please note: Since Q1 - 1.5 x IQR is -$10,768, overall sample minimum $1,121 is used as boxplot min. This is common in skewed data.</p> <p>Sometimes outliers are due to data errors, and we will see how to remove these in the next section. And sometimes we still remove them even if they are correct to improve AI model quality. We will also discuss this later.</p>"},{"location":"tutorial/data-analysis-intro/#group-by-to-summarize-data","title":"Group by to Summarize Data","text":"<p>Since our insurance data also has demographic information such as region, we can summarize (aggregate) based on how we wish to break down our data. </p> <ul> <li>Select Analyze &gt; Group By</li> </ul> <p></p> <ul> <li>Select region and then sex for the Group by section</li> <li>Select charges - Mean (Average), charges - Median (50%), charges - Std. Dev (Standard Deviation), age - Mean(Average), bmi - Variance for the summarize section</li> <li>Click ok </li> </ul> <p></p> <p>You will see the selected charges summaries for region and sex break-down. There is no limit, you can break down for as many columns as you need.</p> <p></p> <p>Now let's create a more advanced multi-layer graph:</p> <ul> <li>Select Analyze &gt; Graph</li> <li>Click on Advanced Options</li> <li>Select region for X, charges_mean for Y</li> <li>Click Add Layer</li> <li>Select region for X, charges_median for Y</li> <li>Click Add Layer again</li> <li>Click ok</li> </ul> <p></p> <p>You will see the mean and median for different U.S. regions. </p> <p></p> <p>Let's say we want to email this graph to someone:</p> <ul> <li>Click on Save</li> <li>Select a file name. e.g. insurance.png</li> </ul> <p></p> <p>You will get a graphics file saved on your computer.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/data-prep-intro/","title":"Introduction to Data Preparation","text":"<p>This section only requires Practicus AI app and can work offline.</p> <p>Let's start by loading boston.csv. We will ignore the meaning of this dataset since we will only use it to manipulate values.</p> <p></p> <p>Click on Prepare button to view some common data preparation actions</p> <p></p> <p></p>"},{"location":"tutorial/data-prep-intro/#sorting-data","title":"Sorting Data","text":"<ul> <li>Click on CRIM column</li> <li>Hit Ctrl (Cmd for macOS) + down arrow to sort ascending</li> <li>Hit Ctrl (Cmd for macOS) + up arrow to sort descending</li> </ul> <p>You can also open the advanced sort menu by clicking Prepare &gt; Sort</p>"},{"location":"tutorial/data-prep-intro/#filtering-data","title":"Filtering Data","text":"<p>There are several ways to filter data: </p> <ul> <li>Click on RM column to view the mini-histogram</li> <li>Click on the left most distribution to view filter menu</li> <li>Select Filter to keep &gt;= 4.605</li> </ul> <p></p> <ul> <li>Click on a cell value in INDUS column, e.g. 6.2</li> <li>Select Filter to keep &lt;</li> </ul> <p>This will remove all INDUS values greater than 6.2</p> <p></p>"},{"location":"tutorial/data-prep-intro/#updating-wrong-values","title":"Updating Wrong Values","text":"<ul> <li>Click on any cell with 12.5 in ZN column  </li> <li>Select Change Values</li> </ul> <ul> <li>Enter 100, click ok</li> </ul> <p>You will see that ALL cells with the value 12.5 in ZN column will be updated to 100</p> <p></p> <p>Please note that Practicus AI does not allow you to update the value of an individual cell only. All updates need to be rule-based. The reason for this is to be able to create production data pipelines. E.g. what you design can be used on fresh data every night, automatically. Individual cell updates do not work for this scenario.</p>"},{"location":"tutorial/data-prep-intro/#formulas","title":"Formulas","text":"<p>Practicus AI supports 200+ Excel compatible functions to write formulas. If you can use formulas in Excel, you can in Practicus AI.</p> <ul> <li>Click on the Formula button to open up the formula designer</li> </ul> <p></p> <ul> <li>Select ISODD function under Math section to find odd numbers  </li> <li>You will be asked to choose a column, select RAD column as Number </li> <li>Click Add</li> </ul> <p>You can use the designer to build the formula or type by hand. You can also create them in Excel and copy / paste. Unlike Excel, you do not use cell references e.g. A5, D8, but column names directly like the below:</p> <p></p> <ul> <li>Leave the column name as suggested: ISODD_RAD</li> <li>Click ok to run the formula </li> <li>You will see a new column named ISODD_RAD added to the dataset</li> <li>Click on ISODD_RAD column to select and hit Ctrl (Cmd in macOS) + left arrow key to move the column to left. Keep doing it until it is next to RAD column</li> </ul> <p></p>"},{"location":"tutorial/data-prep-intro/#advanced-filter","title":"Advanced filter","text":"<ul> <li>Click on INDUS column name to select and then Prepare &gt; Filter</li> <li>Advanced filter designer will open and INDUS column already selected </li> <li>Select &lt;= Less Than and Equal as criteria </li> <li>Click on ... and choose 6.91 select ok </li> <li>Click on Add</li> <li>Now, select our newly created column ISODD_RAD instead of INDUS</li> <li>Leave Is True and click Add</li> </ul> <p>You will see a filter statement is getting created like the below. You can use brackets and keywords such as (, ), and, or, to build the filter you need.  </p> <p></p> <p>After applying your filter you will see the new data set.</p> <ul> <li>Click on INDUS column</li> <li>Hit Ctrl (or Cmd) + up arrow to sort descending</li> <li>Hit Ctrl (or Cmd) + right arrow to move the column next to RAD</li> </ul> <p>You can view the columns we are working on listed together, like the below </p> <p></p>"},{"location":"tutorial/data-prep-intro/#viewing-and-updating-steps","title":"Viewing and updating Steps","text":"<p>You can make mistakes while manipulating data, and it is possible to fix these without starting from scratch.</p> <ul> <li>Click on Steps button</li> </ul> <p></p> <p>You will view the current steps so far</p> <ul> <li>Select the filter step</li> <li>Hit Edit step button</li> <li>Hint: Practicus AI detects the types of variables and does type conversion automatically. You can see this in steps 2 and 3.</li> </ul> <p></p> <ul> <li>Change the filter value from 6.91 to 5</li> <li>Click ok</li> </ul> <p></p> <p>You will see the updated step in green</p> <ul> <li>Click ok to make the change</li> </ul> <p></p> <p>You will see that INDUS column is now less than 5</p> <p></p> <p>Please note that updating steps will reset the column order, such as moving columns left / right or hiding them.</p> <p>Instead of opening the Steps dialog you can quickly undo / redo as well:</p> <ul> <li>Hit Ctrl (or Cmd) + Z to undo a step. </li> <li>Do it a few more times, you will see data is updated automatically </li> <li>Now, hit Ctrl (or Cmd) + Y few times to redo the steps you undid</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/data-prep/","title":"Data Preparation","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/data-profiling/","title":"Data Profiling","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Profiling your data is an extremely powerful way to get a good understanding of your data distribution and correlations.</p> <ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; insurance.csv</li> <li>Select Analyze &gt; Profile </li> </ul> <p></p> <p>After a few minutes, you will see the data profile completed.</p> <p>You will see several statistical calculations about this data. </p> <p></p> <p>When you click Alerts, you will see various alerts, such as high correlation.</p> <p></p> <p>Click Variables on the up-right tab, and Select Variables as Age</p> <p></p> <p>When you click Correlations on the up-right tab, you will see a correlation heatmap.</p> <ul> <li>If you want to see the correlation matrix, you can click Table.</li> </ul> <p></p> <p>In this example, all correlations are positive, so they are displayed with blue. Negative correlations are displayed in red. E.g. If we had a column about how fast someone can run, age column would probably have a negative correlation. I.e. if someone is younger, they would run faster.</p>"},{"location":"tutorial/data-profiling/#profiling-for-difference","title":"Profiling for difference","text":"<p>You can make changes to your data and then create a profile to compare with the original.</p> <ul> <li>Go back to the insurance dataset </li> <li>Under smoker column click on a cell that has the no value </li> <li>Select Filter to keep cell value</li> <li>Select Analyze &gt; Profile again</li> <li>You will be asked if you would like to compare with the original data set, select yes</li> </ul> <p></p> <p>You will now see all the statistics for the original and current data and can compare them side-by-side. </p> <p></p> <p>Click Variables on the up-right tab, and Select Variables as Age</p> <ul> <li>See the descriptive statistics of the comparison of the old version of the age variable with the new version. </li> <li>In addition, you can click More Details to see statistics, histograms, etc.</li> <li></li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/deploy/","title":"Deployment to Production","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/develop_ml_models/","title":"Develop Machine Learning Models with Jupyter Notebooks","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>This section will provide information on how technical and non-technical users can easily intervene in the code and improve the machine learning models they build.</p> <p>You have set up the model and everything is fine. You can either complete the model at this point and save it, or you can easily intervene in the generated Jupyter Notebook code to improve the model. </p> <ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Cloud Worker Files and open the file below </li> <li>Home &gt; samples &gt; titanic.csv</li> <li>Select Model &gt; Predictive Objective </li> <li>Choose Objective Column as Survived and Technique should be Classifications</li> <li>Click OK</li> <li>After the model build is completed you will see a dialog</li> <li>Select Open Jupyter Notebook to experiment further</li> </ul>"},{"location":"tutorial/develop_ml_models/#scale-and-transform","title":"Scale and Transform","text":"<p>In the Scale and Transform section, you can rescale the values of numeric columns in the dataset without distorting differences in the ranges of values or losing information.</p> <p></p> <p>Parameters for Normalization in shortly:</p> <ul> <li>You must first set normalize to True</li> <li>You can choose z-score, minmax, maxabs, or robust methods for normalize</li> </ul> <p>Parameters for Feature Transform in shortly: </p> <ul> <li>You must first set normalize to True</li> <li>You can choose yeo-johnson or quantile methods.</li> <li>For the Target Transform you can choose yeo-johnson or quantile methods</li> </ul> <p></p> <p>If you want to have deeper knowledge for Scale and Transform, you can review this link</p>"},{"location":"tutorial/develop_ml_models/#feature-engineering","title":"Feature Engineering","text":"<p>In the Feature Engineering section, you can automatically try new variables and have them improve the accuracy of the model. You can also apply rare encoding to data with low frequency</p> <p></p> <p>Parameters for Polynomial Features in shortly:</p> <ul> <li>You must first set polynomial_features to True</li> <li>polynomial_degree should be int</li> </ul> <p>Parameters for Bin Numeric Features in shortly:</p> <ul> <li>bin_numeric_features should be list</li> </ul> <p>Parameters for Combine Rare Levels in shortly:</p> <ul> <li>rare_to_value: float, default=None</li> <li>rare_value: default='rare'</li> </ul> <p></p> <p>If you want to have deeper knowledge for Feature Engineering, you can review this link</p>"},{"location":"tutorial/develop_ml_models/#feature-selection","title":"Feature Selection","text":"<p>In the Feature Selection section, you can set which variables to include in the model and exclude some variables based on the relationships between the variables.</p> <p></p> <p>Parameters for Remove Multicollinearity in shortly:</p> <ul> <li>You must first set remove_multicollinearity to True</li> <li>multicollinearity_threshold should be float</li> </ul> <p>Parameters for Principal Component Analysis in shortly:</p> <ul> <li>You must first set pca to True</li> <li>pca_method should be linear, kernel, or incremental</li> <li>pca_components should be None, int, float, mle</li> <li>Hint: Minka\u2019s MLE is used to guess the dimension (ony for pca_method='linear')</li> </ul> <p>Parameters for Ignore Low Variance in shortly:</p> <ul> <li>low_variance_threshold should be float or None.</li> </ul> <p>If you want to have deeper knowledge for Feature Selection, you can review this link</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/end-advanced/","title":"End of Advanced Topics","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/end-basic/","title":"End of Basic Tutorial","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/engines/","title":"Data Processing Engines","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/excel-prep/","title":"Excel Prep","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/explore/","title":"Exploring Cloud Data Sources","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Once you have a Practicus AI Cloud Worker running and ready, you can access and explore different data sources on the cloud. </p>"},{"location":"tutorial/explore/#cloud-worker-files","title":"Cloud Worker Files","text":"<p>Every Cloud Worker has some local storage that you can use to quickly upload/download to use cloud-only features. </p> <ul> <li>Click on Explore</li> <li>Make sure your Cloud Worker is already selected upper left</li> <li>Click Cloud Worker Files to load content </li> <li>Expand samples folder and click on boston.csv</li> <li>Click Load </li> </ul> <p></p> <p>Since this data is loaded using a Cloud Worker, a copy of it is already on the Cloud Worker. </p> <p>Congrats, now you can use all the Practicus AI features, including advanced AI ones such as AutoML.</p> <p></p> <p>For curious minds: When you make changes to your data using the app, you will see the results instantly on the app, and in parallel the changes are applied on the cloud. This avoids wait time.</p>"},{"location":"tutorial/explore/#uploading-files","title":"Uploading Files","text":"<p>There are multiple ways to quickly upload your data to the cloud, so you can take advantage of cloud-only Practicus AI features or use external services such as Jupyter. </p> <ul> <li>Click on Explore </li> <li>Select Cloud Worker Files</li> <li>Click on New Folder</li> <li>Enter a name, such as my_data</li> <li>Select your newly created folder, my_data </li> <li>Click on Upload button</li> </ul> <p></p> <ul> <li>Select one of our local data samples</li> <li>Home &gt; practicus &gt; samples &gt; data &gt; boston.csv</li> <li>A file Transfer tab will open, click Start Transfer</li> <li>Close after done </li> <li>Go back to Explore tab, click Reload </li> </ul> <p>You will see that your data is now uploaded to the local disk of the Cloud Worker</p> <p></p> <p>Tips:</p> <ul> <li>You can upload / download entire directories </li> <li>Your data is compressed during upload / download, saving on bandwidth and time</li> <li>You can also use Copy / Paste between data sources to quickly move data around</li> </ul>"},{"location":"tutorial/explore/#quick-upload","title":"Quick Upload","text":"<p>Sometimes you will not know in advance if you are going to use a cloud-only feature or not, and start working offline. </p> <p>Practicus AI offers a quick way for you to start offline and continue in the cloud later:</p> <ul> <li>Open Explore tab</li> <li>Click on Local Files and load the below file </li> <li>Home &gt; practicus &gt; samples &gt; data &gt; boston.csv </li> <li>Select CRIM column, hit delete or backspace to delete the column</li> <li>Click on Model button (AutoML) </li> </ul> <p>You will see a note telling this is a cloud-only feature and asks if you would like to upload your data to the cloud.</p> <ul> <li>Select Yes</li> </ul> <p></p> <ul> <li>Select a Cloud Worker, such as the one you created in the previous section</li> <li>Click Ok</li> </ul> <p></p> <p>Your data will be uploaded AND all your steps (such as deleting CRIM column) will be applied. </p> <ul> <li>Click Model button again to verify it now works</li> <li>Click cancel, we will visit AutoML later</li> </ul>"},{"location":"tutorial/explore/#working-with-s3","title":"Working with S3","text":"<p>S3 is a very common location for Big Data and AI/ML workloads. Your AWS account might already have access to some S3 buckets (locations)  </p> <ul> <li>Open Explore tab</li> <li>Click on S3</li> <li>Select Load Buckets under Bucket</li> <li>In as moment, you will see all the S3 buckets you have access to under Bucket</li> </ul> <p></p> <p>Notes: </p> <ul> <li>If you need to use a bucket under another AWS account, you can select I will enter my credentials under Cloud Config to manually enter security credentials</li> <li>If you would like to learn how to create a bucket please visit the AWS guide</li> <li>If you would like to experiment using public S3 buckets, please check this registry</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/gpu/","title":"GPU Acceleration","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/model/","title":"Modeling with AutoML","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <ul> <li>You can use Practicus AI for both supervised and unsupervised learning.</li> <li>Hint: In supervised learning, your dataset is labeled, and you know what you want to predict.  In unsupervised learning, your dataset is unlabeled, and you don't know what to do. But with Practicus AI, you can switch from unsupervised to supervised learning.</li> </ul>"},{"location":"tutorial/model/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; insurance.csv and Load </li> <li>Click Model button</li> </ul> <ul> <li>View the below optional sections, and then click ok to build the model</li> </ul>"},{"location":"tutorial/model/#optional-building-excel-model","title":"(Optional) Building Excel Model","text":"<p>By default, models you build can be consumed using Practicus AI app or any other AI system. If you would like to build an Excel model, please do the following. Please note that this feature is not available for the free cloud tier.</p> <ul> <li>In the Model dialog click on Advanced Options</li> <li>Select Build Excel model and enter for how many rows, such as 1500</li> </ul>"},{"location":"tutorial/model/#optional-explaining-models","title":"(Optional) Explaining Models","text":"<p>If you would like to build graphics that explain how your model works, please do the following. Please note that the free tier cloud capacity can take a long time to build these visualizations.  </p> <ul> <li>In the Model dialog click on Advanced Options</li> <li> <p>Select Explain</p> </li> <li> <p>Click ok to start building the model</p> </li> <li> <p>Hint: The Select Top % features by importance setting only includes the most important variables in the model. If two variables are highly correlated, then the model can already predict the target variable with one variable, so the other variable is not included.</p> </li> </ul> <p>If you choose the optional sections, model dialog should look like the below:</p> <p></p> <p>You should see a progress bar at the bottom building the model. </p> <p></p> <p>For a fresh Cloud Worker with regular size (2 CPUs) the first time you build this model it should take 5-10 minutes to be completed. Subsequent model runs will take less time. Larger Cloud Workers with more capacity build models faster and more accurately.  </p> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select all the options</li> <li>Click ok</li> </ul> <p></p> <p>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</p> <p></p> <p>If you requested to build an Excel model, you will be asked if you want to download. </p> <p></p> <p>We will make predictions in the next section using these models, or models that other built. </p>"},{"location":"tutorial/model/#optional-reviewing-model-experiment-details","title":"(Optional) Reviewing Model Experiment Details","text":"<p>If you chose to explain how the model works: </p> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Navigate to relevant graphics, for instance Feature Importance</li> </ul> <p></p> <p>The above tells us that an individual not being a smoker (smoker = 0), their bmi, and age are the most defining features to predict the insurance charge they will pay.</p> <p>Note: You can always come back to this screen later by opening Cloud Workers tab, clicking on MLflow button and finding the experiment you are interested with. </p>"},{"location":"tutorial/model/#optional-downloading-model-experiment-files","title":"(Optional) Downloading model experiment files","text":"<p>You can always download model related files, including Excel models, Python binary models, Jupyter notebooks, model build detailed logs, and other artifacts by going back to Explore tab and visiting Home &gt; models</p> <p>You can then select the model experiment you are interested, and click download  </p> <p></p>"},{"location":"tutorial/model/#optional-saving-models-to-a-central-database","title":"(Optional) Saving Models to a central database","text":"<p>In the above steps, the model we built are stored on a Cloud Worker and will disappear if we delete the Cloud Worker without downloading the model first. This is usually ok for an ad-hoc experimentation. A better alternative can be to configure a central MLflow database, so your models are visible to others, and vice versa, you will be easily find theirs. We will visit this topic later.    </p>"},{"location":"tutorial/model/#modelling-with-time-series","title":"Modelling with Time Series","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; airline.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select Explain </li> </ul> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select Predict on Current data and entry Forecast Horizon. It sets how many periods ahead you want to forecast in the forecast horizon.</li> <li>Click ok</li> </ul> <p></p> <p>Select Visualize after predicting and see below graph.</p> <ul> <li>The orange color in the graph is the 12-month forecast result.</li> </ul> <p></p> <ul> <li>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</li> </ul> <p></p>"},{"location":"tutorial/model/#modelling-with-anomaly-detection","title":"Modelling with Anomaly Detection","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; unusual.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select all the options</li> <li>Click ok</li> </ul> <p>After the model build is completed you will see a dialog like the below.</p> <ul> <li>Select all the options</li> </ul> <p></p> <ul> <li>If you select Predict on current data option in the dialog above, you can see the prediction results in the data set after the model is built.</li> </ul> <p></p> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Click t-SNE(3d) Dimension Plot</li> </ul> <p>You can hover over this 3D visualization and analyze the anomaly results.</p> <p></p>"},{"location":"tutorial/model/#modelling-with-segmentationclustering","title":"Modelling with Segmentation(Clustering)","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; customer.csv and Load </li> <li>Click Model button</li> <li>In the Model dialog click on Advanced Options</li> <li>Select all the options</li> <li>Click ok</li> </ul> <ul> <li>Click on the MLflow tab</li> <li>Select explain folder at the left menu </li> <li>Click Elbow Plot</li> <li>See the optimal number of clusters</li> <li>Hint: The elbow method is a heuristic used to determine the optimum number of clusters in a dataset. </li> </ul> <ul> <li>Click Distribution Plot</li> <li>You can hover over this Bar chart</li> </ul> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/next-steps/","title":"Next Steps","text":"<p>Congrats on completing Practicus AI tutorial!</p> <p>There are still many topics that we haven't covered in our tutorial, and we will be adding sections about these in the near future. So please be on the lookout.</p> <p>Please feel free to experiment with these features yourself, since many of them are very intuitive to use.</p> <p>We have several videos where you can learn about Practicus AI. Please view them on the Demo Videos section.</p> <p>Some topics we haven't covered are:</p>"},{"location":"tutorial/next-steps/#advanced-data-preparation","title":"Advanced Data Preparation","text":"<ul> <li>Joins: You can join any data, from different data sources. E.g. You can join data on S3 to data on a relational database. Please view the join help page in our documentation to learn more. </li> <li>Handling missing values</li> <li>Machine learning related tasks</li> </ul>"},{"location":"tutorial/next-steps/#excel-prep","title":"Excel Prep","text":"<p>If you work with someone who doesn't use Practicus AI, you can share any data over Excel / Google Sheets with them, so they can make changes and send back to you. You can then detect their changes and apply as steps to any data.</p> <p>Please view Excel prep help page to learn more</p>"},{"location":"tutorial/next-steps/#python-coding","title":"Python Coding","text":"<p>You can use the built-in Python editor in Practicus AI app to extend for any functionality. This feature doesn't require the cloud and the app comes with all the relevant libraries pre-installed such as pandas, and requests for API calls.</p>"},{"location":"tutorial/next-steps/#deployment-to-production","title":"Deployment to production","text":"<p>Once you finish preparing your data, building models, making predictions, and other tasks, you can automate what you have designed, so it can run automatically on a defined schedule such as every night or every week.</p> <p>Please view the following pages to learn more:</p> <ul> <li>Code Export (Deploy)</li> <li>Airflow Integration</li> <li>Modern Data Pipelines</li> </ul>"},{"location":"tutorial/next-steps/#observability","title":"Observability","text":"<p>Please view logging help page to learn more.</p>"},{"location":"tutorial/next-steps/#gpu-acceleration","title":"GPU Acceleration","text":"<p>You can choose to use GPU powered Cloud Workers by simply selecting Accelerated Computing family while launching a Cloud Worker. </p>"},{"location":"tutorial/next-steps/#data-processing-engines","title":"Data Processing Engines","text":"<p>If you are a technical user, you can choose an engine of your choice, including pandas, DASK, RAPIDS, RAPIDS+DASK and SPARK. Simply click the Advanced button before loading data. </p>"},{"location":"tutorial/next-steps/#advanced-sampling","title":"Advanced Sampling","text":"<p>If you work with very large data, you can load a different sample size on the Cloud Worker. E.g. original data source can be 1 billion rows, you can read 200 million on the cloud and 1 million on the app. Simply click the Advanced button before loading data.      </p> <p>This concludes our tutorial. Thank you for reading!</p> <p>&lt; Previous</p>"},{"location":"tutorial/observability/","title":"Observability","text":"<p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/plot/","title":"Introduction to Data Visualization","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p> <p>Plotting datasets visually aids in data exploration, revealing patterns, and relationships. Thus, it is very important for decision-making, storytelling and insight generation. For that Practicus AI give you Plot service which plots the data in worker as well as in app.</p> <p>Let's have a look of Plot basics by loading salary.csv.  We will ignore the meaning of this dataset since we will only use it to explain basics of plot.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open salary.csv :</li> <li>samples &gt; salary.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>After loading the data set, click on Plot button to start ploting service.</p> <p></p>"},{"location":"tutorial/plot/#basics-of-plot","title":"Basics of Plot","text":"<p>The first thing we will see at Plot tab is going to be Data Source, from this menu we can select the data sheet which we want to visualize.</p> <ul> <li>Click Data Source drop down menu</li> <li>select salary</li> </ul> <p>After choosing the data sheet then we could chose graphic from Graphic drop down menu which we want use while working on visualizing.</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> </ul> <p></p> <p>After choosing the graphic style we want to work with we will see the options listed down below:</p> <ul> <li>Sampling: This option refers to a subset of data set selected from a larger dataset to represent its characteristics. Smaller samples in large data sets can be plotted more quickly, enhancing the efficiency of exploratory data analysis.</li> <li>X Coordinate: This option refers to the horizontal axis of the plot, representing the column(s) of the data set. Within Bar and H Bar graphic styles axis could get string columns as well as numerical columns.</li> <li>Y Cooridnate: This option refers to the vertical axis of the plot, also representing column(s).</li> <li>Color: This option refers to color which will be the filling of shapes within selected graphic style.</li> <li>Size: This option refers to size of the shapes within selected graphic style,  with the exception of the Bar and H Bar graphic styles, where size refers to the spacing between bars.</li> </ul> <p>Let's have a quick look to these options with a simple examle.</p> <ul> <li>Click to  X Coordinate drop down menu and select YearsExperience</li> <li>Click to  Y Coordinate drop down menu and select Salary</li> <li>Click to Add Layer</li> </ul> <p></p> <p>In the end we will have the plot down below:</p> <p></p>"},{"location":"tutorial/plot/#advanced-techniques-in-plot","title":"Advanced Techniques in Plot","text":"<p>In this section we will try to have look to more advance techniques we can use in Plot such as adding multiple layer of visualizing, dynamic size and color, transparency, tooltip and usage of Geo Map graphic style.</p>"},{"location":"tutorial/plot/#dynamic-size-color","title":"Dynamic Size &amp; Color","text":"<p>One of the most illustrative datasets for demonstrating dynamic size and color options is the Titanic dataset. Let's load it into one of our worker environments.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open titanic.csv :</li> <li>samples &gt; titanic.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>The Titanic dataset is a popular dataset used in machine learning and data analysis. It contains information about passengers aboard the RMS Titanic, including whether they survived or not. Within this data set we will use Circle graphic style from Plot and columns of pclass, fare, age and survived. Let's describe what these columns means for better understanding.</p> <ul> <li>Pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd).</li> <li>Fare: Passenger fare.</li> <li>Age: Passenger's age in years.</li> <li>Survived: Indicates whether the passenger survived or not (0 = No, 1 = Yes).</li> </ul> <p>Let's start our plotting journey,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select titanic</li> <li>Click Graphic drop down menu</li> <li>Select Circle</li> <li>Click to Advanced</li> </ul> <p></p> <p>After open up advanced section you will see the options of Dynamic Size and Dynamic Color. Dynamic size and color in a circle plot refer to adjusting the size and color of circles based on additional variables, beyond the x and y coordinates. Let's have look with the example of titanic data set.</p> <ul> <li>Click to  X Coordinate drop down menu and select age</li> <li>Click to  Y Coordinate drop down menu and select fare</li> <li>Click to Dynamic size drop down menu and select pclass</li> <li>Click to Dynamic color drop down menu and select survive</li> <li>Click to Add Layer</li> </ul> <p>The plot down below should be showed up:</p> <p></p> <p>Hence, we can deduce from the analysis that passengers with smaller data points (indicating lower values of \"Pclass\") paid higher fares and had a better chance of survival. Moreover, it's evident that passengers with lower ages (on the X-axis) had a higher likelihood of surviving.</p>"},{"location":"tutorial/plot/#analyze-over-multiple-layer","title":"Analyze over multiple layer","text":"<p>One of the most illustrative datasets for demonstrating multiple layer anlyze is the Iris dataset. Let's load it into one of our worker environments.</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open iris.csv :</li> <li>samples &gt; iris.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>The Iris dataset is a popular dataset in machine learning and statistics, often used for classification tasks. It consists of 150 samples of iris flowers, each belonging to one of three species: Setosa, Versicolor, or Virginica. Within this data set we will use both Bar and Circle graphic style from Plot. The dataset comprises four features, each representing measurements of the length and width of both the petals and sepals of flowers.</p> <p>Before start let's use label encode and group by on the species and for better visualisation:</p> <ul> <li>Click Snippets</li> <li>Click Advanced</li> <li>Locate and select Label encoder</li> <li>Select species from Text columns drop down menu</li> <li>Click +</li> <li>Click OK</li> </ul> <p></p> <ul> <li>Click Prepare</li> <li>Click Group By</li> <li>Select species from Group by drop down menu</li> <li>Select sepal_length and Mean (Average) from Summarize drop down menus</li> <li>Select sepal_windth and Mean (Average) from Summarize drop down menus</li> <li>Select petal_length and Mean (Average) from Summarize drop down menus</li> <li>Select petal_windth and Mean (Average) from Summarize drop down menus</li> <li>Click OK</li> </ul> <p></p> <p>In the end we should have the table down below:</p> <p></p> <p>Let's start plotting for multiple layer analyze,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select iris</li> </ul> <p>For first layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Bars</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select sepal_length_mean</li> <li>Click to Advanced</li> <li>Select greenish color from Color drop down menu</li> <li>Enter a value of 50 for the Transparency % input</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For second layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select sepal_windth_mean</li> <li>Click to Advanced</li> <li>Select a darker greenish color from Color drop down menu</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For third layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Bars</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select petal_length_mean</li> <li>Click to Advanced</li> <li>Select blueish color from Color drop down menu</li> <li>\u00canter a value of 50 for the Transparency % input</li> <li>Click to Add Layer</li> </ul> <p></p> <p>For fourth layer:</p> <ul> <li>Click Graphic drop down menu</li> <li>Select Line</li> <li>Click to  X Coordinate drop down menu and select species</li> <li>Click to  Y Coordinate drop down menu and select petal_windth_mean</li> <li>Click to Advanced</li> <li>Select a darker blueish color from Color drop down menu</li> <li>Click to Add Layer</li> </ul> <p></p> <p>In the end we sould have a plot like down below:</p> <p></p> <p>As we hover over the bars and lines, data point values will be displayed. Additionally, on the right side of plot, there are options available for zooming in, zooming out, and saving the plot.</p> <p>Observing this multi-layer graph, it becomes evident that both sepal length and petal length play a crucial role in distinguishing between classes. Similarly, the same differentiation can be observed for petal width.</p>"},{"location":"tutorial/plot/#geo-map-tutorial","title":"Geo-map Tutorial","text":"<p>To use the Geo-map feature of Plot, the initial requirement is to define the Google Maps API either through the admin console or within the application itself. If you don't know how to retrieve a Google Maps API key you can check Google's documentetion.</p> <p>Defining a Google Maps API over admin console:</p> <ul> <li>Open Admin Console of Practicus AI</li> <li>Expand (Click) Definitions from left menu</li> <li>Click Cluster Definitions</li> <li>Click GOOGLE_MAPS_API_KEY from table</li> </ul> <p></p> <ul> <li>Enter your key to Value input</li> <li>(Optional) Enter a description to Description input</li> <li>Click Save</li> </ul> <p></p> <p>Defining a Google Maps API within application:</p> <ul> <li>Click Settings frop top menu</li> <li>Click Other tab from opened window</li> <li>Enter your Google Maps API to Personal API Key at down below</li> <li>Click Save</li> </ul> <p></p> <p>After assigning the Google Map API we could have a look to Geo-Map by using car_insurance dataset. This dataset contains information about the insurance company's past customers who have purchased health insurance.  The objective is to use this dataset to train a predictive model that can determine whether these past customers would also be interested in purchasing vehicle insurance from the same company.</p> <p>The features can be listed as:</p> <p>id: A unique identifier assigned to each customer. Gender: The gender of the customer. Age: The age of the customer. Driving_License: Indicates whether the customer possesses a driving license. Region_Code: A distinct code assigned to denote the region of the customer. Previously_Insured: Indicates whether the customer already holds vehicle insurance. Vehicle_Age: The age of the vehicle. Vehicle_Damage: Indicates whether the customer's vehicle has been damaged in the past. Annual_Premium: The yearly premium amount that the customer is required to pay. Policy_Sales_Channel: An anonymized code representing the outreach channel used to contact the customer, including different agents, mail, phone, and in-person visits. Vintage: The duration, in days, for which the customer has been associated with the company. Response: Indicates customer interest. 1 indicates interest, while 0 signifies no interest.</p> <p>Let's load the dataset to our worker:</p> <ul> <li>Open Practicus AI app</li> <li>You will see the Explore tab, click on New Worker</li> <li>Select the worker which has been started</li> <li>Navigate to the samples directory and open airports.csv :</li> <li>samples &gt; car_insurance.csv</li> <li>Click on the file and you will see a preview</li> <li>Click Load</li> </ul> <p></p> <p>Before start let's group the data on the Regeion_Code column for better visualisation:</p> <ul> <li>Click Prepare</li> <li>Click Group By</li> <li>Select Regeion_Code from Group by drop down menu</li> <li>Select Response and sum from Summarize drop down menus</li> <li>Select Previously_Insured and sum from Summarize drop down menus</li> <li>Select Lat and max from Summarize drop down menus</li> <li>Select Lon and max from Summarize drop down menus</li> <li>Click OK</li> </ul> <p></p> <p>Let's start our plotting journey,</p> <ul> <li>Click on Plot</li> <li>Click Data Source drop down menu</li> <li>Select car_insurance</li> <li>Click Graphic drop down menu</li> <li>Select Geo Map</li> </ul> <p>After selecting the \"Geo Map\" graphic style, we observe four distinct options that set it apart from other graphic styles:</p> <ul> <li>Latitude: Indicates distance north or south of the Equator.</li> <li>Longitude: Specifies distance east or west of the Prime Meridian.</li> <li>Map Type: Indicates Google Maps styles.</li> <li>Zoom: Provides an approximation of the number of miles/kilometers that fit into the area represented by the plot.</li> </ul> <p>Let's try to visualize the relation between Response and Previously_Insured on Google Maps by plotting data from the car_insurance dataset:</p> <ul> <li>Select Lon_max from Longitude drop down menu</li> <li>Select Lat_max from Latitude drop down menu</li> <li>Enter 1500 to Zoom input</li> <li>Click Advanced</li> <li>Select Response_sum from Dynamic Size drop down menu</li> <li>Select Previously_Insured_sum from Dynamic Color drop down menu</li> <li>Click Add Layer</li> </ul> <p></p> <p>Let's say we want to email this plot to someone:</p> <ul> <li>Click on Save from menu at right side</li> <li>Select a file name. e.g. us_flight.png</li> </ul> <p></p> <p>You will get a graphic file saved on your computer.</p>"},{"location":"tutorial/predict/","title":"Making Predictions","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"tutorial/predict/#loading-insurance-dataset","title":"Loading Insurance dataset","text":"<ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select Cloud Worker Files </li> <li>Navigate to Home &gt; samples &gt; insurance.csv and Load </li> <li>Click Predict button</li> </ul>"},{"location":"tutorial/predict/#ai-model-locations","title":"AI Model locations","text":"<p>The AI model you or some other person built can be found on a variety of locations or databases. </p> <p>Practicus AI currently support predicting models in the below locations:</p> <ul> <li>MLflow model database</li> <li>Cloud Worker</li> <li>Your computer</li> <li>S3</li> <li>An API endpoint (for programmers)</li> </ul> <p>In this tutorial we will show MLflow usage. </p>"},{"location":"tutorial/predict/#predicting-using-mlflow-database","title":"Predicting using MLflow database","text":"<ul> <li>To locate the model we just built click on the ... to search for it</li> </ul> <ul> <li> <p>In the search dialog that opens, type insurance and hit enter</p> </li> <li> <p>You will find the AI model you built in the previous section of this tutorial.</p> </li> </ul> <p>Cannot find model? It is probably because you are not using a central MLflow database yet, and you built the AI model using a different Cloud Worker. Please check the using a central MLflow database section below to learn more. For now, please read insurance.csv using the same Cloud Worker that you built the AI model with.  </p> <p></p> <ul> <li>Select the model and click ok</li> </ul> <p>You will see the model location (URI) on screen. Although you can click ok to make the  predictions now, we strongly suggest you load the model details (metadata) first. </p> <ul> <li>Click Load to load model details</li> </ul> <p></p> <ul> <li>If you prefer, make the dialog larger</li> <li>Rename predicted column name to charges predicted since we already have a column named charges. If you don't, predicted values will overwrite the original ones.</li> <li>Confirm the features (columns) requested by the model match the ones that we provide</li> </ul> <p>Note: Sometimes the feature names will not match. E.g. AI model could need a feature named children, and our dataset could have a feature named child. Practicus AI uses a simple AI algorithm to match the features names, but it is possible there is a mistake. It is always a good practice to confirm the matching is correct. </p> <ul> <li>Click ok to predict </li> </ul> <p></p> <p>Your predictions will be made.</p> <p></p>"},{"location":"tutorial/predict/#analyzing-model-error","title":"Analyzing model error","text":"<p>Your AI model will almost always have a margin of error. We can estimate the error we expect this model will make. </p> <ul> <li>Click on Formula button </li> <li>Add the below formula, and name the new column Model Error</li> </ul> <p><pre><code>ABS(col[charges] - col[charges predicted])\n</code></pre> We used ABS() function to calculate absolute value E.g. |-1| = 1 so error is always a positive value. </p> <ul> <li>Click ok </li> </ul> <p></p> <p>You will see the errors our model makes. </p> <ul> <li>Click on Model Error column</li> <li>Move your mouse over the mini-histogram</li> </ul> <p>You will see that in ~75% of the cases, we expect that our model will make an error less than ~$2200</p> <p></p> <ul> <li>Move your mouse to the right on the mini-histogram to see in what % of the cases our model makes not so good predictions</li> <li>Click on one of the bad prediction bars on the mini-histogram, and select Filter to Keep &gt;= ...</li> </ul> <p></p> <p>You will see all the cases where our model did not do a good job.</p> <ul> <li>Select Model Errors column </li> <li>Hit Ctrl (or Cmd) + Up Arrow to sort descending</li> </ul> <p>You will now see the worst predictions located at the top. When you select a column, bottom right hand of the screen shows some basic statistics in addition to the mini-histogram. You will see that in 127 cases our model had some room for improvement.</p> <p></p> <p>This might be a good time to analyze the individuals of the not so good predictions, and see if there is anything we can do to get our data in a better shape. Often, better data will lead to better models.</p> <p>You can use the same analytics tasks to visualize and profile this data to see if there is a pattern you can detect. In some cases, your data will have errors, and fixing these will improve the model. In some other cases, you will find out that you are simply missing key features. E.g. for the insurance example, existing health problems could very well be a key indicator for the charges, and we miss this data. </p> <p>Sadly, there is no standard way to improve AI models. Sometimes a technical machine learning coder can greatly improve a model by tweaking parameters. And in some other cases a domain experts' professional experience to optimize the data will be the answer. For some tough problems, you will most likely need both of these personas do their magic, and collaborate effectively. </p>"},{"location":"tutorial/predict/#optional-understanding-ground-truth","title":"(Optional) Understanding Ground Truth","text":"<p>In most real life scenarios, you will make predictions for unknown situations based on what we already know, the ground truth. </p> <p>Then, \"life will happen\", and you will observe the real outcome. </p> <p>For instance, you can make predictions on which customer you \"might\" lose. This is called customer churn prediction AI use case. 6 months later, you will see that (unfortunately) you really lost some customers. And you will have more up-to-date ground truth. </p> <p>With Practicus AI, you can make regular predictions, for instance weekly or nightly, by deploying your models to production. We will discuss how to do this later. </p> <p>Practicus AI allows you to store your predictions in a database, so you can compare them later with the ground truth life brings, This will give you the real errors your models make, in comparison to the estimated errors we discussed in the previous section.   </p>"},{"location":"tutorial/predict/#optional-understanding-model-drift","title":"(Optional) Understanding Model Drift","text":"<p>If you choose to store your prediction results regularly, and then compare these to the ground truth, AND you do this on a regular basis you will have a good feeling of how your models perform. In some cases, a model that was doing great a while back will start to perform not so good. This is called the model drift.</p> <p>With Practicus AI, you can implement systems to automatically detect which of models start drifting. We will discuss this topic later. </p>"},{"location":"tutorial/predict/#optional-using-a-central-mlflow-database","title":"(Optional) Using a central MLflow Database","text":"<p>In this tutorial we used a local MLflow database on a Cloud Worker, which will disappear when you terminate it. This will be perfectly fine if you are just experimenting. For other cases, you can copy your models from the Cloud Worker manually. Or even better, you can store them in a central database at first place. </p> <p>To store models in a central database and share with others, do the following:  - Create or reuse a cloud database, such as MySQL or Postgresql  - Create an S3 bucket to store model artifacts. These are binary files, which can be very large, so it is not a good idea to store them in a relational database  - Open Practicus AI app, go to settings &gt; MLflow - Enter the connection string of the database - Enter S3 bucket</p> <p>Sample MLflow setup </p> <p></p> <p>When you have a central MLflow database configured, your Cloud Workers will automatically switch to using this database, instead of the local ones. </p> <p>With a central model database you can:</p> <ul> <li>Consume other users models to make predictions</li> <li>View other users experiment results </li> <li>View model explanation visuals </li> <li>Predict using models that are not built with Practicus AI </li> <li>Share Practicus AI models with other systems, so they can make predictions without Practicus AI </li> </ul> <p>Central model databases can greatly democratize how AI models are built, and how they are consumed. </p> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/snippets/","title":"Snippets","text":""},{"location":"tutorial/snippets/#what-is-snippets","title":"What is Snippets?","text":"<p>Snippets are blocks of code written in Python, prepared for technical and non-technical users. These snippets are designed to perform specific and frequently encountered programming tasks so that users can manage their data processing faster and more efficiently. Snippets are especially ideal for tasks such as data manipulation because these code blocks eliminate the need for users to rewrite code every time.</p>"},{"location":"tutorial/snippets/#functionality-and-benefits-of-snippets","title":"Functionality and Benefits of Snippets","text":"<ul> <li>Snippets provides standardized and optimized code solutions, allowing users to quickly apply frequently needed operations on data sets with a few clicks. This saves time and minimizes coding errors.</li> </ul>"},{"location":"tutorial/snippets/#advantages-of-using-snippets","title":"Advantages of Using Snippets","text":"<ul> <li> <p>Time Efficiency: Saves time by reducing the need to write code for repetitive tasks.</p> </li> <li> <p>Accessibility: Snippets are open source. Users can share the snippets they create with their teammates or all Practicus AI users.</p> </li> <li> <p>Standardization: Data processing operations are applied in a consistent and repeatable way for each user, ensuring continuity across projects.</p> </li> </ul>"},{"location":"tutorial/snippets/#using-existing-snippets","title":"Using Existing Snippets","text":"<ul> <li> <p>Hint: By default, snippets are located in users' hidden .practicus folder. In this tutorial you will see more than one snippet folder. This is because I have also put the snippets in the other Practicus folder. The advantage is that you can create and test new snippets there and easily customize existing snippets. </p> </li> <li> <p>Open Explore tab </p> </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; airline.csv</li> <li>Click Prepare</li> <li>Select period column and click convert then choose Date Time</li> <li>Click OK</li> <li>Then select Snippets</li> <li>Open the Date time folder and choose Year</li> <li>Then choose Datetime column as period</li> <li>Then set the result name to Year and click on the test, then see the preview.</li> </ul> <p></p> <ul> <li>Then Click OK and see the new variable created</li> </ul> <p></p>"},{"location":"tutorial/snippets/#using-advanced-snippets","title":"Using Advanced Snippets","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; insurance.csv</li> <li>Select Snippets</li> <li>Open the Advanced folder and select Normalize</li> <li>Then choose Number columns as age, bmi, and charges</li> <li>Then set Normalize option as Z-Score Normalization</li> </ul> <ul> <li>Then Click OK and see the new variable created with columnname + _normalized</li> </ul>"},{"location":"tutorial/snippets/#advanced-section-for-technical-users","title":"Advanced Section for Technical Users","text":""},{"location":"tutorial/snippets/#snippets-for-spark-engine","title":"Snippets for SPARK Engine","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Samples &gt; insurance.csv</li> <li>Click customize sampling at the bottom left </li> <li>Click Advanced and set Data Engine as SPARK </li> </ul> <ul> <li>Then click OK and Load</li> <li>Select Snippets</li> <li>Open the Statistical folder and select Corr</li> <li>Then choose Number columns as age, bmi, and charges</li> <li>Then set Corr method option as Pearson (other options kendall and spearman)</li> </ul> <ul> <li>Then Click OK and see that the correlation table is formed </li> </ul> <ul> <li>Hint: Not all snippets work on SPARK Engine. To see which engines the snippets work in, you can look at the Snippets Code at the bottom of the screen when you select a snippet.</li> </ul>"},{"location":"tutorial/snippets/#creating-new-snippets","title":"Creating New Snippets","text":"<ul> <li>Open Explore tab </li> <li>Make sure a Cloud Worker is selected (upper right)</li> <li>Select Worker Files and open the file below </li> <li>Open any csv file</li> <li>Select Snippets and click New at the bottom</li> <li>Then, you will see a template for generating snippets, and in this template you will have access to a detailed explanation of how to create snippets.</li> </ul>"},{"location":"tutorial/sql/","title":"Working with SQL","text":"<p>This section requires a Practicus AI Cloud Worker. Please visit the introduction to Cloud Workers section of this tutorial to learn more.</p>"},{"location":"tutorial/sql/#using-sql-with-a-sql-database","title":"Using SQL with a SQL database","text":"<p>This is quite straightforward, if the database you connect to support SQL, you can simply start by authoring a SQL statement. </p> <p>Practicus AI Cloud Workers come with a simple music artists database.</p> <ul> <li>Open Explore tab</li> <li>Select a Cloud Worker upper right of screen (start new or reuse existing)</li> <li>Select SQLite on the left menu </li> <li>Click Run Query</li> </ul> <p>You will see the result of a sample SQL, feel free to experiment and rerun the SQL</p> <p></p> <p>As you will see later in this tutorial, Practicus AI also allows you to run SQL on the result of your previous SQL, as many times as you need. On a SQL database you would need to use temporary tables to do so, which is a relatively advanced topic.</p>"},{"location":"tutorial/sql/#using-sql-on-any-data","title":"Using SQL on any data","text":"<p>Practicus AI allows you to use SQL without a SQL database. Let's demonstrate using an Excel file. You can do the same on other non-SQL data, such as S3. </p> <ul> <li>Open Explore tab</li> <li>Select Local Files</li> <li>Load Home &gt; practicus &gt; samples &gt; data &gt; worker_aws.xlsx</li> </ul> <p></p> <ul> <li>Click on SQL Query button</li> </ul> <p></p> <ul> <li>After click on the Query tab, the dialog will open. Select Yes.</li> </ul> <p></p> <p>Since SQL is an advanced feature, it will require a Cloud Worker to run. You will be asked if you would like to quickly upload to a Cloud Worker. Click Yes, select a Cloud Worker, and now your Excel file will be on the cloud. </p> <p>Click on SQL Query button again. This time the SQL query editor will be displayed.</p> <ul> <li>Type the below SQL </li> </ul> <pre><code>SELECT \"name\", \"total_price\", \"bang_for_buck\" \nFROM \"node_types\" \nWHERE \"total_price\" &lt; 5\nORDER BY \"bang_for_buck\" DESC\n</code></pre> <p></p> <p>Tip: double-clicking on a column name adds its name to the query editor, so you can write SQL faster. If you select some text before double-clicking, your selected text is replaced with the column name.</p> <ul> <li>Click on Test SQL button</li> </ul> <p>Note: Your first SQL on a particular Cloud Worker (cold run) will take a little longer to run. Subsequent SQL queries will run instantly. </p> <ul> <li>(Optional) Experiment further with the SQL, click</li> <li>When ready, click Apply SQL button</li> </ul> <p>You will get the result of the SQL back in the worksheet.  </p>"},{"location":"tutorial/sql/#optional-visualize-sql-result","title":"(Optional) Visualize SQL result","text":"<ul> <li>Click Analyze &gt; Graph</li> <li>Select bang_for_buck for X and total_price for Y</li> <li>Click ok</li> </ul> <p>You will see the estimated total cost (Practicus AI license cost + Cloud infrastructure cost), and how much cloud capacity value you would expect to get (bang for buck) visualized. </p> <p></p> <p>Note: If you have Practicus AI Enterprise license, your software is already paid for. So this graph would not make any sense.  This is only meaningful for the professional pay-as-you-go license type.</p> <p>&lt; Previous | Next &gt;</p>"},{"location":"tutorial/start/","title":"Practicus AI Studio tutorial","text":"<p>Welcome to Practicus AI Studio hands-on tutorial. </p>"},{"location":"tutorial/start/#topics-covered-in-this-tutorial","title":"Topics covered in this tutorial","text":"<p>This tutorial will help you get started with the below topics using Practicus AI</p> <ul> <li>Explore and Analyze Big Data</li> <li>Build AI models using AutoML </li> <li>Make predictions using AI models </li> <li>Data preparation to manipulate data interactively </li> </ul>"},{"location":"tutorial/start/#how-can-this-tutorial-help-you","title":"How can this tutorial help you?","text":""},{"location":"tutorial/start/#non-technical-users","title":"Non-technical Users","text":"<p>You will learn to be self-sufficient for most AI and Big Data tasks. If things get complicated, you will also learn to effectively collaborate with technical-users.</p>"},{"location":"tutorial/start/#semi-technical-users","title":"Semi-technical Users","text":"<p>Can you write SQL or Excel formulas? In addition to the above, you will also learn to extend the functionality quite a bit.</p>"},{"location":"tutorial/start/#data-scientists","title":"Data Scientists","text":"<p>If you are ok to click for some tasks, this tutorial will teach you to accelerate plenty, especially for data prep.  If you are a die-hard coder, you can use Practicus AI to get support from others, especially domain experts, for data discovery, preparation, quality, and model consumption. </p>"},{"location":"tutorial/start/#data-ml-engineers","title":"Data / ML Engineers","text":"<p>You can use Practicus AI as a modern and interactive ETL tool. You will also get clean production ready code for all the tasks (Data + AutoML + Predictions) that others perform, technical or not.</p>"},{"location":"tutorial/start/#big-picture-summary","title":"Big Picture Summary","text":"<p>The below is a summary of what this tutorial will cover and for whom. Please check our documentation for other topics. </p> <p></p>"},{"location":"tutorial/start/#before-we-begin","title":"Before we begin","text":"<p>If you are viewing this tutorial inside the Practicus AI app, you can right-click the tutorial tab and open it inside your browser as well. This can be useful if you would like to view the tutorial side-by-side with the app.  </p> <p></p> <p>Let's get started! </p> <p>Next &gt;</p>"},{"location":"tutorial/worker-node-intro/","title":"Introduction to Practicus AI Cloud Workers","text":"<p>Some advanced Practicus AI features require to use software in addition to Practicus AI APP or the developer SDK. In this section we will learn how to use Practicus AI Cloud Workers.</p>"},{"location":"tutorial/worker-node-intro/#what-is-a-cloud-worker","title":"What is a Cloud Worker?","text":"<p>Some Practicus AI features such as AutoML, making ** AI Predictions, Advanced Profiling and production deployment** capabilities require a larger setup, so we moved these from the app to a backend (server) system.  </p> <p>You have multiple Cloud Worker options to choose from. You can run them in the cloud or on your computer. Please view help on choosing a Cloud Worker to learn more.  </p>"},{"location":"tutorial/worker-node-intro/#setup-cloud-worker","title":"Setup Cloud Worker","text":"<p>Please check the Setup Guide to learn how to configure Practicus AI Cloud Workers. </p> <p>You can use the free cloud tier for this tutorial, or use containers on your computer as well.</p>"},{"location":"tutorial/worker-node-intro/#launching-a-new-cloud-worker","title":"Launching a new Cloud Worker","text":"<ul> <li>Click on the Cloud button to open the Cloud Workers tab</li> <li>Make sure the selected local for your computer, or the optimal AWS Cloud Region. The closest region geographically will usually give you the best internet network performance</li> <li>Click Start New </li> </ul> <ul> <li>Pick the Cloud Worker Size of your Cloud Worker</li> <li>Click ok to Start new Cloud Worker </li> </ul> <p>The default size will be enough for most tasks. You can also choose the free cloud tier.</p> <p></p> <p>In a few seconds you will see your Cloud Worker is launching, and in 1-2 minutes you will get a message saying your Cloud Worker is ready.</p> <p></p>"},{"location":"tutorial/worker-node-intro/#stopping-a-cloud-worker","title":"Stopping a Cloud Worker","text":"<p>If you use local container Cloud Workers you have less to worry about stopping them.  </p>"},{"location":"tutorial/worker-node-intro/#cloud-workers","title":"Cloud Workers","text":"<p>Similar to electricity, water, or other utilities, your cloud vendor (AWS) will charge you a fee for the hours your Cloud Worker is running. Although Practicus AI Cloud Workers automatically shut down after 90 minutes, it would be a practical approach to shut down your Cloud Workers manually when you are done for the day.</p> <p>For this, you can simply select a clod node and click on the Stop button. The next day, you can select the stopped Cloud Worker, click Start and continue where you are left.</p> <p>Tip: It is usually not a good idea to frequently stop / start instances. Please prefer to stop if your break is at least a few hours for optimal cost and wait time.</p>"},{"location":"tutorial/worker-node-intro/#terminating-a-cloud-worker","title":"Terminating a Cloud Worker","text":"<p>Practicus AI Cloud Workers are designed to be disposable, also called ephemeral. You can choose a Cloud Worker and click Terminate to simply delete everything related to it.</p> <p>Please be careful that if you choose to store data on the local disk of your Cloud Worker, this will also get lost after termination. In this case, you can prefer to copy your data manually, or simply click the Replicate button before terminating a Cloud Worker. </p>"},{"location":"tutorial/worker-node-intro/#optional-using-jupyter-lab","title":"(Optional) Using Jupyter Lab","text":"<p>For technical users.</p> <p>Every Cloud Worker comes with some external services preconfigured, such as Jupyter Lab, Mlflow, Airflow.  </p> <ul> <li>Select a Cloud Worker that is running and ready</li> <li>Click on Jupyter button</li> </ul> <p>This will start the Jupyter Lab service and view inside the app. You can also right-click tab name and select Open in browser to view the notebook on your default browser.</p> <p></p> <p>Notes: </p> <ul> <li>If you shut down the app, the secure connection tunnel to the Cloud Worker notebook service will be lost even if the Cloud Worker continues to run.</li> <li>There are two separate Conda kernels configured for your notebook server. Big data one will have common libraries and data engines, such as DASK, RAPIDS (if you have GPUs) and Spark installed. The ML one, as the name suggests, will have ML related libraries such as scikit-learn, Xgboost, Pycaret ..</li> </ul>"},{"location":"tutorial/worker-node-intro/#optional-using-the-terminal","title":"(Optional) Using the Terminal","text":"<p>For technical users.</p> <p>You can choose a Cloud Worker and click the Terminal button to instantly open up the terminal. You have sudo (super-user) access and this can be a very powerful and flexible way to customize your Cloud Worker.</p> <p></p> <p>&lt; Previous | Next &gt;</p>"}]}